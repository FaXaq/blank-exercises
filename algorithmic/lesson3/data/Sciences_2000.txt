En chimie expérimentale, le but est l'objectif que l'on cherche à atteindre, en respectant les règles élémentaires de sécurité, lors d'une synthèse, d'une analyse ou d'une expérimentation en général. Il constitue le fil conducteur de la manipulation. À la rédaction comme à l'oral, il est bref — deux lignes au plus dans la majorité des cas — et introduit un verbe d'action. En général, il regroupe le travail effectué, ainsi que la méthode.  
La formulation est une opération industrielle consistant à fabriquer un matériau homogène et stable, non toxique (pour une grande majorité d'applications), possédant des propriétés finales spécifiques et répondant aux exigences d'un cahier des charges fonctionnel (CDCF), en mélangeant des substances diverses.
On nomme plan d'expériences (en anglais, design of experiments ou DOE) la suite ordonnée d'essais d'une expérimentation, chacun permettant d'acquérir de nouvelles connaissances en contrôlant un ou plusieurs paramètres d'entrée pour obtenir des résultats validant un modèle avec une bonne économie (nombre d'essais le plus faible possible, par exemple). Un exemple classique est le « plan en étoile » où en partant d'un jeu de valeurs choisi pour les paramètres d'un essai central, on complète celui-ci par des essais où chaque fois un seul des facteurs varie « toutes choses égales par ailleurs ». Un type de plan plus exhaustif est le plan factoriel consistant à choisir des valeurs pour chacun des facteurs en faisant varier simultanément tous les facteurs (de façon exhaustive ou non). Le nombre d'essais peut alors devenir très grand (explosion combinatoire).
Les sciences empiriques explorent le monde sensible, en s'appuyant sur l’expérimentation ou l’observation.
Les sciences de la vie, ou Biologie (du grec bios « la vie » et logos, « discours »), comprennent les domaines de la science qui impliquent l'étude des organismes vivants - tels que les micro-organismes, les plantes, les animaux et les êtres humains - ainsi que des considérations connexes comme la bioéthique. Alors que la Biologie demeure la pièce maîtresse des sciences de la vie, les progrès technologiques de la biologie moléculaire et de la biotechnologie ont conduit à une éclosion de spécialisations et de domaines interdisciplinaires. Certaines sciences de la vie se concentrent sur un type spécifique de la vie. Par exemple, la zoologie est l'étude des animaux, tandis que la botanique est l'étude des plantes. Autres sciences de la vie se concentrent sur les aspects communs à tous ou de nombreuses formes de vie, tels que l'anatomie et de la génétique. Pourtant, d'autres domaines sont intéressés par les avancées technologiques impliquant des êtres vivants, tels que le génie biologique. Une autre branche importante des sciences de la vie, bien plus spécifique, consiste à comprendre l'esprit: les neurosciences. Les sciences de la vie ont des applications dans l'environnement, l'agriculture, la médecine et l'industrie pharmaceutique et de l'alimentation. Il y a un chevauchement considérable entre un grand nombre de sujets d'étude dans les sciences de la vie, qui sont ainsi composées de nombreuses branches et sous-disciplines. Pourtant, un certain nombre de concepts gouvernent toutes les études et la recherche, et en font un ensemble cohérent. De manière générale, la Biologie reconnaît l'espèce comme unité de base de la biodiversité, la cellule comme l'unité de base de l'organisme (qui peut être unicellulaire), le gène comme unité de base de l'hérédité, et la théorie de l'évolution fournit le cadre conceptuel qui régit cet ensemble. Les principales branches des sciences de la vie sont: Agronomie – l'ensemble des sciences exactes, naturelles, économiques et sociales, et des techniques auxquelles il est fait appel dans la pratique et la compréhension de l'agriculture Anatomie – l'étude des relations entre la forme et la fonction chez les organismes (plantes, animaux ou autres, ou spécifiquement humains) Biochimie – l'étude des réactions chimiques au sein du vivant, le plus souvent au niveau infra-cellulaire Bioinformatique – champ interdisciplinaire développant des méthodes de stockage, d'archivage, d'organisation et d'analyse des données biologiques; une activité importante de la bioinformatique est de développer des outils logiciels pour produire de la connaissance en biologie à partir d'informations hétérogènes Biologie cellulaire – l'étude de la cellule comme entité au sein de l'organisme, et des interactions moléculaires et chimiques qui ont lieu dans une cellule vivante Biologie marine – l'étude des écosystèmes marins et océaniques, et de la biodiversité qu'ils abritent Biologie de synthèse – domaine du génie biologique visant à concevoir et construire de nouveaux systèmes et fonctions biologiques Biologie des populations – l'étude de groupes d'organismes de la même espèce, y compris la structuration dans l'espace de ces populations et leur évolution dans le temps Biologie des systèmes (ou biologie intégrative) – intégration de différents niveaux d'information pour comprendre le fonctionnement des systèmes biologiques Biologie du développement – l'étude des processus par lesquels les organismes croissent et se développent Biologie évolutive – l'étude des scénarios et des mécanismes de l’évolution des espèces Biologie structurale – une branche de la biologie moléculaire qui étudie la structure et l'organisation spatiale des macromolécules biologiques Biomécanique – l'étude des propriétés mécaniques des organismes vivants Biophysique – une discipline à l'interface de la physique et la biologie, qui applique à la compréhension des phénomènes biologiques les concepts et les outils d'observation et de modélisation de la physique Biosémantique – une analyse des représentations mentales en termes de fonctions biologiques spécifiques héritées de l'évolution Biotechnologie – l’application à des organismes vivants des principes scientifiques et de l'ingénierie aux fins de la production de connaissances, de biens et de services Écologie – l'étude des interactions des organismes vivants entre eux et avec les éléments abiotiques de leur environnement Écotoxicologie – l'étude des effets de substances chimiques exogènes sur les populations d'organismes vivants Épidémiologie – l'étude des facteurs affectant la santé de populations humaines, animales ou végétales et notamment ce qui a trait à la répartition, à la fréquence et à la gravité des états pathologiques Éthologie – l'étude des comportements animaux Génétique – l'étude des gènes et de l'hérédité Génétique des populations – domaine de la génétique qui étudie les populations d'êtres vivants, sous l'influence des diverses pressions évolutives Génie biologique – l'application des concepts et méthodes de la biologie dans une approche d'ingéniérie du vivantHematology (also known as Haematology) – study of blood and blood-forming organs Histologie – l'étude des tissus biologiques et des organes Médecine – l'étude de l'organisation du corps humain, de son fonctionnement normal et de ses dysfonctionnements pathologiques, appliquée dans la pratique à la compréhension et au rétablissement de la santé humaine, et à la prévention des maladies et des épidémies; la médecine comprend de très nombreuses sous-disciplines Médecine vétérinaire – les approches médicales appliquées aux animaux domestiques ou non Microbiologie – l'étude des êtres vivants microscopiques (micro-organismes) et de leurs interactions entre eux et avec les autres organismes Biologie moléculaire – l'étude des mécanismes de fonctionnement de la cellule au niveau moléculaire Neurosciences – l'étude du système nerveux, notamment humain Phylogénie – l'étude des relations de parenté entre êtres vivants (individus, populations, espèces, etc.) Physiologie – l'étude du fonctionnement des organismes vivants et de leurs organes Sociobiologie – l'étude des bases biologiques des comportements sociaux dans le règne animal Systématique – la science qui organise les taxons sur la base de principes objectifs et partageables Taxonomie – le regroupement des organismes vivants en taxons afin de les identifier, de les nommer et de les classer Toxicologie – l'étude des effets de substances chimiques exogènes sur les organismes vivants Croisant ces champs disciplinaires, une subdivision par type d'organismes étudiés est souvent utilisée: Bactériologie – l'étude des bactéries Botanique – l'étude des plantes Entomologie – l'étude des insectes Herpétologie – l'étude des reptiles et des amphibiens Ichtyologie – l'étude des poissons Malacologie – l'étude des mollusques Mammalogie – l'étude des mammifères Mycologie – l'étude des champignons Ornithologie – l'étude des oiseaux Phycologie – l'étude des algues Nématologie – l'étude des nématodes Virologie – l'étude des virus Zoologie – l'étude des animaux  Portail de la biologie  Portail de la bioéthique
Ceci est une liste des concepts scientifiques classés par ordre alphabétique.
Cet article dresse une liste non exhaustive des découvertes des civilisations anciennes.
Ceci est une liste d'effets scientifiques classés par ordre alphabétique.
Cette page donne une liste des équations et formules par ordre alphabétique, qui contient les équations, formules, relations et autres identités, égalités ou inégalités.
Ceci est une liste des expériences scientifiques, que ce soit dans le domaine des sciences expérimentales (physique, chimie) ou des sciences humaines (psychologie clinique, sociologie...) Elles sont classées par ordre alphabétique.
Cette liste répertorie des fabricants d'instrumentation scientifique. 454 Life Sciences A&D Company (en) ADC Bioscientific (en) ABB AES Laboratoire Agilent Technologies Ametek Analytik Jena (en) Anton Paar GmbH (de) Applied Biosystems Barloworld Scientific (en) Bausch & Lomb Optical Beckman Coulter BioRad BioTek (en) Brookfield Engineering (en) Brown & Sharpe Brüel & Kjær (en) Bruker Büchi Bushnell Corporation (en) Cameca Canberra Carl Zeiss Cecil Instruments (en) CovalX (en) Danaher Corporation Dionex (en) Drägerwerk Elekta Endress+Hauser Eppendorf Eurotherm (en) GE Healthcare (en) Gilson-Rainin Hanna Instruments (en) Harvard Apparatus (en) Heidolph (en) Heraeus (en) Hitachi Honeywell Horiba Huber+Suhner (de) IKA-Werke (de) Imago Scientific Instruments (en) Instron (en) JEOL (en) L. S. Starrett Company (en) Leica LI-COR Biosciences (en) Life Technologies Malvern Instruments (en) McPherson Inc (en) Meade Instruments Measurement Systems division (en) Merci (en) Metrohm Mettler-Toledo Millipore Corporation (en) Minolta Mitutoyo MSE (centrifugeuses) (en) MTS Systems Corporation (en) Nikon Novacam Technologies (en) NTi Audio (en) Olympus Corporation Oxford Instruments (en) PANalytical (en) Pendulum Instruments (en) PerkinElmer Petrotest (en) Philips Medical Systems Polymer Char (en) Pratt & Whitney Measurement Systems (en) Prior Scientific (en) Rex Entertainment (en) Röntec Russell Finex (en) Sanyo Sartorius AG (en) Schott Instruments Shimadzu Corp. Siemens Sigma-Aldrich SIMPO-Bouty Sony Spectracom (en) SPECTRO Analytical Instruments (en) Tektronix TeledyneLeCroy Testo Thermo Fisher Scientific TMI Group of Companies (en) Toshiba Unitron (en) Varian, Inc. (en) Varian Medical Systems VIEW Engineering (en) Waters Corporation (en) Yokogawa Electric Zygo Corporation (en)
Cet article recense des termes utilisés en géomorphologie.
On appelle paquet GNU un paquet logiciel maintenu par le projet GNU et soutenu par la Free Software Foundation. Ces programmes sont destinés à être utilisés dans le cadre du système d'exploitation GNU et de ses variantes, notamment GNU/Linux, bien qu'ils puissent très souvent être utilisés sur d'autres plateformes. Ces logiciels sont tous des logiciels libres, le système GNU ayant été précisément créé dans le but de fournir un équivalent libre au système Unix. Les paquets GNU sont au nombre de 395. Ils sont fédérés au sein de la forge GNU Savannah. Le projet GNU a été fondé par Richard Stallman, auteur du premier paquet GNU Emacs.
Cette page dresse une liste des instruments et équipements scientifiques classés par ordre alphabétique. En toute rigueur, un capteur, une sonde, un détecteur et un transducteur (voir liste non exhaustive en annexe) ne sont pas des instruments de mesure.
Voici une liste de bases de données et de moteurs de recherche académiques. Ces derniers sont présentés par ordre alphabétique. Ils sont également classés par discipline et par type d'accès (libre ou non).
Cette liste d'idées reçues recense des idées reçues et des croyances populaires dans des domaines divers et variés, ayant été démontrées fausses par des sources fiables à travers le monde. Chacune de ces idées reçues, ainsi que son domaine général et les faits le concernant, ont été étudiés dans diverses publications. Cette liste n'a pas vocation à être exhaustive.
Voici une liste de bibliothèques numériques classée par défaut par ordre alphabétique. Il est également possible de classer cette dernière par sujet, nombre de volumes et fournisseurs.
Cet article présente une liste des normes les plus courantes pour les aciers suivant leur région d'origine. Les normes sont en général issues d'un pays ou d'une région donnés, ils peuvent cependant être employés dans d'autres régions. Certaines des normes désignées sont spécifiquement dédiées aux aciers (par exemple les normes AISI), pour d'autres il s'agit de normes industrielles plus générales qui traitent entre autres sujets des aciers (JIS, DIN...). Certaines normes possèdent des normes équivalentes dans d'autres normes
Voici une liste de listes de publications importantes dans différents domaines de la science. Les publications sont organisés par domaine.
On trouvera ci-dessous une liste de découvertes, d'inventions et d'innovations faites par accident, à la suite d'un concours imprévu de circonstances ou par hasard – ce qu'il est devenu courant d'appeler, sous l'influence du livre de Royston Roberts, Serendipity: Accidental Discoveries, les découvertes et inventions faites par sérendipité. Elle regroupe sous ce concept des découvertes et des inventions aussi différentes que l'invention accidentelle de l'hélice de bateau — à la suite d'un vrai accident — mais aussi la découverte de la pénicilline par Alexander Fleming (dont il a su exploiter les propriétés thérapeutiques) ou bien encore la découverte par le plus grand des hasards de la grotte de Lascaux ou des manuscrits de la mer Morte. La sérendipité apparaît alors souvent, tout simplement, comme une manière irrationnelle de faire des découvertes, des inventions et des innovations : par accident ; par un concours de circonstances ; par erreur ; par hasard pur ; par inadvertance ; par maladresse ; par mégarde ; par négligence professionnelle. L'impact économique, social et culturel de ces découvertes, inventions et innovations faites par accident ou par hasard n'a aucun rapport avec leur notoriété médiatique, comme entre l'hélice de bateau de Pettit qui a révolutionné la navigation et le Post-it, toujours cité.[réf. nécessaire]
Liste des lois scientifiques par disciplines, par années de découverte et par ordre alphabétique. Les libellés en italique sont des alias ou des redirections.
Les méthodes expérimentales scientifiques consistent à tester la validité d'une hypothèse, en reproduisant un phénomène (souvent en laboratoire) et en faisant varier un paramètre. Le paramètre que l'on fait varier est impliqué dans l'hypothèse. Le résultat de l'expérience valide ou non l'hypothèse. La démarche expérimentale est appliquée dans les recherches en biologie, physique, chimie, psychologie, ou encore l'archéologie. Certains soutiennent que le savant Ibn Al Haytham (Alhazen),,, a été l'un des premiers à faire la promotion des méthodes expérimentales. Définies par le chimiste Michel-Eugène Chevreul en 1856, elles ont été développées par Claude Bernard en médecine et en biologie. Outil privilégié des sciences de la nature, les méthodes expérimentales sont également utilisées en sciences humaines et sociales.
Cette liste de notations scientifiques est censée être mise à jour et inversement, les articles faisant référence aux notations en question doivent idéalement être maintenus en accord avec cette liste.
Une propriété de matériau est une grandeur intensive généralement avec une unité de mesure qui peut être utilisée comme métrique de la valeur pour comparer les avantages d'un matériau plutôt qu'un autre dans un choix de matériaux (en). Une propriété du matériau peut être une constante, ou une fonction de plusieurs variables indépendantes, telles que la température et la pression.
Cette liste de sons inexpliqués recense des phénomènes sonores ayant été détectés mais dont l'origine n'a pas pu être trouvée. Certains de ces sons ont été enregistrés dans les océans ou dans l'atmosphère et les théories sont parfois nombreuses pour en expliquer l'origine, sans confirmation définitive. Certains sons sont restés longtemps sans origine connue mais ont fini par être expliqués.
Ceci est une liste de sujets d'étude scientifiques classés par ordre alphabétique
La liste des unités mixtes de recherche présente les 826 unités mixtes de recherche (UMR) du CNRS selon les chiffres de février 2012. Au 5 février 2015, elles sont 837, consultables sur l'annuaire des unités du CNRS.
Cet article présente les différentes utilisations des lettres de l'alphabet grec dans les sciences. Il faut noter que, en physique, les variables ou paramètres analogues sont notés en italique : la constante mathématique « pi » est ainsi notée π (symbole pi pas en italique) alors que la parallaxe sera notée π (symbole pi en italique). Note : l'ensemble de l'article se base sur les ouvrages référencés dans la section bibliographie, en particulier les deux ouvrages Formulaire technique et Tables numériques et formulaires.
L'Association française de réalité virtuelle, augmentée, mixte et d'interaction 3D (AFRV) a vu le jour en novembre 2005. Fondée par une douzaine de chercheurs et de cadres de l'industrie, cette association loi de 1901 entend fédérer la communauté française, académique et industrielle, autour de ces thèmes. Elle a pour vocation de : promouvoir et favoriser le développement de la réalité virtuelle, de la réalité augmentée, de la réalité mixte et de l’interaction 3D dans tous leurs aspects : enseignement, recherche, études, développements et applications ; procurer un moyen de communication entre les personnes intéressées par ce domaine ; faire reconnaître cette communauté par les institutions françaises, européennes et internationales.
L'Association France Cichlid (AFC) a été fondée le 16 mai 1980, sous la houlette de Jean-Claude Nourissat (membre fondateur), Jean Carlus, Robert Allgayer.
La Banque espagnole d'algues (BEA, en espagnol : Banco Español de Algas) est un service du Centre de Biotechnologie Marine (CBM) de l’Université de Las Palmas de Gran Canaria (ULPGC). Les objectifs principaux sont l’isolement, l'identification, la caractérisation, la conservation et l'approvisionnement de microalgues et cyanobactéries.
Bioemco est un laboratoire scientifique de recherche français, fruit de la collaboration entre l'UPMC, le CNRS, l'INRA, l'ENS Ulm, l'IRD, AgroParisTech l'Université Paris Est. Bioemco comprend 180 personnes réparties sur cinq sites franciliens et sept sites étrangers, dont 105 chercheurs, enseignants-chercheurs et ITA.
Bioinitiative est un groupe de scientifiques internationaux, dont la première contribution publiée en 2007 est le "rapport Bioinitiative", qui vise à démontrer la dangerosité des champs électromagnétiques (basses fréquences, radio-fréquences, Wi-Fi…). Les associations environnementales l’utilisent comme base scientifique, notamment, pour justifier l’appel au principe de précaution et demander le démontage d’antennes-relais. Cependant, ce rapport est critiqué par de nombreuses instances scientifiques.
Le Board of European Students of Technology (BEST, en français Comité des étudiants européens en technologie) est une organisation internationale, non gouvernementale, apolitique, à but non lucratif, entièrement gérée par des étudiants. Celle-ci s'efforce de rendre les étudiants en technologie plus ouverts vers l'étranger, en encourageant leur mobilité et leur communication interculturelle. BEST utilise l'anglais comme seule langue officielle.  À l'heure actuelle, BEST compte 95 groupes BEST locaux (LBG), dans 32 pays européens, et plus de 3 300 membres actifs, touchant environ 1 700 000 étudiants. L'organisation dispose d'un service d'offres d'emplois, ciblées vers les étudiants en technologie envisageant une carrière internationale, proposant une base de données contenant environ 20 000 CV d'étudiants européens.
Le Comité de données pour la science et la technologie (ou CODATA), créé en 1966, est un comité interdisciplinaire du Conseil international pour la science (ICSU). Son objectif est d'améliorer la collection, l'évaluation critique, et l'accès aux données scientifiques et technologiques majeures. CODATA préconise une liste de valeurs des constantes physiques fondamentales. Il y a déjà eu 7 publications, en 1973, 1986, 1998, 2002, 2006, 2010 et 2014.
Le Conseil culturel mondial (CCM) est une organisation internationale à but non lucratif, avec son siège au Mexique, dont les objectifs sont de promouvoir la culture, les valeurs d’humanisme et la coopération à travers le monde. Elle accorde le prix mondial des sciences Albert Einstein, le prix mondial de l'education José Vasconcelos et le prix mondial des arts Léonard de Vinci à des personnalités remarquables dont le travail a eu un impact positif significatif sur l'héritage culturel de l'humanité. Les membres du Conseil comprennent plusieurs lauréats du prix Nobel. C'est en 1981, sur l'inspiration de 124 éminents universitaires, présidents d'université et dirigeants d'organisations du monde entier, que le CCM a été fondé et, en 1984, la première cérémonie de remise des prix a eu lieu. Le Conseil culturel mondial est constitué d'un organe directeur dirigé par un président d'honneur, un vice-président, un directeur exécutif, un secrétaire général, et un comité interdisciplinaire composé de personnalités scientifiques, artistiques et éducatives mondialement reconnues. Le comité interdisciplinaire évalue annuellement les candidats désignés pour participer aux prix «Albert Einstein», «José Vasconcelos» et «Leonardo da Vinci».  
L'association scientifique EISCAT (European Incoherent Scatter Scientific Association) est une collaboration entre l'Allemagne, la France, le Royaume-Uni, la Finlande, la Norvège et la Suède consacrée à l'étude de la haute atmosphère et de l'ionosphère, et au couplage entre le Soleil et la Terre (interaction entre le vent solaire et la magnétosphère terrestre). EISCAT dispose de deux antennes radar réceptrices situées à Kiruna en Suède et à Sodankylä en Finlande, de deux antennes émettrices-réceptrices situées à Tromsø, et un radar à Svalbard en Norvège. En novembre 1995, le Japon avec l’Institut national pour la recherche polaire du Japon (en anglais : National Institute of Polar Research of Japan) est devenu le septième membre de la Société EISCAT, ouvrant la perspective d'une seconde antenne sur l'île de Svalbard.
La Federation of American Scientists (FAS), que l'on peut traduire par « Fédération des scientifiques américains », est une organisation non gouvernementale fondée en 1945 par des chercheurs du Projet Manhattan qui estimèrent que les scientifiques ont l'obligation morale de partager connaissances et savoirs pour peser sur les grandes décisions nationales. Leurs premières préoccupations furent le contrôle des armes atomiques et la recherche sur le nucléaire civil, thèmes toujours éminents pour le FAS. Avec l'appui de 67 lauréats du Prix Nobel, la FAS émet des analyses critiques sur de très nombreux sujets autour du thème de la sécurité nationale.
La Fondation Marcel Bleustein-Blanchet pour la Vocation, dite aussi Fondation pour la Vocation, est un organisme privé, reconnu d'utilité publique, destiné à encourager toutes les vocations et à aider les jeunes de 18 à 30 ans qui, faute d'appui matériel et personnel, sont freinés dans les efforts qu'ils déploient pour s'accomplir dans le métier de leur choix. Pour ce faire, elle accorde chaque année 20 Prix de la Vocation, des Prix de l’Espérance, un Prix Littéraire et un Prix de Poésie, d’une valeur de 8 000 €. L’accompagnement des Lauréats comporte également un appui immatériel important, à travers les conseils que la fondation prodigue, les démarches qu’elle initie et la mise à disposition de son réseau.
La Fondation Alexander von Humboldt (en allemand Alexander von Humboldt-Stiftung) est une fondation allemande pour la promotion de la coopération internationale dans le domaine de la recherche scientifique. Elle facilite des séjours de recherche de scientifiques étrangers en Allemagne, et soutient les contacts scientifiques et culturelles qui en résultent. La Fondation finance des programmes de bourses sur concours pour des scientifiques titulaires du doctorat et particulièrement pour des séjours de recherche en Allemagne de scientifiques étrangers pour une période de six mois à deux ans. La Fondation accorde également de nombreux prix à des scientifiques de renommée mondiale comme le Prix Humboldt. Le financement est assuré par le gouvernement de la République fédérale d'Allemagne. Le siège de la fondation est à Bonn-Bad Godesberg, où la Fondation possède aussi une maison permettant d'héberger des visiteurs.
Les Petits Débrouillards existent dans de nombreux pays à travers le monde. Ils ont été créés pour la première fois au Canada. Ce mouvement regroupe de nombreuses structures, la plupart du temps associatives, dont le but commun est de faire partager la curiosité scientifique au plus grand nombre. Ainsi, ces associations s'adressent à tous les publics (adultes et enfants). Historiquement, elles s'adressaient aux enfants de 6 à 12 ans en leur proposant de réaliser des expériences ludiques, façon science amusante. Aujourd'hui, ces associations produisent des expositions, des expositions interactives, des livres, des CD-ROM ; elles participent également à la formation professionnelle continue, proposent une expertise didactique et réalisent des outils pédagogiques.
MarsDrive, fondée en 2005, est une fondation internationale à but non lucratif active dans le milieu spatial avec des membres au niveau international ainsi que des branches en Amérique du Nord, Europe et en Asie australe.
Le Mind and Life Institute (français : Institut Esprit et Vie), a pour but de favoriser un apport mutuel entre le bouddhisme et la science. C'est une association à but non lucratif qui s'attache à explorer la relation de la science et du bouddhisme comme méthodologies dans la compréhension de la nature de réalité. L'institut a parmi ses membres, des scientifiques émérites, et des pratiquants bouddhistes, dont le plus connu est le 14e dalaï-lama. Il fut fondé en 1990 aux États-Unis en tant qu'association de la section 501(c)(3) - équivalent de loi française de 1901.
Le National Inventors Hall of Fame est une organisation qui honore les réalisations des inventeurs, peu importe leur nationalité, pour peu que celles-ci soient remarquables. Pour y être admis, de son vivant ou à titre posthume, il faut posséder au moins un brevet déposé auprès du United States Patent and Trademark Office (USPTO). En 2018, l'institution compte 562 intronisés.
La National Space Society (NSS) est une association à but non lucratif d'origine américaine soutenant l'exploration spatiale, la colonisation de l'espace, l'énergie solaire spatiale, l'ascenseur spatial ainsi que le tourisme spatial.
L'Organisation islamique pour l'éducation, les sciences et la culture (ou ISESCO pour Islamic Educational, Scientific and Cultural Organization) est un organisme établi en mai 1979 par l'Organisation de la Conférence Islamique (OCI, actuelle Organisation de la coopération islamique). Son siège est basé à Rabat, au Maroc. Depuis 2006, elle décerne le titre de capitale de la culture islamique à plusieurs villes du monde musulman.
La plateforme océanique des îles Canaries (ou PLOCAN) est un consortium situé à Grande Canarie consacré à la recherche scientifique des secteurs maritime et marine de la région. Son objectif est la combinaison rentable de services comme observatoires, bancs d'essai, support pour véhicules sous-marins, formation et centre d'innovation.
Le Réseau Blaise Pascal (Sciences, Cultures et Foi), créé en avril 2001, est constitué de plus d’une vingtaine de groupes francophones d’inspiration chrétienne qui travaillent sur la question « sciences, cultures et foi. »
La Space Frontier Foundation (SFF) est une association à but non lucratif d'origine américaine soutenant l'exploration spatiale et le développement des activités spatiales. D'inspiration libérale, elle souhaite accroitre le rôle du secteur privé dans le programme spatial, en collaboration avec les gouvernements, et œuvre pour la création de spatioports.
USENIX est une association créée en 1975 qui se présente comme « l'association des systèmes informatiques avancés » (en anglais, The Advanced Computing Systems Association).
DEFT ou Défi fouille de texte est une campagne d'évaluation scientifique francophone portant sur la fouille de textes. Le défi est organisé depuis 2005 par des chercheurs du LRI (Laboratoire de Recherche en Informatique, Orsay) et du LIRMM (Laboratoire d'informatique, de robotique et de microélectronique de Montpellier), puis du LIMSI (Laboratoire d'informatique pour la mécanique et les sciences de l'ingénieur, CNRS) dès 2007 avec le soutien du consortium European Language Resources Association (ELDA/ELRA) (en).
ESTER est une campagne d'évaluation scientifique francophone organisée depuis 2005. Ce projet, initialement connu sous le nom EVALDA / ESTER a été mis en place en France par l'AFCP (Association française de la communication parlée), la Délégation générale pour l'Armement et la European Language Resources Association.
Le Text REtrieval Conference (TREC) est un programme conçu comme une série d'ateliers dans le domaine de la Recherche d'information (RI ou IR). Ce programme est soutenu conjointement par le National Institute of Standards and Technology (NIST) et par l'Advanced Research and Development Activity (ARDA) Center du Département de la Défense des États-Unis. Il a débuté en 1992 dans le cadre du projet TIPSTER. Son but est d'encourager les travaux dans le domaine de la recherche d'information en fournissant l'infrastructure nécessaire à une évaluation objective à grande échelle des méthodologies de recherche textuelle et accroitre la rapidité du transfert de technologie. En 2001 et 2002, la conférence a organisé des campagnes d'évaluation sur la segmentation, l'indexation et la recherche par le contenu dans les vidéo. Cela est devenu en 2003 une campagne d'évaluation indépendante sous le nom de TRECVID.
Le congrès de l'Acfas est un congrès annuel scientifique, multidisciplinaire et interdisciplinaire, organisé par l'Association francophone pour le savoir (Acfas), depuis 1933,. Le congrès de l'Acfas est considéré comme le plus grand rassemblement scientifique multidisciplinaire de la francophonie,,,. Chercheurs et professionnels s'y rassemblent pour présenter leurs plus récents travaux, connaître les derniers développement en recherche, débattre de questions d'actualité et échanger des idées avec des collègues, des associations et des groupes participants.
Dans les milieux académiques, les actes de conférence ou comptes-rendus de conférence sont le recueil des communications faites lors d’un congrès scientifique et publiées avant ou, le plus souvent, à la suite de ce congrès. Les actes de conférence, traditionnellement publiés sous forme imprimée, peuvent aussi être disponibles sur support numérique ou directement en ligne. Le document constitue les archives des présentations orales faites lors du congrès. Bien qu’ayant physiquement l’allure d’une monographie lorsqu’elles sont en format papier, les actes d’une conférence ne constituent pas un texte suivi, mais un ensemble thématique d’articles. Dans certains cas lorsque le sujet des communications est suffisamment uniforme, l’éditeur peut en remanier le contenu afin d’en faire une monographie. Dans ce dernier cas, il ne s’agit plus d’actes de conférence, mais d’une monographie. Les actes de conférence s’adressent généralement à un public restreint spécialisé dans un domaine particulier des sciences, tout comme pour le congrès où ils ont été prononcés. Comme les revues scientifiques, ils constituent une forme privilégiée de communication entre les chercheurs d’un domaine scientifique. Toutefois, comme le processus de sélection des communications à un congrès n’est généralement pas aussi rigoureux que l’évaluation par les pairs des revues scientifiques, il en résulte que la rigueur de l’information des actes de conférence n’est pas aussi élevée comparée aux articles des revues. L’avantage des conférences sur les articles de revue scientifique est la rapidité de la diffusion de l’information : la procédure d’arbitrage des revues scientifiques peut en effet être passablement longue. Les différents textes sont rassemblés dans les actes et présentés par un ou plusieurs directeurs de la publication. La publication de l'ensemble est assurée par l'institution qui a organisé le congrès, ou par une maison d'édition académique.
American Journal of Human Genetics est une revue scientifique mensuelle évaluée par des pairs dans le domaine de la génétique humaine. Elle a été créée en 1948 par l'American Society of Human Genetics (en) (la Société américaine de génétique humaine) et couvre tous les aspects de l'hérédité chez l'homme, y compris l'application de la génétique en médecine de même que les politiques publiques, ainsi que dans les domaines connexes de la biologie moléculaire et cellulaire. Selon le Journal Citation Reports, en 2012, le journal a eu un facteur d'impact de 11.202.
Un appel à papiers ou à communications, ou Call For Papers (CFP) en anglais, est une méthode académique pour rassembler des livres, articles de journaux ou présentations de conférences. L'appel à papiers décrit les thématiques sur lesquelles doivent porter les contributions, il contient aussi les informations concernant les dates limites ainsi que les membres du comité qui vont évaluer les contributions. L'appel à papiers est en général diffusé par courrier électronique sur des listes de diffusion spécialisées, il est aussi mis en ligne sur des sites recensant les appels à papiers.
Un article de revue est un type particulier d'article publié dans les revues scientifiques dont le principe est de dresser un état des lieux dans un domaine particulier de la recherche et de dégager les directions particulières prises dans ce domaine. Sa forme, comme tous les articles scientifiques, peut aller de la rédaction théorique ou technique extrêmement spécialisée, ou aller vers une certaine forme de vulgarisation. En tant qu'article à vocation éminemment scientifique, l'article de revue est soumis aux mêmes exigences que les articles plus classiques, notamment le contrôle avant publication par un comité de lecture. Il est publié également dans des revues scientifiques ou séries comme Annual review of cell and developmental biology ou Annual review of earth and planetary sciences.
L'Association Information et Management (AIM) est Affiliated Chapter (une branche) de AIS, the Association for Information System. Fondée en 1991, elle rassemble enseignants, chercheurs, professionnels et spécialistes des systèmes d'information (SI). Sa vocation est de faciliter les débats relatifs à ce champ disciplinaire dans les communautés francophones et à l'international en général. L'intérêt de concevoir une communauté de personnes impliquées dans les SI est de dynamiser les échanges afin de diffuser les meilleurs pratiques pédagogiques et scientifiques. Enfin, AIM souhaite valoriser les filières de la formation en SI et renforcer leur attractivité.
La communication scientifique et technique est l'activité dont l'objet est de diffuser les problématiques et les résultats de la recherche scientifique fondamentale ou appliquée ou du développement industriel soit en direction des pairs, soit vers un large public (on parle souvent dans ce cas de vulgarisation). La communication scientifique et technique répond à des règles très différentes selon le public visé : règles de plus en plus formalisées aujourd'hui quand il s'agit de la communication des scientifiques à destination des autres membres de la communauté scientifique, règles beaucoup plus souples pour la vulgarisation. Mais, dans tous les cas, qu'elle soit écrite ou orale, elle recherche l'efficacité et la bonne perception des messages émis. Ce type de communication est enseigné dans le cadre de formations initiales de type master, ou par le biais de modules de formation courts. Aussi, beaucoup d'ouvrages qui traitent de communication scientifique et/ou technique se présentent comme des guides ou des manuels de bonnes pratiques. La communication scientifique et technique peut passer par différents médias : conférences, panneaux ou posters, articles, émissions de radio et de télévision, interviews, exposition d'objets, mais aussi images, vidéos, pièces de théâtre, visites d’usines ou de laboratoires de recherche…
Un congrès ou conférence scientifique est un événement qui vise à rassembler des chercheurs et ingénieurs d'un domaine pour faire état de leurs avancées. Cela permet également à des collègues géographiquement éloignés de nouer et d'entretenir des contacts. Les congrès se répètent généralement avec une périodicité fixée, le plus souvent annuelle.
La Déclaration de San-Francisco sur l'évaluation de la recherche (San Francisco Declaration on Research Assessment ; DORA) ou « Déclaration de San-Francisco » est une déclaration initiée par des scientifiques de l'American Society for Cell Biology (ASCB) et un groupe d'éditeurs de journaux scientifiques. Elle remet en cause l'usage croissant du classement bibliométrique comme indice au service de l’évaluation de la Recherche ou des chercheurs. Préparée en 2012, cette déclaration a été publiée en 2013. La DORA se présente comme une initiative mondiale concernant toutes les disciplines scientifiques, que toute personne physique ou morale ou institution sensible aux besoins d'amélioration de l’évaluation de la recherche scientifique peut signer. Et après quelques mois, en mai 2013, plus de 6000 personnes l'avaient signé, et le nombre d'organisations scientifiques signataires est passé de 78 à 231 en 2 semaines. Elle découle des conclusions d'une réunion organisée en 2012 en marge du congrès annuel de l'ASCB qui portait sur le thème des citations scientifiques et de l'évaluation de la recherche scientifique, et mettant en cause la notion de facteur d'impact (tel que calculé par Thomson Reuters, de plus en plus utilisée pour évaluer les chercheurs, malgré les biais qu'elle implique), . Le facteur d'impact est une mesure indirecte de la visibilité et notoriété d'une revue, qui selon les auteurs de la déclaration peut être affecté par des biais importants. Or c'est un des critères utilisé pour l'évaluation des chercheurs et des laboratoires, dont en France par l'AERES (l'autorité administrative indépendante créée en 2007 pour évaluer la Recherche).
L'état de l'art est l'état des connaissances dans tout domaine donné (scientifique, technique, artistique, médical, etc.) à un instant donné. C'est un des objectifs des grandes encyclopédies que de dresser un état de l'art sur les grands sujets culturels, scientifiques et techniques de leur époque, y compris par des illustrations (dont les premières ont été les planches encyclopédiques Diderot et d'Alembert) décrivant les métiers et les techniques existant à son époque.
Une étude littéraire est un travail portant sur le domaine littéraire, que ce soit sur un ou plusieurs auteurs, d'œuvres, ou même d'un genre littéraire. Elle s'appuie en général sur un corpus bibliographique.
Dans les disciplines scientifiques, l'évaluation par les pairs (ou peer review selon l'expression anglophone couramment utilisée) désigne l'activité collective des chercheurs qui jugent de façon critique les travaux d'autres chercheurs (leurs « pairs »). Cette évaluation peut porter sur : une recherche précise soumise pour publication dans une revue scientifique (ou destinée à être présentée à une conférence). L'avis du comité de lecture est essentiel dans la décision d'accepter ou non la publication (ou la présentation) ; un projet de recherche proposé à un financement par une institution publique (comme le CNRS) ou privée (comme une fondation). Il s'agit alors de financer ou non le projet, et si oui à quel montant ; l'ensemble des travaux d'un chercheur, notamment lors de la sélection des candidats à un poste, mais aussi dans le cadre d'une procédure de contrôle (généralement périodique) ; l'ensemble des travaux d'un groupe de chercheurs ou d'un établissement entier, dans le cadre d'une procédure de contrôle (généralement périodique).
Le Festival du film de chercheur de Nancy est un festival de promotion de films scientifiques. Il a été créé en 1996. En parallèle se déroule un concours du film des chercheurs en herbe.
Un mémoire est un document permettant d'exposer son opinion concernant un sujet donné en s'appuyant logiquement sur une série de faits pour en arriver à une recommandation ou une conclusion. Il se veut habituellement court et incisif.
Une monographie est à l'origine un livre ou un traité non périodique, c'est-à-dire complet en un seul volume ou destiné à être complété en un nombre limité de volumes. On peut le définir aussi comme une étude approfondie limitée à un fait social particulier et fondée sur une observation directe qui, mettant en contact avec les faits concrets, participe de l'expérience vécue. De nos jours, le terme monographie est surtout utilisé lorsque l'on parle d'une « étude exhaustive et large portant sur un sujet précis et limité ou sur un personnage ».
Moon Hyung-in est un chercheur en biologie médicale, membre du département de biotechnologie médicale et professeur à l'université Dong-A à Busan en Corée du Sud, auteur de critiques frauduleuses de ses propres publications.
L'expression « publication scientifique » regroupe plusieurs types de communications scientifiques et/ou techniques avancées que les chercheurs scientifiques font de leurs travaux en direction de leur pairs et d'un public de spécialistes. Ces publications ayant subi une forme d'examen de la rigueur de la méthode scientifique employée pour ces travaux, comme l'examen par un comité de lecture indépendant constitué de pairs. La notion de littérature scientifique désigne plus largement l'ensemble des publications scientifiques. Les publications scientifiques permettent de diffuser des informations scientifiques et techniques, produites et utilisées notamment par les chercheurs. Une partie de cette littérature est réutilisée par les revues de vulgarisation scientifique.
La rédaction technique est la conception de documents écrits accessoires à l'utilisation ou la maintenance de machines, instruments, logiciels, pièces détachées, sur support imprimé ou informatique. Les rédacteurs techniques préparent, par exemple, des modes d'emploi, des notices d'utilisation, des aides en ligne (Mallender 2002), aussi bien pour du matériel informatique que des logiciels, de l'électroménager, des pièces d’aéronautique et d’astronautique, à l'intention d'utilisateurs de toute sorte de compétences techniques.
Un certain nombre de phénomènes sont modélisés sous forme de données chiffrées, et ce dans de nombreux domaines : mathématiques, physique, sociologie, géographie, géologie, économie… lorsque ces données sont abondantes, leur analyse et leur partage (communication) peut être facilitée par une représentation graphique.
Les notions de « revue de la littérature » (ou d’« analyse de la littérature ») désignent à la fois une méthode de travail scientifique et une « catégorie » d’études scientifiques. L’article final est parfois dit « article de revue » par les francophones (traduction de l'anglais Review article'. Le premier objectif d’une revue de la littérature est de résumer l'état de l'art ou de la connaissance dans un domaine et pour une période ou un territoire.
Une revue scientifique est un titre de presse à publication périodique édité sous la forme d'une revue. Il s'agit de l'un des types de communications choisi par les chercheurs scientifiques pour faire connaître leurs travaux en direction d'un public de spécialistes, et ayant subi une forme d'examen de la rigueur de la méthode scientifique employée pour ces travaux, comme l'examen par un comité de lecture indépendant. Elle se distingue par son format périodique des autres publications scientifiques, livres, thèses ou littérature grise. Elle a pour fonction de faire connaître des travaux de recherche originaux ou de fond, et de contribuer ainsi au débat scientifique entre spécialistes ; elle se distingue ainsi des revues destinées à la vulgarisation. Les revues scientifiques, dans leur grande majorité, sont spécialisées dans un aspect particulier d'un domaine : sciences, technologie, médecine (STM), sciences humaines et sociales (SHS). Les articles publiés sont examinés auparavant par un comité de lecture composé de chercheurs-pairs, de la même spécialité. La publication d'un travail de recherche dans une revue scientifique permet de le confronter à la communauté scientifique, et d'archiver les résultats pour référence ultérieure. Dans le cadre de l'évaluation des performances de recherche, on tente de comptabiliser le nombre de publications, l'impact des revues scientifiques dans lesquelles elles sont publiées (c'est-à-dire les lectorats potentiels des articles) et le nombre de publications ultérieures citant chacune d'elles. Cette technique d'évaluation, parfois décriée pour ses biais, est appelée bibliométrie. Ces revues peuvent être l'émanation de sociétés savantes ou d'académies des sciences, par exemple, mais peuvent aussi avoir été fondées indépendamment. Des éditeurs de presse se spécialisent dans leur édition, impression et distribution. Il s'agit alors d'édition scientifique.
Le Congrès de Saint-Louis de 1904 (ou Congrès universel des arts et des sciences de Saint-Louis) se tint à Saint-Louis, au Missouri, du 19 au 26 septembre 1904. L'évènement eut lieu durant la tenue de l'Exposition universelle qui fut présentée dans cette ville la même année. Le huitième congrès géographique international y a également tenu une session finale.
Une thèse (du nom grec thesis, se traduisant par « action de poser ») est l'affirmation ou la prise de position d'un locuteur, à l'égard du sujet ou du thème qu'il évoque.
La vulgarisation est une forme de diffusion pédagogique des connaissances qui cherche à mettre le savoir (et éventuellement ses limites et ses incertitudes) à portée d'un public non expert. C'est l'ensemble des actions permettant au public d'accéder à la culture, et en particulier aux cultures scientifiques, techniques, industrielles ou environnementales, c'est-à-dire aux savoirs, savoir-faire et savoir-être de ces disciplines. La vulgarisation est intimement liée à la démarche scientifique, et fait partie des missions des chercheurs. C'est une nécessité pour l'avancée du savoir. Elle est nécessaire à la conservation de la production scientifique (alors vue comme bien commun). Le partage des résultats avec le plus grand nombre facilite la critique positive et méliorative, et est même facteur de sérendipité. [réf. nécessaire] La vulgarisation permet aussi au citoyen de pouvoir se saisir d'un enjeu la communauté scientifique, ce qui se développe via certains partenariats entre recherche et citoyens. Mais cette dernière piste reste encore marginale, comparativement à l'espace occupé par la vulgarisation plus « classique » (magazines, émissions de télévision, livres, musées de science, universités populaires, cours publics, etc.). Enfin, on peut noter que l'enjeu que représente la vulgarisation prend de l'ampleur dans le même temps que sont réalisées les avancées scientifiques et culturelles, en considérant que la somme et la complexité de l'information à diffuser est croissante dans le temps. De plus, internet permet de tendre une certaine idée de démocratisation de cette méthode de diffusion. Son rôle est ainsi exponentiel puisque l'écart entre la population et la connaissance se creuse. Dans cette perspective, la vulgarisation prend une portée quasiment politique, car elle est outil d'information et point de jonction à la fois entre une spécialisation sans limites, et des citoyens pour qui les réalités exposées par les spécialistes paraissent de plus en plus occultes. Ainsi, elle peut être l'objet de manipulations à des fins n'ayant rien à voir avec l'élévation scientifique ou culturelle de la population (voir Approches critique et éthique).
Une controverse scientifique peut être définie comme un débat, amical ou non, opposant des personnalités de la communauté scientifique ou ses observateurs (épistémologues, philosophes des sciences, sociologues des sciences, journalistes…) sur un point de théorie scientifique ou sur les faits historiques ou philosophiques devant être associés à cette théorie. Toute controverse scientifique est par nature conflictuelle. L'histoire des sciences « ne saurait se passer de controverses [car elles] accompagnent les progrès du savoir [et] sont de nature à faire avancer la recherche » ; elle est donc jalonnée de controverses, dont certaines sont restées célèbres. Si les controverses scientifiques font évoluer la science, qui est par nature réfutable, elles contiennent aussi « un débordement, une "contamination" du ou vers le social ». Certaines sont liées à la « paternité » de la découverte ou de l'invention comme l'invention du calcul différentiel, qui de Isaac Newton ou de Gottfried Wilhelm Leibniz.
L'histoire des sciences est jalonnée de controverses scientifiques dont plusieurs sont restées célèbres.
Peter J. d'Adamo, né le 17 juillet 1956, est un naturopathe américain, ancien étudiant de l'université de Bastyr dans les années 1980. Son premier livre est traduit en plus de 65 langues. Actuellement, 6 de ses 10 livres en anglais sont traduits en français.
L'affaire de l'étude de Rind, Tromovitch et Bauserman est une controverse suscitée par la publication en 1998, dans le Psychological Bulletin, revue de la Société américaine de psychologie, d'un article de trois psychologues américains : Bruce Rind, du département de psychologie de l'université de Philadelphie, Philip Tromovitch, de l'université de Pennsylvanie, et Robert Bauserman, de l'université du Michigan. Cette étude, qui porte sur les abus sexuels sur mineurs et les éventuelles séquelles psychologiques observées chez les victimes de ceux-ci, est une méta-analyse de 59 enquêtes effectuées précédemment auprès d'étudiants ayant été victimes ou non d'abus sexuels, le ressenti personnel de ceux qui ont subi ces abus, et les signes cliniques de traumatisme (alcoolisme, dépression, anorexie, paranoïa...), en vue d'en déterminer les conséquences à long terme. La conclusion des auteurs de cette étude est que « le point de vue selon lequel les abus sexuels sont par nature destructeurs, et causent un traumatisme sévère aux garçons comme au filles est propre à la culture américaine et ce point de vue n'est pas soutenu par les résultats de l'étude ». L'étude a provoqué aux États-Unis une vive controverse, portant aussi bien sur ses conclusions que sur la méthode employée par les auteurs. L'étude a été condamnée à l'unanimité par les membres de la Chambre des représentants et du Sénat. La condamnation d'une étude scientifique par le Congrès était, à cette époque, un événement sans précédent. Parmi les trois auteurs de l'étude, deux avaient participé à des conférences organisées par des militants pédophiles et contribué à des publications de cette même mouvance,, et le troisième a fait par la suite des déclarations publiques en faveur de la pédophilie.
L’affaire Séralini est déclenchée suite à la publication d'un article scientifique, « Long term toxicity of a Roundup herbicide and a Roundup-tolerant genetically modified maize » (en français « Toxicité à long terme d'un herbicide Roundup et d'un maïs génétiquement modifié tolérant au Roundup ») écrit par Gilles-Éric Séralini, présentant les résultats d'une étude relative à la toxicité à long terme (deux ans) du Roundup (un herbicide à base de glyphosate) et du NK 603 (un maïs génétiquement modifié tolérant au Roundup) sur des rats de laboratoire. Publiée initialement le 19 septembre 2012 dans la revue scientifique à comité de lecture Food and Chemical Toxicology, cette étude a été menée par Gilles-Éric Séralini et une équipe de chercheurs de l'université de Caen. Elle a coûté plus de 3 millions d'euros ; elle a été financée et soutenue notamment par les fondations Ceres (Consommateurs et Entrepreneurs Responsables) dont font partie les groupes Auchan (dont le fondateur Gérard Mulliez est engagé auprès des groupes antiOGM depuis plusieurs années,[réf. à confirmer] même s'il se défend d'être antiOGM) et Carrefour, et la Fondation Charles Leopold Meyer pour le progrès pour l'homme, ainsi que des fonds provenant de la réserve parlementaire du sénateur français François Grosdidier. L'étude compare des rats nourris avec du maïs génétiquement modifié (cultivé avec ou sans Roundup), des rats ayant reçu du Roundup uniquement (sans maïs OGM) et des rats contrôles, durant deux ans. Les auteurs affirment que les rats femelles ayant ingéré du maïs OGM et/ou du Roundup sont morts plus rapidement que ceux du groupe de contrôle. Les auteurs concluent également que les rats mâles et femelles testés ont développé plus de tumeurs que les contrôles et que l'ingestion de maïs OGM et/ou de Roundup provoque également des problèmes hormonaux et de toxicité au foie et aux reins. Cependant, ces résultats très controversés et la mise en scène médiatique qui les a accompagnés ont fait l'objet d'une levée de boucliers scientifique et médiatique sans précédent,,,,,,,,,,. L'étude est qualifiée par l'Autorité européenne de sécurité des aliments (AESA) comme étant d'une « qualité scientifique insuffisante ». Par exemple, les différences de durée de survie seraient biaisées du fait que deux rats du groupe contrôle aient survécu inhabituellement longtemps (tous les groupes à l'exception du groupe témoin se trouvent à l'intérieur des fourchettes de référence fournies par l'éleveur). Malgré son retrait controversé en novembre 2013 de la revue Food and Chemical Toxicology, cet article a toutefois eu un très fort impact médiatique au niveau international dans le cadre de la controverse relative aux organismes génétiquement modifiés (OGM). Le 24 juin 2014, l'étude est à nouveau publiée dans une version légèrement remaniée avec ses données brutes en accès libre (science ouverte), dans la revue Environmental Sciences Europe du groupe Springer.
La controverse sur le nom des transfermiens ou guerre des transfermiens s'étend des années 1960 à 1997 et concerne l'attribution des noms des éléments chimiques suivant le fermium (élément de numéro atomique 100 dans la classification périodique des éléments), plus particulièrement les éléments de numéros atomiques 102 à 109. Elle a impliqué des équipes de chercheurs travaillant dans trois centres différents : le Centre de recherche sur les ions lourds (GSI) à Darmstadt (Allemagne), l'Institut unifié de recherches nucléaires (JINR) à Doubna (Russie), et le Laboratoire national Lawrence-Berkeley (LBNL) à Berkeley (États-Unis). Ces équipes se sont disputé les découvertes de ces éléments et ainsi le droit de les nommer. L'Union internationale de chimie pure et appliquée (UICPA) et l'Union internationale de physique pure et appliquée (UIPPA) tentèrent de résoudre la controverse autour des éléments 104 et 105 dès 1974 à l'aide d'une commission ad hoc, sans succès. Les deux institutions créèrent le Transfermium Working Group (TWG) en 1986 pour attribuer les découvertes des transfermiens et, in fine, leur donner un nom. Les résultats du TWG furent utilisés, accompagnés des suggestions des équipes impliquées, pour établir les recommandations de l'UICPA de 1994. Ces dernières assignèrent un nom officiel à chacun des éléments 101 à 109. Cependant, elles suscitèrent de fortes critiques, notamment du côté américain, et l'American Chemical Society adopta sa propre nomenclature. Par conséquent, de nouvelles recommandations furent adoptées par l'UICPA en 1997, attribuant un nom définitif à ces éléments.
La crise de la reproductibilité, (replication crisis ou replicability crisis en anglais) fait référence à la crise méthodologique dans le domaine des sciences selon laquelle de nombreux résultats publiés dans des revues scientifiques sont difficiles, voire impossibles à reproduire au cours d'études subséquentes. Initiée au milieu des années 2000, la crise prend de l'ampleur au milieu des années 2010, nourrie par la publication de plusieurs articles sur le phénomène,. Ainsi, selon une étude réalisée auprès de 1 500 scientifiques et publiée par Nature en 2016, plus de 70 % des chercheurs affirment avoir été incapables de reproduire l'expérience scientifique d'un autre chercheur et plus de la moitié affirment avoir échoué à reproduire leur propre expérience. Un autre sondage sur la reproductibilité des recherches sur le cancer montre que 50 % des répondants affirment avoir fait l'expérience d'au moins un épisode d'incapacité à reproduire des données publiées. Plusieurs étant incapable de déterminer la source du problème même en interrogeant les auteurs originaux. En 2015, les résultats du Reproductibility project (en), lancé en 2011 afin de mieux quantifier le phénomène en psychologie, montrent que moins de la moitié des expériences dans ce domaine ont pu être reproduites. La crise n'est pas propre à un domaine unique bien qu'elle semble moins toucher les sciences fondamentales et appliquées que les sciences médicales. Les facteurs qui en sont responsables semblent nombreux. Des pistes d'amélioration de la reproductibilité au sein des publications scientifiques, dont notamment l'amélioration des critères de publication, sont explorées.
Les dinosaures au Paléocène seraient des familles ou genres de dinosaures non-aviens[Lesquels ?] qui auraient survécu à l’extinction Crétacé-Tertiaire (aussi appelée « limite K-T »), il y a 65,5 millions d'années environ. Bien que presque tous les témoignages fossiles indiquent que les dinosaures non-aviens ont tous disparus à la limite K-T, quelques fossiles font polémique, bien que pour l'instant, aucun squelette complet (c'est-à-dire plus que quelques os) n'ait été découvert.
L’eau polymérisée était une forme hypothétique polymérisée d’eau qui a été un sujet de controverse scientifique à la fin des années 1960. En 1969, la presse généraliste s'y est intéressée. En 1970, des doutes sur l'authenticité de l'eau polymérisée ont commencé à se faire jour,,. À la fin de l'année 1973, il apparut clairement que l'eau polymérisée n'existait pas. À présent, l'eau polymérisée est utilisée comme exemple de science pathologique.
Selon l'hypothèse de l'effet barbecue, les personnes vivant en milieu urbain dense et ayant au quotidien un mode de vie avec une grande efficacité énergétique, seraient au final plus énergivores en raison d'une mobilité occasionnelle ayant un fort impact, notamment les voyages utilisant le transport aérien. Cette mobilité occasionnelle serait due au besoin de quitter une forme urbaine dense. Au contraire, des personnes ayant un mode de vie plus énergivore, par exemple avec un pavillon de banlieue et une voiture, pourraient faire des activités locales pendant leurs loisirs, comme un barbecue dans leur jardin. Elles auraient donc un impact global moins élevée. Cet effet contre-intuitif est largement controversé.
L’effet Mpemba est le nom donné au phénomène qui apparaît lorsque de l’eau chaude gèle plus vite que de l’eau froide dans des conditions de refroidissement similaires. Cet effet est aussi parfois nommé « paradoxe Mpemba » car l'eau doit obligatoirement repasser par une température inférieure en refroidissant, et prendre a priori plus de temps à refroidir qu'à une température plus basse. Ce phénomène n'est pas systématique et n'apparaît que sous certaines conditions. Supposément connu depuis l'Antiquité, cet effet a été redécouvert en 1963 par un élève tanzanien, Erasto B. Mpemba, et diffusé dans la communauté scientifique à partir de cette date.
L'expression médiatique fusion froide désigne des réactions supposées « nucléaires à température et pression ambiante ». La plus connue est celle qui semble être une fusion nucléaire réalisée selon des techniques dérivées d'une expérience réalisée par Martin Fleischmann et Stanley Pons en mars 1989. Cette expérience se caractérisait par un dégagement de chaleur non explicable par la quantité d'énergie électrique reçue (faisant fondre l'électrode). Le terme de « fusion froide » apparaît en 1956 dans un article du New York Times décrivant le travail de Luis W. Alvarez sur la catalyse par muon. E. Paul Palmer de l'université Brigham Young a aussi utilisé le terme « fusion froide » en 1986 dans son investigation sur la « géo-fusion » : la possible existence de la fusion dans le cœur des planètes. Les phénomènes de ce domaine de recherche sont aussi appelés LENR (low-energy nuclear reactions pour « réactions nucléaires à basse énergie »), CANR, LANR, CMNS, BL, Sonofusion, Bubble fusion, CNT ou « transmutations biologiques ». L'expression fusion froide n'est pas admise par la majorité de la communauté scientifique, parce que l'expérience de Pons et Fleischman est difficilement reproductible et a déclenché une polémique mondiale sur la vérification effectuée par les comités de lecture. Le principe même de la fusion froide reste controversé, certains n'hésitant pas à assimiler ces expériences à celles de l'alchimie et des tentatives de transmutation du plomb en or ; les processus physiques reconnus permettant d'aboutir de façon avérée à des réactions de fusion nucléaire, utilisables pour la production d'énergie, nécessitent en effet des pressions et des températures extrêmement élevées.
Gordon Gould, né le 17 juillet 1920 à New York et mort dans cette même ville le 16 septembre 2005, est un physicien américain. Il est largement crédité de l'invention du laser, mais pas universellement, certains attribuant cette invention à Theodore Maiman. Gould est du coup connu pour son combat de trente ans avec l'United States Patent and Trademark Office pour obtenir des brevets concernant le laser et ses technologies connexes. Il est également à l'origine de batailles judiciaires avec les fabricants de laser pour faire respecter les brevets obtenus.
L’homéopathie ou homœopathie (du grec όμοιος / hómoios, « similaire » et πάθος / páthos, « souffrance » ou « maladie ») est une pratique pseudo-scientifique de médecine alternative inventée par Samuel Hahnemann en 1796. Le principe de fabrication des médicaments homéopathiques est de diluer des substances qui, si elles étaient concentrées, provoqueraient des symptômes similaires à ceux du patient. Mais en raison des dilutions extrêmes utilisées, les remèdes homéopathiques sont dépourvus de principes actifs. L'homéopathie ne constitue pas un traitement plausible, étant donné que les principes sur lesquels la méthode de traitement repose, que ce soit à propos du fonctionnement des médicaments, des maladies, du corps humain, des fluides et des solutions, sont contredits par un large ensemble de découvertes faites en biologie, psychologie, physique et chimie dans les deux siècles suivant son invention,,,,. Les études cliniques à grande échelle ont montré que l'homéopathie ne présente aucune efficacité supérieure à l'effet placebo, et ce pour toutes les maladies considérées, ce qui suggère que les effets subjectifs ressentis sont dus à l'effet placebo et à l'évolution naturelle de la maladie,,,,. Bien que certains articles aient rapporté des résultats positifs,, de multiples revues systématiques indiquent que cela est dû au hasard, à des méthodes de recherche discutables, ou encore aux biais de publications. La persistance de l'utilisation de l'homéopathie par certaines populations en dépit de son manque avéré d'efficacité, a fait que dans les communautés scientifiques et médicales, elle est considérée comme une absurdité, du charlatanisme, et une imposture. Elle a été critiquée sur le plan éthique lorsqu'elle se fait au détriment de traitements efficaces,, et l'Organisation mondiale de la santé met en garde contre son utilisation dans le traitement de maladies graves comme le SIDA ou bien le paludisme. Son utilisation lors de désordres psychogènes ou psychosomatiques permet cependant aux médecins de bénéficier d'un traitement sans effets secondaires, et dont l'efficacité repose sur l'effet placebo, afin d'éviter de prescrire des thérapeutiques conventionnelles potentiellement iatrogènes ; cette méthode pose de sérieux problèmes éthiques et fait l'objet de nombreux débats. Entre autres, des évaluations par le National Health and Medical Research Council (en) en Australie, le House of Commons Science and Technology Committee (en) au Royaume-Uni et l'Office fédéral de la santé publique en Suisse ont toutes conclu que l'homéopathie est inefficace et se prononcent contre tout financement de cette pratique,. En septembre 2017, le Conseil scientifique des académies des sciences européennes, qui réunit l'ensemble des académies des sciences, a publié un rapport dénonçant d'une part l'inefficacité de cette méthode alternative par rapport à l'effet placebo, mais aussi que « la promotion et l'usage de produits homéopathiques posent des risques significatifs. Tout d'abord, en entrainant un délai pour le patient de rechercher des soins médicaux appropriés et basés sur des preuves ou même d'empêcher les patients d'avoir simplement recours à des soins médicaux »,,.
Impostures intellectuelles est un ouvrage d'Alan Sokal et Jean Bricmont publié en français en 1997. L'ouvrage constitue une critique assez dure envers ce que les auteurs regroupent sous le nom de « philosophie postmoderne ». Il vise en particulier des penseurs qui utilisent les concepts ou le vocabulaire des mathématiques ou de la physique, relève des erreurs, dénonce des pensées vides de sens et commente des extraits de livres de Jacques Lacan, Julia Kristeva, Bruno Latour, Gilles Deleuze, Luce Irigaray, Jean Baudrillard, et Félix Guattari. Cet ouvrage a été publié en anglais l'année suivante sous le titre Fashionable Nonsense: Postmodern Intellectuals' Abuse of Science. Une nouvelle édition française, revue et augmentée, est sortie en 1999.
Gérard Lucotte, né le 27 août 1941, est un généticien français.
La « mémoire de l’eau » est le nom donné, en 1988, au cours d'une controverse médiatique, à une hypothèse du chercheur, médecin immunologue, Jacques Benveniste selon laquelle l’eau qui a été en contact avec certaines substances conserve une empreinte de certaines propriétés de celles-ci alors même qu'elles ne s’y trouvent statistiquement plus. Une série d'expériences réalisées pour valider cette hypothèse est alors présentée par des tenants de l'homéopathie (qui pratique une dilution très importante des principes actifs) comme une validation scientifique de celle-ci. Cependant, aucune explication satisfaisante n'ayant été proposée, une reproduction de l'expérience est menée par des chercheurs anglais, avec au final un résultat négatif. Les connaissances scientifiques montrent que l'eau liquide ne retient pas de réseaux ordonnés de molécules pendant plus d'une petite fraction de nanoseconde. Les résultats des expériences originales sur la mémoire de l'eau découlent probablement d'un artéfact expérimental ou d'une fraude scientifique, et ces hypothèses sont aujourd'hui unanimement considérées comme pseudo-scientifiques. Ce phénomène continue cependant d'être étudié par certains scientifiques, menés par le professeur Luc Montagnier, lauréat du Prix Nobel de médecine en 2008,. Ami de Benveniste, il estime que ce dernier avait globalement raison, malgré des résultats qui « n'étaient pas reproductibles à 100 % ». Il s'est depuis fait remarquer pour prétendre avoir téléporté de l'ADN sur plusieurs centaines de kilomètres de distance, passant pour un charlatan et étant la risée de la communauté scientifique,.
Les rumeurs sur le programme Apollo sont une série de récits populaires prenant le contrepied des informations officielles, selon laquelle les vaisseaux du programme Apollo ne se seraient jamais posés sur la Lune et qu'il s'agirait d'une mise en scène réalisée sur Terre. Ces rumeurs, peu diffusées à l'époque du débarquement sur la Lune, ont pris de l'ampleur dans les années 1970 grâce à une couverture médiatique importante. Les arguments invoqués peuvent être divisés en trois catégories : ceux qui mettent en doute l'authenticité des documents ; ceux qui mettent en doute la véracité des missions ; ceux qui présentent les mobiles à l'origine d'une telle désinformation. En 2009, les photographies, des sites d'atterrissage, des objets laissés par les différentes missions Apollo sur le sol lunaire (étages d'atterrissages des LEM, traces d'activité, équipements ALSEP...), et des traces sur le sol lunaire ont donné une nouvelle actualité au débat.
Les science wars (littéralement guerres de la science) réfèrent à une série d'échanges entre des tenants du réalisme scientifique et du postmodernisme concernant la nature de la méthode scientifique. Ces échanges se sont grandement tenus lors des années 1990 dans des publications académiques et populaires américaines. Lors de ces échanges, les réalistes ont accusé les postmodernistes d'avoir rejeté la méthode scientifique en général et l'objectivité (en) en particulier. Des réalistes comme Norman Levitt, Paul R. Gross, Jean Bricmont et Alan Sokal ont soutenu que le savoir scientifique est réel et que les postmodernistes pensent qu'il n'est pas réel. Ils ont également affirmé que de grands pans de l'éducation rejettent l'objectivité et le réalisme sous l'influence de poststructuralistes tels Jacques Derrida, Gilles Deleuze et Jean-François Lyotard. Les réalistes ont critiqué les approches de disciplines telles les Cultural Studies, l'anthropologie culturelle, les études féministes, la littérature comparée, la sociologie des médias et les études des sciences et technologies. Ils ont également affirmé que les critiques postmodernistes ne savent pas de quoi ils parlent.
L'acrylamide ou 2-propénamide (amide acrylique) est un composé organique de formule brute C3H5NO. L'acrylamide est une substance CMR (cancérogène, mutagène et reprotoxique). Il est classé parmi les composés du groupe 2A (agents probablement cancérogènes) selon la classification du CIRC. Il est considéré par l'OMS comme présentant un risque pour la santé humaine. Il se forme spontanément lors de la cuisson (friture, rôtissage, etc.) à haute température (supérieure à 120 °C) d'aliments riches en glucides (amidon, sucres) et en protéines.
Le bitartrate de potassium est un corps composé organique de formule chimique développée COOH-CHOH-CHOH-COOK ou compacte C4O6H5K, sous-produit de la vinification. Il est aussi connu sous les noms tartrate de monopotassium, tartrate acide de potassium, hydrogénotartrate de potassium, « crème de tartre », « tartre blanc », « cristal (ou cristaux) de tartre ». Le tartrate monopotassique ou additif E336(i) autorisé en France est utilisé en tant que stabilisant et régulateur de l’acidité.
Le brunissement enzymatique est un processus naturel rendant brun certains organismes, en particulier la nourriture. Ce brunissement peut être souhaitable, comme pour une amélioration du goût du thé, ou indésirable, comme quand une pomme brunit après avoir été coupée ou avoir subi une simple meurtrissure. Les aliments, y compris les boissons, peuvent subir un brunissement enzymatique ou non-enzymatique (réaction de Maillard).
L'énergie métabolisable (fraction de l'énergie brute ou énergie totale) d'un aliment doit être exprimée (comme toute autre forme d'énergie) en kilojoule(s) (symbole : kJ) et, en principe, jamais plus en kilocalorie(s) (symbole : kcal) dans le Système international d'unités). Le Facteur de conversion est : 1 kcal = 4,1868 kJ. L'énergie métabolisable est proportionnelle à la quantité d'aliments ingérés, et lorsque l'on donne un apport pour un gramme d'aliment, on note kJ.g-1 et on lit kilojoule par gramme. Apports énergétiques pour 1 gramme de   Glucides : » 17 kJ.g-1 (exactement 16,72 kJ.g-1, jadis 4 kcal.g-1), Protéines : » 17 kJ.g-1 (exactement 16,72 kJ.g-1, jadis 4 kcal.g-1), Lipides : » 38 kJ.g-1 (exactement 37,62 kJ.g-1, jadis 9 kcal.g-1), Alcool Éthylique : » 29 kJ.g-1 (exactement 29,26 kJ.g-1, jadis 7 kcal.g-1).
Les valeurs sont données pour 100 g de denrées comestibles de l'aliment. Base de calcul de la valeur calorique pour 1 gramme (1 kcal = 4,18 kJ) : Glucides : 4 kcal ou 17 kJ Protéines : 4 kcal ou 17 kJ Lipides : 9 kcal ou 38 kJ Alcool éthylique : 7 kcal ou 29 kJ (non métabolisables)[réf. nécessaire]
Les valeurs sont données pour 100 grammes de denrées comestibles de l'aliment. Base de calcul de la valeur calorique pour 1 gramme (1 kcal = 4,18 kj) : Glucides : 4 kcal ou 17 kJ Protéines : 4 kcal ou 17 kJ Lipides : 9 kcal ou 38 kJ
La composition nutritionnelle des fruits récapitule la liste des nutriments que l’on trouve en moyenne dans 100 grammes de fruit. Le terme de fruit est ici utilisé dans son acception vernaculaire. Les valeurs sont données pour 100 grammes de denrées comestibles de l'aliment Base de calcul de la valeur calorique pour 1 gramme (1 kcal = 4,18 kJ) : Glucides : 4 kcal ou 17 kJ Protéines : 4 kcal ou 17 kJ Lipides : 9 kcal ou 38 kJ alcool éthylique : 7 kcal ou 29 kJ (non métabolisables !) (7kcal par g d'alcool mais 5,6 kcal par ml d'alcool !)
La composition nutritionnelle des légumes récapitule la liste des nutriments que l’on trouve en moyenne dans 100 grammes de légumes frais ainsi que dans leurs dérivés secs. Le terme de légume est ici utilisé dans son acception vernaculaire.
Les valeurs sont données pour 100 grammes de denrées comestibles de l'aliment. Base de calcul de la valeur calorique pour 1 gramme (1 kcal = 4,18 kJ) : Glucides : 4 kcal ou 17 kJ Protéines : 4 kcal ou 17 kJ Lipides, 9 kcal ou 38 kJ
Le lait UHT est un lait de longue conservation, stérilisé par Upérisation à Haute Température.
La réaction de Maillard est une réaction chimique que l'on peut observer lors de la cuisson d'un aliment ; elle correspond à l'action des sucres sur les protéines, et contribue notamment au goût des viandes rôties. C'est durant cette réaction que les acrylamides — considérés comme probablement cancérigènes — sont formés.
La rétro-olfaction est le mécanisme physiologique permettant de percevoir à partir du système olfactif les caractéristiques aromatiques, dites flaveurs, des aliments qui sont contenus dans la bouche. On parle aussi de « voie rétronasale » ou d' « olfaction rétronasale », en effet les arômes suivent une trajectoire passant en arrière du palais pour atteindre l'épithélium olfactif dans les fosses nasales. Cette voie rétronasale ou indirecte (notion d'arôme) est utilisée lorsque l'aliment est en bouche, s'opposant à la voie orthonasale ou directe (notion d'odeur) en inspirant ou « humant » directement l’odeur de l’aliment non ingéré. La rétro-olfaction et l’ortho-olfaction sont les deux mécanismes de la perception olfactive.
La saccharification est un processus biochimique qui consiste à transformer les sucres complexes, comme la cellulose ou l'amidon, en sucres plus simples, tels le fructose et le glucose. Il s'agit généralement d'un processus enzymatique. Les enzymes peuvent être présentes dans le produit de base, comme dans le cas du malt, produites par des levures ou d'autres champignons microscopiques, ou apportées directement pour déclencher la réaction. En brasserie, c'est une autolyse, consistant en la transformation des sucres du malt dans de l'eau chauffée afin d'activer les enzymes, principalement l'amylase.
Claude Allègre, né le 31 mars 1937 à Paris, est un géochimiste et un homme politique français. Ses travaux scientifiques et sa carrière de chercheur ont notamment été récompensés par le prix Crafoord en 1986 et la médaille d'or du CNRS en 1994. Il est membre de l'Académie des sciences. Il a été ministre de l'Éducation nationale, de la Recherche et de la Technologie dans le gouvernement Lionel Jospin de 1997 à 2000. En parallèle de sa carrière scientifique, Claude Allègre a publié de nombreux ouvrages de vulgarisation scientifique et pris des positions publiques sur les thèmes de l'université française et de la recherche. Connu pour son franc-parler, il a suscité de fortes controverses, en particulier par ses prises de position sur l'origine et l'évolution du réchauffement climatique et précédemment par son opposition au désamiantage du campus de Jussieu.
Luc Amyotte est un professeur et écrivain scientifique canadien (québécois). Il a rédigé plusieurs ouvrages de mathématiques surtout destinés au niveau collégial québécois.
Peter William Atkins, né le 10 août 1940 à Amersham dans le Buckinghamshire, est un chimiste britannique, professeur de chimie à l'université d'Oxford. C'est un écrivain prolifique de livres de chimie, en particulier dans les domaines de la chimie physique, de la chimie inorganique et de la mécanique moléculaire, domaines dans lesquels ses ouvrages font référence dans le monde entier, dont le plus célèbre (Physical Chemistry) est à sa neuvième édition. Il est également l'auteur de livres de vulgarisation scientifique parmi lesquels on peut citer Le parfum de la fraise et Le doigt de Galilée. Atkins (qui se revendique comme athée) a également écrit sur des sujets comme l'humanisme, l'athéisme, et sur ce qui lui apparaît comme une incompatibilité entre science et religion.
Jeremy Bernstein (né le 31 décembre 1929 à Rochester) est un physicien théoricien, un essayiste scientifique et un historien de la physique américain.
Deborah Blum, née le 19 octobre 1954 à Urbana (Illinois), est une professeur, écrivain, journaliste et blogueuse américaine. Professeur de journalisme scientifique à l'Université du Wisconsin, elle travaille comme journaliste scientifique depuis le début des années 1990,.
David Joseph Bohm (né le 20 décembre 1917, mort le 27 octobre 1992) est un physicien américain qui a réalisé d'importantes contributions en physique quantique, physique théorique, philosophie et neuropsychologie. Il a participé au projet Manhattan et conduit des entretiens filmés avec le philosophe indien Krishnamurti.
Marcel Boll, né le 15 septembre 1886 à Paris et mort en 1971, est un ingénieur ESPCI, agrégé et docteur ès Sciences Physiques, professeur de chimie et d'électricité à l'École des hautes études commerciales de Paris (HEC), journaliste scientifique et membre fondateur de l'Union Rationaliste.
Sir Charles Vernon Boys (15 mars 1855 – 30 mars 1944) est un physicien britannique, connu pour ses travaux expérimentaux.
Robert Brown (né le 23 mars 1842 à Camster, dans l'ancien comté écossais de Caithness – mort à Camden dans la nuit du 26 octobre 1895) est un botaniste et explorateur écossais.
Boris (Ber) Davidovich Brutskus (en russe: Борис (Бер) Давыдович Бруцкус; en hébreu: בוריס (בר/דּוֹב) בֶּן־דָּוִד ברוצקוס), né le 3 octobre 1874 à Polangen dans le gouvernement de Courlande en Russie (maintenant en Lituanie) et décédé le 7 décembre 1938 à Jérusalem, est un économiste russe, statisticien, agronome et activiste social, membre du Comité central de l'Association juive de colonisation, journaliste et frère du ministre du gouvernement lituanien YD Brutskus. Il est l'auteur de nombreuses publications entre autres sur l'économie et l'agronomie en Russie et sur la vie juive.
Bernard Cabane, né le 12 octobre 1945, est un physicien et chimiste français, directeur de recherche au CNRS à l'ESPCI ParisTech, spécialiste de la mécanique des fluides et de matière molle et membre correspondant de l'Académie des Sciences.
Franz Ludwig von Cancrin, (1738-1816), minéralogiste, métallurgiste, ingénieur et architecte hessois, à qui l'on doit un Traité des mines et salines. Il est le père du ministre comte Georges von Cancrin.
Paul-Antoine Gratacap, dit Cap, né à Mâcon (Saône-et-Loire) le 2 avril 1788 et mort à Paris le 12 novembre 1877, est un pharmacien, naturaliste et écrivain scientifique français.
David C. Cassidy est un historien des sciences américain spécialisé en physique.
Valérie Chansigaud, née le 29 mars 1961 à Lyon, est une historienne des sciences et de l’environnement française.
Pierre-Louis-Napoléon Chernoviz (en polonais, Piotr Ludwik Napoleon Czerniewicz, et, en portugais, Pedro Luiz Napoleão Chernoviz), né le 11 septembre 1812 à Łuków (Pologne) et décédé en 30 août 1881 à Paris, est un docteur en médecine, écrivain scientifique et éditeur d'origine polonaise. Il fut le médecin de l'empereur Pierre II du Brésil.
Brian Clegg (né en 1955) est un écrivain scientifique britannique. On lui doit une biographie du philosophe Francis Bacon publiée en 2003.
L’abbé Charles Combaluzier (1903 † à Marseille, le 22 novembre 1991), docteur ès sciences, est un pédagogue et prêtre catholique français. Ancien aumônier du lycée Thiers à Marseille, disciple de Pierre Teilhard de Chardin, son essai philosophique Dieu demain a été couronné du prix Montyon en 1973.
Alexander Comfort (né le 10 février 1920 et mort le 26 mars 2000) est un écrivain et physicien britannique surtout connu pour avoir écrit The Joy of Sex, un manuel de sexe, en 1972. Il a également été auteur de romans, gérontologue, anarchiste, pacifiste et objecteur de conscience.
Paul François Jean Couderc (né le 15 juillet 1899 à Nevers , mort le 5 février 1981 dans le 14e arrondissement de Paris,) est un astronome français et un auteur prolifique réputé pour ses travaux de vulgarisation scientifique.
Hilaire Cuny, écrivain et vulgarisateur scientifique français, né en 1913, mort le 22 mars 2003. Auteur d'ouvrages sur Albert Einstein, Louis Pasteur, Werner Heisenberg, Camille Flammarion, lauréat de l'Institut. Son travail de vulgarisation s'étend à l'ensemble du savoir scientifique (de l'astronomie à l'évolution du vivant). Il a notamment collaboré avec les Lettres françaises. Il obtient en 1971 le prix Fabien de l'Académie française pour Alexis Carrel.
David Darling (1953-) est un astronome et écrivain anglais. Il a publié plusieurs œuvres de vulgarisation scientifique telles que Life Everywhere: The Maverick Science of Astrobiology (en) (2001) et le livre à succès The Universal Book of Mathematics (en) (2004). Il est également connu pour son encyclopédie en ligne Encyclopedia of Science.
Paul Henry De Kruif (né le 2 mars 1890 à Zeeland, Michigan, et mort le 28 février 1971 à Holland, Michigan) est un microbiologiste américain, auteurs de plusieurs ouvrages de vulgarisation. Il est surtout connu pour son best-seller, Les Chasseurs de Microbes (1926), qui a fait naître la vocation de nombreux chercheurs, comme le biochimiste britannique Aaron Klug ou le biologiste moléculaire Charles Weissmann.
Sir James Dyson, né le 2 mai 1947 à Cromer au nord du Norfolk, est un inventeur et designer industriel britannique, fondateur de la société Dyson et directeur de département « recherche et développement » de son entreprise. Dyson est connu grâce à ses aspirateurs à séparation cyclonique, sans sac et sans perte d'aspiration. Il a également inventé un système de lave-linge plus efficace, une brouette qui ne s'enfonce pas dans la boue, un lanceur de bateaux flottant, etc. Ses plus récentes innovations sont le sèche-main en dix secondes avec de l'air froid, le ventilateur sans hélice et un robinet sèche-mains. Il a été un partisan du « Brexit » lors des débats sur le retrait du Royaume-Uni de l'Union européenne en vue du référendum de 2016.
Giulia Enders est une étudiante en médecine allemande, née en 1990 à Mannheim, et l'auteur du livre à succès Darm mit Charme traduit en français par Le Charme discret de l'intestin.
Paul Sophus Epstein (20 mars 1883 à Varsovie, Empire russe - 8 février 1966 à Pasadena, Californie) est un physicien mathématicien américain d'origine russe. Il est surtout pour ses travaux théoriques en mécanique quantique. Son nom est associé aux physiciens Lorentz, Einstein, Minkowski, Thomson, Rutherford, Sommerfeld, Röntgen, von Laue, Bohr, de Broglie, Ehrenfest et Schwarzschild.
Siegfried Flügge (16 mars 1912 à Dresde, Allemagne - 15 décembre 1997 à Hinterzarten, Allemagne) est un physicien théoricien allemand spécialiste de physique nucléaire. Après l'obtention de son doctorat, il travaille au Kaiser-Wilhelm Institut für Chemie puis participe aux recherches atomiques sous le régime nazi. De 1956 à 1984, il est éditeur du Handbuch der Physik, une encyclopédie de plus de 50 volumes.
Pierre Fontanel (né au XIXe siècle - mort au XXe siècle) est un écrivain scientifique canadien (québécois). Il a rédigé des ouvrages en minéralogie et géologie appliquées, ainsi que sur la chimie industrielle. Il a aussi publié des articles de vulgarisation dans la revue L'école sociale populaire.
Patrick Forterre, né le 21 août 1949 à Paris, est un chercheur en biologie, professeur d'université et écrivain scientifique français. Il a été chef d’unité et Professeur à l’Institut Pasteur  , . Il est reconnu pour ses travaux sur les Archaea, les virus et l'évolution du vivant.
Ben Michael Goldacre, né en 1974, est un écrivain scientifique britannique, docteur en médecine et psychiatre. Il est l'auteur de la rubrique Bad Science (« mauvaise science ») dans le journal The Guardian  et du livre du même nom édité par Fourth Estate en septembre 2008. Goldacre est le fils des Australiens Michael Goldacre, professeur de santé publique à l'université d'Oxford, et de la chanteuse pop Susan Traynor, alias Noosha Fox, le neveu du journaliste scientifique Robyn Williams, et l'arrière-arrière-petit-fils de Henry Parkes.
Thomas P. Grazulis (né le 17 août 1942) est un météorologue américain qui a beaucoup écrit sur les tornades et est à la tête de la compagnie Tornado Project. Il est particulièrement connu pour ses livres sur la climatologique de ce phénomène aux États-Unis et pour ses films documentaires sur le sujet.
Amédée Guillemin, né le 5 juillet 1826 à Pierre-de-Bresse où il est mort le 2 janvier 1893, est un écrivain scientifique français.
Thomas Hager, né le 18 avril 1953 est un écrivain américain spécialisé en vulgarisation scientifique.
Gerald James Holton, né le 23 mai 1922 à Berlin, est un physicien, écrivain scientifique un professeur américain. Établi aux États-Unis pendant la Seconde Guerre mondiale, ses intérêts se portent surtout sur la philosophie et l'histoire des sciences.
James Hopwood Jeans (11 septembre 1877 à Ormskirk – 16 septembre 1946 à Dorking) est un physicien, astronome et mathématicien britannique.
Waldemar Kaempffert (né le 27 septembre 1877 à New York et mort le 27 novembre 1956) est un écrivain scientifique et un directeur de musée américain.
Masao Kawai (河合 雅雄, Kawai Masao), né le 2 janvier 1924, est un primatologue japonais qui met en œuvre le concept de kyokan (en) comme moyen d'étudier les primates dans son ouvrage Life of Japanese Monkeys (1969).
Jeffrey Kluger, né le 21 mai 1954, est un journaliste et écrivain américain. Il publie au Time Magazine et est l'auteur de neuf livres sur des sujets divers. Il est principalement connu pour son livre Lost Moon: The Perilous Voyage of Apollo 13 (en) (1994) qu'il a écrit avec l'astronaute James Lovell et qui a servi de base au scénario du film Apollo 13 réalisé par Ron Howard et sorti en 1995.
André Langaney (né en 1942) est un généticien et vulgarisateur scientifique français, spécialiste de l'évolution et de la génétique des populations.
Hervé Lehning (né le 10 décembre 1948) est un professeur et écrivain scientifique français. Il a notamment rédigé des ouvrages de mathématiques.
Jonah Lehrer (né en 1981 à Los Angeles) est un auteur et vulgarisateur américain. Il écrit sur des sujets tels la psychologie, les neurosciences ainsi que la relation entre les sciences et les lettres. Il a travaillé plusieurs années comme assistant de recherche à l'Université Columbia.
François Le Lionnais (3 octobre 1901 à Paris - 13 mars 1984 à Boulogne-Billancourt, France) est un ingénieur chimiste, mathématicien épris de littérature, doublé d’un écrivain passionné de sciences.
Primo Levi, né le 31 juillet 1919 à Turin et mort le 11 avril 1987 à Turin, est un docteur en chimie italien rendu célèbre par son livre Si c'est un homme, dans lequel il relate son emprisonnement au cours de l'année 1944 dans le camp de concentration et d'extermination d'Auschwitz-Monowitz. Juif italien de naissance, chimiste de profession et de vocation, il entre tardivement dans une carrière d'écrivain orientée par l'analyse scientifique de cette expérience de survivant de la Shoah, dans le but de montrer, retranscrire, transmettre, expliciter. Il est l'auteur d'histoires courtes, de poèmes et de romans.
Roger Lewin (né en 1944) est un anthropologue et écrivain scientifique britannique.
Harold « Hal » Lewis (1er octobre 1923 à New York - 26 mai 2011) est un physicien américain, un professeur émérite de physique et un ancien directeur du département de physique de l'université de Californie à Santa Barbara. En 2010, après 67 ans d'adhésion, il démissionne de l’American Physical Society, critiquant sa politique sur le réchauffement climatique.
Warren Kendall Lewis (21 août 1882 – 9 mars 1975) est un professeur du Massachusetts Institute of Technology (MIT) qui a été surnommé le père du génie chimique moderne. Il est coauteur d'un manuel important sur le sujet publié en 1923 qui introduit le concept d'opération unitaire.
Gokulananda Mahapatra, né le 24 mai 1922 à Bhadrak dans l’État d'Odisha et mort le 10 juillet 2013 (à 91 ans) à Ahmedabad, est un scientifique et un écrivain de science-fiction indien.
Bernard Maitte est un écrivain et universitaire français, né en 1942 à Saint-Quentin, historien des sciences et responsable culturel scientifique. Cristallographe de formation, il est professeur émérite de l'Université de Lille spécialiste en histoire et épistémologie de la physique. S'attachant à faire connaître la pensée scientifique, à mettre en débats citoyens ses applications et implications, il compte parmi les précurseurs ayant permis d'initier la politique de développement des centres de culture scientifique, technique et industrielle en France,,. Conjointement à son travail de chercheur scientifique, Bernard Maitte manifeste un engagement soutenu en faveur de la popularisation de la science, notamment au travers d'entretiens radiophoniques, de contributions périodiques ou ponctuelles dans diverses revues, en tant que conférencier ou comme conseiller scientifique.
Simon Mitton, né le 18 décembre 1946, est un astronome et écrivain britannique. Il est basé au St Edmund's College, à Cambridge. Il a publié de nombreux œuvres reconnues telles The Cambridge Encyclopedia of Astronomy (1978), Exploring the Galaxies (1974) et Cambridge Scientific Minds (2000). Son ouvrage le plus fameux est sans doute sa biographie de son collègue astronome de Cambridge Fred Hoyle.
Alexandre Moatti, né le 6 décembre 1959 à Boulogne-Billancourt, est un entrepreneur, haut fonctionnaire et historien des sciences français.
Théophile Moreux dit l'abbé Moreux, né le 20 novembre 1867 à Argent-sur-Sauldre (Cher) et mort le 13 juillet 1954 à Bourges, est un astronome et un météorologue français, célèbre par de nombreuses publications de vulgarisation destinées à faire connaître l'état des sciences du début du XXe siècle au plus large public possible.
Kazuhiro Nakaya (仲谷 一宏, Nakaya Kazuhiro), né le 27 juillet 1945, est un ichthyologiste japonais. Il est diplômé de l'université de Hokkaidō avec un BA en 1968 et un PhD en 1972. Professeur d'environnement et des ressources marines au laboratoire marin pour la biodiversité, il est spécialiste de la taxonomie et de l'évolution des requins, des rajiformes, des chimaeras et des poissons du lac Tanganyika. Il est l'auteur de nombreux articles et livres sur les requins et les poissons. En 1995, il a été chargé de disséquer et préparer le 7e échantillon du très rare requin grande-gueule.
Christopher John Nowinski, né le 24 septembre 1978 à Arlington Heights en Illinois, est un catcheur américain. Il est également l'auteur d'un ouvrage intitulé Head Games: Football's Concussion Crisis qui étudie les effets à long terme des blessures crâniennes sur les athlètes.
Jay Myron Pasachoff (né en 1943) est un astronome américain. Professeur au Williams College, il a rédigé plusieurs manuels et livres sur l'astronomie, la physique et la mathématique.
David Beaumont Peakall (17 mars 1931 - 18 août 2001) était un toxicologue britannique internationalement reconnu. Sa recherche sur les effets de dichlorodiphenyldichloroethylène et du DDT sur les coquilles d'œuf a contribué à l'interdiction du DDT aux États-Unis. Il a prouvé que ces produits chimiques ont causé une fragilité des coquilles, entraînant à une réduction des populations de diverses espèces d'oiseau. Il a aussi été un pionnier de la recherche sur les effets de PCB sur les oiseaux. Il publia en 1986 une suite au Printemps silencieux de Rachel Carson, intitulé Beyond Silent Spring, coécrit avec Helmut F. van Emden.
Bernard Peters (né Bernhard Pietrowski en 1910 à Posen, Allemagne - 2 février 1993 à Copenhague) est un physicien américain qui s'est distingué dans l'étude des rayons cosmiques.
Melba Newell Phillips (1er février 1907 à Hazleton (Indiana) (en), États-Unis - 8 novembre 2004 à Petersburg (Indiana)) est une physicienne, enseignante et écrivaine scientifique américaine. En 1935, avec Robert Oppenheimer, elle décrit le processus Oppenheimer-Phillips. En 1952, pendant le maccarthysme, elle refuse de témoigner devant une sous-commission du Sénat des États-Unis, ce qui lui vaut d'être renvoyée par le Brooklyn College.
Ronald Hans Anton Plasterk (prononcé en néerlandais : [ˈroːnɑlt ˈɦɑns ˈɑntɔn ˈplɑstɛrk]), né le 12 avril 1957 à La Haye, est un scientifique et un homme politique néerlandais membre du Parti travailliste. Il est ministre de l’Intérieur entre 2012 et 2017 dans le cabinet Rutte II, après avoir été ministre de l'Éducation, de la Culture et de la Science du 22 février 2007 au 23 février 2010 dans le cabinet Balkenende IV. Représentant à la Seconde Chambre des États généraux du 17 juin 2010 au 5 novembre 2012, Plasterk est un professeur reconnu, cité et primé dans le domaine de la génétique moléculaire. Depuis 1995, il est également auteur et chroniqueur pour plusieurs publications nationales.
Edward Regis souvent aussi dénommé Ed Regis (né Edward Regis, Jr en 1944) est un philosophe, professeur et écrivain scientifique américain. Il écrit sur la science, la philosophie et l'intelligence.
Robert Resnick (11 janvier 1923 à Baltimore, Maryland, États-Unis - 29 janvier 2014 à Pittsburgh, Pennsylvanie) est un physicien et professeur de physique américain. Il a rédigé plusieurs manuels de physique de niveau universitaire largement utilisés de par le monde.
Michel Rival (né en 1953) est un historien français,.
Jean Rostand, né le 30 octobre 1894 à Paris (17e arrondissement) et mort le 4 septembre 1977 à Ville-d'Avray (Hauts-de-Seine), est un écrivain, moraliste, biologiste, historien des sciences et académicien français.
Frank P. Ryan, né le 23 juillet 1944, à Limerick (Irlande), est un médecin et biologiste de l'évolution britannique. Il est membre honoraire du Department of Animal and Plant Sciences à l’Université de Sheffield (à Sheffield, au Royaume-Uni). Écrivain scientifique, Frank Ryan est également auteur de livres de fiction.
Antoine Joseph Fort Camille de Saporta, né à Aix-en-Provence le 26 juillet 1855 et mort à Montpellier le 14 avril 1914, est un écrivain scientifique français. Fils du paléobotaniste Gaston de Saporta, il a contribué de nombreux articles à La Nature, à la Revue scientifique et à la Revue des deux Mondes.
Martin J. Sherwin est un historien américain. Il rédige surtout sur l'histoire du développement de l'énergie nucléaire et la prolifération nucléaire.
Rebecca L. Skloot est une journaliste et femme de lettres américaine qui se concentre sur la science et la médecine. Son premier livre, The Immortal Life of Henrietta Lacks (2010), a été l'un des meilleurs vendeurs aux États-Unis en 2010, demeurant sur la New York Times Best Seller list pendant plus de deux ans et se plaçant en juin 2012 en tête de cette liste.
Vaclav Smil, né le 9 décembre 1943, est un chercheur et analyste politique canadien d'origine tchèque. En date de 2006, il avait rédigé 23 livres sur différents sujets, dont l'énergie et l'environnement en Chine. Il affirme s'intéresser aux sujets interdisciplinaires.
Victor John Stenger, né le 29 janvier 1935 à Bayonne (New Jersey), et mort le 27 août 2014 à Hawaï (Honolulu), est un physicien, écrivain et vulgarisateur américain. Il est un grand défenseur de la pensée scientifique et du scepticisme scientifique. Depuis, juin 2010, il est l'auteur de 9 livres grand public sur la physique, la mécanique quantique, la cosmologie, la philosophie, la religion, l'athéisme et la pseudo-science, son dernier titre étant The New Atheism: Taking a Stand for Science and Reason publié en septembre 2009. Stenger a annoncé qu'il travaillait à la rédaction d'un dixième livre intitulé pour l'instant The Fallacy of Fine-Tuning: How the Universe is Not Designed for Humanity.
Jean-Louis Tassoul est un astrophysicien belge.
Edwin F. Taylor est un physicien et professeur américain. Il est surtout connu pour ses manuels de physique.
Colin Tudge (né le 22 avril 1943) est un écrivain et journaliste scientifique britannique. Biologiste de formation, il est l'auteur de nombreux ouvrages de vulgarisation sur l'alimentation, l'agriculture, la génétique et la diversité des espèces.
André Warusfel, né le 1er décembre 1936 à Douai et décédé le 6 juin 2016, ancien élève de l'École normale supérieure (promotion 1956), fut successivement professeur, inspecteur général et historien des mathématiques.
Quentin Duane Wheeler, né le 31 janvier 1954, est un entomologiste, taxinomiste, auteur et chroniqueur américain, et il est le directeur fondateur de l'Institut international d'exploration des espèces. Depuis 2014, il est le quatrième président de l'université d'État de New York Collège de la science de l'environnement et de la foresterie, à Syracuse dans l'État de New York. Auparavant, il était professeur d'entomologie à l'université Cornell et l'université d'État de l'Arizona, il a été conservateur et chef de l'entomologie au musée d'histoire naturelle de Londres, puis directeur de la Division de biologie environnementale de la National Science Foundation.
Robert Whitaker est un journaliste et écrivain américain. Il rédige surtout sur la médecine, la science et l'histoire. Par ses écrits, il s'inscrit dans le mouvement de l'antipsychiatrie.
Robert Rathbun Wilson (4 mars 1914 - 16 janvier 2000) est un physicien américain qui a été chef de projet lors du Projet Manhattan et l'un des responsables de la création du Fermilab, duquel il est aussi directeur de 1967 à 1978.
Mark Wolverton est un écrivain scientifique américain.
Leona Woods (9 août 1919 – 10 novembre 1986), appelée plus tard Leona Woods Marshall puis Leona Woods Marshall Libby, est une physicienne américaine qui a collaboré à la construction du premier réacteur nucléaire dans le cadre du Projet Manhattan,. Elle a été l'épouse du physico-chimiste américain Willard Frank Libby.
Le biais de financement désigne en science le fait que les résultats des recherches ont tendance à être plus favorables au financeur direct ou indirect. Ce biais a été mis en évidence dans de nombreux domaines, où des industries ont un intérêt fort (pour des raisons de régulation) à ce que les résultats aillent plutôt dans son sens : recherche sur les effets du tabac, de la nourriture, des pesticides, des perturbateurs endocriniens, des OGM, recherches dans le secteur biomédical. Il ne s'agit pas d'un biais unique à proprement parler mais d'un ensemble de biais dont les contours ne sont pas bien identifiés.
L'expérience relève du vécu, c'est-à-dire de la connaissance acquise à travers l'expérience sensible (sensation et sens), par opposition à ce qui relève d'une connaissance pure et a priori. Elle se confond alors avec la signification courante lorsqu'on dit de quelqu'un qu'il est expérimenté: il a appris un savoir par la pratique, et non de façon théorique.
Les méthodes expérimentales scientifiques consistent à tester la validité d'une hypothèse, en reproduisant un phénomène (souvent en laboratoire) et en faisant varier un paramètre. Le paramètre que l'on fait varier est impliqué dans l'hypothèse. Le résultat de l'expérience valide ou non l'hypothèse. La démarche expérimentale est appliquée dans les recherches en biologie, physique, chimie, psychologie, ou encore l'archéologie. Certains soutiennent que le savant Ibn Al Haytham (Alhazen),,, a été l'un des premiers à faire la promotion des méthodes expérimentales. Définies par le chimiste Michel-Eugène Chevreul en 1856, elles ont été développées par Claude Bernard en médecine et en biologie. Outil privilégié des sciences de la nature, les méthodes expérimentales sont également utilisées en sciences humaines et sociales.
Un bioréacteur, appelé également fermenteur ou propagateur, est un appareil dans lequel on multiplie des micro-organismes (levures, bactéries, champignons microscopiques, algues, cellules animales et végétales) pour la production de biomasse (écologie), ou pour la production d'un métabolite ou encore la bioconversion d'une molécule d'intérêt. Dans les années 1800, Pasteur, Kutzing, Schwann, et Cagniard-Latour ont démontré que la fermentation était causée par des levures, qui sont des organismes vivants (Hochfeld, 2006). Le terme « fermentation » prend en compte aussi bien le métabolisme aérobique qu’anaérobique. Elle consiste à multiplier la biomasse de microorganismes vivants, et éventuellement à utiliser son métabolisme. Contrairement aux systèmes plus simples utilisés pour faire pousser des micro-organismes, comme les fioles, le bioréacteur permet de contrôler les conditions de culture (température, pH, aération, etc.), et de ce fait, il permet de récolter des informations de plus grande fiabilité. Les modèles de laboratoire vont de 0,1 à 15 litres. Les modèles employés pour les tests en vue de l'industrialisation (appelés "pilotes") vont de 20 à 1 000 litres, alors que ceux destinés à la production industrielle peuvent dépasser les 1 000 m3 (cas de la production d'éthanol). Des modèles de bioréacteurs jetables existent sur le marché depuis 1995, utilisés principalement pour des volumes allant du millilitre à quelques centaines de litres. En ingénierie tissulaire, le terme de bioréacteur peut designer un système permettant la culture de tissu. Le but n'est pas ici de produire des métabolites mais bien un tissu complet composé de cellules et de la matrice extracellulaire.
Un dépistage génétique ou dépistage de mutagénèse est une technique expérimentale utilisée pour identifier et sélectionner des individus possédant un phénotype d'intérêt au sein d'une population à laquelle on a appliqué une mutagénèse. Un dépistage génétique est donc un type de dépistage phénotypique. Les dépistages génétiques peuvent fournir d'importantes informations sur la fonction d'un gène ainsi que sur les événements moléculaires à la base d'un processus biologique ou d'une voie métabolique. Alors que les projets de séquençage de génomes ont permis d'identifier un vaste inventaire de gènes dans beaucoup d'organismes différents, les dépistages génétiques peuvent fournir des informations précieuses sur la façon dont ces gènes fonctionnent,,,,.
La méthode des doubles différences (ou méthode des différences de différences) est une méthode statistique utilisée pour estimer l'effet d'un traitement et consistant à comparer la différence entre le groupe de contrôle et le groupe traité avant et après l'introduction du traitement. Cette méthode est notamment utilisée en évaluation des politiques publiques pour estimer l'effet d'un traitement dans le cadre théorique du modèle causal de Neyman-Rubin.
En théorie des probabilités, une expérience aléatoire est une expérience renouvelable (en théorie si ce n'est en pratique), dont le résultat ne peut être prévu, et qui, renouvelée dans des conditions identiques –pour autant que l'observateur puisse s'en assurer– ne donne pas forcément le même résultat à chaque renouvellement. Une succession de lancers d'une même pièce en est un exemple classique. Le tirage au hasard d'un élément dans un ensemble en est un autre exemple. Chaque renouvellement de l'expérience est appelé une épreuve. Une épreuve peut combiner plusieurs épreuves élémentaires, consécutives ou simultanées. Par exemple, le jet de deux dés peut être considéré comme la combinaisons de deux épreuves élémentaires. D'autre part, les épreuves peuvent être plus ou moins indépendantes. Le tirage des épreuves est fait dans un ensemble soit global soit dans un échantillon. Il convient de rechercher l'homogénéité de l'échantillon afin d'y appliquer les règles statistiques de base. L'ensemble des résultats possibles, ou issues, d'une expérience aléatoire constitue l'univers de cette expérience. La notion d'issue débouche sur celle d'événements, plus large.  Portail des probabilités et de la statistique
L'expérience de Miller (dite encore de Miller-Urey), destinée à mettre en évidence une éventuelle origine chimique de l'apparition de la vie sur Terre, consista à simuler les conditions supposées régner originellement après la formation de la croûte terrestre. Elle avait pour objectif de mettre à l'épreuve l'hypothèse d'Oparin et de Haldane, selon laquelle les conditions existant alors sur Terre auraient favorisé les réactions chimiques susceptibles de faire apparaître des composés organiques à partir de composés inorganiques. Considérée comme classique dans le domaine de l'origine de la vie, cette expérience fut menée en 1953 par Stanley Miller et Harold Clayton Urey à l'Université de Chicago,,. Elle donna naissance au concept de « soupe primitive (ou primordiale) de la vie », qui a ensuite gagné en popularité. En 1953, Stanley Miller, accompagné de Harold Urey, a voulu reproduire les conditions de la Terre primitive. Ils ont enfermé dans un ballon des gaz (méthane CH4, ammoniac NH3, hydrogène H2 et eau H2O) et soumis le mélange à des décharges électriques pendant sept jours. Ils ont obtenu des molécules organiques, les briques du vivant, et notamment de l'urée (CON2H4), du formaldéhyde (H2CO), de l'acide cyanhydrique (HCN), des bases et des acides aminés (AA), certains composés étant présents à plus de 2 %.
Une expérience de terrain est une expérience scientifique qui a lieu dans le monde réel plutôt que dans un laboratoire. Ce protocole est souvent utilisé en économétrie appliquée pour évaluer l'effet causal d'une politique publique.
On appelle expérience naturelle est une expérience dans laquelle l'assignation aléatoire au traitement est provoquée par des causes naturelles et/ou politiques. On oppose ainsi les expériences naturelles aux expériences contrôlées dans laquelle l'assignation au traitement est aléatoirement déterminée pour les besoins de l'étude. Par exemple, David Card a utilisé l'exode de Mariel comme une expérience naturelle pour mesurer l'effet de l'immigration sur le marché du travail en Floride.
L’expérimentation animale consiste à utiliser des animaux comme substitut ou « modèle », pour mieux comprendre la physiologie d'un organisme et ses réponses à divers facteurs (alimentation, environnement, agents pathogènes) ou substances (pour en tester, vérifier ou évaluer l'efficacité, l'innocuité ou la toxicité), et tout particulièrement pour tenter de prévoir ce qui se passe chez l'Homme.  Pour des raisons de taille, d'accumulation de connaissances, de standardisation, de prix et de temps, la très grande majorité des expérimentations animales se font sur des rongeurs. La souris commune étant de mieux en mieux connue au point de vue génétique, son usage augmente plus que celui des autres espèces, mais il existe d'autres animaux vertébrés ou invertébrés utilisés comme organismes modèles. L'expérimentation animale est une pratique controversée, certaines personnes pensant qu'on fait ainsi souffrir des animaux, sans apporter aucun bénéfice ni pour eux, ni pour les humains. selon un sondage IPSOS de 2003, 64 % des Français sont plutôt ou tout à fait défavorables à l'expérimentation animale, le chiffre montant à 85 % pour une interdiction de celle-ci si des méthodes substitutives existent. Ce dernier point est d'ailleurs déjà mis en vigueur par la réglementation européenne et française qui interdit l'utilisation d'animaux en recherche si d'autres méthodologies existent. Le chiffre descend à 60 % d'opinions favorables à une interdiction de l'expérimentation animale relative aux cosmétiques. D'autres sondages donnent des indications sur l'opinion du public sur le sujet de l'expérimentation animale en ce qui concerne le domaine médical. Un sondage IPSOS réalisé en 2007 pour le Gircor, indique que 69 à 77 % des Français sont favorables à l'usage de l'expérimentation animale pour lutter contre les maladies graves. 51 % désapprouvent toute expérimentation sur les chiens et les singes, même si cela peut aider à résoudre des problèmes de santé pour les humains, selon l'Eurobaromètre 2010, alors que seulement 18 % désapprouvent l'expérimentation sur souris si cela doit régler des problèmes de santé. Cependant, les institutions scientifiques et autorités affirment la nécessité d'avoir recours à cette méthodologie pour garantir le progrès scientifique et médical. Selon un rapport de la Commission européenne, 11,5 millions d'animaux ont été utilisés en 2011 par les 27 États membres, dont 80 % de lapins et de rongeurs. La directive européenne, sur la protection des animaux utilisés à des fins scientifiques a été révisée et mise à jour en 2010 sous le code Directive 2010/63/UE. Elle affirme le principe des 3R qui demande que l'utilisation d'animaux soit remplacée, réduite et améliorée autant que possible. Elle impose un examen critique des projets d'étude sous l'angle bénéfice pour la recherche et contrainte pour les animaux et la publication de résumés non-techniques. Elle impose des conditions d'hébergement minimales. Le développement des méthodes alternatives est encouragé de différentes façons. Tous les États membres ont transposé cette directive dans leur réglementation.
Le Générateur poïétique est une œuvre d'art télématique, précurseur de nombreux jeux et réseaux sociaux sur Internet, imaginée par Olivier Auber en 1986 et développée en tant qu'œuvre d'art libre depuis 1987. Le jeu défini par le Générateur poïétique se déroule à l’intérieur d’une matrice à deux dimensions comme les jeux de tabliers et son principe s'inspire de celui du jeu de la vie et des cadavres exquis des surréalistes. Le Générateur poïétique s’écarte néanmoins de ces modèles sur plusieurs points. Ce n'est pas un algorithme de type Conway, mais bien des joueurs humains qui contrôlent en temps réel les éléments graphiques de la matrice globale, à raison d'une unité par personne. Contrairement au cadavres exquis dans lesquels il y a toujours des parties cachées, ici toutes les actions des joueurs sont visibles en permanence par chacun d’eux. Enfin, à la différence des jeux de tabliers, il n’y a pas de notion de gagnant ou de perdant, le but du jeu étant simplement de faire apparaître collectivement des formes reconnaissables par tous et d’observer ensemble comment elles se créent. L'appellation « Générateur poïétique », qui dérive du concept d'autopoïèse en sciences du vivant, et de celui de poïétique en philosophie de l'art, traduit le processus d’auto-organisation à l’œuvre dans l'émergence continue de l'image globale. Depuis son origine, le Générateur poïétique a été conçu par son auteur comme un élément d'une recherche-action plus vaste en vue de créer un « art de la vitesse ». Depuis 1986, l'appellation « générateur poïétique » a été reprise par d'autres auteurs dans d'autres contextes, notamment celui de la musique électronique.
Ceci est une liste des expériences scientifiques, que ce soit dans le domaine des sciences expérimentales (physique, chimie) ou des sciences humaines (psychologie clinique, sociologie...) Elles sont classées par ordre alphabétique.
Le modèle murin est un modèle d'expérimentation animale utilisant la souris ou le rat ou le cobaye,....les rongeurs en général. La souris est le vertébré le plus utilisé en raison de sa disponibilité, de sa petite taille, de son faible coût, de sa manipulation aisée et de son taux élevé de reproduction. Elle représente le modèle de base pour étudier les maladies génétiques humaines et partage 99 % de ses gènes avec l'Homme. Avec les nouvelles techniques de génie génétique, des souris génétiquement modifiées peuvent être créées à la demande.[réf. nécessaire] En Angleterre en 2004, 1 910 110 souris, 464 727 rats et 37 475 autres rongeurs ont été utilisés, ce qui représente 84,5 % du total des animaux utilisés dans l'année. En 2005, les chiffres sont similaires avec 1 955 035 souris, 414 335 rats et 40 856 autres rongeurs.
Moving to Opportunity (ou Moving to Opportunity for Fair Housing ou encore MTO) est une expérience de terrain menée aux États-Unis dans les années 1990 auprès de 4 600 familles à bas revenus ayant des enfants vivant dans une situation de grande pauvreté. L'expérience a été financée par le Département du Logement et du Développement urbain des États-Unis et à Baltimore, Boston, Chicago, Los Angeles et New York. Les familles qui ont volontairement participé à l'expérience ont été réparties aléatoirement en trois groupes. Le premier groupe a reçu un bon (en anglais voucher (en)) pour pouvoir emménager dans un quartier à faible taux de pauvreté et des conseils pour prendre la meilleure décision ; le second groupe a reçu un bon pour déménager dans le quartier de son choix ; le troisième groupe (groupe de contrôle) n'a pas reçu d'aide particulière. Cette expérience a donné lieu à de nombreuses recherches sur le lien entre le voisinage et la réussite scolaire ou économique. Ainsi, Raj Chetty et ses co-auteurs montre des effets positifs sur la fréquentation de l'université, les revenus et la probabilité de devenir parent célibataire pour les enfants ayant bénéficié du programme avant l'âge de 13 ans. En revanche, ils ne trouvent pas les mêmes effets positifs pour les enfants ayant bénéficié du programme à partir de 13 ans
Le programme Phyt’air est un programme français de recherche portant sur la faisabilité de l'épuration de l’air à l'intérieur des bâtiments par des plantes, et sur leur capacité de bioindication de la qualité de l'air intérieur.
On appelle « rat de laboratoire » des souches ou lignées de rats sélectionnées et élevés et reproduits pour les besoins de l'expérimentation animale en laboratoires, ou parfois pour les leçons d'anatomie et de dissection. Le rat étant bien plus facile à élever que les singes (plus proches génétiquement de l'homme), il est devenu l'une des espèces les plus utilisées pour l'expérimentation animale. Après la souris, c'est le rat qui est le mammifère le plus fréquemment utilisé en expérimentation animale (il compte pour à peu près 20 % du nombre total de mammifères utilisés dans la recherche). Toutes les souches de laboratoire ont été produites par sélection à partir de reproducteurs choisis par les éleveurs au sein d'élevages de Rat brun, dont les premiers exemplaires provenaient de l'espèce sauvage Rattus norvegicus. Remarque : Le rat albinos de compagnie et la plupart des rats de compagnie sont des descendants de rats de laboratoires.
En économétrie, la régression sur discontinuité (en anglais regression discontinuity design ou encore RDD) est une méthode statistique qui permet d'évaluer l'effet causal d'une variable (ou d'un traitement) sur une autre en regardant ce qui se passe autour d'un seuil de discontinuité. La méthode de régression sur discontinuité s'apparente à ce que les économistes appellent une expérience naturelle.
La reproductibilité d'une expérience scientifique est une des conditions qui permettent d'inclure les observations réalisées durant cette expérience dans le processus d'amélioration perpétuelle des connaissances scientifiques. Cette condition part du principe qu'on ne peut tirer de conclusions que d'un événement bien décrit, qui est apparu plusieurs fois, provoqué par des personnes différentes. Cette condition permet de s'affranchir d'effets aléatoires venant fausser les résultats ainsi que des erreurs de jugement ou des manipulations de la part des scientifiques. Le critère de reproductibilité est une des conditions sur lesquelles le philosophe Karl Popper distingue le caractère scientifique d'une étude. Pour toutes les sciences expérimentales, les probabilités fournissent un modèle mathématique expliquant la variabilité des résultats.
Le Projet STAR (Student-Teacher Achievement Ratio) est une expérimentation à grande échelle menée dans l'État du Tennessee à partir de 1985 pour évaluer l'effet de la taille des classes sur les résultats scolaires des élèves. Dans le groupe de contrôle, les élèves ont été affectés dans de petites classes de 13 à 17 élèves pendant 4 ans alors que dans le groupe de contrôle les élèves sont dans des groupes de 22 élèves. L'objectif est de comparer leurs résultats au bout de quatre ans. L'expérience a montré que les résultats des élèves dans les petites classes sont significativement meilleurs que dans les grandes classes. L'expérience a été initiée par le gouverneur du Tennessee Lamar Alexander.
Un voltamétre est un dispositif d'électroanalyse qui permet la mesure d'une quantité d'électricité par le suivi de l'avancement d'une réaction électrochimique quantitative. Il s'agit d'une méthode coulométrique.
Le voltamètre de Hofmann est un appareil pouvant réaliser l'électrolyse de l'eau et inventé par August Wilhelm von Hofmann (1818-1892). La voltamétrie est une méthode d'électroanalyse qui permet la mesure d'une quantité d'électricité par le suivi de l'avancement d'une réaction électrochimique quantitative. Il s'agit d'une méthode coulométrique.
Un éditeur scientifique est une personne ou une organisation (entreprise, association) industrielle ou artisanale dont l'activité principale, comme tout éditeur, est la production et la diffusion publique de textes (des livres, des revues). Les activités principales exercées (code APE) sont alors 5811Z - Édition de livres ou 5814Z - Édition de revues et périodiques mais qui s'est spécialisé dans l'édition de textes de recherches scientifiques ou d'ouvrages techniques. Selon la nature de l'œuvre publiée il s'agira d'une maison d'édition (livres) ou d'un éditeur de presse (presse) s'il s'agit de revues scientifiques. Les éditeurs scientifiques sont soit des maisons d'édition universitaire liées à un ou plusieurs établissements d’enseignement supérieur, soit des sociétés privées spécialisées. Selon la destination des ouvrages il s'agira de publications académiques, ou d'ouvrages scolaires. Un éditeur scientifique est aussi une personne chargée de la direction éditoriale d'un ouvrage scientifique publié par un collectif d'auteurs, d'une collection d'ouvrages scientifiques ou d'une revue scientifique. Il assume la responsabilité scientifique de l'ensemble des textes édités sous son autorité.
Voici une liste de bases de données et de moteurs de recherche académiques. Ces derniers sont présentés par ordre alphabétique. Ils sont également classés par discipline et par type d'accès (libre ou non).
Voici une liste de bibliothèques numériques classée par défaut par ordre alphabétique. Il est également possible de classer cette dernière par sujet, nombre de volumes et fournisseurs.
L'expression « publier ou périr » (issue de l'anglais « publish or perish ») vise à dénoncer la pression exercée sur les professionnels du milieu académique, en particulier les chercheurs scientifiques, à travers l'obligation, pour avancer dans la carrière, de publier le plus régulièrement possible les résultats de travaux de recherche dans les revues scientifiques. Cette expression cherche notamment à pointer du doigt le manque de prise en compte d'autres aspects du travail académique, comme la production de prépublications ou l'organisation et la conduite des enseignements, la quantité de publications étant considérée, dans le cadre de l'évaluation, comme un moyen non sans biais de mesure de l'activité académique et des possibilités d'avancement (accès aux postes, soutien financier des projets de recherche, augmentation du revenu, notoriété ,,). Précisément, sont mis en cause les indicateurs bibliométriques, tels l'indice h ou g (en), basés sur le nombre de publications et de citations de celles-ci par d'autres auteurs, la prééminence du facteur d'impact des revues dans lesquelles l'auteur publie, ainsi que le processus d'évaluation par les pairs. Les parutions reconnues des pairs apportent des avantages à leurs auteurs et à l'institution qui l'emploie. Cette dernière peut voir ses revenus corrélés aux publications du chercheur. Ceux qui enseignent à des élèves qui ne sont pas inscrits dans les cycles supérieurs ou qui complètent des expériences qui ne cadrent pas avec les orientations des publications scientifiques peuvent voir leurs chances d'avancement réduites. La pression imposée par cette approche serait l'une des causes de la piètre qualité de maintes publications scientifiques,.
PubPeer est un site qui permet aux utilisateurs de discuter et d'examiner des productions scientifiques. Ce site a permis à des universitaires de participer à des analyses et des discussions post-publication. Il a mis en évidence des lacunes dans plusieurs articles de grande envergure, qui ont dans certains cas conduit à des rétractations et à des accusations de fraude scientifique,,,, comme remarqué par Retraction Watch.
L'Analyse des modes de défaillance, de leurs effets et de leur criticité (AMDEC) est un outil de sûreté de fonctionnement (SdF) et de gestion de la qualité. AMDEC est la traduction de l'anglais FMECA (Failure Modes, Effects and Criticality Analysis, litt. « analyse des modes, des effets et de la criticité des défaillances »), désignation d'une méthode élaborée par l'armée américaine dans les années 1940. L'AMDEC se distingue de l'AMDE (Analyse des modes de défaillance et de leurs effets, traduction de l'anglais FMEA ou Failure Modes and Effects Analysis) par une quantification portée par la notion de criticité C. La criticité d'un mode de défaillance se détermine généralement par le produit (indice de fréquence) × (indice de gravité) × (indice de détection). Ces indices sont définis par le client, l'entreprise qui fixe également un seuil d'acceptabilité, au-dessus duquel toute criticité doit être réduite, par un moyen à définir (reprise de conception, plan de maintenance, action de surveillance, …). Pour exemple, imaginons une machine équipée de pneumatiques, pour diminuer la criticité d'une crevaison jugée inacceptable, on pourrait décider de reprendre la conception et minimiser : l'indice de fréquence, en améliorant la structure du pneu, voire en utilisant un pneu increvable, l'indice de gravité, en utilisant des roues jumelées, l'indice de détection, en équipant le poste de conduite de témoins de pression pneumatique. De telles analyses peuvent être adaptées à toute interrogation dans tout domaine. Elles peuvent servir de base, entre autres, aux analyses fiabilité, maintenabilité, disponibilité, qualité et testabilité. Le but est de hiérarchiser les actions d'amélioration à conduire sur un processus, un produit, un système en travaillant par ordre de criticité décroissante.
Selon la norme CEI-300-3-9 (CEI 300-3-9, 1995), l’analyse préliminaire des risques (APR) « est une technique d’identification et d’analyse de la fréquence du danger qui peut être utilisée lors des phases amont de la conception pour identifier les dangers et évaluer leur criticité ».
L'analyse spatiale est une approche géographique qui étudie les localisations et les interactions spatiales en tant que composantes actives des fonctionnements sociétaux. Elle part du postulat selon lequel l'espace est acteur organisé. C'est une science nomothétique donc elle vise à proposer une approche modélisée de l'espace géographique en mettant en évidence des formes récurrentes d'organisation spatiales et des théories, notamment à travers diverses notions-clés : distance, réseaux, structure, situation… L'espace n'est donc pas seulement considéré comme un simple support mais comme un élément décisif d'une organisation sociale. Il s'agit de prendre en compte un ensemble complexe de données physiques et humaines pour analyser les distributions spatiales de divers phénomènes, en prenant garde de ne pas tomber dans le déterminisme strict. Les raisonnements d'analyse spatiale s'appuient beaucoup sur les données statistiques et sur des espaces isotropes (cela permet d'élaborer des théories qui sont ensuite appliquées à l'espace réel, par essence anisotrope, en tenant compte des particularités). Depuis la généralisation des systèmes d'information géographiques, l'analyse spatiale dispose de nombreux outils pour interroger les configurations spatiales observées ou proposer des simulations d'organisation spatiale. Ils permettent ainsi de modifier les objets spatiaux, de mesurer les relations entre objets en fonction de leur distance, d'identifier des configurations spécifiques ou encore d'offrir de nombreuses méthodes d'interpolation spatiale. L'analyse spatiale peut alors conduire à des modélisations de type géostatistique. De nombreux logiciels -comme GeoDa- existent désormais offrant des outils d'analyse spatiale à mettre en œuvre à partir de bases de données spatialisées.
Les chaînes de Markov sont couramment employées en sûreté de fonctionnement pour les calculs de fiabilité et de disponibilité des systèmes techniques, en particulier pour modéliser des successions de pannes, réparations, changements de configuration.
Un arbre de défaillances (aussi appelé arbre de pannes ou arbre de fautes) est une technique d’ingénierie très utilisée dans les études de sécurité et de fiabilité des systèmes statiques (un système statique est un système dont la défaillance ne dépend pas de l'ordre de défaillance de ses composants). Cette méthode consiste à représenter graphiquement les combinaisons possibles d’événements qui permettent la réalisation d’un événement indésirable prédéfini. Une telle représentation graphique met donc en évidence les relations de cause à effet. Cette technique est complétée par un traitement mathématique qui permet la combinaison de défaillances simples ainsi que de leur probabilité d'apparition. Elle permet ainsi de quantifier la probabilité d'occurrence d'un événement indésirable, également appelé « événement redouté ».
La balance bénéfice-risque est la comparaison du risque d'un traitement avec ses éventuels bénéfices. L'exposition à un risque personnel est habituelle dans la vie de tous les jours.
Un banc de test est un système physique permettant de mettre un produit en conditions d'utilisation paramétrables et contrôlées afin d'observer et mesurer son comportement. Le banc de test est largement utilisé dans l'industrie, au point de représenter une part importante du budget de développement d'un produit. Les tests sont essentiellement destinés à vérifier les fonctionnalités du produit à l'état de carte électronique mais aussi sous la forme définitive (produit fini), ce sont alors des bancs de tests fonctionnels. Les besoins de test étant très différents selon le format définitif du produit à tester
En statistiques, un biais d'autosélection peut se produire lorsque les individus formant les groupes que l'on cherche à comparer n'ont pas été assignés aléatoirement mais ont eux-mêmes choisi à quel groupe ils appartenaient. Par exemple, dans une étude clinique concernant l'efficacité de deux traitements (A vs. B), on prendra soin d'éviter que les individus du traité par A n'avaient pas au préalable des caractéristiques qui les ont fait opter pour A plutôt que B, par exemple, si leur état était moins sévère, on pourrait à tort conclure que A est plus efficace que B. Cette source d'erreur est particulièrement critique dans les situations où l'on peut supposer que certaines caractéristiques des individus peuvent les avoir incités à choisir l'un des groupes plutôt que les autres, comme cela peut être le cas en sociologie, en psychologie ou en économie. Le biais d'autosélection aboutit donc à un échantillon biaisé de la population étudiée. En sociologie, ces termes désignent aussi le processus par lequel certaines dispositions peuvent conduire tel ou tel individu à se lancer dans telle ou telle carrière professionnelle et, notamment en criminologie, le fait de suivre une trajectoire criminelle plutôt que non. En écologie cette question essentielle, et épineuse du fait de la diversité des types de protocoles couramment employés par les observatoires de l'environnement, fit le sujet d'une monographie fondamentale de Hurlbert en 1984
Le biais de confirmation, également dénommé biais de confirmation d'hypothèse, désigne le biais cognitif qui consiste à privilégier les informations confirmant ses idées préconçues ou ses hypothèses (sans considération pour la véracité de ces informations) et/ou à accorder moins de poids aux hypothèses et informations jouant en défaveur de ses conceptions. Les personnes manifestent ce biais en rassemblant des éléments ou se rappelant les informations mémorisées, de manière sélective, et les interprètant d'une manière biaisée. On dit aussi que les personnes « tirent la réalité » à elles. Les biais de confirmation apparaissent notamment autour de questions de nature affective et concernant des croyances établies. Par exemple, pour s'informer d'un sujet controversé, les personnes victimes d'un biais de confirmation préfèrent généralement lire des sources qui confirment ou affirment leur position actuelle. Elles ont aussi tendance à interpréter des preuves équivoques pour appuyer leur position actuelle. Les biais dans la recherche, l'interprétation et le rappel de la mémoire ont été invoqués pour expliquer l'attitude de polarisation (quand un désaccord devient plus extrême, même si les différentes parties sont confrontées à la même preuve), de persévérance de conviction (quand la croyance persiste après que les preuves la soutenant sont démontrées fausses), l'effet de primauté irrationnelle (une plus forte importance pour les premières données rencontrées) et l'illusion de corrélation (par laquelle les personnes perçoivent à tort une association entre deux événements ou situations). Une série d'expériences dans les années 1960 suggère que les individus sont biaisés en faveur de la confirmation de leurs croyances actuelles. Des travaux ultérieurs ont expliqué ces résultats par une tendance à évaluer les idées d'une manière unilatérale, mettant l'accent sur une possibilité unique et ignorant les alternatives. En combinaison avec d'autres effets, cette stratégie de pensée peut biaiser les conclusions qui sont atteintes. Pour expliquer les biais observés, on invoque notamment le rôle du désir dans la pensée et les limitations de la capacité humaine au traitement de l'information. Une autre hypothèse est que les individus montrent un biais de confirmation, car d'une manière pragmatique ils évaluent le coût d'être dans l'erreur, plutôt que d'enquêter d'une manière neutre ou scientifique. Les biais de confirmation contribuent à l'excès de confiance dans les croyances personnelles et peuvent maintenir ou renforcer les croyances face à des preuves contraires. Ils peuvent donc conduire à des décisions désastreuses, en particulier dans des contextes organisationnels, militaires, politiques ou sociaux.
Le biais de financement désigne en science le fait que les résultats des recherches ont tendance à être plus favorables au financeur direct ou indirect. Ce biais a été mis en évidence dans de nombreux domaines, où des industries ont un intérêt fort (pour des raisons de régulation) à ce que les résultats aillent plutôt dans son sens : recherche sur les effets du tabac, de la nourriture, des pesticides, des perturbateurs endocriniens, des OGM, recherches dans le secteur biomédical. Il ne s'agit pas d'un biais unique à proprement parler mais d'un ensemble de biais dont les contours ne sont pas bien identifiés.
Un biais de publication, désigne en science le fait que les chercheurs et les revues scientifiques ont bien plus tendance à publier des expériences ayant obtenu un résultat positif (statistiquement significatif) que des expériences ayant obtenu un résultat négatif (soutenant l'hypothèse nulle). Ce biais de publication donne aux lecteurs une perception biaisée (vers le positif) de l'état de la recherche.
Dans une étude statistique, le terme biais de sélection désigne une erreur systématique faite lors de la sélection des sujets à étudier. Ce terme regroupe tous les biais pouvant conduire à ce que les sujets effectivement observés lors d'une enquête ne constituent pas un groupe représentatif des populations censées être étudiées et ne permettent donc pas de répondre aux questions posées dans le protocole. Les biais de sélection se produisent lors de l'échantillonnage, c'est-à-dire lors de la sélection d'un échantillon représentatif de la population étudiée. Ils sont liés aux critères d'inclusion des individus dans l'étude et se produisent lorsque les sujets inclus dans l'étude ne constituent pas un groupe représentatif de la population cible. Cela se produit lorsque les sujets sélectionnés dans l'échantillon ont des caractéristiques qui les distinguent de l'ensemble de la population-mère, ce qui est inévitable puisque ce sont ces caractéristiques qui déterminent l'échantillon de personnes. Le risque de biais de sélection est faible dans les études expérimentales et longitudinales, élevé dans les études cas-témoins et considérable dans les études transversales. Il faut noter qu'en santé, un biais de sélection aura lieu si après constitution dudit groupe, il y a ce que l'on nomme des « perdues de vues », c'est-à-dire des personnes ayant quitté l'enquête épidémiologique en cours.
Le biais du survivant est une forme de biais de sélection consistant à surévaluer les chances de succès d'une initiative en concentrant l'attention sur les sujets ayant réussi mais qui sont des exceptions statistiques (des « survivants ») plutôt que des cas représentatifs.
En science, un biais méthodologique est une erreur dans la méthode scientifique, le non-respect des règles de protocole, qui engendre des résultats erronés.
En chimie expérimentale, le but est l'objectif que l'on cherche à atteindre, en respectant les règles élémentaires de sécurité, lors d'une synthèse, d'une analyse ou d'une expérimentation en général. Il constitue le fil conducteur de la manipulation. À la rédaction comme à l'oral, il est bref — deux lignes au plus dans la majorité des cas — et introduit un verbe d'action. En général, il regroupe le travail effectué, ainsi que la méthode.  
Un cadre conceptuel est un outil d'analyse comptant plusieurs variations et contextes. Il est utilisé pour faire des distinctions conceptuelles et organiser des idées. Les cadres conceptuels forts saisissent quelque chose de réel et le font d'une manière facile à retenir et à appliquer. Isaiah Berlin emploie la métaphore d'un renard et d'un hérisson pour faire des distinctions conceptuelles dans la façon dont les philosophes et les auteurs importants voient le monde. Berlin décrit les hérissons comme ceux qui utilisent une seule idée ou principe d'organisation pour voir le monde (les exemples donnés comprennent Dante, Pascal, Dostoïevski, Platon, Ibsen et Hegel). Les renards d'un autre côté, incorporent un type de pluralisme et voient le monde à travers des lentilles multiples et parfois contradictoires (les exemples qu'il donne comprennent Goethe, Joyce, Shakespeare, Aristote, Hérodote, Molière, Anderson, Balzac). Les économistes emploient le cadre conceptuel de l'« offre » et de la « demande » pour faire la distinction entre le comportement des consommateurs et les systèmes d'incitation des entreprises. Comme beaucoup de cadres conceptuels, l'offre et la demande peuvent être présentées par le moyen de représentations visuelles ou graphiques.
La cartographie systématique est une « méthode de collecte, d’évaluation et de synthèse des connaissances scientifiques et techniques qui s’est développée initialement dans le domaine des sciences médicales, puis des sciences sociales, puis celles de l'environnement ». Elle s'appuie notamment sur les Revues systématiques. Dans un monde touché par de grandes crises globales (climatique et de biodiversité notamment) et où la science évolue rapidement, ces cartographies sont des « représentations du paysage de la connaissance en réponse à une question ou une problématique très large. Elles répondent à des questions qui s’apparentent à celles des revues systématiques dans leur structuration (PICO/ PECO). Elles ont pour objectif de mettre en évidence la répartition des connaissances selon des critères explicites (métadonnées) ». Ce type d'étude produit de plus en plus les « Evidence bases », sur lesquelles s’appuient (ou non ) les décideurs,.
En sciences, le terme cas idéal définit le domaine d'application d'une théorie qui a été simplifiée jusqu'à sa plus simple expression. Cette simplification est souvent nécessaire pour pouvoir adapter et résoudre les équations mathématiques qui seraient beaucoup trop complexes en considérant l'intégralité du phénomène.
Cochrane (précédemment la Collaboration Cochrane) est une organisation à but non lucratif indépendante qui regroupe plus de 28 000 volontaires dans plus de 100 pays. Cette collaboration s'est formée à la suite d'un besoin d'organiser de manière systématique les informations concernant la recherche médicale. De telles informations consistent en des preuves scientifiques pour la prise de décision médicale, fondées sur des essais cliniques bien menés. Les preuves scientifiques sont nécessaires pour prendre des décisions de soin efficaces et pour mettre en lumière les domaines où les données sont insuffisantes et où plus de recherches sont nécessaires. La collaboration a pour but de regrouper des données scientifiquement validées de manière accessible et résumée. Elle conduit des revues systématiques (et des méta-analyses) d'essais randomisés contrôlés d'interventions en santé,. Ces travaux sont publiés dans la bibliothèque Cochrane (en anglais : Cochrane library). La collaboration a gagné des relations officielles avec l'Organisation mondiale de la santé (OMS) en janvier 2011 en tant qu'organisation non gouvernementale. Elle a un siège à l'organisation mondiale de la santé pour y apporter des contributions,.
Le consensus scientifique est le jugement, la position, et l'opinion collectifs des personnes de la communauté scientifique qui travaillent sur un domaine particulier d'étude. Le consensus implique un accord général, mais pas nécessairement à l'unanimité. Le consensus scientifique n'est, en lui-même, pas un argument scientifique, et il ne fait pas partie de la méthode scientifique. Néanmoins, le consensus peut être basé sur la méthode et l'argument scientifiques.
Le Consolidated Standards of Reporting Trials, ConSORT statement ou Standards fusionnés dans la rédaction d'essais thérapeutiques est une proposition du groupe ConSORT, pour améliorer la rédaction et la transparence des essais randomisés contrôlés. Il se présente sous la forme d'une liste d'items à vérifier par l'auteur, l'éditeur ou le lecteur pour évaluer la qualité de la rédaction de l'essai. L'objectif de ces conseils est d'obtenir un article le plus transparent, le plus complet et le plus objectif possible. Il ne permet pas d'évaluer la qualité d'une méthode, mais de repérer plus facilement les bons et les mauvais essais. La version la plus récente est la version de 2010. Elle consiste en une liste de vérification (checklist) de 25 items et d'un graphique le diagramme de flux (flow chart). À cette liste, est associée un document d'explication, d'illustrations des recommandations par des exemples dont la lecture est conseillée. Pensé comme un document créé de manière collaborative et en évolution, les propositions ConSORT sont sujet à des changements périodiques au fur et à mesure que les pratiques et les données de la science évolueront qui peuvent être retrouvées sur leur site.
La crise de la reproductibilité, (replication crisis ou replicability crisis en anglais) fait référence à la crise méthodologique dans le domaine des sciences selon laquelle de nombreux résultats publiés dans des revues scientifiques sont difficiles, voire impossibles à reproduire au cours d'études subséquentes. Initiée au milieu des années 2000, la crise prend de l'ampleur au milieu des années 2010, nourrie par la publication de plusieurs articles sur le phénomène,. Ainsi, selon une étude réalisée auprès de 1 500 scientifiques et publiée par Nature en 2016, plus de 70 % des chercheurs affirment avoir été incapables de reproduire l'expérience scientifique d'un autre chercheur et plus de la moitié affirment avoir échoué à reproduire leur propre expérience. Un autre sondage sur la reproductibilité des recherches sur le cancer montre que 50 % des répondants affirment avoir fait l'expérience d'au moins un épisode d'incapacité à reproduire des données publiées. Plusieurs étant incapable de déterminer la source du problème même en interrogeant les auteurs originaux. En 2015, les résultats du Reproductibility project (en), lancé en 2011 afin de mieux quantifier le phénomène en psychologie, montrent que moins de la moitié des expériences dans ce domaine ont pu être reproduites. La crise n'est pas propre à un domaine unique bien qu'elle semble moins toucher les sciences fondamentales et appliquées que les sciences médicales. Les facteurs qui en sont responsables semblent nombreux. Des pistes d'amélioration de la reproductibilité au sein des publications scientifiques, dont notamment l'amélioration des critères de publication, sont explorées.
En sciences expérimentales, le relevé de données se représente fréquemment sur un graphique et prend alors souvent la forme d'une courbe. Le dépouillement de la courbe est l'opération qui consiste à extraire des informations de cette courbe ; l'information ne provient pas des valeurs individuelles des points, mais de leur organisation, de la forme de la courbe. Lorsque cette courbe est un spectre, on parle de dépouillement d'un spectre.
Avec le développement d'Internet et des nouvelles technologies, le phénomène du plagiat scolaire s'est beaucoup développé, en particulier dans le milieu universitaire. De nombreux enseignants ont cherché des moyens et méthodes efficaces pour lutter contre le plagiat. Il existe de nombreuses méthodes de détection du plagiat.
L'échelle de Newcastle–Ottawa est une méthode d'évaluation de la qualité d'études non-randomisées (notamment études cas témoin et de cohorte). L'échelle donne un score avec des étoiles. Au maximum 9 étoiles pour : la qualité de la sélection (cas bien définis, lieu (hôpital ou ambulatoire), témoins bien définis (pas d'histoire de maladie), la comparabilité entre les cas et les témoins, l'aveugle de la mesure des résultats des participants à l'étude, étude suffisamment longue pour la pathologie étudiée, nombre de perdus de vue Elle évalue aussi la facilité d'utilisation du contenu pour une méta-analyse. La méthode a été développée entre les universités de Newscastle en Australie et d'Ottawa au Canada.
Un essai clinique, ou étude clinique, ou encore essai thérapeutique, est une étude scientifique réalisée en thérapeutique médicale humaine pour évaluer l'efficacité et la tolérance d'une méthode diagnostique ou d'un traitement. L'objectif d'un essai n'est pas d'apporter un bénéfice thérapeutique au volontaire. Le Comité international des rédacteurs de revue médicales en donne la définition suivante : « Tout projet de recherche qui affecte de façon prospective des sujets humains à des groupes d'intervention et de comparaison afin d'étudier la relation de cause à effet entre un acte médical et l'évolution d'un état de santé ». Ces études sont souvent effectuées après des études expérimentales non-cliniques (sur des modèles animaux ou cellulaires) pour confirmer leur pertinence et leur sécurité. Elles nécessitent aussi l'accord des autorités de santé ou d'éthique du pays où elles ont lieu. En fonction du type d'étude et du stade du développement du médicament, les investigateurs enrôlent des volontaires sains ou des patients. Les études peuvent être monocentriques avec un faible nombre de participants (par exemple études pilotes). À l'extrême inverse elles peuvent être multicentriques et inclure des milliers de patients. La fiabilité de ces études repose sur une méthode scientifique rigoureuse et éprouvée afin de limiter tout biais, toute erreur de collecte des données ou d'interprétation des résultats. Les Bonnes Pratiques Cliniques sont une norme internationale relative à la bioéthique s'appliquant aux essais cliniques réalisés sur des sujets humains. Les résultats sont publiés dans des revues médicales et présentés lors de congrès. Dans le cas des médicaments, ils servent à établir le dossier permettant d'en valider l'utilisation auprès d'instances nationales ou internationales.
Les essais cliniques indépendants peuvent être promus et financés par divers sponsors, notamment les sociétés pharmaceutiques, les gouvernements, les organismes de bienfaisance de la recherche, des fondations, des organisations médicales, et des groupes de volontaires, tels que les associations de patients.
Un essai clinique randomisé par grappes est un type d'essai clinique où des groupes entiers de sujets (à opposer à des sujets répartis un par un) sont alloués de façon aléatoire dans les bras de l'essai,. Ces groupes pré-existent souvent à l'essai, ils ne sont pas créés par les chercheurs : villes, classes, écoles, entreprises, hôpitaux… Les essais cliniques randomisés par grappes sont aussi appelés en anglais Cluster Randomised Trials (CRT), cluster randomization trials, group-randomised trials, et place-randomized trials. Une étude bibliométrique de 2004 montre que le nombre de publications médicales fondées sur des essais cliniques randomisés par grappes croît depuis les années 1980.
Un essai randomisé contrôlé (ERC) (randomized controlled trial (RCT) en anglais) est un type d'étude scientifique utilisé en médecine et plus récemment en sciences sociales (économie). C'est le gold standard, ou pour le dire en français, le mètre étalon, en ce qui concerne les essais cliniques. Les ERC sont souvent utilisés pour tester l'efficacité de plusieurs approches thérapeutiques dans une population de patients. Ils peuvent aussi collecter des informations sur les effets secondaires des traitements. Après l'évaluation pour éligibilité dans l'étude puis recrutement, les sujets sont aléatoirement répartis (randomisation) parmi les groupes correspondants à chaque approche thérapeutique testée. Ensuite, on vérifie que les deux populations sont proches en comparant les caractéristiques de base dont les caractéristiques démographiques (souvent tableau 1-données initiales). L'intérêt de la randomisation est qu'elle limite les biais de sélection et donc permet une répartition homogène entre les groupes des facteurs pronostiques connus et inconnus. Elle permet une comparabilité initiale. Ensuite, l'intervention thérapeutique commence. Les sujets, les thérapeutes et les évaluateurs sont dans la mesure du possible en aveugle (on dit aussi en insu), c'est-à-dire qu'ils ne savent pas dans quel groupe est le patient. Ceci permet que la seule variable qui soit différente entre les groupes soit le traitement. L'aveugle est maintenu en utilisant des techniques de placebo (traitement médicamenteux) ou de fausse procédure (traitement non médicamenteux). Cette stratégie permet un maintien de la comparabilité.
Une étude écologique est une étude épidémiologique dans laquelle les critères analysés concernent une population plutôt que des individus. Par exemple, une étude écologique peut étudier l'association entre le tabagisme et les décès par cancers du poumon dans des pays différents en prenant les statistiques de chaque pays et non pas en étudiant tous les individus. Une étude écologique est souvent considérée comme inférieure à d'autres types d'études épidémiologiques comme les études de cohorte ou les études cas-témoins à cause de l'erreur écologique. Un exemple d'une étude écologique est l'analyse des effets des soins de désinfections chez les nouveau-nés. Cette étude a utilisé les données de 108 villes du Massachusetts comme unité d'analyse. Les études écologiques peuvent facilement être confondues avec des études de cohorte, surtout quand différentes cohortes se retrouvent dans des lieux différents. La différence est que dans le cas d'étude écologiques, il n'y a pas d'information recueillies à propos des sujets des populations étudiées (par exemple comparaison de plusieurs pays à propos de la prévalence moyenne des maladies respiratoires dans un pays en étudiant la pollution moyenne dans un pays. Au contraire, on connait les données (exposition et maladie) pour chaque individu dans une étude de cohorte. Malgré leur faiblesses, les études écologiques sont utiles car elles peuvent être menées facilement, rapidement et pour un faible coût. Elles utilisent des données qui sont généralement déjà disponibles. Si des relations intéressantes et fortes sont observées, les résultats d'études écologiques peuvent fournir des idées de nouvelles études avec des protocoles plus rigoureux et moins sujets aux biais (études cas-témoin, études de cohorte).
L'étude randomisée en double aveugle, avec répartition aléatoire, randomisée ou hasardisée pour les Québécois, à double insu (ou en double aveugle) est une démarche expérimentale utilisée dans de nombreuses disciplines de recherche telles que la médecine, les sciences sociales et la psychologie, les sciences naturelles telles que la physique et la biologie. En pharmacie, elle est utilisée dans le développement de nouveaux médicaments et pour évaluer l'efficacité d'une démarche ou d'un traitement. Le rôle d'un tel protocole, relativement lourd à mettre en place, est de réduire au mieux l'influence sur la ou les variables mesurées que pourrait avoir la connaissance d'une information (utilisation d'un produit contenant un principe actif ou d'un placebo, par exemple) à la fois sur le patient (premier « aveugle ») et sur l'examinateur (deuxième « aveugle »). C'est la base de la médecine fondée sur les faits.
L'étude sur le terrain, l'enquête de terrain, le travail de terrain ou plus simplement terrain est le fait de se rendre sur le lieu de collectes des données. On définit ainsi le travail de terrain ou l'enquête de terrain, par opposition au travail post-observatoire d'analyse des données.
La figure de pôles est une manière de représenter les orientations dans l'espace.
La formulation est une opération industrielle consistant à fabriquer un matériau homogène et stable, non toxique (pour une grande majorité d'applications), possédant des propriétés finales spécifiques et répondant aux exigences d'un cahier des charges fonctionnel (CDCF), en mélangeant des substances diverses.
Un groupe contrôle est utilisé en sciences humaines et sociales pour mesurer les effets d'un test. Il s'agit d'un ensemble d'individus qui n'ont pas été concernés par la manipulation d’une (ou plusieurs) variable(s) consistant le test (c'est la « condition contrôle »). Il est comparé au groupe expérimental qui, lui, a subi la manipulation afin d'en déterminer les effets et sert de valeur témoin de référence. Dans un essai randomisé contrôlé on parle aussi de bras témoin pour désigner le groupe contrôle.
Une hypothèse scientifique est une proposition admise soit comme donnée, soit pour la démonstration d'un théorème (= axiome postulat)  Portail des sciences
L'illusion des séries (en anglais clustering illusion) est la tendance à percevoir à tort des coïncidences dans des données au hasard. Cela est dû à la sous-estimation systématique par l'esprit humain de la variabilité des données.
L’interdisciplinarité est l'art de faire travailler ensemble des personnes ou des équipes issues de diverses disciplines scientifiques. L'intérêt est d'enrichir les approches et solutions en favorisant la créativité et la sérendipité, de faciliter l'atteinte d'un but commun en confrontant des approches différentes d'un même problème. Des concepts proches et complémentaires sont la pluridisciplinarité, la transdisciplinarité et la métadisciplinarité. L'interdisciplinarité est reconnue comme nécessaire pour résoudre les questions globales et complexes (changement climatique, santé publique, crise écologique ou sociopolitique, paix dans le monde, etc.), mais la structuration de l'enseignement et du monde de la recherche n'a pas favorisé la coopération et l'interdisciplinarité. De plus, les études interdisciplinaires sont d'autant plus difficilement financées qu'elles mobilisent des disciplines éloignées les unes des autres et si elles trouvent un financement, elles seront statistiquement moins citées que les autres (moindre facteur d'impact).
En physique, une loi en carré inverse est une loi physique postulant qu'une quantité physique (énergie, force, ou autre) est inversement proportionnelle au carré de la distance de l'origine de cette quantité physique. Cette loi fut d'abord suggérée par l'astronome français Ismaël Boulliau, puis mise en forme par Isaac Newton après que Robert Hooke lui eut proposé l'idée dans une lettre. Robert Hooke accusa plus tard Newton de plagiat.
Une méta-analyse est une démarche scientifique systématique combinant les résultats d'une série d'études indépendantes sur un problème donné, selon un protocole reproductible. La méta-analyse permet une analyse plus précise des données par l'augmentation du nombre de cas étudiés et de tirer une conclusion globale. La méta-analyse fait partie des méthodes d'analyse dite secondaires en ce sens qu'elles s'appuient sur la ré-exploitation de données existantes. Pour certains, les enjeux sont de produire des connaissances nouvelles en prenant appui sur des connaissances existantes réduisant le temps et le coût de la recherche ; pour d'autres, les enjeux portent plus sur une réinterprétation voire un contrôle des connaissances existantes. Il est possible de distinguer les méta-analyses selon deux critères , : en fonction du type de variable : quantitative (agrégation d'études quantitatives) ou qualitative (agrégation d'études qualitatives) en fonction du matériau traité : sur données résumées (informations globales sur un ensemble d'individus) ou sur données individuelles (par individu). Ces méta-analyses peuvent néanmoins elles-mêmes être sujettes à un biais de publication, par exemple les chercheurs pouvant avoir moins tendance à publier une étude concluant à une absence de résultat. D'autres biais peuvent venir de l'usage des données résumées.
La méthode hypothético-déductive est une méthode scientifique qui consiste à formuler une hypothèse afin d'en déduire des conséquences observables futures (prédiction), mais également passées (rétrodiction), permettant d'en déterminer la validité. Elle est à la base de la démarche expérimentale, théorisée en particulier par Roger Bacon (à ne pas confondre avec Francis Bacon) en 1267 dans De Scientia experimentali, une des sept parties de son Opus maius (« Œuvre majeure »). La question de la vérification d'une hypothèse renvoie en particulier au problème de l'induction, au cœur de la philosophie des sciences empiriste. L'approche hypothético-déductive et les autres approches L'approche hypothético-déductive consiste à émettre des hypothèses, à recueillir des données, puis à tester les résultats obtenus pour réfuter ou appuyer les hypothèses. Elle contraste avec d'autres approches comme l'approche inductive ou la recherche dite enracinée. Dans la méthodologie de percolation des données, l'approche hypothético-déductive est encadrée dans un paradigme pragmatique et l'émission des hypothèses doit se faire selon une modélisation axée sur quatre grands groupes de relations entre les variables considérées: descriptif, d'influence, longitudinal et causal. Les variables sont elles-mêmes classées selon qu'elles sont structurelles ou fonctionnelles, ce qui modifie la formulation, dicte les tests statistiques des hypothèses et guide la recherche pour en augmenter l'efficacité.
La méthode QMU (Quantification of Margins and Uncertainties), en français Quantification des Marges et Incertitudes, a été développée aux États-Unis par les laboratoires nationaux Lawrence Livermore et Los Alamos. Elle est principalement appliquée, dans le cadre du programme de maintien de l'arsenal nucléaire (Stockpile stewardship (en)), aux vérifications de sûreté et de fiabilité des armes. C'est un cadre méthodologique permettant de déterminer le niveau de confiance que l'on peut accorder au bon fonctionnement de chacune des armes de l'arsenal nucléaire. Il utilise un formalisme standard de présentation des données, ce qui permet de faciliter la communication entre les diverses équipes collaborant au programme.
Il serait abusif de faire remonter la notion de méthode scientifique jusqu’à l’Antiquité, tant il est délicat d’identifier ce que nous nommons « science » avec les démarches de production de nouveaux savoirs aux époques proto-historiques. On peut cependant reconnaître dans les réflexions des anciens philosophes les prémisses d’une théorie de la connaissance congruante avec les pratiques scientifiques contemporaines, mais pas des vrais méthodes. Au cours des siècles, différents philosophes enrichiront la réflexion sur la notion de méthode en en explorant différents aspects (déduction, induction, méthode expérimentale, méthode analytique, réfutation, etc.), sans qu’il soit toujours fait un lien entre eux. Cette histoire n’est donc pas linéaire, mais se présente plutôt comme un buissonnement d’idées qui s’agrègent aujourd’hui dans la notion de méthode scientifique. Il faut cependant distinguer l’histoire de la méthode en tant que notion normative de l’histoire de la méthode en tant que pratique scientifique effective. Tandis que les philosophes visent, souvent dans une perspective normative, à éclaircir la notion de méthode scientifique, les savants ne se préoccupent pas toujours de ses considérations, et n’ont pas toujours une démarche réflexive. Il ne faut cependant pas verser dans l’excès inverse, et imaginer que travail scientifique et travail sur la science s’ignorent. De nombreux savants, et non des moindres, portent attention aux discours sur la science, tandis que les philosophes, historiens ou sociologues faisant porter leurs réflexions sur la méthode scientifique peuvent influencer, directement ou indirectement, l’organisation de la science. Enfin, indépendamment de la dimension normative de la notion de méthode, il faut porter attention à l’évolution des regards portés sur cette méthode. Les analyses des historiens, puis des sociologues, ont évolué au fil du temps, faisant du même coup évoluer notre représentation de la méthode scientifique. Au bout du compte, il faut entrelacer trois perspectives : l’histoire de la méthode comme pratique ; l’histoire de la méthode comme concept normatif ; l’histoire des discours sur la méthode.
La méthode scientifique désigne l'ensemble des canons guidant ou devant guider le processus de production des connaissances scientifiques, qu'il s'agisse d'observations, d'expériences, de raisonnements, ou de calculs théoriques. Très souvent, le terme de « méthode » engage l'idée implicite de son unicité, tant auprès du grand public que de certains chercheurs, qui de surcroît la confondent parfois avec la seule méthode hypothético-déductive. L'étude des pratiques des chercheurs révèle cependant une si grande diversité de démarches et de disciplines scientifiques que l'idée d'une unité de la méthode est rendue très problématique. Ce constat ne doit cependant pas être entendu comme une forme d'anarchisme épistémologique. Si la question de l'unité de la méthode est problématique (et ce problème sera abordé plus en détail ci-dessous), cela ne remet pas en question l'existence d'une pluralité de canons méthodologiques qui s'imposent aux chercheurs dans leurs pratiques scientifiques.
Les méthodes qualitatives regroupent un ensemble de méthodes de recherche utilisées dans les études qualitatives. Elles trouvent leur utilité notamment en Sciences humaines et sociales. Elles laissent délibérément de côté l'aspect quantitatif pour gagner en profondeur dans l'analyse de l'objet d'étude. Pour cela diverses techniques, fondées sur l'administration de questions ouvertes et l'exploration du langage, sont mises en œuvre : les « focus groupes » ou tables rondes, l'analyse de contenu ou le desk research (en), l'observation participative, sont les plus importants. Les « focus groupes » consistent à réunir des sujets et à les interroger sur leur attitude vis-à-vis d'un produit, d'une idée, d'une publicité, etc. Les méthodes qualitatives sont couramment utilisées conjointement aux méthodes quantitatives. L'usage de méthodes qualitatives permet soit de dégrossir un sujet d'étude pour poursuivre avec une étude quantitative soit pour interpréter les nombres produits par des méthodes quantitatives[pas clair].
Un modèle scientifique est une représentation simplifiée, et souvent idéale, de la réalité d'un phénomène permettant d'élaborer une théorie plus ou moins précise adhérant aux observations et de prévoir ce qu'il se passerait dans certaines conditions. Dans la plupart des cas, un modèle reste limité à un domaine d'application (les valeurs minimales et maximales des différentes variables) en dehors duquel ledit modèle n'est plus applicable. Un modèle aide les scientifiques à concevoir, à analyser ou à imaginer des concepts scientifiques. La communauté scientifique ne s'entend toutefois pas sur la délimitation du concept de «modèle scientifique» et celui de «théorie scientifique». Mais, d'une façon globale, on peut dire que les deux sont utilisées (voir essentielles) dans toutes les branches de la science.
L’observation est une expérience de sélection et de recueil d'informations sur un phénomène, un objet d'étude, en vue de dégager des hypothèses ou de vérifier celles découlant d'observations antérieures. Ce stade de la recherche est indispensable dans tous les domaines scientifiques, dans les sciences naturelles comme dans les sciences humaines et sociales, par exemple en psychologie.
Le sigle OHERIC (Observation, Hypothèse, Expérience, Résultats, Interprétation, Conclusion) désigne la succession d’étapes d’un modèle idéalisé de démarche scientifique. Il correspond à une critique formulée à l’encontre d’une telle présentation linéaire dans l’enseignement des sciences, qui laisse de côté les errements, les tâtonnements et les fausses pistes habituellement suivies dans le cheminement réel de la recherche, parcours sinueux dans lequel la solution est progressivement construite à coups d’hypothèses fausses successivement rectifiées.
L’opérationnisme, opérationisme ou opérationnalisme « consiste à définir les concepts de telle manière qu'ils puissent être établis et éprouvés en termes d'opérations concrètes et répétables, par des observateurs indépendants ». Cette approche, introduite au début du XXe siècle par le physicien Percy Williams Bridgman, facilite le développement de la mécanique quantique. Elle est ensuite appliquée en psychologie.
Le terme phénoménologie appliqué à la science est utilisé pour décrire un corps de connaissance reliant de nombreuses observations empiriques entre elles, de façon cohérente avec la théorie fondamentale, mais n'en étant pas issu. Une théorie phénoménologique exprime mathématiquement le résultat de l'observation d'un phénomène sans s'attarder à sa signification fondamentale. Le mot dérive de « phénomène » ( du grec φαινόμενoν, pl. φαινόμενα - phenomena, traduit par "chose manifeste, évidente, qui apparaît", et -λογία - -logia, traduit par « étude de » ou « recherche »), et s'applique à tout événement observable. Ainsi, des expressions algébriques peuvent être utilisées afin de modéliser des observations ou des résultats expérimentaux de différentes échelles de longueurs, masses, ou temps, et pour effectuer des prédictions sur les résultats d'autres observations ou expériences, bien que ces expressions ne soient pas issues d'approximations d'une théorie proposée pour ce domaine de connaissance. Une autre façon d'appréhender la phénoménologie en science est de considérer qu'elle constitue l'intermédiaire entre l'expérience et la théorie. Elle est plus logique et comprend plus d'étapes logiques que l'expérience, mais est plus directement liée à cette dernière qu'à la théorie. Les limites entre théorie et phénoménologie, et entre phénoménologie et expérience, sont parfois floues et dépendent en quelque sorte des préconceptions du scientifique les décrivant et du champ particulier de travail. La plupart des scientifiques diraient qu'une modélisation phénoménologique d'un phénomène ne constitue pas une compréhension du phénomène, mais agréeront la pertinence de son rôle dans les sciences. La philosophe des sciences Nancy Cartwright ne croit pas dans les lois fondamentales scientifiques, mais plutôt dans ses lois phénoménologiques.
Preuve empirique, données ou connaissance, aussi appelée expérience des sens, est un terme collectif pour désigner la connaissance ou les sources de la connaissance acquise au moyen des sens, en particulier par l'observation et l'expérimentation. Le terme vient du mot grec ancien pour expérience, ἐμπειρία (empeiría). Après Emmanuel Kant, il est habituel en philosophie d'appeler une connaissance ainsi acquise connaissance a posteriori. Cela en opposition à une connaissance a priori, connaissance accessible à partir de la pensée spéculative seule.
Le protocole opératoire est l'ensemble des événements programmés et volontaires devant aboutir au parfait déroulement d'une expérience ou d'une opération particulière. Le protocole opératoire comporte éventuellement des éléments liés à la sécurité, et ce, dans tous les domaines où il y a danger pour l'environnement immédiat ou bien les opérateurs.
Pour approcher cette notion aux contours flous et aux synonymes multiples (on parlera tantôt de « recherche-intervention », tantôt de « recherche-expérimentation »), on retiendra deux acceptions spécifiques de la « recherche-action » où se sont cristallisés ses usages, avec des recoupements possibles, notamment autour des années 1970, où la recherche de l'émancipation et de l'autonomie (l'empowerment - le pouvoir-faire), dans la tradition communautaire anglo-saxonne) était un thème politique mobilisateur et transversal aux deux usages. L'une, utilisée en Grande-Bretagne depuis la fin des années 1940 (action research), désigne un mode d'action sociale centrée sur les obstacles et les ressources de la dynamique des groupes, « sociaux » ou « naturels ». L'autre, apparue dès les années 1960, désigne les voies très variées par lesquelles les savoirs de l'homme sortent de leur enceinte institutionnelle (du soin, de l'éducation, de l'organisation, de la recherche) pour dynamiser le changement social. En 1986, lors d'un colloque à l'Institut national de recherche pédagogique (INRP, Paris), les chercheurs sont partis de la définition suivante : il « s'agit de recherches dans lesquelles il y a une action délibérée de transformation de la réalité ; recherches ayant un double objectif : transformer la réalité et produire des connaissances concernant ces transformations ». Une conception « classique » de la recherche-action consiste à penser que cette méthodologie nouvelle n'est qu'un prolongement particulier de la recherche traditionnelle en sciences sociales, s'appuyant sur l'idée que l'humain et le social, en tant qu'objets d'études, présentent des caractéristiques spécifiques qui appellent à la mise en place d'une méthodologie différente de celle qui a cours dans les « sciences dures » : interiorité, non-déterminisme et singularités. Elle implique dans le processus de construction de la recherche, aussi bien le chercheur que les acteurs participant à l'expérimentation.
En statistiques, en économétrie et en apprentissage automatique, un modèle de régression linéaire est un modèle de régression qui cherche à établir une relation linéaire entre une variable, dite expliquée, et une ou plusieurs variables, dites explicatives. On parle aussi de modèle linéaire ou de modèle de régression linéaire. Parmi les modèles de régression linéaire, le plus simple est l'ajustement affine. Celui-ci consiste à rechercher la droite permettant d'expliquer le comportement d'une variable statistique                         y                 {\displaystyle y}    comme étant une fonction affine d'une autre variable statistique                         x                 {\displaystyle x}   . En général, le modèle de régression linéaire désigne un modèle dans lequel l'espérance conditionnelle de                         y                 {\displaystyle y}    sachant                         x                 {\displaystyle x}    est une transformation affine en les paramètres. Cependant, on peut aussi considérer des modèles dans lesquels c'est la médiane conditionnelle de                         y                 {\displaystyle y}    sachant                         x                 {\displaystyle x}    ou n'importe quel quantile de la distribution de                         y                 {\displaystyle y}    sachant                         x                 {\displaystyle x}    qui est une transformation affine en les paramètres. Le modèle de régression linéaire est souvent estimé par la méthode des moindres carrés mais il existe aussi de nombreuses autres méthodes pour estimer ce modèle. On peut par exemple estimer le modèle par maximum de vraisemblance ou encore par inférence bayésienne. Bien qu'ils soient souvent présentés ensemble, le modèle linéaire et la méthode des moindres carrés ne désignent pas la même chose. Le modèle linéaire désigne une classe de modèles qui peuvent être estimés par un grand nombre de méthodes, et la méthode des moindres carrés désigne une méthode d'estimation. Elle peut être utilisée pour estimer différents types de modèles.
En analyse mathématique et plus particulièrement en géométrie analytique, la représentation graphique d'une fonction mathématique consiste à en dessiner le tracé, c'est-à-dire une image de l'ensemble des valeurs que peut prendre cette fonction. Ce tracé peut être plus ou moins complexe à réaliser suivant la façon dont est définie la fonction en question (expression analytique de la forme                                    f           (           x           )           =           .           .           .                          {\displaystyle \scriptstyle f(x)=...}   , solution d'une équation ou d'une inéquation, etc.) et des espaces de départ et d'arrivée de cette fonction (une ou plusieurs variables, réelles ou non).
Une revue systématique est un travail de collecte, d'évaluation critique et de synthèse des connaissances existantes sur une question donnée. Cette question bien définie est issue de l'étude d'une problématique posée par un commanditaire, un gestionnaire, un praticien, un chercheur... Il s'agit, contrairement à de simples revues de littérature, de minimiser les biais pouvant être inhérents soit à la matière première (données, connaissances) soit à la conduite de la revue elle-même, afin d'atteindre la plus grande objectivité possible. Les biais ne pouvant pas être réduits à zéro, il s'agit donc de les mettre en lumière et de les prendre en compte dans le travail de synthèse afin que tout lecteur des résultats et conclusions de la revue puisse se les approprier en toute connaissance de cause, en en comprenant les limites et le niveau de confiance (incertitude) des résultats. La revue systématique peut notamment être utilisée pour produire une cartographie systématique. « Une revue systématique est la synthèse rigoureuse et reproductible des résultats de toutes les études originales existantes répondant à une même question de recherche. Une méta-analyse est la synthèse statistique des études incluses dans la revue systématique. »
Le score de Jadad, parfois aussi appelé « score de qualité d'Oxford », est une procédure pour évaluer de manière indépendante la qualité méthodologique d'un essai clinique,. C'est une des évaluations les plus répandues dans le monde. En 2008, son article de séminaire a été cité dans plus de 3 000 travaux scientifiques. Alejandro R. Jadad Bechara (en), un médecin colombien, lui a donné son nom. Il travaillait dans un centre anti-douleur. Il est convaincu que les essais randomisés contrôlés sont très importants dans l'avancée des sciences médicales. Ce score va de 0 (très faible) à 5 (rigoureux).
En statistique, le test de Student, ou test t, est un ensemble de tests statistiques paramétriques où la statistique de test calculée suit une loi de Student lorsque l’hypothèse nulle est vraie.
En statistique, une taille d'effet est une mesure de la force de l'effet observé d'une variable sur une autre et plus généralement d'une inférence. La taille d'un effet est donc une grandeur statistique descriptive calculée à partir de données observées empiriquement afin de fournir un indice quantitatif de la force de la relation entre les variables et non une statistique inférentielle qui permettrait de conclure ou non si ladite relation observée dans les données existe bien dans la réalité. En ce sens, la taille de l'effet est complémentaire d'autres mesures statistiques telle que la valeur p d'un test t. Les mesures de taille d'effet sont particulièrement utiles pour conduire des méta-analyses qui exigent de comparer entre eux des résultats issus de différentes études scientifiques pour en faire la synthèse ou pour conduire des analyses de puissance destinées à établir si un protocole expérimental est adapté pour mesurer le phénomène que l'on cherche à étudier. Quelques exemples de mesures de taille d'effet : le r d'une corrélation de Pearson mesure la force d'association entre les deux variables que l'on cherche à corréler ; le d de Cohen ou d’ permet de caractériser la magnitude d'un effet associé dans une population donnée par rapport à une hypothèse nulle. Traditionnellement, un d autour de 0.2 est décrit comme un effet « faible », 0.5 « moyen » et 0.8 comme « fort » ; ω dans une ANOVA.  Portail des probabilités et de la statistique
La théorie ancrée (Grounded theory pour les anglophones) est une méthode systématique des sciences sociales, notamment l'ethnographie et la sociologie, dont la construction d'une théorie repose sur la collecte et l'analyse méthodique des informations,,. C'est une méthodologie de recherche de type inductive, par opposition à la méthode hypothético-déductive. En effet, elle vise à construire des théories non pas à partir d'hypothèses prédéterminées mais à partir des données du terrain et de situation de terrain que le chercheur a collecté ou peut collecter. Une étude utilisant la théorie ancrée commence habituellement par une question ou même par une collection de données qualitatives. Au fur et à mesure que le chercheur analyse les données collectées, il remarque que des idées, des concepts et des éléments se répètent. Il code alors ces répétitions, puis avec les analyses suivantes il groupe les codes en concepts, puis en catégories. Ces catégories peuvent devenir la base d'une nouvelle théorie. Cette théorie est beaucoup utilisée en « recherche qualitative », (par rapport à la « recherche quantitative » qui s'appuie sur des modèles prédéterminés d'analyse de chiffres et statistique), mais peut aussi s'appliquer dans la recherche quantitative, avec une dimension « postmoderne »,. Il existe de nombreuses variantes comme l'analyse par théorisation ancrée ou la Méthodologie de la théorisation enracinée.
En géométrie et trigonométrie, la triangulation est une technique permettant de déterminer la position d'un point en mesurant les angles entre ce point et d'autres points de référence dont la position est connue, et ceci plutôt que de mesurer directement la distance entre les points. Ce point peut être considéré comme étant le troisième sommet d'un triangle dont on connaît deux angles et la longueur d'un côté. Par analogie, la triangulation fait également référence à l'usage croisé de techniques de recueil de données en étude qualitative, notamment en sciences sociales.
La trilatération est une méthode mathématique permettant de déterminer la position relative d'un point en utilisant la géométrie des triangles tout comme la triangulation. Mais contrairement à cette dernière, qui utilise les angles et les distances pour positionner un point, la trilatération utilise les distances entre un minimum de deux points de référence.
En philosophie, la vérité par consensus est une représentation considérée comme fidèle à la réalité parce que faisant consensus. D'après le philosophe Nigel Warburton, ce n'est pas parce qu'il y a consensus sur une version des faits que cette version est fiable, d'autant que les individus sont crédules, manipulables et enclin à prendre leurs désirs pour des réalités.
Une vérité scientifique est une proposition construite par un raisonnement rigoureux, et vérifiée par l'expérience. Pour cette raison elle est réutilisable par d'autres scientifiques, qui pourront à partir d'elle énoncer d'autres propositions de ce type. La vision d'une vérité scientifique « pure » doit néanmoins être tempérée : les propositions reposent souvent sur des consensus établis par convention sur des questions pour lesquelles il n'y a pas assez d'éléments pour répondre. Différentes affirmations peuvent faire l'objet d'une controverse scientifique en attendant qu'un élément nouveau tranche définitivement. L'idée que la science permette d'accéder à une forme de vérité est présente aussi bien chez les philosophes que chez les scientifiques. Ainsi, le sous-titre du Discours de la méthode de René Descartes est « pour bien conduire sa raison, et chercher la vérité dans les sciences ». La vérité scientifique, pour mériter ce nom, ne doit pas dépendre d'une idéologie. (cf l'exemple emblématique de la controverse fameuse soulevée par les travaux de Lyssenko).
Le ministère fédéral de la Science et de la Recherche (Bundesministerium für Wissenschaft und Forschung, BMWF) est le département ministériel responsable de la recherche scientifique et de l'enseignement supérieur en Autriche. Il est dirigé depuis le 18 décembre 2017 par le conservateur Heinz Fassmann.
Le ministère de la Science et de l'Innovation d'Espagne (Ministerio de Ciencia e Innovación de España) était le département ministériel responsable de la recherche scientifique et du développement technologique d'Espagne. Le siège central du ministère se trouvait Calle Albacete, à Madrid.
Le ministère de l'Éducation universitaire, de la Science et de la Technologie (Ministerio del Poder Popular para la Educación Universitaria, Ciencia y Tecnología, en espagnol, littéralement, « ministère du Pouvoir populaire pour l'Éducation universitaire, la Science et la Technologie ») est un ministère du gouvernement du Venezuela, créé en 2014. Son titulaire actuel est Hugbel Roa depuis le 4 janvier 2017.
L'Alchimiste découvrant le phosphore (anglais : The Alchemist Discovering Phosphorus) est une peinture de 1771 du Britannique Joseph Wright of Derby. Elle est conservée au Derby Museum and Art Gallery de Derby. Le titre complet de la peinture est The Alchymist, in Search of the Philosopher’s Stone, Discovers Phosphorus, and prays for the successful Conclusion of his operation, as was the custom of the Ancient Chymical Astrologers, c'est-à-dire « L'alchimiste, à la recherche de la pierre philosophale, découvre le phosphore, et prie pour la réussite de son expérience, comme c'était la coutume des anciens astrologues chimistes ». Il a été suggéré que « l'alchimiste » désigne l'alchimiste de Hambourg Hennig Brandt qui a découvert le phosphore en 1669. Cette histoire a été souvent imprimée dans des livres de chimie du vivant de Wright et a été largement diffusée.
An Experiment on a Bird in the Air Pump (littéralement Une Expérience sur un oiseau dans une pompe à air) est une peinture à l'huile de 1768 du Britannique Joseph Wright of Derby appartenant à une série de peintures représentant des scènes éclairées à la lueur de bougies, peintes pendant les années 1760. Cette œuvre s'écarte des conventions de l'époque en dépeignant un sujet scientifique de façon révérencieuse. Ce style était cependant jusqu'alors réservé à des scènes d'inspiration religieuse ou historique. Wright était étroitement intriqué dans la représentation de la Révolution industrielle et des avancées scientifiques des Lumières. Bien que ses peintures fussent reconnues originales par ses pairs de l'époque, son statut de provincial et le choix de ses sujets a fait que son style n'a jamais été largement imité. La peinture est une possession de la National Gallery depuis 1863 et est toujours considérée comme l'un des chefs-d'œuvre de l'art britannique. L'œuvre représente un savant, précurseur des scientifiques modernes, reproduisant une des expériences de Robert Boyle avec une pompe à air, dans laquelle un oiseau est privé d'oxygène sous le regard d'une audience variée. Ce groupe montre des réactions diverses, mais la plupart d'entre eux montre plus d'intérêt à l'aspect scientifique qu'au sort de l'oiseau. Le personnage central dirige son regard hors champ, vers le spectateur, comme s'il l'invitait à rejoindre le dénouement de cette expérience.
L'Astronome (De astronoom), ou L'Astrologue, est un tableau de Johannes Vermeer (huile sur toile, 51 × 45 cm), peint vers 1668, et actuellement conservé au musée du Louvre.
La Clinique du docteur Gross est une peinture de 1875 par l'artiste américain Thomas Eakins. C'est une huile sur toile de 240 × 200 cm. Le dr. Samuel D. Gross (en), un professeur de soixante-dix ans, vêtu d'une redingote noire, enseigne à un groupe d'étudiants du Jefferson Medical College. Au sein du groupe se trouve un auto-portrait d'Eakins, assis à la droite de la rampe du tunnel, en train de faire une esquisse ou d'écrire. Au-dessus de l'épaule droite de M. Gross, le Dr Franklin Westest, responsable de la clinique, prend des notes sur l'opération. La signature de Eakins est peinte sur la toile, devant la table d'opération.
La Fée Électricité est une peinture de Raoul Dufy. « Mettre en valeur le rôle de l'électricité dans la vie nationale et dégager notamment le rôle social de premier plan joué par la lumière électrique », tel était l'objectif de la commande passée à Dufy par la Compagnie parisienne de distribution d'électricité pour être montrée au Pavillon de l'Électricité à l'Exposition Universelle de 1937. Aidé de son frère et de deux autres assistants, Raoul Dufy réalise, dans un hangar mis à sa disposition de la Centrale électrique de Saint-Ouen, une peinture aux dimensions monumentales, commencée en avril 1936 et achevée un an plus tard. Elle a été commandée par la mairie de Paris (compagnie d'électricité) dans le cadre de l’exposition internationale des arts et techniques dans la vie moderne du 25 mai au 25 novembre 1937 se déroulant à Paris. Le tableau est formé de 250 panneaux en contreplaqué indéformable parqueté sur bois et cintré (afin d'épouser la courbure de la charpente métallique du Palais de la Lumière de Robert Mallet-Stevens), mesurant chacun 2 m de hauteur sur 1,20 m de largeur. Il utilise une peinture à l'huile très légère, conçue par le chimiste Jacques Maroger, donnant une illusion de gouache et séchant très rapidement. Les personnages sont dessinés à l'encre de chine, puis les couleurs sont portées par-dessus. Ce tableau, avec ses 600 m2, a longtemps été considéré comme le plus grand tableau du monde, mais il a été détrôné largement par le Bauernkriegspanorama de Werner Tübke, tableau réalisé entre 1976 et 1987 et qui totalise 1 890 m2 sur une toile d'un seul tenant. Les deux-tiers du temps prévu pour l'exécution de la Fée Électricité ont été consacrés à la documentation sur les hommes et les machines. Dans la partie inférieure, 110 savants et penseurs, qui ont contribué à l'invention de l'électricité sont représentés,. Parmi eux : Zénobe Gramme Werner von Siemens William Thomson Marie Curie Pierre Curie Samuel Morse Thomas Edison Alexander Graham Bell Michael Faraday André-Marie Ampère Benjamin Franklin Thalès Johann Wilhelm Hittorf Émile Baudot Gustave Ferrié James Watt Alessandro Volta Archimède Aristote Galilée Léonard de Vinci Blaise Pascal James Prescott Joule Johann Wolfgang von Goethe Isaac Newton Henry Moseley Henri Poincaré Heinrich Rudolf Hertz Dmitri Mendeleïev Gottfried Wilhelm Leibniz
Le Géographe (De geograaf) est un tableau de Johannes Vermeer (huile sur toile, 53 × 46,6 cm) peint vers 1668-1669, aujourd'hui conservé au Städelsches Kunstinstitut, à Francfort-sur-le-Main.
Gutenberg inventant l’imprimerie est un tableau peint par Jean-Antoine Laurent en 1831. Il est conservé au musée de Grenoble depuis 1992. En 2014, il est prêté au musée des beaux-arts de Lyon dans le cadre de l'exposition L'invention du passé. Histoires de cœur et d'épée en Europe, 1802-1850.  Portail de la peinture  Portail des années 1830  Portail de la monarchie de Juillet  Portail Grenoble Métropole
La Leçon d'anatomie du docteur Tulp (ou La Leçon d'anatomie du docteur Nicolaes Tulp ou La Leçon d'anatomie du professeur Tulp) est une peinture à l'huile sur toile de 169,5 × 216,5 cm réalisée par Rembrandt en 1632.
La Leçon d'Anatomie du docteur Deyman (orthographe alternative Deijman) est un tableau fragmentaire de Rembrandt, peint en 1656 et conservé au Musée d'Amsterdam. C'est un portrait de groupe montrant une dissection.
Philosophe faisant un exposé sur le planétaire, ou Philosophe faisant un exposé sur le planétaire dans lequel le soleil est remplacé par une lampe (A Philosopher giving a Lecture on the Orrery in which a lamp is put in place of the Sun) de son nom complet, est un tableau du Britannique Joseph Wright of Derby, exposé pour la première fois en 1766. Il montre un conférencier faisant la démonstration d'un planétaire devant une assistance restreinte. La toile, aujourd'hui conservée au Derby Museum and Art Gallery de Derby, précède dans le temps Expérience sur un oiseau dans la pompe à air, sur un thème similaire.
Portrait de Nicolaus Kratzer est un tableau peint par Hans Holbein le Jeune en 1528. Il mesure 81,9 cm de haut sur 64,8 cm de large. Il est conservé au musée du Louvre à Paris[réf. nécessaire].
Le Portrait du médecin Alphonse Leroy est un tableau peint par Jacques-Louis David en 1783 et conservé au musée Fabre de Montpellier. L'attention portée sur les détails naturalistes et la tonalité vive de la toile marque chez David une influence des peintres flamands qu'il a vus lors de son séjour dans les Flandres en 1781. Jean-François Garneray, l'un de ses élèves, l'assista pour la peinture des étoffes et de la main. Le tableau est exposé au Salon de peinture et de sculpture de 1783, et fait aujourd'hui partie des collections du musée Fabre, qui l'a acheté en 1829.
Une leçon clinique à la Salpêtrière est un tableau peint en 1887 par l'artiste français Pierre Aristide André Brouillet.
Cicada 3301 (de l'anglais cicada voulant dire cigale) est une série de défis organisée sur Internet et mettant en jeu à titre principal des compétences en cryptographie et en informatique. Une série nouvelle de défis a été lancée chaque année autour du 5 janvier en 2012, 2013, 2014 et 2016, dans le but affiché de recruter « des individus très intelligents ». L'identité des personnes ou organisations qui organisent ces défis demeure inconnue.
L'éthique de l'intelligence artificielle est le domaine de l'éthique de la technologie propre aux robots et autres entités artificiellement intelligents. Il est généralement divisé en roboéthique, qui se préoccupe de l'éthique humaine pour guider la conception, la construction et l'utilisation des êtres artificiellement intelligents, et l'éthique des machines, préoccupée par le comportement moral des agents moraux artificiels. Pour l'aspect philosophique de l'intelligence artificielle, voir Philosophie de l'intelligence artificielle.
Le grand filtre a été défini en 1998 par Robin Hanson comme une suite de barrières qui nuit à l'émergence d'une civilisation extraterrestre durable dans le temps.,. Cet obstacle à surmonter pour toute civilisation peut être situé dans le passé ou bien dans le futur. Hanson a initialement élaboré sa théorie dans le but de trouver une explication au fait qu'aucune civilisation extraterrestre, ni même la trace de celle-ci, n'a été détectée dans l'univers observable. D'après lui, si l'humanité n'a pas été en mesure de trouver d'autres formes de vie dans l'univers observable, cela est dû au fait que le développement de la matière inerte vers la matière vivante et structurée est peut-être une vue géocentrée. Le grand filtre est ainsi l'une des solutions possibles au paradoxe de Fermi.
La philosophie de l'intelligence artificielle tente de répondre à des questions telles que : Une machine peut-elle agir intelligemment ? Peut-elle résoudre n'importe quel problème qu'une personne voudrait résoudre par la réflexion ?  L'intelligence humaine et l'intelligence artificielle sont-elles les mêmes ? Le cerveau humain est-il un ordinateur ? Une machine peut-elle avoir un esprit, état d'esprit, et une conscience similaire à celle de l'humain ? Peut-elle se sentir comment les choses sont ? Ces trois questions reflètent les intérêts divergents des chercheurs en IA, des scientifiques cognitifs et des philosophes, respectivement. Les réponses scientifiques à ces questions sont en cours de discussion. Elles dépendent des définitions de « l'intelligence » et de la « conscience », et de quelles « machines » il s'agit. Les propositions importantes en philosophie de l'IA comprennent : Test de Turing : « Si une machine se comporte aussi intelligemment qu'un être humain, alors elle est aussi intelligente qu'un être humain. ». La proposition Dartmouth : « Chaque aspect de l'apprentissage ou de toute autre caractéristique de l'intelligence peut être si précisément détaillé qu'une machine pourrait être créée pour simuler ceux-ci. ». L'hypothèse du système de symbole physique de Newell et Simon : « Un système de symbole physique dispose des moyens nécessaires et suffisantes pour l'action intelligente générale. ». L'hypothèse de l'IA forte de Searle : « L'ordinateur programmé de manière appropriée avec les bonnes entrées et sorties aurait ainsi un esprit exactement pareil que celui des êtres humains. ». Mécanisme de Hobbes : « En effet la raison, en ce sens, n’est rien d’autre que le calcul (c'est-à-dire l’addition et la soustraction) des suites des noms généraux sur lesquels nous nous sommes mis d’accord pour marquer et signifier nos pensées ».
La pseudo-science (du grec ancien : ψευδἡς : « faux, trompeur, mensonge » et du latin : scientia : « savoir ») est une connaissance ou une discipline qui est présentée sous des apparences scientifiques ou « faussement attribué[e] à la science » , mais qui n'en a pas la démarche, ni la reconnaissance. Elle se situe en opposition avec la science. Le terme de « pseudo-science » est souvent utilisé pour dénoncer la tromperie autour de certaines connaissances, c'est-à-dire ceux qui les présentent utilisent, sciemment ou non, des termes et des démarches qui semblent scientifiques ou logiques dans le but de s'attribuer le crédit que la science possède. Ils utilisent parfois un langage et des axiomes scientifiques, mais ne respectent pas les critères de la méthode scientifique, tels les principes intangibles de réfutabilité, de non-contradiction et de reproductibilité. La pseudo-science se rapproche de la para-science (« auprès de, à côté de la science ») dont le terme est perçu comme étant moins péjoratif, et exprimant l'idée de proximité ou de contiguïté avec la science. Les disciplines ou connaissances dites para-scientifiques sont, au mieux, trop peu étayées pour être considérées comme parties intégrantes de la science. Jusqu’à preuve du contraire (reconnaissance par les institutions scientifiques), les thèses se réclamant de la para-science sont donc à placer en pseudo-science.
L’agriculture biodynamique, aussi appelée communément biodynamie, est un système de production agricole issu du courant ésotérique de l'anthroposophie. Ses bases dogmatiques ont été posées par Rudolf Steiner dans une série de conférences données aux agriculteurs en 1924 et développées ensuite par des agriculteurs anthroposophes. L'agriculture biodynamique de Steiner ne donne aucun mécanisme explicatif, et son fondateur refuse la méthode expérimentale, en appelant uniquement à la foi de ceux qui voudront bien le croire. Cette démarche utilise le concept d'« organisme agricole » qui consiste à regarder toute exploitation agricole comme un organisme vivant, le plus diversifié et le plus autonome possible, avec le moins d'intrants en ce qui concerne le vivant (plants, semences, fumure…). Cette méthode utilise des préparations à base de plantes censées activer ou maîtriser les « forces cosmiques » des planètes, présentes dans le sol, afin de soutenir un bon processus végétatif et limiter le développement des parasites. C'est l'emploi de ces préparations reposant sur des principes ésotériques, dont la prise en considération de l'influence supposée des rythmes lunaires et planétaires, qui différencient principalement l'agriculture biodynamique de l'agriculture biologique. La biodynamie s'adresse à tous les domaines de l'activité agricole tels que la production de semences, l'élevage, l'apiculture, la viticulture biodynamique ou le jardinage, et a été mise en pratique notamment sur des domaines viticoles,, pour la production du thé en Inde ou encore du coton. Tout comme l'agriculture biologique, la biodynamie s'attache au fonctionnement biologique des sols et des végétaux et cherche l'amélioration de la qualité des produits, mais elle en diverge fortement, par ses dogmes et croyances occultes. Ce système de production n'est pas plus efficace que l'agriculture biologique, dont il respecte les principes de base. Du point de vue du scepticisme scientifique, les deux approches se distinguent par les dimensions pseudo-scientifiques de l'agriculture biodynamique, et l'efficacité revendiquée de la biodynamie relève de la pensée magique.
Le terme alignement de sites désigne une notion controversée relevant de la pseudo-science : celle de lignes imaginaires reliant certains grands sites préhistoriques. Elle est différente de l’alignement mégalithique, qui est un ensemble de menhirs posés sur une ou plusieurs rangées. Il faut au moins trois menhirs pour parler d'alignement et ceux-ci peuvent compter jusqu'à plusieurs milliers de pierres comme à Carnac. Un alignement de sites se caractérise par un alignement à grande échelle de monuments ou d’œuvres importantes. Les lignes ainsi tracées sont plus connues sous le nom de ley lines.
L'anthroposophie se présente comme une spiritualité mais aussi comme une science, dont les principaux traits (rejet de la notion de réfutabilité, de la science contemporaine et du matérialisme) lui confèrent toutefois un caractère pseudo-scientifique. Elle fut principalement développée par le philosophe occultiste autrichien Rudolf Steiner. Avant lui, des penseurs du XIXe siècle avaient aussi utilisé le terme « anthroposophie » pour décrire une nouvelle science de l'être humain mais l'acception que lui donne Rudolf Steiner se démarque explicitement de ses prédécesseurs. Partant de la conviction qu'il existe une dimension spirituelle accessible à l'intuition, Steiner développe une théorie de la connaissance (épistémologie) qui s'appuie d'abord sur son étude des écrits scientifiques du poète romantique allemand Goethe, qu'il développe dans sa thèse de doctorat puis dans différents ouvrages à caractère philosophique. Lorsqu'il débuta ses premiers travaux sur le sujet, Rudolf Steiner n'avait que 22 ans et avait été chargé d'éditer les œuvres scientifiques de Goethe. Une vingtaine d'années plus tard, lorsqu'il intègre la théosophie de Blavatsky, sa pensée s'enrichit des nombreuses traditions spirituelles : en particulier la mystique allemande, les traditions chrétiennes et des influences philosophiques orientales, qui imprégnaient fortement le mouvement théosophique, ainsi que de nombreuses traditions occultes, astrologiques, religieuses, mythiques et philosophiques de l'humanité. L'anthroposophie, telle que Rudolf Steiner l'a développée, va s'appliquer à de nombreux domaines : éducation (école Steiner-Waldorf), arts (eurythmie, peinture, théâtre, sculpture, musique, etc.), santé (médecine anthroposophique, pharmaceutique, cosmétique (Weleda), mouvement Camphill), économie (La Nef, GLS Gemeinschaftsbank (de), Triodos Bank), politique (tripartition sociale), agriculture (biodynamie), religieux (communauté des chrétiens). Les travaux de Steiner présentent comme une nouvelle forme de science toutefois largement critiquée pour son caractère pseudo-scientifique, notamment en matière de santé et d'agriculture. Steiner, qui s'oppose notamment à Kant, défend ainsi la capacité de l'être humain, par le travail de la pensée, du développement moral et d'une pratique méditative variée, de hisser sa conscience jusqu'à percevoir la dimension spirituelle des choses, par une intuition transcendantale qu'il qualifiera lui-même d'« occulte ». Aujourd'hui, l'institution principale est le Goetheanum, situé en Suisse, qui est le siège international de la Société anthroposophique universelle et de l'École libre de science de l'esprit qui comporte onze départements : sciences sociales, pédagogie, art de la scène, arts plastiques, médecine, belles-lettres, mathématique-astronomie, agriculture, sciences naturelles, jeunesse, anthroposophie générale. Plusieurs sociétés et établissements se réclament également de l'anthroposophie, comme l'entreprise pharmaceutique Weleda, l'organisme de certification Demeter, ou les différentes écoles « Steiner-Waldorf ».
L'astroarchéologie, créée par Erich von Däniken, est une discipline très controversée qui vise à rechercher dans les vestiges des civilisations du passé des « preuves » de visites extraterrestres sur notre planète. Considérée comme une pseudo-science et du charlatanisme par la communauté scientifique, l'astroarchéologie essaie de se présenter comme une science en s'appuyant sur l'archéologie pour étoffer l'ufologie. Très liée à la théorie des anciens astronautes (ou néo-évhémérisme), l'astroarchéologie s'intéresse particulièrement aux fresques préhistoriques, aux vestiges anciens, aux légendes et témoignages oraux ainsi qu'aux formes et aux reliefs étranges présents sur d'autres planètes.
L'astrologie est un ensemble de croyances et de pratiques qui n'entrent pas dans le domaine du rationnel basées sur l'interprétation symbolique des correspondances supposées entre les configurations célestes (la position et le mouvement des planètes dans le système solaire ou des constellations dans le cosmos) et les affaires humaines, collectives ou individuelles. Ce parallélisme conjecturé fait que l'astrologie, outre l'analyse de l'existant, est souvent utilisée comme outil divinatoire. Le présent article est consacré à l'astrologie occidentale, à laquelle renvoie généralement le terme astrologie. Il s'agit de celle qui est partie de Sumer, a influencé l'Égypte, a dominé Babylone, et s'est démocratisée en Grèce, d'où elle nous est parvenue grâce à la médiation des Arabes. Si, selon Wilhelm Knappich, la forme la plus ancienne sous laquelle a été pratiquée l'astrologie est l'astrologie mondiale, les horoscopes des revues ou les affinités des signes du zodiaque sont ses versions populaires actuelles à l'heure de l'individualisme moderne. Si ces dernières sont généralement considérées comme des échos lointains et déformés de l'astrologie historique, elles en restent la manifestation et l'expression la plus répandue. Les scientifiques considèrent l'astrologie comme une pseudo-science,, ou une superstition, l'ensemble des recherches menées depuis l'Antiquité ayant abouti au fait que l'astrologie se place, par sa méthode-même, en dehors du domaine scientifique. Face à ces considérations, les défenseurs de l'astrologie tendent presque tous à en réduire le caractère déterministe, élément nécessaire pour utiliser le mot de « science rationnelle ». Luc Bigé, Docteur ès science en biologie et chercheur en astrologie, affirme que l'astrologie n'est pas une « science rationnelle ». Selon d'autres astrologues, leur discipline n'a même pas pour but premier la prédiction de l'avenir, l'astrologie pouvant notamment être une voie du développement personnel,. Les croyances associées à l'astrologie restent populaires (voir ci-après). Un sondage mené en France indique qu'entre 30 et 40 pour cent des concitoyens accorderaient du crédit à l'astrologie.
L'étude statistique de l'astrologie cherche à déterminer si une corrélation peut être observée entre les phénomènes étudiés traditionnellement par l'astrologie (planètes, satellite, signes, domification, aspects, transits…) et des évènements objectifs dans la psychologie ou la destinée humaine. Ces recherches, qui tentent d'établir à travers la méthode expérimentale la validité de l'astrologie, ont parfois été jusqu'à revendiquer un statut d'astrologie scientifique mais elles sont généralement perçues comme une démarche pseudo-scientifique.
La biokinergie est une technique thérapeutique alternative, inventée dans les années 1980 par le kinésithérapeute-ostéopathe français Michel Lidoreau,. Comme pour toute technique non conventionnelle, le conseil national de l'ordre des masseurs kinésithérapeutes considère que son utilisation par un masseur-kinésithérapeute constitue une dérive thérapeutique.
Selon Wilhelm Reich, les bions sont des « vésicules » d'« énergie » représentant des stades intermédiaires entre la substance minérale et la substance vivante. Selon cet auteur, ils se forment continuellement dans la nature par un processus de désintégration de la matière organique et inorganique, processus que cet auteur prétend avoir reproduit expérimentalement. Ils sont chargés d'un concept appelé « orgone » et se transforment, toujours selon cet auteur, dans leur développement en protozoaires et en bactéries ou en bacilles T. La biologie ne retient pas cette hypothèse.
Pour la science de l'interaction entre lumière et objets biologiques, voir biophotonique Le biophoton (du grec βιο signifiant « vie » et φωτο voulant dire « lumière ») est un photon hypothétiquement d'origine biologique n'étant pas issu de produits d'une réaction enzymatique spécifique. Il s'agirait donc d'une chimiluminescence d'origine biologique qui se distingue de la bioluminescence par son absence de mécanisme enzymatique dédié et par une magnitude ou intensité ultra-faible (de l'anglais, ultra-weak spontaneous photon emission, ou parfois plus simplement ultra-weak photon emission). Cependant, plus récemment, il a été prétendu qu'en « récoltant l'énergie des biophotons », de supposées « cures » naturelles contre le cancer seraient possibles,. Les produits commercialisés et les services basés sur ces dernières affirmations sont à l'heure actuelle considérés au mieux comme de la pseudo-science sans fondements.
La biorésonance est un concept utilisé en médecine non conventionnelle, notamment en médecine quantique. Elle permettrait de faire des bilans de terrain, repérer éventuellement des « anomalies électromagnétiques » au sein des organes et de les rectifier en envoyant des signaux de très faible intensité. Ce concept de médecine non conventionnelle est non intrusif. Il représente la capacité de capter et d'émettre des rayonnements par les êtres vivants. La biorésonance englobe également les méthodes et appareils basés sur l'émission de rayonnements électromagnétiques destinés à rééquilibrer l'énergie corporelle. La biorésonance est considérée comme pseudo-scientifique, elle n'est pas reconnue en médecine scientifique. Les études scientifiques n'ont pas montré d'effet supérieur à l'effet placebo,,,,,,,. Cependant, les concepts de biorésonance ainsi que les différents appareils de bio-résonances sont une aide à la décision thérapeutique pour beaucoup de médecine non traditionnelle.
Les biorythmes sont une croyance qui affirme que depuis la naissance et jusqu’au moment de la mort, chaque être vivant subirait l'influence de trois cycles principaux : physique, émotionnel et intellectuel dont les phases sont dites positives ou négatives. Mais, à ces cycles naturels, se sont ajoutés d'autres comme les cycles intuitifs, esthétiques, spirituels ou passionnels qui donneraient lieu à des calculs pour évaluer les moments favorables et défavorables pour l'accomplissement de certaines actions. Le terme « biorythme » est une traduction récente du mot anglais : biorhythm adapté à la langue française en 1972. La théorie, ne pouvant être vérifiée par l'approche scientifique, est assimilée à une pseudo-science.
L'affaire Bogdanoff (ou affaire Bogdanov, les deux orthographes étant employées), selon le mot du physicien-mathématicien John Baez, est une controverse sur la validité des travaux scientifiques des producteurs et animateurs de télévision français Igor et Grichka Bogdanoff. Les critiques à l'origine de ce débat touchent à la fois à leur thèse de doctorat et à leur façon de vulgariser les sciences dans plusieurs ouvrages et à la télévision.
Une bougie d'oreille (également appelée bougie Hopi ou bougie auriculaire) est un tube de tissu recouvert de cire d’abeille. Elle se place dans le conduit auditif le temps d'une combustion complète. Cette pratique de médecine alternative, censée améliorer la santé générale et le bien-être, a été prouvée comme étant à la fois dangereuse et inefficace par la recherche médicale., L'affirmation d'un fabriquant, selon laquelle les bougies d'oreille seraient originaires du peuple des Hopis, est également fausse.
La boule magique, boule lavante ou boule de lavage est un produit utilisé pour le lavage du linge, d'inspiration pseudo-scientifique.
Un bracelet à hologramme est un petit bracelet en silicone contenant un petit hologramme. Les fabricants de ces bracelets disaient que l'hologramme « optimisait le flux naturel d'énergie autour du corps et améliorait ainsi la force, l'équilibre et la flexibilité des athlètes ». Seules des témoignages anecdotiques et peu vérifiés ont soutenues ces revendications et des tests menés par Australian Skeptics, l'université métropolitaine de Cardiff et l'école des sciences de la santé de l'Institut royal de technologie de Melbourne ont été incapable de discerner des effets sur les performances sportives.
Brilliant Light Power est une entreprise fondée par Randell Mills qui prétend avoir découvert une nouvelle source d'énergie. Cette énergie serait obtenu en forçant un atome d'hydrogène à se comprimer et serait donc l'expression d'un nouvel état de l'atome d'hydrogène qu'il appelle « hydrino ». Le résultat serait un seul proton autour duquel circulerait un électron qui se trouverait plus près que dans l'atome d'hydrogène à l'état fondamental. Les scientifiques ont majoritairement rejeté cette hypothèse parce qu'invalide.
La carte de la langue, ou encore la carte du goût, traduit une idée reçue selon laquelle la langue est divisée en zones spécialisées dans la perception d'une des saveurs primaires. Tandis qu'elle est encore largement enseignée, il a été démontré par différentes recherches scientifiques que toutes les zones de la langue sont capables de percevoir l'ensemble du spectre des saveurs sans prédisposition ou exclusivité particulière, sans toutefois remettre en cause le fait que certaines zones de la langue sont plus sensibles que d'autres à la perception de saveurs (notion de seuils gustatifs),.
Le catalyseur d'énergie, ou E-Cat (pour « Energy Catalyzer »), est un appareil supposé inventé en 2010 par deux Italiens, Andrea A. Rossi et le professeur Sergio Focardi. Cet appareil aurait eu pour but de fournir de l'énergie à partir d'un procédé de fusion froide. L'appareil serait, selon ses créateurs, le premier équipement destiné à l'usage domestique de fusion froide du nickel. Après plus de six ans d'étude et plusieurs millions d'euros investis, le fonctionnement de l'E-cat n'a pas été démontré par Rossi ni ses partenaires selon les standards de la communauté scientifique. L'invention bien que brevetée, n'est pas reconnue par la communauté scientifique, qui d'ailleurs ne voit en la fusion froide qu'une pseudo-science. Actuellement le projet est arrêté. Son initiateur, Andrea Rossi, une personne déjà impliqué dans des arnaques,,, est en procès contre le dernier investisseur du projet, Industrial Heat.
Le champ morphogénétique (ou « champ morphique », « résonance morphique » ou « champ de forme ») est une expression qui définit un champ hypothétique qui contiendrait de l'énergie ou de l'information sans être constitué de matière (atome, électrons, etc.). Ces champs seraient déterminants dans le comportement des êtres vivants notamment en ce qu'ils hériteraient d’habitudes de l’espèce par « résonance morphique » et que leurs actions influenceraient les dits « champs de forme ». Il s'agit d'un concept qui n'est pas scientifiquement validé, donc qui se limite à une croyance. Les promoteurs de cette croyance, dont actuellement Rupert Sheldrake, rapprochent cette notion de celle du champ de force, mais contrairement aux champs mesurables par des appareils de mesure, les champs de forme n'ont aucun support vérifiable ni réfutable, et échappent donc à toute expérimentation actuellement. L'idée a été adoptée par diverses pensées pseudo-scientifiques. Les ondes et champs de forme ont été popularisés par la spiritualité New Age.
Robert Charroux, de son vrai nom Robert Grugeau, né le 7 avril 1909 à Payroux (Vienne) (France) et mort le 24 juin 1978 à l'âge de 69 ans à Charroux (Vienne), est un journaliste, écrivain français et essayiste qui a développé des thèses pseudo-scientifiques et pseudo-historiques. Il est l'un des promoteurs de la théorie des anciens astronautes. Pourfendeur de l'archéologie préhistorique qui étudie les civilisations anciennes par les vestiges matériels et partisan de la théorie du complot, il a déclaré : « Ainsi le globe se révéla plein d'insolite dense et fascinant, (…), et le désir de faire partager mes vues, de dénoncer les impostures, me poussa à écrire mes livres. ». Il est inhumé au cimetière de Charroux dans le département de la Vienne sous un énorme menhir.
Un chembuster est un appareil à base d'orgonite, composant qui aurait des propriétés qui attireraient l'orgone, un médium pseudo-scientifique, et de six tubes en cuivre. Ses promoteurs disent qu'il peut nettoyer le ciel des chemtrails, une pollution atmosphérique volontaire dont l'existence n'a pas été prouvée et ressemblant aux trainées de condensation des avions à réaction.
La chirologie ou chirognomonie, discipline considérée[Par qui ?] comme une pseudo-science, étudie les corrélations entre la forme de la main ou les plis de sa peau et certaines tendances physiques, psychiques et mentales.
Un cloudbuster (ou cloud buster) est un appareil pseudo-scientifique conçu par le psychanalyste autrichien Wilhelm Reich, qui assurait que son invention pouvait produire de la pluie par une manipulation de ce qu'il appelle « l'énergie de l'orgone » présent dans l'atmosphère.
La complexité irréductible est la thèse selon laquelle certains systèmes biologiques sont trop complexes pour être le résultat de l'évolution de précurseurs plus simples ou « moins complets », du fait de mutations au hasard et de la sélection naturelle. Le terme a été inventé et défini en 1996 par le professeur de biochimie Michael Behe, un système de complexité irréductible étant « composé de plusieurs parties ajustées et interagissantes, qui contribuent chacune à sa fonction élémentaire, alors que l'absence d'une quelconque de ces parties empêche le fonctionnement du système ». Les exemples cités par Behe, la coagulation en cascade, le moteur (ou corps basal) des flagelles cellulaires et le système immunitaire, ne pourraient donc être le résultat de l'évolution naturelle : tout système précurseur au système complet ne fonctionnerait pas, et ne constituerait donc pas un avantage sélectif. De façon plus générale, cet argument est utilisé par les partisans du créationnisme et du dessein intelligent pour réfuter la théorie scientifique actuelle de l'évolution et prouver l'implication d'une cause divine ou intelligente dans la création de la vie. Ces thèses sont anciennes et reprennent l'argument téléologique de l'analogie du grand horloger. En dehors des systèmes biochimiques présentés par Behe, un exemple très couramment avancé de système trop complexe pour être le résultat de l'évolution est l'œil. La thèse de la complexité irréductible est rejetée par une très large majorité de la communauté scientifique ; elle est souvent considérée comme pseudoscientifique. Des travaux scientifiques ont montré que les exemples présentés par Behe ne répondaient pas à sa définition, et des précurseurs ont été identifiés pour certains d'entre eux. Les critiques considèrent que la thèse de la complexité irréductible est fondée sur une incompréhension du fonctionnement de ces systèmes biochimiques, et une méconnaissance des mécanismes de l'évolution (en particulier l'exaptation). Elle est également considérée comme un excellent exemple d'argumentum ad ignorandam (argument d'ignorance, sophisme par lequel on déclare fausse une proposition qui n'a pas été démontrée vraie). Bien qu'elle ait été rejetée en tant que théorie scientifique lors du procès de Dover, à l'issue duquel la cour a jugé que « La thèse du Professeur Behe sur la complexité irréductible a été réfutée par des articles scientifiques publiés dans des revues à comité de lecture, et a été rejetée par la communauté scientifique dans son ensemble » procès de Dover (p. 64), le concept de complexité irréductible reste un argument courant pour les partisans du dessein intelligent et d'autres créationnistes.
La contestation de la responsabilité du VIH dans le sida est un mouvement dissident hétérogène qui met en doute l'origine virale du sida. Ces dissidents, des activistes, journalistes et scientifiques, soutiennent que le consensus de la communauté scientifique sur le lien de cause à effet entre le VIH et le sida a provoqué des diagnostics inexacts, une peur collective, des traitements toxiques et un gaspillage des fonds publics, en même temps qu'un mésusage sans précédent des normes et méthodes scientifiques. Notamment Kary Mullis, lauréat du prix Nobel de chimie en 1993, le virologue Peter Duesberg et The Perth Group (en) affirment qu'il n'existe pas de preuve formelle du lien entre le VIH et le sida,. Cependant, la majorité de la communauté scientifique considère que les éléments démontrant la causalité entre le VIH et le sida sont concluants, et rejette ces théories, qui sont, pour elle, négationnistes et essentiellement basées sur des arguments pseudo-scientifiques et des théories du complot.
La craniométrie est une pratique relevant de la biométrie et consacrée à l'étude des mensurations des os du crâne. Elle se base sur l'hérédité des traits morphologiques (on ressemble à ses parents, et des caractères comme le nez des Bourbons ou le menton des Habsbourgs se transmettent génétiquement), de sorte qu'avant les travaux sur l'ADN, ou en l'absence d'ADN dans le cas des fossiles, elle sert à définir les affinités entre individus ou populations. Elle est utilisée en médecine légale pour identifier l'origine géographique d'un crâne, ce qui requiert des bases de données comparatives larges. Historiquement, elle se distingue de la phrénologie, ou cranioscopie, qui était une tentative ancienne de localiser des fonctions cérébrales (en fait des vices et des vertus) dans le cerveau, et de la physiognomonie, qui est l'étude des traits du visage. Ces trois disciplines ont toutes prétendu pouvoir prédire l'intelligence d'un individu et, en ce qui concerne la craniométrie, en se basant sur une corrélation, vraie au plan évolutif mais fausse au plan individuel, entre capacité crânienne et performances cognitives. L'anthropologie physique a très tôt réfuté la phrénologie, qui relevait plutôt de la neurologie, et n'a jamais considéré la physiognomonie comme une science sérieuse (alors qu'elle est toujours utilisée par certains cabinets de recrutement).  La craniométrie a donc été utilisée par les experts qui voulaient déterminer de quelle race appartenait un individu et, au XIXe siècle, le fait que le cerveau de l'homme, après correction pour la taille du corps, fasse 100 g de plus en moyenne que celui de la femme a permis à des anthropologues comme Paul Broca ou à son élève comme Gustave Le Bon de vouloir y voir une preuve objective d'une prétendue infériorité intellectuelle de la femme, idée qui a perduré tout au long du XXe siècle avant d'être infirmée dans les années 1980 avec le développement de l'imagerie médicale qui permet les premières études sérieuses sur les différences structurales entre le cerveau de l'homme et de la femme.
Le créationnisme désigne au sens large une doctrine d'ordre religieux selon laquelle un ou plusieurs êtres divins sont les créateurs de la vie, et qui s'oppose généralement au principe d'évolution du vivant fondé sur la sélection naturelle. Depuis que sont décrits des phénomènes évolutifs en astronomie, en géologie et en biologie, les créationnistes entretiennent la polémique à cet égard, car l'explication scientifique de ces phénomènes n'est pas compatible avec leur interprétation des textes religieux. Le débat relève d'enjeux politiques importants, notamment en matière d'enseignement, de recherche scientifique, de liberté d'opinion et de croyances. Les courants créationnistes montrent une grande diversité, depuis ceux qui soutiennent le fixisme en élaborant une théorie de la nature théiste (« créationnisme Jeune-Terre » et « Vieille-Terre ») à ceux aux positions plus déistes qui embrassent la théorie transformiste (hypothèse du dessein intelligent et de la panspermie dirigée). Le « créationnisme Jeune-Terre » lit la Bible ou le Coran comme s'ils étaient des livres de sciences naturelles et d'histoire, véhiculant la croyance selon laquelle le récit de la création de l'univers tel que fourni par les textes religieux, donne une description littéralement exacte de l'origine de l'Univers. Cette interprétation littérale de textes comme la Genèse s'appuie sur la conviction que ces textes ont été « dictés par Dieu » comme vérités absolues, définitives et indiscutables (cas de certaines Églises protestantes, majoritaires dans le Bible Belt des États-Unis). Ce courant de pensée est généralement associé au refus de toute idée d'évolution biologique et géologique,,. La plupart des traditions religieuses monothéistes (judaïsme, christianisme et islam) postulent la création du monde par Dieu. La lecture fondamentaliste est refusée par la majorité des Églises chrétiennes actuelles, qui privilégient une lecture herméneutique. Pour les catholiques, la création de l'univers par Dieu ne s'oppose pas en soi à l'évolution : la création est avant tout la relation entre les créatures et un Créateur, leur premier principe. Le créationnisme ne se restreint néanmoins pas aux seuls courants interprétant des textes religieux de façon littérale, mais inclut également divers créationnismes dits « Vieille Terre » qui admettent que l'univers a plus de 6000 ans, les partisans du dessein intelligent, des courants qui admettent des aspects de la théorie de l'évolution mais en excluent l'Homme, l'évolution théiste qui admet que l'évolution des espèces a lieu mais qu'elle est dirigée ou influencée par des divinités ou un Créateur qui donnerait naissance à l'univers, au vivant et aux mécanismes leur permettant ensuite d'évoluer par eux-mêmes.
Le créationnisme Jeune-Terre est la forme la plus radicale du créationnisme, qui interprète la Bible comme un livre de sciences naturelles et d’histoire, véhiculant la croyance selon laquelle le récit de la création de l’univers tel que fourni par les textes religieux, donne une description littéralement et scientifiquement exacte de l’origine de l’Univers. Cette interprétation littérale de textes comme la Genèse s’appuie sur la conviction que ces textes ont été « dictés par Dieu » comme vérités absolues, définitives et indiscutables. Selon cette interprétation, le ciel et la Terre ont été créés le 23 octobre -4004, Adam et Ève ont littéralement vécu et péché dans le jardin d'Eden, et les fossiles retrouvés sont des objets créés tels quels par Dieu comme ébauches ou pour éprouver la foi des êtres humains, ou encore par le Diable pour les tromper. Ce courant de pensée est généralement associé au refus de toute idée d’évolution biologique et géologique,,. Depuis que sont enseignés dans les écoles des phénomènes évolutifs en astronomie (concernant les origines de l’univers et les phénomènes cosmiques), en géologie (concernant le passé et le présent de la Terre) et en biologie (concernant les espèces vivantes passées et présentes), les créationnistes Jeune-Terre entretiennent la polémique à cet égard, car l’explication scientifique de ces phénomènes n’est pas compatible avec leur interprétation littérale des textes religieux. Les Jeune-Terre ont tenté d'élaborer une « géologie du déluge », dans le but de rendre crédible leur interprétation. Le créationnisme Jeune-Terre est partagé par une partie importante de l'opinion publique américaine, notamment dans le Bible Belt. Il a été combattu dans certains États américains, ce qui a donné lieu aux célèbres procès du singe. Les créationnistes Jeune-Terre sont moins nombreux que les créationnistes Vieille-Terre. Peu de mouvements chrétiens partagent leurs croyances, et comme le Coran ne précise pas l'âge de la Terre, les musulmans adhèrent dans leur majorité aux conclusions des scientifiques estimant l'âge de la Terre à 4,6 milliards d'années.
Le créationnisme Vieille-Terre est une forme de créationnisme qui accepte de reconnaitre l'âge avancé de la Terre, tout en adhérant à la croyance selon laquelle l'événement du Big Bang serait un événement créateur avec Dieu pour auteur. Il se distingue et s'oppose sur certains points au créationnisme Jeune-Terre.  Portail des religions et croyances
Don Croft, né en 1949, à Kansas City, Missouri, est un inventeur et homme d'affaires américain. Il lutte principalement contre les chemtrails (des traînées de condensation qui seraient d'origine chimique) et les organisations qui en seraient à l'origine. La théorie derrière les chemtrails est souvent assimilée aux théories de la conspiration.
La cryptobotanique est l'étude de plantes exotiques dont l'existence n'a pas été validée par la communauté scientifique, alors qu'elles sont décrites dans des mythes, dans la littérature ou des signalements anciens non confirmés.
La cymatique (du substantif grec « κῦμα » « vague », par l'allemand « Kimatik ») est une théorie ésotérique proposée au XXe siècle qui relie les vibrations sonores, mises en évidence sur des plaques ou à la surface de fluides, avec l'ensemble des phénomènes naturels. La cymatique fonde les discours de la sonothérapie ; elle a servi à la genèse d'œuvres d'art. En musique, l'Alphabet pour Liège de Karlheinz Stockhausen en adopte et en présente les principes ; dans les arts plastiques, ses expériences spectaculaires servent à des installations et des vidéos.
Le dessein intelligent (intelligent design en anglais) est une théorie pseudo-scientifique,, qui prétend que « certaines observations de l'Univers et du monde du vivant sont mieux expliquées par une cause « intelligente » que par des processus non dirigés tels que la sélection naturelle. » Cette thèse a été développée par le Discovery Institute, un cercle de réflexion conservateur chrétien américain. Le dessein intelligent est présenté comme une théorie scientifique par ses promoteurs mais, dans le monde scientifique, il est considéré comme relevant de la pseudo-science, par des arguments aussi bien internes à la biologie (les promoteurs du dessein intelligent apparaissant aux biologistes comme ne tenant pas compte de nombreuses observations) qu'épistémologiques (en particulier le critère de réfutabilité de Karl Popper). La plupart des commentateurs et des scientifiques y voient une résurgence du créationnisme, dissimulée sous une apparence de scientificité ; le biologiste britannique Richard Dawkins le désigne même comme un « créationnisme affublé d'un costume bon marché ». Le dessein intelligent est désormais classé aux États-Unis dans les théories néo-créationnistes, en particulier à la suite de la publication du Wedge document. Le dessein intelligent ne s'applique qu'au domaine de la biologie, et ne traite pas de l'origine de l'Univers. Ainsi, il ne doit pas être confondu avec le principe anthropique. Les principaux acteurs du mouvement du dessein intelligent acceptent un Univers âgé de plus de 13 milliards d'années et la théorie du Big Bang, avec pour opinion personnelle qu'il est causé par le Dieu de la Bible, mais rejettent le mécanisme de mutation aléatoire couplé à une sélection naturelle comme moteur de l'apparition de nouvelles espèces. William Dembski, l'un des principaux fondateurs du concept du dessein intelligent, reconnaît l'existence de preuves solides pointant vers un ancêtre commun à toutes les espèces vivantes et se déclare ouvert à cette idée, tandis que des publications du Discovery Institute remettent en question ce point ou le rejettent, en usant principalement de l'interprétation créationniste de l'explosion cambrienne.
La détoxication est le processus par lequel un organisme inactive les substances toxiques d'origine interne ou externe, qui consiste, d'une part, en la réduction de l'activité pharmacologique ou toxicologique de la substance, en général par un processus enzymatique et, d'autre part, par la solubilisation de la substance, ce qui en facilite élimination rénale. L'assistance pharmacologique des capacités de détoxication de l'organisme est un objectif à long terme de la recherche biomédicale. La modulation des concentrations de molécules impliquées dans le métabolisme de détoxication (en particulier le glutathion et la vitamine C) font partie des objectifs de la nutrition clinique (en). Certains médias et certaines médecines non conventionnelles recommandent des pratiques visant une hypothétique stimulation de la détoxication, sur des bases plus ou moins rationnelles. Récemment, la mode de la cure de « détox » a connu un grand succès public, mais n'est fondée sur aucune base scientifique et n'a aucune efficacité prouvée.
La dianétique est une théorie d'éveil spirituel ou de développement personnel créée par l'auteur de science-fiction et fondateur de la Scientologie L. Ron. Hubbard. S'appuyant sur de nouveaux concepts de psychologie non enseignés en Faculté de psychologie, elle vise à l'identification et à la réduction systématique d'images mentales négatives inconscientes nommées engrammes. « Dianétique » est un néologisme qui veut signifier « à travers l'âme » (du grec dia « à travers » et noos « âme »). Selon ses détracteurs, le suffixe « ique » sert aux pseudo-sciences à suggérer l'idée d'une science exacte. Aucune étude scientifique, à ce jour, ne valide les présupposés et les pratiques de la dianétique,,.
La divination mathématique se sert « des calculs, des nombres, des quantités élémentaires et des noms »[réf. souhaitée] pour prévoir des événements à venir. Cette activité était notamment pratiquée par les astrologues, qui ont longtemps porté le nom de « mathématiciens »[réf. souhaitée]. Cependant, cette pratique va se différencier progressivement de l'astrologie en utilisant des méthodes spécifiques[Lesquelles ?].
Le drainage lymphatique manuel existe sous différentes formes. L'une d'entre elles est une technique thérapeutique alternative, inventée dans les années 1930 par Emil Vodder (en).
La drapétomanie était une prétendue maladie mentale, décrite et inventée par le médecin américain Samuel Cartwright en 1851, pour expliquer l'évasion vers le Nord des Etats-Unis des esclaves . Son article, ré-imprimé dans le Sud, suscite de nombreuses moqueries et satires au Nord (dont l'une dans l'éditorial du Buffalo Medical Journal, en 1855  ). L'architecte Frederick Law Olmsted, dans son livre de 1856, A Journey in the Seaboard Slave States, prenant appui sur le constat de la fuite de domestiques « engagés » (une sorte de servitude), affirme avec humour que la prétendue maladie serait en fait d'origine blanche, et introduite en Afrique par les commerçants . La drapétomanie est aujourd'hui citée comme exemple de théorie pseudoscientifique et racialiste.
L’effet Allais, aussi appelé anomalie d’Allais, est un possible phénomène physique qui est parfois observé lors d'éclipses solaires et qui est lié à des perturbations de mouvement de pendules ou d'instruments de mesure gravitationnelle. Il fut observé pour la première fois de façon fortuite par un collaborateur de l'économiste Maurice Allais lors de l'éclipse du 30 juin 1954, lors de mesures de l'azimuth du plan d'oscillation d'un pendule paraconique. Il n'existe pas de consensus de la communauté scientifique sur l'existence (ou la non-existence) du phénomène et sur son interprétation.
L'effet CREIL est le nom donné par ses auteurs à un phénomène physique hypothétique jamais observé expérimentalement d'interaction entre lumière et matière. L'acronyme CREIL signifie Coherent Raman Effect on Incoherent Light, soit « Effet Raman Cohérent agissant sur une Lumière temporellement Incohérente ». L'effet CREIL a été proposé par deux frères, Jacques et Jean Moret-Bailly et n'a fait l'objet d'aucune publication dans des revues scientifiques internationales d'astronomie à comité de lecture. Cependant, il n'est pas une alternative au Big-bang et ne s'intéresse qu'à l'explication des glissements de fréquences observés sur les rayonnements lointains.
Les élixirs floraux de Bach, parfois désignés simplement sous les noms de fleurs de Bach ou de remèdes de Bach, sont des macérations alcooliques de plantes, dits "élixirs floraux", réalisés à partir des fleurs de trente-sept espèces de plantes, auxquelles s'ajoute l'eau de roche, ce qui donne donc 38 « élixirs » au total. Par extension, ils désignent la pratique thérapeutique qui les utilise, conceptualisée entre 1928 et 1936 par le Dr Edward Bach, médecin et homéopathe anglais. La conception de cette pratique repose sur la croyance qu'à chaque état psychologique négatif défini correspondrait une espèce de plante, selon un choix intuitif d'Edward Bach ne reposant pas sur la méthode expérimentale. Par ailleurs, le classement qu'il établit en sept états psychologiques négatifs, comprenant trente-huit sous-états psychologiques négatifs, est lui aussi arbitraire et ne repose sur aucune étude empirique. Les essais cliniques systématiques de remèdes à base de fleurs de Bach sur les êtres humains adultes n'ont pas montré d'effet au-delà de l'effet placebo. Ainsi, sa promotion en tant que thérapie efficace relève de la pseudo-science.
L'Emotional Freedom Technique (littéralement : « Technique de liberté émotionnelle»), abrégée en EFT, représente une pratique psycho-corporelle fondée aux États-Unis en 1993 par un ingénieur du nom de Gary Craig aujourd'hui en retraite et qui selon son auteur aurait valeur de psychothérapie. C'est une technique de la même famille que la TAT, Tapas Acupressure Technique. L'EFT a pour but d’alléger les souffrances émotionnelles et psychologiques des personnes. Elle se pratique par l'entretien thérapeutique et la stimulation de points situés sur le trajet des méridiens répertoriés par la médecine chinoise, d’où cette appellation de technique dite « méridienne ».
Masaru Emoto (江本 勝), né le 22 juillet 1943 et mort le 17 octobre 2014, est un auteur japonais connu pour ses théories, qui n'ont jamais été confirmées, sur les effets de la pensée et des émotions sur l'eau.
L'étiopathie est une médecine non conventionnelle manuelle dans la tradition des rebouteux, proche de l'ostéopathie et de la chiropratique.
Nicolas Camille Flammarion, né le 26 février 1842 à Montigny-le-Roi (Haute-Marne) et mort le 3 juin 1925 à Juvisy-sur-Orge (Essonne), est un astronome français. Il fut un membre très actif de maintes sociétés savantes et d’associations pour la vulgarisation des sciences positives. Ses découvertes scientifiques l’ont placé et le maintiennent encore au XXIe siècle au premier rang des vulgarisateurs français, en mettant à la portée du grand public les problèmes de l'astronomie, de l’atmosphère terrestre et du climat. Il a, par les côtés mystiques et spirites de certaines de ses œuvres, ajouté à la notoriété de son nom.
La Flat Earth Society (aussi appelée International Flat Earth Society ou International Flat Earth Research Society, IFERS) est une organisation soutenant l'idée de la Terre plate, fondée en 1956 par l'Anglais Samuel Shenton puis dirigée ensuite par l'Américain Charles K. Johnson. À la mort de Johnson en 2001, l'organisation a connu quelques années d'inactivité jusqu'à sa reprise en 2004 par Daniel Shenton (actuel dirigeant). Son siège était basé à Lancaster en Californie.
La fringe science (littéralement « science marginale ») est un terme anglo-saxon désignant les recherches scientifiques, au sein de sciences académiques reconnues, qui s'éloignent significativement des théories généralement admises. La fringe science est donc la science à la frontière des disciplines scientifiques actuellement établies. Elle s'oppose à la mainstream science, c'est-à-dire la science conventionnelle (mainstream signifie « courant principal » en anglais). Les scientifiques mainstream considèrent typiquement que la fringe science repose sur des concepts très spéculatifs ou réfutés, contrairement aux protosciences qui représentent des pistes plausibles pour faire émerger des sciences nouvelles.
Les géants constituent une figure familière dans plusieurs mythologies et folklores. L'hypothèse de leur existence a perduré tardivement sur la foi de témoignages et d'éléments matériels ambigus ou mal interprétés. Les progrès des connaissances scientifiques ont conduit à considérer ce genre comme cryptide, même si certaines maladies humaines (acromégalie, gigantisme...) peuvent provoquer une croissance hors norme. La croyance en des « géants primordiaux » peuplant jadis la Terre continue parfois d'être exprimée dans des textes relevant de l'ésotérisme et de l'occultisme.
La géobiologie est la discipline qui prétend traiter des relations de l'environnement, des constructions et du mode de vie avec le vivant , de l'ensemble des influences de l'environnement sur le vivant, et notamment des ondes liées aux champs magnétiques et électriques, courants d'eau souterrains, réseaux dit « géobiologiques », failles géologiques, etc. Étymologiquement, il s'agit d'une association de la racine géo (la terre) et biologie (l'étude de tout ce qui est vivant). Elle se distingue nettement de son homonyme, la géobiologie scientifique, branche de la paléontologie liée à l'histoire naturelle. L'Association française pour l'information scientifique et en général les physiciens, les médecins et les géologues classent la géobiologie comme une pseudo-science,. Ses méthodes d'investigation ne suivent pas une démarche scientifique et ses résultats n'ont pas de confirmations par les disciplines scientifiques. Elle est ainsi clairement décrite par les scientifiques comme un champ d'étude irrationnel et, de ce fait, elle est souvent assimilée à un ensemble de croyances. Elle tente en effet d'aborder les liens existants entre un lieu et la vie humaine, animale ou végétale, avec une approche éloignée de la méthode scientifique. Est dénoncée notamment l'utilisation abusive du langage scientifique pour décrire des concepts sans fondement expérimental ou théorique et la prétention qu'ont certains de ses promoteurs à lui conférer un statut de discipline scientifique et à délivrer des diplômes prétendument scientifiques alors que la géobiologie n'a aucun des attributs d'une science .
La graphologie est une technique d'analyse de l'écriture dont la scientificité n'est pas établie qui affirme pouvoir déduire systématiquement des caractéristiques psychologiques de la personnalité d’un individu à partir de l’observation de son écriture manuscrite. Celle-ci est décrite sous forme de signes graphiques qui, groupés en syndromes, amènent le graphologue à faire correspondre un type d'écriture ou de mouvement d'écriture, à un type de personnalité ou de caractère dans une classification préexistante. Le fondement de la graphologie est analogue à l'idée que les formes du visage, du corps ou des gestes corporels permettant de faire des déductions psychologiques : on pourrait faire de même à partir de la trace graphique laissée par les mouvements d'écriture. Elle fonctionne selon les mêmes principes que les tests projectifs. Elle s'est développée parallèlement à la morphopsychologie. Par la suite, la graphologie a été utilisée pour diagnostiquer des pathologies, ou comme technique d'investigation dans des procédures de recrutement, avec un succès très contesté,. La grande majorité des études reposant sur une méthode expérimentale scientifique démontrent que les hypothèses et les résultats obtenus par la graphologie sont invalides, elle est aujourd'hui considérée comme une pseudo-science. Malgré l'accumulation de preuves contre sa validité prédictive, elle a longtemps été utilisée dans certains pays, notamment en France comme outil d'évaluation de candidats à l'embauche. La graphologie doit être distinguée de l'expertise en écriture qui est une technique d'investigation visant à attribuer un écrit manuscrit à son scripteur, que ce soit pour l'identification judiciaire de l'auteur d'un écrit anonyme ou pour l'attribution historique de documents manuscrits.
L’halothérapie ou la thérapie par le sel est l'usage de sel dans le but de créer des conditions hypothétiquement favorables à la guérison. Le contact avec le sel peut se faire de différentes façons : application de compresses, ingestion ou respiration. Dans ce dernier cas, la technique exige un lieu relativement hermétique (mine de sel ou grotte, par exemple) ayant une atmosphère à concentration élevée de sel, dans le but de créer des conditions ayant d'hypothétiques bénéfices pour la santé de l'appareil respiratoire, notamment le soulagement ou la guérison de l'asthme, (certains auteurs utilisent le terme « spéléothérapie » dans ce cas).
Le harcèlement électronique, torture électromagnétique ou harcèlement électromagnétique (HEM) consiste en l'emploi de rayonnements électromagnétiques,de masers, d'armes à énergie dirigée, des radars, des techniques de surveillance et transmission de sons par effet de Frey (en) et ostéophonie à des fins de harcèlement, et de contrôle mental. Ces bombardements pourraient être le fait de gouvernements, de la police, de l'armée... mais aussi de toute personne considérée comme appartenant à un groupe maléfique (journalistes, politiciens...), voire située dans le voisinage direct habituel. Dès lors, les détracteurs du harcèlement électronique peuvent parler d'une théorie du complot. Les personnes qui prétendent être victimes de ces expériences se dénomment « personnes visées », « individus ciblés » ou TI (de l'anglais « Targeted Individual ». En Amérique (États-Unis et Canada principalement), beaucoup ont rejoint des groupes de soutien et de défense.
Burkhard Heim (né le 9 février 1925 à Potsdam, décédé le 14 janvier 2001 à Northeim) était un allemand ayant réalisé des travaux assez médiatisés en physique. Il fut gravement handicapé à l'âge de 19 ans à la suite de l'explosion survenue dans le laboratoire de recherche où il travaillait, pendant la Seconde Guerre mondiale. L'explosion lui causa l'amputation de ses deux mains et le laissa quasiment sourd et aveugle[réf. nécessaire]. Il put néanmoins poursuivre ses études à l'université de Göttingen, d'où il fut diplômé à la suite d'un travail sur la nébuleuse du Crabe[réf. nécessaire]. Il intégra ensuite la Société Max-Planck qu'il quitta à la fin des années 1950. C'est dans ces circonstances qu'il effectua, dans le plus grand isolement, les travaux plus tard présentés sous le nom de théorie de Heim, présentée comme théorie du tout, et censée concilier relativité générale et physique quantique. Mais cette théorie est considérée aujourd'hui comme une pseudoscience. Notices d'autorité : Fichier d’autorité international virtuel • International Standard Name Identifier • Bibliothèque du Congrès • Gemeinsame Normdatei • Bibliothèque universitaire de Pologne • WorldCat  Portail de l’Allemagne  Portail du scepticisme rationnel
Richard C. Hoagland (né en avril 1945) est un essayiste et théoricien du complot américain. Il s'intéresse à différents thèmes concernant l'astronomie que la majorité des astronomes considèrent comme extravagants et relevant de la pseudo-science. Ses affirmations s'appuient sur l'idée qu'il existerait dans le système solaire une civilisation avancée antérieure à la nôtre, en particulier sur Mars et sur la Lune.
L’homéopathie ou homœopathie (du grec όμοιος / hómoios, « similaire » et πάθος / páthos, « souffrance » ou « maladie ») est une pratique pseudo-scientifique de médecine alternative inventée par Samuel Hahnemann en 1796. Le principe de fabrication des médicaments homéopathiques est de diluer des substances qui, si elles étaient concentrées, provoqueraient des symptômes similaires à ceux du patient. Mais en raison des dilutions extrêmes utilisées, les remèdes homéopathiques sont dépourvus de principes actifs. L'homéopathie ne constitue pas un traitement plausible, étant donné que les principes sur lesquels la méthode de traitement repose, que ce soit à propos du fonctionnement des médicaments, des maladies, du corps humain, des fluides et des solutions, sont contredits par un large ensemble de découvertes faites en biologie, psychologie, physique et chimie dans les deux siècles suivant son invention,,,,. Les études cliniques à grande échelle ont montré que l'homéopathie ne présente aucune efficacité supérieure à l'effet placebo, et ce pour toutes les maladies considérées, ce qui suggère que les effets subjectifs ressentis sont dus à l'effet placebo et à l'évolution naturelle de la maladie,,,,. Bien que certains articles aient rapporté des résultats positifs,, de multiples revues systématiques indiquent que cela est dû au hasard, à des méthodes de recherche discutables, ou encore aux biais de publications. La persistance de l'utilisation de l'homéopathie par certaines populations en dépit de son manque avéré d'efficacité, a fait que dans les communautés scientifiques et médicales, elle est considérée comme une absurdité, du charlatanisme, et une imposture. Elle a été critiquée sur le plan éthique lorsqu'elle se fait au détriment de traitements efficaces,, et l'Organisation mondiale de la santé met en garde contre son utilisation dans le traitement de maladies graves comme le SIDA ou bien le paludisme. Son utilisation lors de désordres psychogènes ou psychosomatiques permet cependant aux médecins de bénéficier d'un traitement sans effets secondaires, et dont l'efficacité repose sur l'effet placebo, afin d'éviter de prescrire des thérapeutiques conventionnelles potentiellement iatrogènes ; cette méthode pose de sérieux problèmes éthiques et fait l'objet de nombreux débats. Entre autres, des évaluations par le National Health and Medical Research Council (en) en Australie, le House of Commons Science and Technology Committee (en) au Royaume-Uni et l'Office fédéral de la santé publique en Suisse ont toutes conclu que l'homéopathie est inefficace et se prononcent contre tout financement de cette pratique,. En septembre 2017, le Conseil scientifique des académies des sciences européennes, qui réunit l'ensemble des académies des sciences, a publié un rapport dénonçant d'une part l'inefficacité de cette méthode alternative par rapport à l'effet placebo, mais aussi que « la promotion et l'usage de produits homéopathiques posent des risques significatifs. Tout d'abord, en entrainant un délai pour le patient de rechercher des soins médicaux appropriés et basés sur des preuves ou même d'empêcher les patients d'avoir simplement recours à des soins médicaux »,,.
Le modèle de la conscience sur huit circuits/niveaux est une théorie sur la structure de la conscience proposée par le Dr Timothy Leary. Leary pensait que l'esprit est le mieux représenté comme un assemblage de huit « niveaux » ou « circuits », aussi appelés « vitesses » ou « mini-cerveaux ». Chaque niveau représenterait une étape supplémentaire d'évolution par rapport au précédent. Ce modèle fut influencé par ceux proposés par les travaux du psychologue suisse Jean Piaget. Leary a présenté une structure basée sur la latéralisation des fonctions cérébrales, mais en poussant ses conclusions bien plus loin que les standards de la neuropsychologie académique. Les quatre premiers niveaux, selon Leary, résideraient dans le lobe cérébral gauche, concernent la survie des organismes dans leur environnement ; les quatre autres, résidant dans le lobe droit toujours selon Leary, seront utiles dans l'évolution future de l'espèce, et restent actuellement inactifs dans la majorité de l'humanité.
L’hygiène raciale (en allemand Rassenhygiene) est un concept raciste introduit par le médecin Alfred Ploetz en 1904 dans un article de la publication Archives de biologie raciste et sociale. D'emblée, les propositions de ce texte visant à soigner une population plutôt qu'un individu le placent dans le registre des pseudo-sciences. L'exploitation de ce concept, son instrumentalisation et son application par le pouvoir national-socialiste ont fait l'objet de nombreuses études,, et de rappels tant institutionnels que médiatiques,,. Des décrets et lois de l'année 1933 seront précurseurs du programme Aktion T4 et d'un programme de stérilisation contrainte, programmé par la Loi allemande sur la stérilisation forcée du 14 juillet 1933. Cette action programmée sous le Troisième Reich sera l'objet de diverses procédures judiciaires en 1945, dont le procès des médecins à la suite de prétendues expérimentations médicales. Dès la deuxième moitié du XXe siècle, ce concept et les théories qui l'accompagnaient sont invalidés et condamnés par les instances internationales,.
L'influence lunaire est un lien non vérifié scientifiquement entre les étapes du cycle lunaire terrien et les changements comportementaux chez les êtres humains et les plantes qui ne peut être simplement expliqué que par la variation de la lumière.
Connue depuis l'invention des moteurs à combustion interne, la technique d'injection d'eau a fait l'objet de nombreuses expérimentations, mais est finalement peu utilisée. Certaines personnes ont proposé de mettre de l'eau dans le carburant, et ce en affichant des résultats et des améliorations plus ou moins validés scientifiquement. Un documentaire réalisé par Patrick Lefrère et Cyril Flouard (De l'eau dans le gasoil) a pour objectif de synthétiser la masse d'informations disponible sur internet.
L'intégration neuro-émotionnelle par les mouvements oculaires, couramment appelée EMDR (d'après l'anglais eye movement desensitization and reprocessing), est un type d'intervention à visée psychothérapeutique mise au point par Francine Shapiro à partir de 1987. Cette thérapie est notamment utilisée dans le traitement du syndrome de stress post-traumatique. La particularité de l’EMDR réside dans la stimulation sensorielle généralement appliquée sous une forme bilatérale alternée et le plus souvent par le biais des mouvements oculaires.
L'iridologie est une pratique de médecine alternative, c'est-à-dire non-basée sur des preuves et sans efficacité avérée. Les iridologues prétendent et considèrent qu'à chaque secteur de l'iris correspond un organe. Ainsi selon eux, si un secteur donné présente des anomalies, cela indiquerait que l'organe correspondant est affecté par un dysfonctionnement. À ce jour, les études cliniques n'ont mis en évidence aucune efficacité de l'iridologie, qui échoue notamment au test de la reproductibilité : pour un même patient, les iridologues testés font des diagnostics très différents, voire contradictoires. C'est la raison pour laquelle il faut prendre avec beaucoup de scepticisme la prétention de l'iridologie à fournir des diagnostics pertinents.
Joseph Issels (21 novembre 1907 – 11 février 1998) était un médecin allemand connu pour avoir promu un régime de thérapie de cancer alternatif et très contesté, le traitement Issels. Il a revendiqué guérir les cancéreux que l'on avait déclarés incurables par des traitements du cancer conventionnels.
Le terme junk science (« science poubelle ») est un anglicisme désignant la pseudo-science qui déforme les données scientifiques afin de servir des intérêts idéologiques ou commerciaux en donnant une valeur scientifique à ce qui n’en a pas.   Portail des sciences
Corentin Louis Kervran est un fonctionnaire, un ingénieur et un scientifique français né le 3 mars 1901 à Quimper et mort le 2 février 1983 à Quimperlé. Scientifique chargé de l'étude des effets radiologiques pour la médecine du travail, Maître de conférences de l'Université de Paris et membre de la commission du Conseil supérieur de la recherche scientifique, il est surtout connu pour ses théories sur la transmutation biologique des éléments chimiques. Ces derniers travaux ont été et sont toujours considérés aujourd'hui comme de la pseudo-science, voire de l'alchimie, ce qui lui a fait attribuer le prix parodique Ig Nobel en 1993.
Georges Lakhovsky né à Minsk (12 septembre 1870, Russie – 31 août 1942 New York, États-Unis) était un inventeur controversé d'appareils de médecine alternative, et auteur. Il s'est fait remarquer par une hypothèse pseudo-scientifique d'une communication entre les cellules vivantes au moyen d'énergie à haute fréquence et l'invention d'appareils qui seraient soi-disant en mesure de guérir le cancer.
Le centième singe est une expression désignant, à l'origine, un phénomène supposé par lequel un apprentissage se serait répandu depuis un petit groupe de singes à toute la population des singes de la même espèce, une fois qu’un certain nombre d’entre eux aurait été atteint. Dans le courant New Age, l'expression se rapporte à une propagation d’une idée, d’un savoir ou d’une capacité au sein d'une population humaine (comme dans le concept de résonance morphique de Rupert Sheldrake) sans qu’il y ait de transmission visible et une fois qu’un nombre clé de personnes aurait acquis ce savoir ou cette capacité.
Ernst Lecher (Vienne, 1er juin 1856 - 19 juillet 1926) est un physicien autrichien.
Long Island Medium est une émission de télé-réalité américaine diffusée sur TLC depuis le 25 septembre 2011. La deuxième saison a été lancée le 25 mars 2012 et la troisième, le 9 septembre de la même année. Le 2 novembre, TLC annonce le renouvellement de la série pour une quatrième saison, présentant un format d'une demi-heure dont le lancement s'effectuera en mars 2013.
Le Blue Monday, blues du lundi ou Lundi blues est le nom donné au jour le plus déprimant de l'année par une campagne publicitaire britannique pour la chaîne de télévision Sky Travel (en) en 2005. En général, on célèbre cette journée le troisième lundi de janvier. L'expression fait parfois plus généralement référence aux variations hebdomadaires de l'humeur et à l'idée que le lundi est le jour le plus triste de la semaine.
Le lyssenkisme (en russe : Лысе́нковщина) est une politique agricole formulée en Union des républiques socialistes soviétiques (URSS) par Trofim Lyssenko et ses successeurs à partir de la fin des années 1920, puis mise en application au cours des années 1930 lorsque Lyssenko, soutenu par les autorités, prend la tête de l'Académie Lénine des sciences agronomiques (en). Cette politique a été maintenue officiellement jusqu'en 1964.
Trofim Denissovitch Lyssenko (en russe : Трофим Денисович Лысенко), né le 29 septembre 1898 à Karlivka (aujourd'hui en Ukraine) et mort le 20 novembre 1976 à Kiev, était un technicien agricole soviétique. Il est à l'origine d'une théorie génétique pseudo-scientifique, la « génétique mitchourinienne », qu'il promeut pendant la période stalinienne en Union soviétique où elle accède en 1948 au rang de théorie officielle exclusive opposée à une « science bourgeoise », fausse par essence. En pleine guerre froide, les travaux de Gregor Mendel, de Morgan et des autres généticiens sur la théorie chromosomique de l'hérédité sont interdits d'enseignement en URSS, les laboratoires de génétique fermés et les chercheurs — ceux qui ont survécu aux purges d'avant-guerre — limogés. Ses travaux dans le domaine de l'agriculture lui valurent le titre de héros de l'Union soviétique et lui permirent de dominer la recherche biologique en URSS, jusqu'à son discrédit dans les années 1960. Depuis, le terme lyssenkisme désigne par extension une science corrompue par l'idéologie, où les faits sont dissimulés ou interprétés de manière scientifiquement erronée.
Oscar Kiss Maerth (né en 1914 et mort en 1990) est un auteur pseudo-scientifique né en Hongrie. Durant sa vie, il effectua de nombreux voyages en Asie, en Amérique du Sud ainsi qu’en Australie avant de s’installer, avec sa femme et ses trois enfants, en Italie sur les bords du lac de Côme. Il s’attela à la restauration de la Villa Passalacqua.
La magnétothérapie fait partie des médecines non conventionnelles. Les praticiens magnétothérapeutes affirment pouvoir soigner diverses maladies en utilisant des aimants. La magnétothérapie est considérée comme non scientifique par le corps médical car nombreuses études médicales n'ont pu montrer une efficacité supérieure à l'effet placebo (par exemple en cas de douleurs arthrosiques, de talalgies, de douleurs plantaires, de douleurs lombaires basses) tandis que d'autres ont montré un effet largement supérieur au placebo (sur les douleurs des poliomyélitiques ou des diabétiques), faisant elles-mêmes suite à des observations hospitalières préalables (par exemple sur la suppression de crampes nocturnes ou les raideurs d'épaules). La magnétothérapie fait l'objet d'un marché important, estimé à 252 millions d'euros dans le monde. La magnétothérapie ne doit pas être confondue avec les techniques d'intervention médicale conventionnelles qui font appel à des champs magnétiques pulsés comme la stimulation magnétique transcranienne (TMS).
La maladie du Nobel est l'incapacité ou l'impossibilité pour les nobélisés scientifiques de reproduire ou poursuivre des recherches scientifiques après leur retour de la cérémonie du Nobel,,. Elle est devenue, depuis 2012, après sa popularisation par un oncologue américain ayant décidé de lutter contre la désinformation,, une métaphore pour la tendance de certains lauréats du prix Nobel à devenir défenseurs de théories pseudo-scientifiques ou de théories du complot après avoir reçu cette récompense. On dénombre une trentaine de nobélisés touchés par cette tendance.
Marshall Masters, né à Los Angeles, est un journaliste et essayiste américain sur l'ufologie, les théories du complot, la science et les pseudosciences en général avec l'astronomie en particulier ainsi que le survivalisme.
Le matérialisme dialectique, ou dialectique matérialiste, ou diamat, est l'emploi, dans la pensée marxiste, de la méthode dialectique pour analyser la réalité à travers un prisme matérialiste. L'élaboration matérialiste de la dialectique se situe dans le prolongement du matérialisme historique, conçu par Marx et Engels sous le nom de conception matérialiste de l'histoire pour aboutir à une « connaissance scientifique de l'histoire », soit une évaluation objective des formations de la conscience en les rapportant à leur fondement réel et social ; la dialectique marxiste unifie deux éléments que Marx trouve séparés dans la vie intellectuelle de son époque : le matérialisme philosophique basé sur la science de la nature d'une part, et la dialectique de Hegel, soit la théorie des contradictions, d'autre part. Appliquant au processus historique les lois de la nature, le matérialisme dialectique vise à analyser les évolutions des sociétés humaines, y compris les périodes révolutionnaires où l'évolution naturelle s'accélère. L'expression « matérialisme dialectique » n'apparaît cependant jamais chez Marx ; Engels lui-même n'utilise le terme qu'en 1886. Le concept de matérialisme dialectique semble avoir été ensuite forgé par Joseph Dietzgen et Gueorgui Plekhanov, afin de développer l'idée d'un matérialisme ayant su assimiler et intégrer les enseignements de la dialectique idéaliste de Hegel. A posteriori, l'expression a parfois été utilisée pour désigner dans son ensemble la dimension philosophique du marxisme , Dans l'optique du matérialisme historique, les conditions matérielles déterminent les relations de production — soit la technologie, les inventions, les formes de propriété - lesquelles déterminent à leur tour les philosophies, les formes de gouvernement, les lois, la culture et les principes moraux des organisations sociales. L'évolution quantitative des conditions matérielles conduit à des évolutions qualitatives : le matérialisme dialectique, se présentant comme prolongation du matérialisme historique, consiste à étendre la méthode dialectique au-delà de l'étude de la société, pour l'appliquer à celle de la nature. La pensée matérialiste de Marx et Engels s'approprierait la « forme » de la dialectique de Hegel, mais en la dépouillant de son « idéalisme » : alors que la dialectique hégélienne consistait en une dialectique de la pure pensée, Marx et Engels aspirent à une connaissance scientifique de la réalité, leur conception de la dialectique devant représenter le mouvement du réel dans son développement immanent. Pour le philosophe Henri Lefebvre, le terme de matérialisme dialectique englobe la conception marxiste du monde prise dans toute son ampleur. Récupéré par le stalinisme et érigé en philosophie fondamentale et obligatoire pour tout communiste, le matérialisme dialectique est ensuite utilisé dans les régimes communistes, non plus comme une méthode d'analyse, mais comme une doctrine à laquelle sont subordonnées les sciences dans leur ensemble. La déstalinisation, puis le déclin de l'idéologie communiste, entraînent un discrédit progressif du matérialisme dialectique, remis en cause jusque dans les écrits d'intellectuels marxistes ou marxiens contemporains,.
La médecine anthroposophique est l'ensemble des pratiques thérapeutiques issues de l'anthroposophie, un courant ésotérique occultiste développé par Rudolf Steiner. Ces pratiques sont apparues en 1920 dans le cadre d'une collaboration entre Steiner et la doctoresse Ita Wegman. Médecine non conventionnelle, elle fait l'objet d'une controverse quant à ses objectifs et son efficacité au regard des standards de la médecine fondée sur les faits.
La théorie de la mélanine (Melanin Theory) est une théorie pseudo-scientifique afrocentriste selon laquelle le taux plus élevé de mélanine des individus à la peau foncée leur donne un avantage physique, spirituel et intellectuel,,,,. Plus une personne a un taux de mélanine élevé dans la peau et plus elle aura la peau foncée. Un taux de mélanine élevé dans la peau protège la peau contre les effets nocifs des rayons UV. Certains auteurs afrocentristes prêtent à la mélanine des propriétés non reconnues par la science, établissant des liens erronés entre mélanine et capacités cognitives, créativité, équilibre psychologique, capacités motrices, voire entre mélanine et pouvoirs surnaturels. Selon Bernard Ortiz De Montellano, les théoriciens de la mélanine se servent de la littérature scientifique de manière non pertinente et/ou biaisée pour justifier leurs assertions afrocentristes. Une des théories les plus communes est que les blancs sont des mutants (albinos ou issus d'une mutation récessive de la mélanine),. La théorie de la mélanine est soutenue par Leonard Jeffries, professeur au City College of New York, qui, selon Time Magazine, « croit que la mélanine, le pigment de la peau foncée, donne aux noirs une supériorité intellectuelle et physique sur les blancs ».
La « mémoire de l’eau » est le nom donné, en 1988, au cours d'une controverse médiatique, à une hypothèse du chercheur, médecin immunologue, Jacques Benveniste selon laquelle l’eau qui a été en contact avec certaines substances conserve une empreinte de certaines propriétés de celles-ci alors même qu'elles ne s’y trouvent statistiquement plus. Une série d'expériences réalisées pour valider cette hypothèse est alors présentée par des tenants de l'homéopathie (qui pratique une dilution très importante des principes actifs) comme une validation scientifique de celle-ci. Cependant, aucune explication satisfaisante n'ayant été proposée, une reproduction de l'expérience est menée par des chercheurs anglais, avec au final un résultat négatif. Les connaissances scientifiques montrent que l'eau liquide ne retient pas de réseaux ordonnés de molécules pendant plus d'une petite fraction de nanoseconde. Les résultats des expériences originales sur la mémoire de l'eau découlent probablement d'un artéfact expérimental ou d'une fraude scientifique, et ces hypothèses sont aujourd'hui unanimement considérées comme pseudo-scientifiques. Ce phénomène continue cependant d'être étudié par certains scientifiques, menés par le professeur Luc Montagnier, lauréat du Prix Nobel de médecine en 2008,. Ami de Benveniste, il estime que ce dernier avait globalement raison, malgré des résultats qui « n'étaient pas reproductibles à 100 % ». Il s'est depuis fait remarquer pour prétendre avoir téléporté de l'ADN sur plusieurs centaines de kilomètres de distance, passant pour un charlatan et étant la risée de la communauté scientifique,.
Stanley Allen Meyer (1940-1998) aurait conçu un moteur à eau utilisant un système permettant de provoquer la fission des molécules d'eau pour en extraire l'hydrogène et l'oxygène qu'elles contiennent. Cela en consommant relativement peu d'énergie électrique en comparaison de l'énergie sous forme chimique produite (le mélange de di-hydrogène et de di-oxygène peut être brûlé). D'après ses dires le rendement de son "séparateur de molécule" serait supérieur à 1 000 %, ce qui porte à controverse. Stanley Meyer est condamné pour fraude en 1996 car la cour juge que son invention n'est qu'une électrolyse classique, s'éloignant fortement du rendement annoncé.
La morphopsychologie est une méthode qui cherche des correspondances entre la morphologie des traits du visage d'un individu et sa psychologie. Ignorant la méthode scientifique, aucun de ses postulats n'a été vérifié. Elle est l'objet de critiques récurrentes la classant dans les catégories « ésotérisme » ou « pseudo-science ».
Le système PMC Pantone ou Gillier Pantone est un procédé d'injection non conventionnelle du carburant dans des moteurs à combustion interne. Ce système n'a jamais été proposé sur des moteurs neufs. Il est proposé en rétrofitage sur des moteurs initialement à injection conventionnelle. Sa mise en place est assez simple et ne nécessite pas de connaissances et compétences poussées. Toutefois, il est difficile avec les systèmes d'injection électronique d'installer cette technologie. Le dispositif pour le système PMC Pantone consiste à vaporiser un mélange d'eau et de carburant en exploitant un système de pulvérisation (cette étape se déroule dans ce qu'on appelle un « bulleur »). Le mélange est ensuite réchauffé en exploitant les gaz d'échappement chauds (cette étape se déroule dans ce qui est nommé un « réacteur endothermique »). Les vapeurs générées sont ensuite mélangées à de l'air neuf avant d'être introduites dans le cylindre où la combustion du carburant s'effectue comme dans un moteur à combustion interne « classique ». Le dispositif pour le système Gillier Pantone, principalement adapté pour les moteurs Diesel, consiste à générer de la vapeur d'eau. Cette vapeur est ensuite comprimée et chauffée dans le « réacteur endothermique ». Le gaz obtenu ainsi modifié est directement aspiré avec l'air neuf de l'admission du moteur. L'injection en carburant, elle, reste conventionnelle. Ce système est sujet à controverse, les fondements scientifiques du procédé restant obscurs et peu d'études scientifiques ayant été consacrées au sujet.
Charles A. Musès (28 avril 1919 - 26 août 2000), également connu sous le nom Musaios, était le fondateur du Lion Path, un mouvement chamaniste. Il eut des vues inhabituelles concernant les mathématiques, la physique, la philosophie et d'autres domaines. Musès naquit à Jersey City, et grandit à Long Island. Il reçut un M.A. en philosophie de l'université Columbia en 1947, suivi d'un Ph.D. en 1951. Musès a proposé une méthode astrologique appelée la chronotopologie, qui, disait-il, pouvait mesurer la structure qualitative multidimensionnelle du temps. Il publia un livre sur ce sujet intitulé Chronotopology: Destiny and Control in Human Systems. (Chronotopologie : destinée et contrôle dans les systèmes humains). Musès a aussi envisagé un système de nombre mathématique, les hypernombres muséens, qui incluent les algèbres de nombres hypercomplexes telles que les nombres complexes et les nombres complexes déployés comme types primitifs. Il leur assigna des niveaux basés sur certaines propriétés arithmétiques qu'ils peuvent posséder. Alors que beaucoup de questions restent en suspens, en particulier à propos de la définition des relations entre ces niveaux, Musès a décrit un large éventail d'applications pour ce concept. Certaines de celles-ci sont basées sur les propriétés des carrés magiques [1], et même reliées aux croyances religieuses. Il croyait que ces hypernombres étaient essentiels pour la compréhension de la conscience.
Mysticisme quantique est une expression contemporaine désignant un ensemble de croyances métaphysiques et de pratiques connexes qui cherchent à établir un rapport entre la conscience, l’intelligence, certaines philosophies orientales et les théories de la mécanique quantique et ses interprétations,,,,, venant soutenir une vision panthéiste de l'univers. Du point de vue de la majorité de la communauté scientifique, le mysticisme quantique repose sur des interprétations erronées ou insuffisamment fondées de la mécanique quantique.
La numérologie est un ensemble de croyances et de pratiques fondées sur l'attribution de propriétés à des nombres, propriétés variables selon le contexte (dépendant par exemple de la source alphabétique d'un mot, latin, grec, copte, hébreu, etc.). La numérologie est une pseudo-science. L'une des origines de la numérologie serait la gématrie, technique herméneutique traditionnelle dans le judaïsme et la Kabbale. Une autre serait l'arithmancie pythagoricienne. En 1905, L’américaine écrivain et conférencière, de son nom d’épouse, L. Dow Balliett, (1847/1929), née Sarah Joanna Dennis, publie le premier livre sur le sujet «The Power in Numbers ». Elle publiera ensuite de nombreux autres livres sur le même thème. En 1907, le mot « numérologie » aurait fait son apparition, il aurait été conseillé par l’une de ses amies, l’américaine Julia Seaton Sears, écrivaine et conférencière (1862/1950). La fille du Julia Seaton Sears, le Docteur Juno Jordan (1884/1984), centenaire à 2 mois près, psychologue de formation, et élève de L. Dow Balliett, fut la première réformatrice de la numérologie. Elle publiera en 1965 «The Romance in Your Name». D’autres numérologues américains se succèderont et la numérologie s’étendra en Europe. Les nombres et les chiffres ont toujours fasciné toutes les civilisations antiques comme contemporaines, mais la numérologie prend sa source, son origine, aux États-Unis, au début du XXe siècle. Les origines ancestrales de la numérologie sont le fruit de pures extrapolations ou de manœuvres commerciales.
L'orgone est un terme inventé par le docteur en médecine, psychiatre et psychanalyste Wilhelm Reich pour désigner une forme d'« énergie » dont il affirmait avoir découvert l’existence. Ses résultats ne furent jamais reproduits et la théorie de l'orgone est considérée comme non scientifique. Nota : Dans ce texte, de l'historique à l'interprétation non scientifique, sont utilisés certains termes scientifiques tels que les a utilisés Reich. Le sens de ces termes peut fortement différer voire être incompatible avec celui utilisé dans la science contemporaine à Reich et plus encore dans la science actuelle.
L'orgonite est une invention ésotérique et pseudo-scientifique faite d'un mélange de métaux et de matière à base de carbone (la « matrice ») et qui contient la plupart du temps des cristaux ou des minéraux et parfois d'autres additifs. L'orgonite a été initialement inventée par Karl Welz et améliorée par Don Croft, le tout étant fondé sur des travaux antérieurs de Wilhelm Reich. L'orgonite est utilisée dans le cadre de médecines parallèles qualifiées de pseudo-sciences par la communauté scientifique. Par comparaison avec les fabrications de Wilhelm Reich, à base d'alternances de couches métalliques et organiques, et qui auraient été des accumulateurs d'orgone, l'orgonite aurait la propriété de convertir l'orgone négative en orgone positive. Il existe différents types d'orgonite en fonction de leur usage supposé.
La corrélation d'Orion est une théorie proposée par certains égyptologues ou archéo-astronomes (comme Robert Bauval), selon laquelle il existerait une corrélation entre la position des pyramides d'Égypte et la position des étoiles, notamment entre les trois pyramides de la nécropole de Gizeh (pyramide de Khéops, pyramide de Khéphren, pyramide de Mykérinos) et les trois étoiles centrales de la constellation d'Orion (δ Orionis, ε Orionis et ζ Orionis) – constitutives de l'astérisme appelé Baudrier d'Orion.
Les para-sciences (parasciences) ou sciences parallèles, est un terme apparu au début du XXe siècle pour remplacer les expressions de « sciences occultes » et de « pseudo-sciences ». Le préfixe « para », du grec παρά « à côté de », fait référence à ce qui n'utilise pas des méthodes scientifiques reconnues aujourd'hui par la communauté scientifique.
La phrénologie (du grec : φρήν, phrēn, "cerveau" et λόγος, logos, "connaissance", terme probablement forgé et utilisé pour la première fois par Thomas Ignatius Forster en 1815) est une théorie selon laquelle les bosses du crâne d'un être humain reflètent son caractère. Son fondateur est Franz Joseph Gall. En 1825, François Magendie qualifia la phrénologie de pseudo-science. Cette conception, bien ancrée dans son temps, est un exemple de méthode expérimentée biaisée dans le cadre de l'étude de l'histoire de la médecine.
La physiognomonie est une méthode fondée sur l'idée que l'observation de l'apparence physique d'une personne, et principalement les traits de son visage, peut donner un aperçu de son caractère ou de sa personnalité. Voici la définition du plus célèbre physiognomoniste, Johann Kaspar Lavater : « La physionomie humaine est pour moi, dans l’acception la plus large du mot, l’extérieur, la surface de l’homme en repos ou en mouvement, soit qu’on l’observe lui-même, soit qu’on n’ait devant les yeux que son image. La physiognomonie est la science, la connaissance du rapport qui lie l’extérieur à l’intérieur, la surface visible à ce qu’elle couvre d’invisible. Dans une acception étroite, on entend par physionomie l’air, les traits du visage, et par physiognomonie la connaissance des traits du visage et de leur signification. » La physiognomonie connut son essor au XIXe siècle, en particulier avec les thèses du criminologue Cesare Lombroso, portées dans son ouvrage L'Homme criminel (ce qui vaut encore à cette théorie d'être parfois appelée le lombrosianisme). Cette théorie permit notamment l'avènement d'une école positiviste italienne, qui visait à « mettre la science au service de l’ordre social ». Cette théorie a été profondément critiquée par le corps médical, des philosophes, ainsi que par des juristes. Dénuée de méthodologie scientifique, cette pseudo-science est, d'après ces critiques, un élément du mouvement de racisme scientifique qui s'est développé au cours du XIXe siècle, et du nazisme Dans l'Antiquité, la physiognomonie avait déjà ses détracteurs, comme l'atteste une anecdote racontée par Cicéron. Zopyre, un mage thrace qui, selon Aristote, aurait prédit à Socrate une mort violente, « faisait profession de discerner les mœurs des hommes et leur naturel d'après leur corps, leurs yeux, leur visage, leur front ». Or, face à Socrate, il aurait déduit qu'il avait affaire à un homme « stupide et abruti », et aurait même ajouté : porté sur les femmes. Sur ce, Alcibiade aurait éclaté de rire. Le même Zopyre aurait attribué à Socrate une multitude de vices. Socrate se défendit contre les railleurs en disant : « les vices étaient bien là, mais j'en ai triomphé par la raison ».
Le Pigasus Award et le nom donné à une récompense annuelle de type tongue-in-cheek présenté par le sceptique James Randi. La récompense cherche à exposer les fraudes parapsychologiques, paranormale ou psychique que Randi a noté l'année précédente. Les annonces ont lieu le 1er avril.
Power Balance, appartenant à l'entreprise Power Balance Technologies Inc est une marque de bracelets dits « énergétiques » originaire d'Orange Country, une ville californienne aux États-Unis. D'après ses fabricants, le produit utilise la technologie holographique pour apporter un bienfait sur l'énergie naturelle du corps. Aucune étude en méthode scientifique ne permet de démontrer les effets prétendus de ce bracelet. L'entreprise utilise le marketing viral et le placement de produit auprès de célébrités notamment sportives, rémunérées pour vendre ses accessoires de mode. Parmi eux on compte des athlètes professionnels connus comme Drew Brees, Darren Sproles ou Ruby Gay. Jusqu'en 2011 le bracelet Power Balance a été vendu à plusieurs millions d’exemplaires à travers les Etats-Unis et plus de 40 autres pays.
Antoine Priore (francisé ultérieurement en Prioré), né à Trieste en Italie, le 10 avril 1912 et mort le 9 mai 1983 à Bordeaux, était un technicien en électricité et électronique qui construisit, à partir de 1960, plusieurs machines destinées à traiter le cancer. Malgré certains résultats spectaculaires sur des animaux, ayant donné lieu à une dizaine de communications officielles à l’académie des sciences, leur efficacité sur les tumeurs cancéreuses humaines n'a pas été scientifiquement démontrée. Il s'en est suivi de longues et virulentes controverses entre scientifiques de très haut niveau. Obsédé par le souci de garder le secret sur ses découvertes, Antoine Priore est mort sans laisser de documents permettant de connaître le détail du fonctionnement de son procédé.
La psychogénéalogie est une pratique clinique développée dans les années 1970 par Anne Ancelin Schützenberger selon laquelle les événements, les traumatismes, les secrets et les conflits vécus par les ascendants d'un individu conditionnent ses faiblesses constitutionnelles, ses troubles psychologiques, ses maladies, voire ses comportements étranges ou inexplicables. Elle s'est fondée sur ses propres observations et aussi sur des concepts issus de la psychanalyse, de la psychologie, de la psychothérapie et de la systémique. Cette pratique clinique a été théorisée par la suite par d'autres psychanalystes, tels Françoise Dolto, Didier Dumas ou Nicolas Abraham et Maria Torok. Ils donneront naissance à la psychanalyse transgénérationnelle. Cette approche fait l'objet de critiques sur plusieurs plans. La base scientifique de cette approche est remise en doute par plusieurs chercheurs, et les dérives sectaires sont pointées par plusieurs associations, notamment en raison des risques de faux souvenirs induits liés aux pratiques de certains praticiens.
La psychokinèse ou psychokinésie (PK), ou télékinésie par la pensée, est une faculté métapsychique hypothétique de l'esprit qui permettrait d'agir directement sur la matière. Les expériences concernant la psychokinèse ont été critiquées pour leur manque de contrôles scientifiques appropriés et leur non reproductibilités,,,. Il n'existe pas de preuve convaincante que la psychokinèse soit un phénomène réel, ce sujet est généralement considéré comme une pseudoscience,,,.
Le pouvoir des pyramides réfère à de prétendues propriétés surnaturelles ou paranormales des pyramides d'Égypte antiques et des objets de forme similaire. Ces propriétés sont l'une des nombreuses théories pseudo-scientifiques au sujet des pyramides, théories désignées collectivement sous le nom de pyramidologie.
La pyramidologie est un terme qui réfère aux diverses spéculations pseudo-scientifiques concernant les pyramides, le plus souvent concernant la nécropole de Gizeh et sa Grande pyramide. Les interrogations des « pyramidologues », qualifiés d'« obsédés du merveilleux » par l'égyptologue Pascal Vernus, portent le plus souvent sur les modes de construction, mettant en doute les capacités des Egyptiens à effectivement édifier ces monuments, ainsi que sur les propriétés et fonctions du monument, qui donnent lieux à toutes sortes de théories ésotériques . Ces appréciations se portent également sur différentes structures monumentales, depuis les monuments mégalithiques de Stonehenge jusqu'aux statues de l'île de Pâques en passant par les murailles de Machu Picchu.
Les « pyramides de Bosnie », ou « pyramides bosniennes » ou encore « pyramides bosniaques » (en bosnien, Bosanska piramida), sont un groupe de flatirons (en) – formations géologiques naturelles présentant une face plate en forme de semelle de fer à repasser – situées près de la ville de Visoko, au nord-ouest de Sarajevo. Une de ces pyramides, la « colline de Visočica », haute de 213 mètres et ancien centre de la capitale médiévale bosnienne Visoko, a fait l'objet, en octobre 2005, sous l'appellation de « pyramide du soleil » que lui a donnée son inventeur, d'une campagne médiatique la présentant comme la plus ancienne des pyramides édifiées de main d'homme. À partir de la constatation que la colline, vue sous certains angles, a une forme symétrique pyramidale, l'entrepreneur américano-bosnien Semir Osmanagić a formulé la théorie à prétentions archéologiques que cette forme serait d'origine humaine. Cette théorie n'a reçu aucune reconnaissance de la part de la communauté archéologique internationale et a été dénoncée par une pétition d'archéologues comme étant une « imposture » et un « gaspillage de ressources ». En dépit du désaveu de ce qu'il appelle « la science officielle », Semir Osmanagić poursuit son projet. Les fouilles continuent à être financées par les autorités et les pyramides sont présentées aux enfants des écoles de Bosnie comme faisant partie de leur patrimoine.
Le Quantic potential measurement (QPM) est un test fait à partir d'une machine produite par la société QPM et supposé déterminer la personnalité d'un sujet en trente secondes grâce à la « mesure de l'activité bioélectronique de l'organisme ». Un article du quotidien Le Monde le compare à un « Madame Soleil version informatique », soulignant qu'aucune validation scientifique n'a pour l'instant été faite et redoutant qu'il soit utilisé dans les services de recrutement et de DRH. Une analyse critique a été produite par l'association française pour l'information scientifique montrant le côté pseudo-scientifique de cette « machine miracle » et le manque de rigueur de la journaliste du Monde qui lui a fait une bonne publicité. La société qui fait la promotion de la machine a éliminé de son site une partie du charabia pseudo-scientifique le plus évident suite aux critiques publiées dans la presse. Le même article de l'AFIS, ainsi qu'un autre de Rue89 évoquent comme explication l'effet Barnum.
Francesco Racanelli – né à Sannicandro di Bari (Pouilles) en 1904, mort à Florence en 1977 – est un médecin et écrivain italien, à l’origine d’une thérapie non conventionnelle qu’il dénomme « médecine bioradiante ».
La radiesthésie est un procédé divinatoire de détection reposant sur la croyance selon laquelle les êtres vivants seraient sensibles à certaines radiations qu'émettraient différents corps, permettant ainsi de localiser des sources, retrouver un objet perdu, un trésor ou une personne disparue, établir un diagnostic médical, déterminer la profondeur d'un puits, etc. Les études rigoureuses qui ont été menées (notamment la grande expérience de l'université de Munich réalisée entre 1986 et 1988) n'ont mis en évidence aucune efficacité de la radiesthésie. Le mouvement de la baguette serait dû à l'effet idéomoteur,. On doit le mot « radiesthésie » (du latin radius, « rayon » et du grec aisthêsis, « sensibilité ») aux abbés Bouly et Bayard . La radiesthésie est, selon la définition de l'abbé Bouly, la croyance en la faculté d'exercer cette sensibilité pour découvrir, grâce au pendule ou à la baguette, ce qui est caché aux facultés normales mais dont l'existence est réelle ou imaginaire. Ce procédé divinatoire était anciennement appelé rhabdomancie (du grec ῥάβδος, rhabdos, « bâton » et μαντεία, manteia, « divination », technique utilisant la baguette de coudrier) ou encore sourcellerie.
Un régime alcalin est un régime alimentaire basé sur la croyance que certains aliments peuvent influencer l'acidité ou le pH des fluides du corps humain, y compris l'urine ou le sang, et affirmant que cela permet de prévenir ou guérir de certaines maladies. Devant le manque de preuves de l'intérêt de ces méthodes, elles sont déconseillées par les diététiciens et professionnels de la santé.
Le réseau Hartmann, en radiesthésie en sourcellerie et autres pseudo-sciences, serait un treillis de carrés entrelacés de "champs énergétiques" différents, "les réseaux telluriques", principalement composés des champs Hartmann et Curry (du nom de leurs "découvreurs"). Pour la quasi-totalité de la communauté scientifique, ce treillis de lignes est imaginaire et n'a aucune réalité quantifiable ni mesurable. Il est utilisé par différents groupes de charlatans, se servant de l’ignorance des gens et de la peur des ondes et de ce treillis imaginaire pour leur revendre par la suite différents ustensiles ou méthodes « miracles » tels que des soit-disant « couvertures de matelas anti-rayonnement ». Le réseau Hartmann est en lien avec la géobiologie de l'habitat, une pseudoscience rattachée à la radiesthésie. La géobiologie est définie comme une « science ou technique qui traite de la qualité vitale d'un lieu par l'analyse des ondes pouvant influer sur le développement des organismes (végétaux, animaux, humains) y séjournant. » Concernant l'habitat, elle « s'intéresse aux terrains à construire et aux maisons d'habitation. » . C’est une pseudo-science qui n’a aucun fondement scientifique et se base sur des croyances et des idées dont la quasi-totalité vont soit à l’encontre de la science (« réseaux de cadrillage d’énergie ») soit sont prouvées fausses (nocivité de l’éléctricité). Ne pas confondre la géobiologie de l'habitat avec son homonyme la géobiologie scientifique qui s'intéresse à l'étude scientifique des êtres vivants à l'échelle des temps géologiques.
La rumpologie (de l’anglais rumpology, de rump, « croupe », et -logy, « -logie ») est un néologisme qui désigne une pseudo-science, apparentée à la physiognomonie, consistant à dire l’avenir d’une personne par l’étude des caractéristiques de ses fesses et de son sillon interfessier (fissures, fossettes, verrues, grains de beauté, plis, etc.). Cette pratique divinatoire est analogue à la chiromancie et sa pseudo-science associée, la chirologie.
Ruggero Maria Santilli, né le 8 septembre 1935, est un physicien italo-américain. Il a enseigné et fait de la recherches pour plusieurs universités et organismes gouvernementaux américains. Il a fondé les revues Hadronic Journal, Hadronic Journal Supplement et Algebras, Groups and Geometries, dans lesquelles il publie ses travaux ainsi que ceux d'autres chercheurs. Ces revues sont éditées par Hadronic Press, une firme dirigée par son épouse Carla Santilli. Santilli est CEO et scientifique en chef de MagneGas Corporation. Certains de ses travaux sont jugés comme étant de type fringe science.
François de Sarre ou François Charousset de Sarre, né le 16 mai 1947 à Dudweiler (Allemagne), est un zoologiste franco-allemand spécialisé en ichtyologie, en sciences de l'évolution et en paléoanthropologie.
La schizophrénie torpide ou schizophrénie à évolution lente (russe : вялотеку́щая шизофрени́я, vyaloteushchaya shizofreniya) est une catégorie de diagnostic utilisée en Union soviétique pour décrire une prétendue schizophrénie à évolution lente : les symptômes usuels de la schizophrénie ou des troubles psychotiques ne sont pas présents, mais on présume qu'ils vont se développer ultérieurement. Dès lors, des personnes menant une vie normale, et ne se plaignant d'aucune souffrance psychique particulière, se voient désignées comme souffrant de schizophrénie torpide. Malgré cette absence de symptôme, les patients sont souvent traités avec des médicaments prévus pour le traitement de la schizophrénie. Le concept de schizophrénie torpide a été développé dans les années 1960 par le psychiatre soviétique Andrei Snezhnevsky et ses collègues. La majorité des experts s'accordent à dire que ce concept a été fabriqué à la demande du parti et du KGB et que ses auteurs étaient parfaitement conscients du caractère idéologique et abusif de leur démarche. Ce diagnostic n'a été utilisé qu'en URSS et dans certains pays du Bloc de l'Est (notamment la Roumanie) et ce jusqu'à la chute du communisme en 1989,,. La schizophrénie torpide n'a jamais été reconnue par l'Organisation mondiale de la santé (OMS) et est largement considérée comme un exemple frappant de l'utilisation abusive de la psychiatrie à des fins politiques en Union soviétique. En effet, le diagnostic de schizophrénie torpide a été massivement utilisé à l'encontre des dissidents pour justifier leur internement forcé en hôpital psychiatrique. En outre, la validité scientifique de cette classe nosologique a été réfutée par la recherche psychiatrique. Dans les années 80, les psychiatres soviétiques admettent qu'il y a sans doute trop de personnes diagnostiquées comme souffrant de schizophrénie torpide, mais ils continuent néanmoins à utiliser cette classification.
Une science pathologique est un domaine de recherche où les « gens sont forcés de faire le mauvais choix … par les effets de la subjectivité, de l’illusion et des interactions ». Le terme a été pour la première fois utilisé par Irving Langmuir, le chimiste vainqueur du prix Nobel, durant un colloque en 1953 au Knolls Atomic Power Laboratory (en). Langmuir a déclaré qu'une science pathologique ne disparaîtrait pas après avoir été abandonnée car jugée fausse par la majorité des scientifiques d'un domaine. Il les qualifia de « sciences des choses qui ne sont pas ce qu'elles sont ». Bart Simon liste notamment parmi ces sciences : « les catégories … telles que … la pseudoscience, la science amateure, la science déviante et frauduleuse, la mauvaise science, la junk science, et la science vulgarisée … la science pathologique, la science de type culte du cargo, et la science vaudou »,.
Zecharia Sitchin, né le 11 juillet 1920 à Bakou en Union soviétique et décédé le 9 octobre 2010 à New York, est un écrivain américain d'origine soviétique. Ses théories, mêlant celle du néo-évhémérisme et celle du dessein intelligent de type extraterrestre, sont considérées comme pseudo-scientifiques par les scientifiques et les historiens.
Rudolf Steiner, né le 27 février 1861 à Donji Kraljevec, Croatie/Empire d'Autriche, et mort le 30 mars 1925 (à 64 ans) à Dornach, Suisse, est un philosophe occultiste. Il est le fondateur de l'anthroposophie, qu'il qualifie de « chemin de connaissance », visant à « restaurer le lien entre l'Homme et les mondes spirituels ». Membre de la Société théosophique puis secrétaire général de la section allemande en 1902, il s'en sépare dix ans plus tard pour fonder la Société anthroposophique. Inspirée de certains penseurs et poètes du romantisme allemand comme Goethe, la pensée de Steiner émane presque entièrement de ses « intuitions ». De nombreux avatars modernes du mouvement anthroposophique sont souvent revisités à travers une influence New Age. L'UNADFI alerte sur les risques que représente l’enseignement Steiner-Waldorf ainsi que l'endoctrinement à l'anthroposophie dans les écoles Steiner-Waldorf, et aujourd'hui le Goetheanum. En outre, l'anthroposophie fait l'objet d'une suspicion de dérives sectaires en France,,.
Steorn Ltd. est une petite société privée basée à Dublin, en Irlande. La société a brièvement attiré l'attention des médias en août 2006 en plaçant des publicités sur une page du journal The Economist, qui déclaraient qu'elle avait développé une technologie produisant une énergie « libre, propre et permanente » tout en demandant à la communauté scientifique de réfuter cette annonce (d'un point de vue théorique, les lois de la physique écartent d'emblée cette affirmation). Des physiciens avaient été invités à une démonstration pour tenter de prouver ou bien l'absence de fonctionnement, ou bien la présence de trucage. En l'occurrence, l'appareil ne fonctionna pas, selon ses fabricants à cause d'interférences dues aux projecteurs de la scène. Physicsworld ironisa sur le fait que Steorn aurait pu accuser la première loi de la thermodynamique, autrement dit la conservation de l'énergie : en effet, cette loi démontre bien que l'appareil de Steorn ne peut pas fonctionner, et pour les physiciens il est évident que cet échec démontre que le système n'a jamais fonctionné comme annoncé. Du 15 décembre 2009 au 31 janvier 2010, au Waterways Centre à Dublin, l'appareil dénommé Orbo, censé être un moteur sur-unitaire, est en démonstration au public pour être évalué et examiné. Les premiers retours sur son authenticité doivent avoir lieu dans les jours qui suivent. Malgré la démonstration, les observateurs restent dubitatifs. Steorn ne fournit de toute manière pas assez d'informations pour permettre de reproduire ses résultats. Or il s'agit d'un critère incontournable pour qu'un résultat soit scientifique.
La téléportation de l'ADN est une affirmation pseudo-scientifique selon laquelle l'ADN produirait des signaux électromagnétiques, mesurables lorsqu'il est fortement dilué dans l'eau, qui pourraient être enregistrés, transmis électroniquement et ré-émis sur un autre échantillon d'eau pure éloigné, où l'ADN pourrait alors être répliqué par une simple réaction PCR malgré l'absence d'ADN à copier pour les enzymes dans le nouvel échantillon d'eau. Cette idée a été développée par le Français Luc Montagnier en 2009, lauréat du prix Nobel de médecine en 2008 pour avoir participé à la découverte du virus du sida. Elle est semblable en principe à la mémoire de l'eau, un autre concept pseudo-scientifique popularisé en 1988 par Jacques Benveniste, un scientifique français ami de Montagnier, et depuis largement discrédité. Benveniste affirmait déjà en 1997 avoir transmis par téléphone des « effets biologiques » entre Chicago et Paris,. Aucune recherche indépendante n'a soutenu cette revendication, et la science établie ne supporte pas cette théorie, ni ne fournit de mécanisme explicatif plausible par lequel elle pourrait être valable. En 2015, l'équipe de Montagnier publie d'autres résultats semblables à ceux de 2009, mais à l'aide d'ADN bactérien et viral. Dans ce nouvel article, ils prétendent que les ondes électromagnétiques pourraient être expliquées en termes d'effet quantique. Ces travaux ont été critiqués pour leur manque de reproductibilité, les méthodes publiées étant évasives et manquant de contrôles, et les affirmations de Montagnier ont fait l'objet de critiques acerbes, assimilées au mysticisme quantique ou au charlatanisme. Bien que les résultats de ces « études » prétendent bouleverser la physique et la biologie moléculaire modernes, ils sont donc bonnement et simplement ignorés par la communauté scientifique « pour une bonne raison, à savoir qu'il sont absolument invraisemblables ». Si Montagnier a été la risée du monde scientifique, et diagnostiqué par les sceptiques comme atteint de la « maladie du Nobel » (consistant pour un prix Nobel à se mettre à travailler sur des sujets où il n'a aucune compétence ou sur des théories pseudo-scientifiques), prises aux sérieux ses affirmations pourraient avoir des conséquences dramatiques en matière de santé publique. Ainsi, lorsque Montagnier est pressenti pour présider un laboratoire de recherches au Cameroun en 2012, 44 autres prix Nobel signent une lettre au président du pays pour dénoncer « [les solutions de Montagnier] qui n’ont aucun début de preuves scientifiques » et le prévenir d'« un impact désastreux sur la qualité du système de santé au Cameroun ».
La théorie de Heim (ou HQT, pour Heim Quantum Theory) est une tentative de « théorie unifiée » des interactions fondamentales, selon les termes de ses promoteurs, issue de travaux non publiés du physicien allemand Burkhard Heim (9 février 1925-14 janvier 2001), sur une longue période s'étendant des années 1950 aux années 1980. Le physicien Walter Dröscher y a également contribué, ainsi que Jochem Hauser. Cette théorie n'a pas fait l'objet de publications dans des revues scientifiques à comité de lecture et n'est pas citée dans d'autres travaux du milieu académique. Elle apparaît aujourd'hui relever du domaine pseudo-scientifique, mais jouit néanmoins d'une certaine popularité auprès du grand public, en partie grâce au fait qu'elle a récemment bénéficié (fin 2005) d'une certaine publicité à la suite de l'annonce diffusée dans la presse généraliste que cette théorie permettrait un nouveau mode de propulsion spatiale révolutionnaire.
La théorie des chemtrails /ˈkɛmtreɪls/ avance que certaines traînées blanches créées par le passage des avions en vol sont composées de produits chimiques délibérément répandus en haute altitude par diverses agences gouvernementales pour des raisons dissimulées au grand public. Cette théorie est rejetée par la communauté scientifique qui indique qu'il s'agit de simples traînées de condensation. Le nom lui-même est un néologisme et un anglicisme, plus précisément un mot-valise anglais, construit par la contraction de « chemical trail », soit « traînée de produits chimiques », sur le modèle de, et par opposition à, « contrail », contraction de « condensation trail ». Ces termes ne s’appliquent pas aux autres formes d'épandage aérien comme l'épandage agricole, l'ensemencement des nuages, l'écriture dans le ciel ou la lutte contre les incendies. Les partisans de cette théorie du complot avancent que ces dispersions de produits chimiques seraient destinées à lutter contre le réchauffement climatique, à réguler la population, à modifier le temps, à effectuer des recherches militaires sur la guerre chimique et qu'elles sont la cause de maladies.
L'oppression de l'énergie libre est une théorie conspirationniste qui avance que les technologies avancées permettant de remplacer les méthodes de production énergétique actuelles existent, mais qu'elles sont supprimées par des groupements d'intérêt pour lesquels le statu quo est avantageux. Le concept d'énergie libre n'a aucun fondement scientifique et est en contradiction directe avec le premier principe de la thermodynamique. L'énergie libre (au sens de gratuite, à ne pas confondre avec la notion de thermodynamique éponyme) se rapproche souvent du concept de mouvement perpétuel, lui-même en contradiction avec le deuxième principe de la thermodynamique.
La théorie du primate aquatique (anglais Aquatic Ape Hypothesis, AAH ou Aquatic Ape Theory, AAT) est une hypothèse qui propose que des ancêtres de l'homme moderne se seraient adaptés à une vie dans un milieu humide, en bord de mer ou d'autres étendues d'eau. Cela aurait contribué à l'apparition de divers traits propres aux Hominidés par rapport aux proches primates, notamment l'absence de fourrure et la bipédie. Cette théorie est très marginale parmi les professionnels de la paléoanthropologie. L'intégralité de la proposition de «primate aquatique» reste très controversée et est plus populaire auprès du public que des scientifiques.
La théorie synergétique ou simplement la synergétique (appelée encore la Synergie par son auteur) est une théorie pseudo-scientifique développée par René-Louis Vallée et diffusée à partir de 1971, date de la publication de son livre L'énergie électromagnétique matérielle et gravitationnelle. Le magazine Science et Vie publie quelques articles sur le sujet, puis relate en 1975 une expérience qui aurait produit plus d'énergie que celle qui a été apportée au système : commence alors une longue controverse sur cette découverte d'une énergie libre. Dans l'année qui suit, le magazine La Recherche s'intéresse au livre de Vallée et, sur ses indications, fait faire une expérience rigoureuse à des physiciens, permettant de vérifier ou de réfuter le premier test. Les résultats se montrent négatifs : aucun excès d'énergie n'est observé. L'analyse de la théorie elle-même révèle de nombreuses incohérences, il s'avère que l'auteur est parti de ses propres convictions pour écrire un ensemble de formules qui ne sont pas vraiment liées entre elles. En effet, Vallée était contre la physique moderne dont les développements théoriques du XXe siècle étaient devenus trop complexes à ses yeux et ne pouvaient être conformes à la réalité. Inscrit au Cercle de physique Alexandre Dufour et soutenu par les adeptes des énergies libres jusque dans les années 2000, Vallée parvient à obtenir une certaine présence médiatique avant que la synergétique ne disparaisse de cet espace.
L'expression Terre creuse fait référence à des théories selon lesquelles la planète Terre serait creuse, ces théories étant presque toujours associées à l'idée que la Terre possède une surface interne habitable. Bien que des romans d'aventure les aient un temps rendues populaires, elles reçoivent peu de soutien au XXIe siècle ; les théories géodésiques actuelles les contredisent, et la majorité de la communauté scientifique les rejette, considérant qu'il s'agit de théories pseudo-scientifiques.
Une thérapie de conversion, parfois appelée thérapie de réorientation sexuelle ou bien encore thérapie réparatrice par ses défenseurs, est un ensemble de traitements d'origines diverses utilisés dans le but controversé de tenter de changer l'orientation sexuelle d'une personne de l'homosexualité à l'hétérosexualité.
L'expression thérapies des fascias renvoie à des approches de médecine non conventionnelle, généralement manuelles ou gestuelles, qui recourent aux fascias. Andrew Taylor Still, fondateur de l'ostéopathie, s'est le premier intéressé aux fascias dans une démarche thérapeutique. La fasciapulsologie, la fasciathérapie, le rolfing et le myofascial release recourent principalement aux fascias dans leurs fondements théoriques. La fasciapulsologie et la fasciathérapie sont en France les approches les plus connues. Actuellement, il existe un mouvement de scientifisation des thérapies des fascias. Ces études ne suffisent pas à valider les méthodes selon les exigences du modèle biomédical.
Uwe Topper, né à Breslau en 1940, est un artiste et essayiste allemand, auteur de livres en espagnol et en allemand sur l’histoire, l’ethnographie et l’anthropologie.
Le toucher thérapeutique, ou parfois toucher thérapeutique sans-contact, est une technique thérapeutique alternative, créée en 1972 par Dolorès Krieger, et Dora van Gelder Kunz (en).
L'unité Bovis est une unité de mesure parfois utilisée en radiesthésie. Son invention a été attribuée au radiesthésiste français Alfred Bovis par son petit-fils Jacques Bovis. Elle exprimerait le supposé taux vibratoire ou la supposée énergie cosmo-tellurique d'un lieu ou d'un corps, mais sa définition varie suivant les auteurs puisqu'il n'existe aucun moyen de la mesurer avec les outils de la physique conventionnelle. L'origine du nom Bovis est un sourcier français nommé André Bovis (1871-1947), parfois prénommé Alfred ou Antoine selon les radiesthésistes. L'ensemble théorique sous jacent de cette unité Bovis est scientifiquement inoccupé, les chiffres obtenus sont intuitifs et ne sont pas falsifiables.
L'Université interdisciplinaire de Paris (UIP) est une association loi de 1901 fondée en 1995 sous l'impulsion de Jean Staune, secrétaire général, et Jean-François Lambert, président. L'UIP se donne pour vocation la diffusion et la rencontre des savoirs, dans les domaines de la science, la philosophie, les différentes traditions de l'humanité, l'économie et le management afin de montrer leur implications sur l'évolution de la société. L'UIP n'est pas un établissement public à caractère scientifique, culturel et professionnel telle que définie par la loi Savary (1984) et n'est donc pas reconnue par le Ministère de l'Enseignement supérieur et de la Recherche comme une université.
Un vin biodynamique est un vin produit selon les principes de l'agriculture biodynamique. Les pratiques biodynamie peuvent s'appliquer, selon le producteur, tant aux méthodes de culture de la vigne (viticulture biodynamique) qu'à l'élaboration du vin (vinification biodynamique). La biodynamie considère que la plante, le sol et la terre sont un écosystème, dont il s'agit d'assurer l'équilibre. Elle se traduit dans des pratiques qui tendent à abolir tout intrant de synthèse, à apporter des soins favorisant la qualité de la terre et l'enracinement de la vigne, à travers notamment l'utilisation de préparations biodynamiques, le travail du sol souvent au cheval et l'application de calendriers solaire et lunaire,. Ces méthodes ont été adoptées par plusieurs domaines réputés tels que le domaine de la Romanée-Conti, M. Chapoutier, Cristal Roederer ou le domaine Zind-Humbrecht, mais elles ne font pas l'unanimité dans la filière viti-vinicole. Les tests scientifiques ont démontré que les pratiques de l'agriculture biodynamique ne produisaient pas de résultats différents de ceux de l'agriculture biologique traditionnelle, dont elle se révèle être une simple variante agrémentée d'ésotérisme. La biodynamie fait l'objet de critiques qui vont de la simple imposture à la crainte de la banalisation de l'anthroposophie dont elle découle, au même titre que d'autres méthodes découlant des théories de Rudolf Steiner.
La viticulture biodynamique est une approche de la culture de la vigne fondée sur les concepts de l'agriculture biodynamique. Les raisins qui en sont issus peuvent être vinifiés selon ces mêmes principes et donner un « vin biodynamique ». La biodynamie, approche ésotérique inventée par l'occultiste allemand Rudolf Steiner dans les années 1920-1930, se base principalement sur la notion d'« organisme agricole », c'est-à-dire sur la complémentarité entre les systèmes minéraux, végétaux et animaux, dans l'équilibre de l'écosystème agricole. En complément des techniques de l'agriculture biologique, la biodynamie utilise entre autres des préparations spécifiques « dynamisées[pas clair] » (à base de plantes, de quartz ou de bouse de vache, suivant une technique proche de l'homéopathie), censées tirer parti de « forces cosmiques » ou spirituelles. Elle suit aussi les rythmes lunaires, planétaires et astrologiques pour la planification de travaux agricoles. Cette méthode fait donc appel à un certain nombre de pratiques qui n'ont à ce jour jamais été validées scientifiquement ni dans leur démarche ni dans leurs résultats éventuels. Les pratiques de l'agriculture biodynamique testées par la science se sont révélées aussi efficaces que l'agriculture biologique traditionnelle, dont elle se révèle être une variante agrémentée d'ésotérisme. L'efficacité et l'intérêt des pratiques biodynamiques ne font pas l'unanimité dans la filière viti-vinicole. Certains voient l'agriculture biodynamique comme une simple imposture, ou font de Rudolf Steiner un « cinglé », quand d'autres y pressentent la banalisation des théories mystiques de Steiner, à tendance sectaire (comme la médecine anthroposophique ou la pédagogie Steiner).
Les indicateurs statistiques de recherche et développement (R-D)) décrivent les principaux aspects des systèmes de R-D. Ils sont des outils essentiels d'évaluation des politiques publiques de R-D, et sont au cœur des dispositifs de pilotage de ces activités.
Le tableau ci-dessous présente, pour la plupart des pays du monde et pour les grandes régions géographiques et économique, un indicateur possible parmi les nombreux indicateurs disponibles calculant l'évolution de leur production scientifique respective entre 1988 et 2003, en nombre d'articles en science ou ingénierie publiés chaque année. Les articles retenus sont ceux publiés dans les revues scientifiques couvertes par le Science Citation Index (SCI) et le Social Sciences Citation Index (SSCI). Les nationalités sont attribuées d'après la ou les adresses institutionnelles des auteurs indiquées sur l'article. La méthode adoptée est celle du comptage fractionnaire : pour les articles publiés par des auteurs de différents pays, chaque pays se voit attribuer une fraction de la publication sur la base de la proportion d'auteurs de ce pays parmi les signataires de l'article. Par exemple, pour un article cosigné par des chercheurs d'une institution française et britannique, la France se verra attribuer 0,5 article, ainsi que la Grande-Bretagne.
La recherche scientifique est, en premier lieu, l’ensemble des actions entreprises en vue de produire et de développer les connaissances scientifiques. Par extension métonymique, on utilise également ce terme dans le cadre social, économique, institutionnel et juridique de ces actions.
La recherche scientifique est une fonction importante de l'activité économique. Son pilotage, et en particulier la gestion des fonds publics dévolus au financée, requiert le développement d'une batterie d'indicateurs permettant d'évaluer l'efficacité des différents systèmes de recherche et la pertinence des politiques de recherche qui leur sont appliquées. Cette page présente, pour les années récentes, les données relatives aux principaux indicateurs de R&D.
L’association Bernard Gregory est une association française destinée à préparer et aider à la poursuite de carrière en entreprise des titulaires d'un doctorat. Elle a été fondée le 25 juin 1980, et nommée en l'honneur du physicien des particules Bernard Gregory. Elle propose des formations pour les doctorants et docteurs et dispose d'un site emploi pour la diffusion et la consultation d’offres d’emploi, de sujets de doctorat et de Master 2, ainsi que d’une CVthèque de candidats issus de toutes disciplines scientifiques.
Créée en 1953, l'Association nationale de la recherche et de la technologie (ANRT) rassemble les acteurs publics et privés de la Recherche et développement (R&D) en France. Son objectif est d’aider à améliorer l’efficacité du système français de recherche et d’innovation et en particulier les relations public-privé. Ses trois actions principales sont les Conventions CIFRE, la plate-forme de prospective FutuRIS et, avec le service Europe, l’amélioration des pratiques de recherche partenariale.
L'association science et bien commun (ASBC) est une association, créée en 2011 dont l'objet est la promotion d'une science ouverte.
L'autonomie de la science est la capacité de la communauté scientifique à produire ses propres règles de fonctionnement et à déterminer elle-même ses priorités de recherche, et la façon dont ses recherches doivent être menées puis évaluées, et cela indépendamment des considérations économiques, politiques ou idéologiques. Il s'agit d'un principe apparu avec ce que l'on appelle la modernité. Cette autonomie, défendue par les scientifiques et justifiée par nombre de sociologues et de philosophes par des considérations épistémologiques, se heurte aujourd'hui à diverses volontés politiques et économiques enjoignant aux communautés scientifiques de répondre à des impératifs extra-scientifiques. Elle est également l'objet de critiques émanant de certains courants de la sociologie des sciences. Enfin, elle est au cœur de nombreux débats publics, en particulier autour des questions d'éthique. L'autonomie de la science est un concept à la fois normatif et descriptif. Ces deux dimensions ne sont cependant pas indépendantes l'une de l'autre : les justifications de l'autonomie repose sur des analyses revendiquant leur objectivité. Inversement, la dénonciation de cette autonomie, et la défense d'une certaine hétéronomisation de la science, reposent de leur côté sur une critique de ces analyses.
En science, un biais méthodologique est une erreur dans la méthode scientifique, le non-respect des règles de protocole, qui engendre des résultats erronés.
Un cadre conceptuel est un outil d'analyse comptant plusieurs variations et contextes. Il est utilisé pour faire des distinctions conceptuelles et organiser des idées. Les cadres conceptuels forts saisissent quelque chose de réel et le font d'une manière facile à retenir et à appliquer. Isaiah Berlin emploie la métaphore d'un renard et d'un hérisson pour faire des distinctions conceptuelles dans la façon dont les philosophes et les auteurs importants voient le monde. Berlin décrit les hérissons comme ceux qui utilisent une seule idée ou principe d'organisation pour voir le monde (les exemples donnés comprennent Dante, Pascal, Dostoïevski, Platon, Ibsen et Hegel). Les renards d'un autre côté, incorporent un type de pluralisme et voient le monde à travers des lentilles multiples et parfois contradictoires (les exemples qu'il donne comprennent Goethe, Joyce, Shakespeare, Aristote, Hérodote, Molière, Anderson, Balzac). Les économistes emploient le cadre conceptuel de l'« offre » et de la « demande » pour faire la distinction entre le comportement des consommateurs et les systèmes d'incitation des entreprises. Comme beaucoup de cadres conceptuels, l'offre et la demande peuvent être présentées par le moyen de représentations visuelles ou graphiques.
Le titre de Candidat des Sciences (Russian: кандидат наук, Kandidat nauk) est un grade de l'enseignement supérieur d'origine soviétique et largement repris dans les autres pays de l'ancien bloc de l'Est, qui l'ont depuis abandonné. Il est toujours utilisé en Russie. Correspondant globalement à un doctorat français, ou un PhD anglo-saxon, il récompense un travail de recherche original dans les disciplines scientifiques. Ce diplôme qualifie son détenteur comme Maître de conférences, mais pour devenir professeur d'université, un titre de Docteur des sciences est requis à l'instar de l'Habilitation en Allemagne  Selon les directives russes d'équivalence des diplômes : dans les pays disposant de deux grades distincts de doctorats, le grade de candidat des Sciences devrait équivaloir au 1er grade dans les pays disposant d'un unique grade de doctorat devrait équivaloir à ce grade Selon la Classification internationale de type de l'éducation de l'UNESCO, il équivaut à un PhD.
La cartographie systématique est une « méthode de collecte, d’évaluation et de synthèse des connaissances scientifiques et techniques qui s’est développée initialement dans le domaine des sciences médicales, puis des sciences sociales, puis celles de l'environnement ». Elle s'appuie notamment sur les Revues systématiques. Dans un monde touché par de grandes crises globales (climatique et de biodiversité notamment) et où la science évolue rapidement, ces cartographies sont des « représentations du paysage de la connaissance en réponse à une question ou une problématique très large. Elles répondent à des questions qui s’apparentent à celles des revues systématiques dans leur structuration (PICO/ PECO). Elles ont pour objectif de mettre en évidence la répartition des connaissances selon des critères explicites (métadonnées) ». Ce type d'étude produit de plus en plus les « Evidence bases », sur lesquelles s’appuient (ou non ) les décideurs,.
Le Centre béninois de la recherche scientifique et technique (ou CBRST) est un centre de recherche public dont le siège est situé à Cotonou, la capitale économique du Bénin.
Le Centre commun de recherche (CCR, Joint Research Centre en anglais) est le laboratoire de recherche scientifique et technique de l'Union européenne. Cette direction générale de la Commission européenne a été créée dans le but d'apporter les conseils scientifiques et le savoir-faire technique nécessaires pour soutenir les orientations politiques choisies par l'Union.
Le Centre d'analyse et de recherche interdisciplinaire sur les médias (CARISM) regroupe les enseignants-chercheurs de l’Institut français de presse (IFP)(Département Information et Communication de l'université Panthéon-Assas) autour de programmes de recherche définis par l’équipe. Centre pionnier en France dans la recherche sur les médias, ses travaux se fondent sur une approche transdisciplinaire puisant dans le droit, l’économie, la géopolitique, la sociologie et la sémiotique.
Le Centre de compétences suisse en sciences sociales (FORS) est une infrastructure de recherche nationale, créée en 2008. Financée par le Secrétariat d’État à l'éducation et la recherche, le Fonds national suisse de la recherche scientifique et l'Université de Lausanne, son but est de fournir des services à la communauté scientifique en sciences sociales, de mener des recherches scientifiques et d'en publier et diffuser les résultats.
Le centre de recherche sur le mycétome (en anglais : Mycetoma Research Center ou MRC) est un centre de recherche soudanais situé à Khartoum.
Le Centre de recherche sur les civilisations de l'Asie orientale (CRCAO), fondé en 2006, est une unité mixte de recherche du CNRS (UMR 8155), domiciliée aux instituts d’Extrême-Orient du Collège de France.
Le Centre de recherche sur les ions, les matériaux et la photonique, ou CIMAP, est un laboratoire de recherche du CEA/DSM/IRAMIS, du CNRS/INP, de l'ENSICAEN et de l'Université de Caen Normandie. Ses missions sont centrées autour de trois activités : recherche et innovation, accueil des recherches interdisciplinaires au GANIL et enseignement.
Le Centre européen pour la construction de lanceurs d'engins spatiaux ou CECLES également connu par son acronyme en anglais ELDO, (European Launcher Development Organisation) est une organisation européenne créée en 1963 pour mettre au point un lanceur européen. Le développement du lanceur Europa est arrêté fin 1971 après une série d'échecs et le rôle du CECLES est repris en 1975 par l'Agence spatiale européenne.
Le Centre national d'appui à la recherche (ou CNAR) est un centre de recherche public dont le siège est situé à Ndjamena, la capitale du Tchad.
Le Centre national de la recherche scientifique et technologique (ou Cenarest) est un centre de recherche public dont le siège est situé à Libreville, la capitale du Gabon.
Présentées en octobre 2008 par Valérie Pécresse, les chaires d'excellence (officiellement appelées "chaires mixtes") offrent à de jeunes enseignants-chercheurs une prime annuelle de 6 000 à 15 000 euros et une enveloppe financière de 10 000 à 20 000 euros annuels pour mener à bien leurs travaux. Elles ont pour but de rendre la carrière universitaire plus attractive. Elles sont destinées aux jeunes maîtres de conférences et chargés de recherche. Elles constituent une des mesures de revalorisation des carrières des chercheurs d'un à quatre ans d'expérience. Ils bénéficient pendant cinq ans (durée renouvelable une fois selon la note d'orientation du 9 décembre 2008 et la convention signée par le CNRS avec les Universités) d'une décharge de deux tiers de leur temps d'enseignement afin de mener leurs recherches. Les chaires d'excellence sont gérées par les organismes de recherche et les universités. Elles représentent un budget de 4,5 millions d'euros.
La communauté scientifique désigne, dans un sens assez large, l'ensemble des chercheurs et autres personnalités dont les travaux ont pour objet les sciences et la recherche scientifique, selon des méthodes scientifiques. Parfois cette expression se réduit à un domaine scientifique particulier : la communauté des astrophysiciens pour l'astrophysique, par exemple. La sociologie des sciences s'intéresse à cette communauté, à la façon dont elle fonctionne et s'inscrit dans la société. Le terme est d'usage ancien, et permet à un chercheur de se référer à ses pairs afin de légitimer ses travaux de recherche. Cet usage s'appuie sur l'évaluation de travaux de recherche par les pairs au sein de comités de lecture lors de leur communication. Toutefois, l'expression « communauté scientifique » ne doit pas être prise trop littéralement. En effet, se posent les problèmes suivants : la communauté scientifique dans sa globalité n'est pas une communauté organisée, au sens où elle n'a pas de représentant légal, de porte-parole reconnu (même si certaines disciplines se dotent parfois d'une académie ou d'un Conseil de l'Ordre) ; la communauté scientifique n'est pas un ensemble de personnes totalement identifié, et l'appartenance à celle-ci n'est pas « labellisée » : voir par exemple le problème des pseudo-sciences ; la communauté scientifique n'est pas univoque. Dans un même domaine scientifique, y compris dans les sciences exactes, plusieurs avis différents peuvent coexister. Le problème de flou de l'expression est le même que pour la communauté internationale.
Le Conseil européen de la recherche (CER ; en anglais : European Research Council, ERC) est un organe de l'Union européenne chargé de coordonner les efforts de la recherche entre les États membres de l'UE et la première agence de financement pan-européenne pour une « recherche à la frontière de la connaissance ». Il a été institué officiellement le 27 février 2007 dans le cadre du septième programme-cadre. Son président est Jean-Pierre Bourguignon.
Le Conseil National pour la Recherche Scientifique et Technologique, ou Consejo Nacional para Investigaciones Científicas y Tecnológicas (CONICIT), est une institution autonome de la République du Costa Rica pour la promotion de la recherche scientifique et technologique. L'organisme gère des fonds pour la réalisation de projets de recherche en biologie, chimie, physique et lesMathématiques, entre autres. Le CONICIT est fondé le 22 août 1972, sous l'administration du Président José Figueres Ferrer et ouvre ses portes au public le 1er août 1973. Le CONICIT est organisé en plusieurs sections telles que le Conseil d'Administration et le Secrétariat exécutif chargé du département de Promotion de la Science et Technologie, ainsi que du département de gestion de l'organisme. Le Conseil d'Administration est l'organe supérieur en hiérarchie, formé de 5 membres, pour une période de 5 ans choisis par le Conseil du Gouvernement de la République.
Le Consortium pour le management de la recherche fondamentale et appliquée en Afrique au sud du Sahara (COMREFAS) est une institution internationale pluridisciplinaire dont le but est de structurer et de valoriser la recherche fondamentale et appliquée en Afrique dans différentes disciplines scientifiques.
Dans le domaine de la recherche scientifique, une convention de recherche est un contrat de prestation intellectuelle. La convention lie deux parties, appelées alors cocontractants, autour d'un projet de recherche. Elle est définie dans le temps : elle a une date de notification, une date d'effet et une date de fin. Elle peut contenir des clauses financières ou non.  Portail des sciences
En France, une convention industrielle de formation par la recherche (CIFRE) est un dispositif de financement de thèse qui aide les entreprises pour le recrutement de jeune chercheur-doctorant.
COST est un programme financé par la Commission Européenne, direction Générale de la Recherche, via Horizon 2020 lié à la Recherche et au Développement . COST est un programme intergouvernemental, gouverné par le "CSO", regroupant des représentants des 37 membres. (36 membres et un pays coopérant) COST a été créé en 1971. COST permet de coordonner et d’aider la circulation du savoir à l’échelle pan-européenne. COST finance la mise en réseau de chercheurs financés nationalement afin d'assurer une position forte à l'Europe dans le domaine des sciences et des technologies. Cela permet aussi de créer un effet de levier au niveau national, puisque COST permet une coordination des programmes de recherche au niveau de ses pays membres. COST permet aussi d'assurer une intégration des chercheurs au niveau de tous ses pays membres, ainsi qu'un partage des connaissances au niveau européen. (CF COST Position paper). Les différents réseaux financés par COST, les Actions COST, sont financées pour 4 ans, via le COST Grant System. La Sélection des Actions se fait deux fois par an via une procédure d'appel. En effet, toutes les Actions financées proviennent des chercheurs eux-mêmes. Chaque proposition provenant des chercheurs est évalués par un comité d'experts internationaux. Durant FP6 et FP7, COST était implémenté par l'European Science Foundation. Depuis Horizon 2020, les pays membres de COST ont créée une Association dédiée, la COST Association, qui est responsable de l'implémentation des Actions, et ce via un Accord de partenariat signé avec la Commission Européenne. Le CSO est devenu, pour le faire, l'assemblée générale de l'association.
Le Centro Studi E laboratori Telecomunicazioni (CSELT) était un institut de recherche de Turin dans le domaine des télécommunications,, le principal en Italie et l'un des plus importants en Europe et au niveau international,,. Il peut être considéré comme l'équivalent du CNET français avec qui il a collaboré à plusieurs reprises. Né en tant que centre de recherche pour l'ensemble du conglomérat IRI-STET, l'autorité du centre a été reconnue dans le monde entier dans la recherche appliquée. Il a souvent été pionnier dans les télécommunications, avec une orientation particulière vers la commutation numérique, la fibre optique, les technologies de codage de la voix, et la normalisation internationale des protocoles et des technologies (par exemple, la création du mp3, coordonnée par le centre; c'est probablement le développement le plus connu du grand public). Après une réduction considérable de ses effectifs au tournant du millénaire, la plupart des structures ont été transférées d'abord dans Telecom Italia Lab (une société du groupe Telecom Italia), puis à TIM, ainsi qu'à d'autres entreprises nées en tant que spin-off comme Loquendo (en 2001), active dans le domaine de la reconnaissance et la synthèse vocale. Le matériel historique de CSELT est conservé dans les Archives Historiques de TIM,. Le site web du CSELT est disponible dans les archives du Web depuis 1998; c'est ici que sont stockés les résumés des Rapports Techniques (CSELT Rapporti Tecnici) de 1995 à 1999, l'une des principales publications scientifiques de la société, publiées depuis le mois de juillet 1973. Les listes de brevets CSELT sont également disponibles sur le Web,. En 2011, le projet CSELTMUSEUM, a commencé avec le but de recueillir et de publier sur le Web des documents historiques, des inédits, qui sont difficiles à trouver, liés aux activités du Centre. Le groupe Facebook CSELTMUSEUM, après 6 ans d'activité, est devenu public, compte près de 500 membres, et contient à ce jour plus de 250 articles et 2000 images avec les discussions. Ce matériel est régulièrement réuni et publié sur différents canaux Web (par exemple, Researchgate et Slideshare). Ces collections et d'autres documents historiques sont également disponibles sur le Web. La naissance en 1961 CSELT a été fondée à Turin en 1961, d'abord sous le nom de CSEL - comme un centre d'études de Stipel, la mise en œuvre du désir exprimé depuis 1955 par son directeur général Giovanni Oglietti, qui était alors le premier président, jusqu'en 1968, et président d'honneur jusqu'en 1986. Après la naissance de SIP en 1963, il a fusionné avec le groupe IRI-STET et a pris le nom définitif « CSELT » le 5 décembre 1964. Le CSELT a étudié, depuis ses débuts, la fiabilité des équipements de commutation téléphonique du réseau italien, qui en 1963 était en train de devenir une organisation nationale unifiée. Giovanni Oglietti pose comme objectif principal pour ce Centre, l'unification et l'orientation des choix à moyen et à long termes des compagnies de téléphonie acquises par STET. C'est ainsi que naquirent plusieurs recherches de pointe, telles que la transmission et la commutation numérique du signal téléphonique, l'étude du trafic pour le réseau de service et de sa gestion. Le CSELT selon la volonté de son fondateur et est né sur le modèle de référence des Bell Labs, à l'intérieur d'un contexte dans lequel la société mère, l'IRI, a été inspirée par les modèles des États-Unis dans le domaine de la recherche et du développement. Cependant, son rôle et son pouvoir de décision au sein du groupe STET n'ont jamais été clairement définis, à la différence de ce qui s'est passé dans des centres de recherche similaires, tels que le CNET en France ; en particulier, on ne lui a pas affecté assez clairement un rôle de "Transfert de Technologie", par manque de décisions politiques dans ce sens, bien que, à plusieurs reprises, il ait réussi à agir efficacement dans ce domaine.
Pour comprendre le concept de découverte scientifique, il est nécessaire de le resituer dans le contexte de la science moderne, telle qu'elle a émergé depuis la Renaissance. En effet, pour Francis Bacon, le projet de la science est de percer les secrets de la Nature, et d'en extraire tout le savoir possible. Dans cette démarche, la découverte scientifique correspond à l'identification d'un fait ou d'un phénomène naturel original. Pour que ce fait ou ce phénomène soit reconnu, il est nécessaire que sa valeur universelle soit incontestable. Chaque découverte est donc soumise à l'épreuve des faits, par confrontation à l'expérience. Pour l'épistémologue Karl Popper, c'est la seule façon pour qu'une idée, aussi géniale soit-elle, ne reste pas à l'état d'hypothèse, mais qu'elle acquiert la valeur de découverte. Les découvertes archéologiques, géographiques et paléontologiques ont un statut à part.
Le développement consiste à transformer le principe d'un nouveau produit, procédé ou service en réalisation industrielle prête à être commercialisée ou exploitée. Selon le Manuel de Frascati, le développement est une phase de la recherche et développement (R-D) correspondant à la mise au point d'une invention, d'un procédé, d'un composé chimique ou d'un produit. Ainsi, le développement d'un véhicule automobile consiste à transformer un avant-projet conçu par le bureau de style en un véhicule, ainsi qu'à concevoir l'ensemble de la chaîne de production qui le produira, tant chez le constructeur que chez le sous-traitant.  Portail des technologies
Un doctorant est un chercheur en début de carrière s'engageant, sous la supervision d'un directeur de thèse, dans un projet de recherche sur une durée variable selon les pays et les statuts, comprenant la rédaction et la soutenance d'une thèse dans le but d'obtenir le grade de docteur. Même s'il s'agit d'une expérience professionnelle, le doctorant est obligatoirement inscrit dans un établissement habilité à délivrer le doctorat (généralement une université ou certaines « grandes écoles ») et, à ce titre, a le statut d'usager de son établissement d'inscription, comme c'est le cas pour les chercheurs passant leur habilitation à diriger des recherches.
Le doctorat (du latin doctorem, de doctum, supin de docere, enseigner) est généralement le grade universitaire le plus élevé. Le titulaire de ce grade est le docteur. Selon les pays et les époques, le doctorat peut être un grade d'État (cas en Russie, en France à partir de l'époque napoléonienne jusqu'en 1897) ou un grade d'établissement (cas en Amérique du Nord, au Royaume-Uni, en Allemagne, en France avant la Révolution ou après 1984), voire mixte (cas en France de 1897 à 1984 avec la réintroduction progressive du doctorat d'établissement et l'élimination progressive du doctorat d'État), et peut être régi de manière globale, ou par discipline. En France, on effectue ses recherches dans une formation de minimum 3 ans qui est une école doctorale et c'est celle-ci qui délivre le doctorat et pas seulement l'établissement d'inscription administrative. Au XXIe siècle, quatre principaux types de doctorat existent dans le monde : le doctorat de recherche, qui est l'aboutissement d'un premier travail de recherche scientifique original, suivi de la rédaction d'une thèse et de sa soutenance devant un jury académique. Il correspond au standard international du PhD ; le doctorat supérieur de recherche, qui est conféré à la suite de la réalisation de plusieurs travaux de recherches au cours d'une carrière de chercheur ; le doctorat d'exercice ou doctorat professionnel, dont l'obtention n'est pas liée à un travail de recherche mais à l'aboutissement d'un premier cycle de formation universitaire. Il conduit à l'exercice d'une profession, notamment dans les disciplines de la santé (M.D) ou du droit anglo-saxon (J.D). Ce doctorat de premier cycle ne confère pas le grade de Ph D. Les titulaires de doctorats d'exercice souhaitant poursuivre leur carrière académique doivent préalablement effectuer une thèse de doctorat de type Ph D. après avoir suivi la scolarité de deuxième et de troisième cycle universitaire. le doctorat honoris causa, qui est honorifique. Avant le développement du doctorat de recherche existait un doctorat d'érudition et d'éloquence, fondé sur la dispute oratoire.
Le grade de doktor nauk ou doktor naouk (en russe : доктор наук, « docteur ès sciences ») est un grade de l'enseignement supérieur, le plus haut dans l'ex-Union soviétique, et maintenant la Russie, et dans beaucoup de pays de la CEI.
Les données de la recherche sont l'ensemble des informations produites et utilisées par la recherche scientifique. Elles peuvent recouvrir différents types de données selon le point de vue adopté, notamment par les différents métiers qui composent la recherche scientifique ainsi que l'information scientifique et technique.
L’EA-41 est une fusée expérimentale développée par la France à partir de 1941. C'est la première fusée à propergol liquide développée en France.
ELIPS (European programme for Life and Physical sciences in Space and applications utilising the International Space Station) est un programme européen de recherche autour de la microgravité, mis en œuvre à bord de la Station spatiale internationale. Les recherches portent sur des domaines variées, comme : la médecine cardio-vasculaire les neurosciences les physique des plasmas l'exobiologie  Portail des sciences  Portail de l’astronautique
L’Encyclopédie du conte (en allemand : Enzyklopädie des Märchens, abrégé en EM), est un ouvrage de référence en langue allemande et en 15 volumes, dont le projet a été initié par le germaniste et folkloriste Kurt Ranke pour servir à la recherche internationale sur les contes.
EOLE est un prototype de missile balistique développé par l'officier d'artillerie français Jean-Jacques Barré entre 1946 et 1952.
L’Espace européen de la recherche (EER mais plus connu sous l'acronyme anglais ERA) est un concept créé par la Communauté européenne pour décrire sa politique en matière de recherche et d'innovation. Son but est de promouvoir une approche unifiée de la recherche de la part de la Communauté européenne élargie par rapport aux programmes strictement nationaux. L'incitation à un accroissement de la coopération en recherche de la part de la Commission commence en 1984. La définition de l'EER est un pas supplémentaire qui dépasse le concept de coopération et parle d'intégration. Il est aussi un instrument pour mettre en place la Stratégie de Lisbonne. L'EER a été lancé avec le Sixième programme-cadre et a été renforcé dans le Septième programme-cadre. La brochure de présentation[réf. nécessaire] le décrit comme un « marché commun » pour la recherche.
La Fédération internationale de la sclérose en plaques (MSIF) a été créée en 1967 en tant qu'organe international reliant les activités des sociétés nationales de la SEP dans le monde. La Fédération cherche à travailler en partenariat mondial avec les sociétés membres et la communauté scientifique internationale pour éliminer la sclérose en plaques et ses conséquences, et pour parler au nom des personnes touchées par la sclérose en plaques.
La Fédération mondiale des travailleurs scientifiques (FMTS, (en) : World Federation of Scientific Workers) est une organisation rassemblant des scientifiques du monde entier fondée en 1946 par l'Association britannique des travailleurs scientifiques. Son premier président fut Frédéric Joliot-Curie.
FutuRIS (acronyme signifiant « Futur Recherche Innovation Société ») est un centre indépendant d'analyse et de prospective stratégique, étudiant les évolutions du système français et européen de recherche et d’innovation (SFERI). Il a pour mission d’éclairer la prise de décision, de formuler des recommandations de politique publique et d’accompagner le déploiement de stratégies concertées. Pour cela, FutuRIS développe une approche fondée sur des repères solidement documentés et sur la confrontation de visions d’acteurs et d’experts issus d’horizons divers. FutuRIS est animé par l'Association nationale de la recherche et de la technologie et est soutenu par une trentaine de contributeurs (ministères, agences, institutions de recherche, entreprises), avec l’appui de l’Académie des sciences et de l’Académie des technologies.
Un groupe de travail est un regroupement d'un petit nombre (moins de 15) d'individus dont l'objet est la réalisation d'un travail commun. Un groupe de travail n’est ni une famille, ni une bande, ni un groupe d’amis, c’est un groupe de personnes qui ont des relations interpersonnelles de face à face. Il est caractérisé par la convergence des efforts, par l’exécution d’une tâche qui sera l’œuvre commune. Il existe de nombreux exemples de groupes de travail, tels que : des cercles de qualité ; des groupes interdisciplinaires de recherche scientifique.
HumGen.org est le site Web du Centre de génomique et politiques (CGP) affilié à l’Université McGill et au Centre d’innovation Génome Québec. Le Centre a été lancé pour répondre aux besoins en matière de politique publique et d’analyses éclairées sur les enjeux socio-éthiques liés à la recherche en génétique humaine aux niveaux international, national et provincial. Le site Web HumGen vise à offrir aux décideurs et à la population un accès à des énoncés de politique sur le thème de la recherche en génétique.
L'indice de démocratie est un indice créé en 2006 par le groupe de presse britannique The Economist Group qui permet selon ses critères d'évaluer le niveau de démocratie de 167 pays dont 166 sont des États souverains et 165 sont membres de l'Organisation des Nations unies. Cette étude est publiée pour la première fois en 2006 puis actualisée en 2008, 2010, 2011, 2012, 2013, 2014, 2015 et 2016. Le calcul est basé sur 60 critères regroupés en cinq catégories : le processus électoral et le pluralisme, les libertés civiles, le fonctionnement du gouvernement, la participation politique et la culture politique. La notation se fait selon une échelle allant de 0 à 10 et à partir de cette note les pays sont classifiés selon quatre régimes : démocratique, démocratique imparfait, hybride ou autoritaire. Le plus faible score enregistré est celui de la Corée du Nord en 2008 atteignant 0,86. Le score le plus élevé correspond à celui de la Norvège qui en 2014 est de 9,93. La Corée du Nord, quant à elle, avec une moyenne de 1,08 sur 10, occupe toujours en 2014 la position 167, la dernière du classement.
L'Indice Kardashian (ou Indice K) est une mesure de l'écart entre la popularité numérique d'un chercheur scientifique et son dossier de publication. Baptisé d'après Kim Kardashian, l'Indice K est lancé en 2014 par le biologiste Neil Hall dans une volonté de dénonciation humoristique de l'hypertrophie médiatique de certains de ses collègues.
Un institut de recherche est un établissement, laboratoire ou organisme de recherche et d'enseignement spécialisé dans les domaines de la recherche scientifique, de la recherche historique ou dans le domaine de la sociologie et des sciences sociales. Les instituts de recherche peuvent se spécialiser dans la recherche fondamentale ou peuvent être orientés vers la recherche appliquée. Les instituts de recherche peuvent être liés en partenariat à des universités, des musées, des entreprises et des ministères. Les instituts de recherche sont à la pointe de la production mondiale d'articles scientifiques et publient des comptes rendus de leurs travaux, exposés et conférences scientifiques dans de nombreuses publications scientifiques. Les instituts de recherche expérimentent d'une part la méthode scientifique qui désigne l'ensemble des processus de production des connaissances scientifiques, qu'il s'agisse d'observations, d'expériences, de raisonnements, ou de calculs théoriques ; et d'autre part la méthode expérimentale qui est une démarche scientifique qui permet de tester par des expériences répétées la validité d'une hypothèse en obtenant des données nouvelles, qualitatives ou quantitatives, conformes ou non à l'hypothèse initiale.
En 2006, le Centre Pompidou, sous l’impulsion du philosophe Bernard Stiegler et de Vincent Puig, a créé en son sein l'Institut de recherche et d'innovation (IRI) pour anticiper les mutations des pratiques culturelles permises par les technologies numériques. En août 2008, l'IRI a acquis un statut d'association de recherche autonome  cofondée par le Centre Pompidou, le Centre de culture contemporaine de Barcelone (CCCB) et Microsoft France. Depuis cette date ils ont été rejoints par le Goldsmiths College de l'université de Londres, l'ENSCI, l'Institut Télécom, l'université de Tokyo, Alcatel Bell labs et France Télévisions.
L'Institut de recherche industrielle, Inc. (IRI) est une association à but non lucratif basée dans le Comté d'Arlington en Virginie. La mission déclarée de l'IRI, fondé par le Conseil national de la recherche en 1938, est « d'améliorer l'efficacité de l'innovation technologique en mettant en réseau les meilleurs praticiens du monde et des faiseurs d'opinion pour rechercher, partager, apprendre et créer ». L'IRI est une organisation non partisane, fondée sur un système d'appartenance, qui rassemble les chefs de la R & D, pour découvrir et partager les meilleures pratiques dans la gestion de l'innovation technologique.
L'Institut de recherches apicoles (en russe : Научно-исследовательский институт пчеловодства) est la principale institution de Russie dans le domaine de l'apiculture. Il est situé à Rybnoïe. Cet institut dépendant de l'académie des sciences de Russie. L'histoire de l'institut commence en 1930.
L'Institut pour la protection et la sécurité des citoyens (IPSC), localisé à Ispra en Italie, est un des sept instituts du Centre commun de recherche de la Commission européenne. La mission de l'Institut pour la protection et la sécurité des citoyens est de fournir des résultats de recherche et de soutenir les décideurs de l'Union européenne dans leurs efforts en matière de sécurité mondiale et de la protection des citoyens européens contre les accidents, les attaques délibérées, la fraude et les actions illégales contre les politiques de l'Union européenne.
L'Institut togolais de recherche agronomique (ou ITRA) est un institut public de recherche en sciences agronomiques établi à Lomé, la capitale du Togo.
L'Instituto de ciencia y tecnología agrícolas (ICTA) (Institut de science et technologie agricoles) est un institut de recherche agronomique guatémaltèque dont le siège est situé à Villa Nueva (département de Guatemala). C'est un établissement public fondé en 1972 et rattaché au ministère de l'Agriculture, de l'Élevage et de l'Alimentation (Ministerio de Agricultura, Ganadería y Alimentación) (MAGA).
Le réseau des instituts français de recherche à l’étranger (désigné par les acronymes IFRE ou UMIFRE) comprend 27 centres de recherche répartis, avec leurs antennes, dans près de 40 pays sur tous les continents. Ces instituts de recherche sont placés sous la cotutelle du ministère français chargé des Affaires étrangères (Direction générale de la mondialisation - Direction de la coopération culturelle, universitaire et de la recherche (DCUR) - Sous-direction des échanges scientifiques et de la recherche) et du CNRS. Ces instituts ont été créés en plusieurs vagues, depuis les années 1920 jusqu'à la fin du XXe siècle . Les 27 instituts français de recherche à l'étranger coopèrent avec les institutions universitaires et de recherche des pays d’accueil et les organisations françaises de recherche en sciences humaines et sociales, comme, en particulier l’Institut de recherche pour le développement (IRD), les Maisons des sciences de l’homme (MSH), l’École pratique des hautes études (EHPE), l’École des hautes études en sciences sociales (EHESS) et des universités françaises. L'ensemble des activités et des productions scientifiques du réseau des Instituts français de recherche dans le monde est présenté sur le site officiel des IFRE, http://www.ifre.fr (agenda, actualités, publications, programmes de recherche, ressources électroniques).
L'Inter-Agency Space Debris Coordination Committee (IADC, Comité interagence de coordination des débris spatiaux) a été constitué en 1993 par la NASA, l'Agence spatiale européenne (ESA) et les agences spatiales civiles russe et japonaise à la suite des discussions des années 1980 et du début des années 1990.
Le terme jardin des plantes, plantes sous-entendues médicinales, se réfère à une spécificité de la langue française. En France, un jardin des plantes est aussi bien un jardin botanique qu'un jardin médicinal (du moins, médicinal en référence aux origines de ce type de jardin en France). Alors que, historiquement, les jardins médicinaux sont strictement liés à la production de plantes médicinales, les jardins des plantes couvrent aussi les autres domaines botaniques dans leur ensemble. Aussi, les jardins médicinaux, quoique parfois très anciens, sont très souvent privés ou appartenant à des institutions qui en font un usage privatif. Au Moyen Âge, les médecins et les monastères possédaient très souvent des jardins médicinaux qui ne pouvaient être assimilés à des jardins des plantes parce qu'ils étaient fermés au public et ne permettaient pas le partage des connaissances et des espèces botaniques. Les jardins des plantes sont bien au contraire remarquables pour leur histoire, leur rôle dans l’avancement de la science, les hommes éminents qui y ont travaillé et la richesse de leurs collections (arbres, plantes endémiques rares…).
Le Laboratoire d'électronique, systèmes de communication et microsystèmes (ESYCOM), est un laboratoire de recherche français qui regroupe des enseignants-chercheurs de trois établissements d'Île-de-France : CNAM Paris-centre, ESIEE-Paris, UPEM.
Le labyrinthe à bras radial a été conçu par Olton et Samuelson en 1976 pour mesurer l'apprentissage et la mémoire spatial chez les rats. L'appareil d'origine se compose de huit bras équidistants, chacun d'environ 1,2 m de long et rayonnant à partir d'une petite plate-forme circulaire centrale (des versions ultérieures ont utilisé seulement trois bras et jusque 48 bras). À la fin de chaque bras il y a un site contenant de la nourriture. Les contenus ne sont pas visibles depuis la plate-forme centrale. Deux types de mémoire sont évalués au cours de ce test : la mémoire de référence et la mémoire de travail. La mémoire de référence est évaluée lorsque les rats visitent uniquement les bras du labyrinthe contenant une récompense. L'échec (absence de nourriture) se traduire par une erreur de mémoire de référence. La mémoire de travail est évaluée lorsque les rats entrent dans chaque bras une seule fois. La ré-entrée dans les bras se traduit par une erreur de mémoire de travail[réf. insuffisante]. La conception assure que, après vérification de la présence de nourriture à l’extrémité de chaque bras, le rat est toujours forcé de retourner à la plate-forme centrale avant de faire un autre choix. Par conséquent, le rat a toujours huit options possibles. Des mesures sont utilisées pour s'assurer que les rats ne peuvent pas tout simplement utiliser leur sens de l'odorat, soit pour détecter des objets alimentaires ou pour détecter leurs propres pistes. Olton et Samuelson ont constaté que les rats ont d'excellents souvenirs pour les bras visités et non visités. Environ 7,0 nouvelles entrées sur 8,0 (88 %) sont des bras non visités, et donc correctement identifiés comme tels. Un choix aléatoire serait de 5.3 nouvelles entrées dans les 8 premiers choix (66 % correct). Olton et Samuelson ont également découvert, après avoir permuté des bras déjà visités avec des bras non visités, que les rats avaient tendance à visiter les emplacements du labyrinthe non visités, même si les bras correspondant avaient déjà été parcourus avant d’être permutés. Et inversement, les rats tendaient à éviter les bras des emplacement du labyrinthe déjà visités, même si les bras n'avaient pas visités. Il semble donc que le souvenir des emplacements dans les bras du labyrinthe ne reposent pas sur des indices des bras, mais plutôt sur des indices extérieurs.
Le labyrinthe de Morris, ou piscine de Morris (du nom de Richard G. Morris qui l'a conçu en 1984), est un dispositif aquatique circulaire très utilisé en neurosciences comportementales pour évaluer la mémoire du rongeur. Il est divisible virtuellement en quadrants et une plateforme immergée (invisible) est localisée dans l'un de ceux-ci. Le principe de son utilité réside dans la motivation de l'animal à échapper à l'aversion causée par l'eau, celui-ci devant trouver et grimper le plus rapidement possible sur la plateforme. La position de la plateforme reste inchangée, contrairement à la position de départ du rongeur qui, elle, varie de quadrant en quadrant au fil des essais. Des signaux (bureau, frigidaire, poster, ...), répartis aux alentours de la piscine, sont utilisés pour permettre à l'animal de s'y diriger plus facilement. Les données généralement analysées sont la latence à trouver la plateforme, la distance parcourue et la vitesse pour l'atteindre. Il est ainsi attendu que la latence soit de plus en plus courte et que le chemin parcouru soit plus direct et plus rapidement effectué au fil des essais et lorsque l'intervalle de rétention entre la phase de mémorisation et la phase de rappel est court.
Les méthodes expérimentales scientifiques consistent à tester la validité d'une hypothèse, en reproduisant un phénomène (souvent en laboratoire) et en faisant varier un paramètre. Le paramètre que l'on fait varier est impliqué dans l'hypothèse. Le résultat de l'expérience valide ou non l'hypothèse. La démarche expérimentale est appliquée dans les recherches en biologie, physique, chimie, psychologie, ou encore l'archéologie. Certains soutiennent que le savant Ibn Al Haytham (Alhazen),,, a été l'un des premiers à faire la promotion des méthodes expérimentales. Définies par le chimiste Michel-Eugène Chevreul en 1856, elles ont été développées par Claude Bernard en médecine et en biologie. Outil privilégié des sciences de la nature, les méthodes expérimentales sont également utilisées en sciences humaines et sociales.
Il serait abusif de faire remonter la notion de méthode scientifique jusqu’à l’Antiquité, tant il est délicat d’identifier ce que nous nommons « science » avec les démarches de production de nouveaux savoirs aux époques proto-historiques. On peut cependant reconnaître dans les réflexions des anciens philosophes les prémisses d’une théorie de la connaissance congruante avec les pratiques scientifiques contemporaines, mais pas des vrais méthodes. Au cours des siècles, différents philosophes enrichiront la réflexion sur la notion de méthode en en explorant différents aspects (déduction, induction, méthode expérimentale, méthode analytique, réfutation, etc.), sans qu’il soit toujours fait un lien entre eux. Cette histoire n’est donc pas linéaire, mais se présente plutôt comme un buissonnement d’idées qui s’agrègent aujourd’hui dans la notion de méthode scientifique. Il faut cependant distinguer l’histoire de la méthode en tant que notion normative de l’histoire de la méthode en tant que pratique scientifique effective. Tandis que les philosophes visent, souvent dans une perspective normative, à éclaircir la notion de méthode scientifique, les savants ne se préoccupent pas toujours de ses considérations, et n’ont pas toujours une démarche réflexive. Il ne faut cependant pas verser dans l’excès inverse, et imaginer que travail scientifique et travail sur la science s’ignorent. De nombreux savants, et non des moindres, portent attention aux discours sur la science, tandis que les philosophes, historiens ou sociologues faisant porter leurs réflexions sur la méthode scientifique peuvent influencer, directement ou indirectement, l’organisation de la science. Enfin, indépendamment de la dimension normative de la notion de méthode, il faut porter attention à l’évolution des regards portés sur cette méthode. Les analyses des historiens, puis des sociologues, ont évolué au fil du temps, faisant du même coup évoluer notre représentation de la méthode scientifique. Au bout du compte, il faut entrelacer trois perspectives : l’histoire de la méthode comme pratique ; l’histoire de la méthode comme concept normatif ; l’histoire des discours sur la méthode.
La méthode scientifique désigne l'ensemble des canons guidant ou devant guider le processus de production des connaissances scientifiques, qu'il s'agisse d'observations, d'expériences, de raisonnements, ou de calculs théoriques. Très souvent, le terme de « méthode » engage l'idée implicite de son unicité, tant auprès du grand public que de certains chercheurs, qui de surcroît la confondent parfois avec la seule méthode hypothético-déductive. L'étude des pratiques des chercheurs révèle cependant une si grande diversité de démarches et de disciplines scientifiques que l'idée d'une unité de la méthode est rendue très problématique. Ce constat ne doit cependant pas être entendu comme une forme d'anarchisme épistémologique. Si la question de l'unité de la méthode est problématique (et ce problème sera abordé plus en détail ci-dessous), cela ne remet pas en question l'existence d'une pluralité de canons méthodologiques qui s'imposent aux chercheurs dans leurs pratiques scientifiques.
MISTRALS (Mediterranean Integrated STudies at Regional And Local Scales) est un programme de recherche consacré à l'étude du bassin méditerranéen et de son environnement, l'objectif étant de « mieux comprendre l'impact des facteurs globaux sur cette région et d'anticiper l'évolution, sur un siècle, de ses conditions d'habitabilité ». Créé en août 2008 sous l'égide du CNRS, il est rapidement devenu un programme de collaboration de nombreux organismes de recherches d'abord français, puis de nombreux autres pays méditerranéens. « La gouvernance générale est assurée par un Comité de Pilotage International et d’un Comité de Pilotage Inter-Organisme, qui valident les directions scientifiques générales de chaque programme thématique et procurent un fonds de roulement financier ». En 2011, sa direction scientifique est assurée par le CNRS et L'IRD (ses deux codirecteurs sont Étienne Ruellan du CNRS/INSU et Abdelghani Chehbouni de l'IRD), et y participent également onze autres organismes de recherche français (ADEME, BRGM, CEA, CEMAGREF, CIRAD, CNES, IFP Énergies nouvelles, IFREMER, INRA, IRSN et Météo-France). En 2011, plus de 1000 scientifiques participent aux sept programmes scientifiques chapeautés par MISTRALS, ces scientifiques sont répartis sur la plupart des pays méditerranéens et certains pays européens. Les programmes de recherches financés sont interdisciplinaires,, : les thèmes abordés sont, notamment, les écosystèmes marins et terrestres, la pêche, le climat, la géologie, l'urbanisme, l'agriculture, la disponibilité de l'eau douce, l'économie, la sociologie, l'anthropologie.  
Le sigle OHERIC (Observation, Hypothèse, Expérience, Résultats, Interprétation, Conclusion) désigne la succession d’étapes d’un modèle idéalisé de démarche scientifique. Il correspond à une critique formulée à l’encontre d’une telle présentation linéaire dans l’enseignement des sciences, qui laisse de côté les errements, les tâtonnements et les fausses pistes habituellement suivies dans le cheminement réel de la recherche, parcours sinueux dans lequel la solution est progressivement construite à coups d’hypothèses fausses successivement rectifiées.
L’opérationnisme, opérationisme ou opérationnalisme « consiste à définir les concepts de telle manière qu'ils puissent être établis et éprouvés en termes d'opérations concrètes et répétables, par des observateurs indépendants ». Cette approche, introduite au début du XXe siècle par le physicien Percy Williams Bridgman, facilite le développement de la mécanique quantique. Elle est ensuite appliquée en psychologie.
Paris-Saclay est un pôle scientifique et technologique (cluster) en cours d'aménagement à vingt kilomètres au sud de Paris, sur une zone couvrant 27 communes des départements de l'Essonne et des Yvelines. Sa construction, lancée en 2006, doit durer quinze à vingt ans pour permettre de regrouper à terme entre 20 et 25 % de la recherche scientifique française. Le projet Paris-Saclay vise à regrouper organismes de recherche, grandes écoles, universités et entreprises privées afin de créer un pôle d'excellence scientifique et technique de dimension internationale, comparable à la Silicon Valley ou à la région de Cambridge. Les premières implantations d'organismes de recherche datent de l'après-guerre. Le territoire s’est par la suite développé à plusieurs reprises pendant les années 1970 et 2000, qui ont vu l'installation de grandes écoles et de centres de recherche et développement de grandes entreprises. Plusieurs projets ont été lancés pour poursuivre le développement du site : la construction de trois ZAC, deux sur le territoire de la communauté d'agglomération Paris-Saclay, autour de l’École polytechnique et de CentraleSupélec, et une à Versailles-Satory ; la construction de la ligne 18 du métro de Paris ; et la constitution de l'Université Paris-Saclay. Plusieurs grandes écoles, dont AgroParisTech, l'ENS Paris-Saclay, Télécom ParisTech et l'ESTACA doivent compléter le campus en déménageant sur le plateau de Saclay à l'horizon 2019. Le projet Paris-Saclay représente plus de 1 300 000 m2 de locaux d’enseignement, de bureaux, de logements à construire d’ici 2020 sur un territoire de 7 700 hectares. L'établissement public Paris-Saclay est créé en 2010 et devient l'établissement public d'aménagement Paris-Saclay en 2015 pour superviser les opérations d'aménagement et œuvrer au développement économique du cluster. L'EPPS a été présidé par Pierre Veltz, ancien directeur de l'École nationale des ponts et chaussées, de sa création en 2010 à octobre 2015. Il est actuellement présidé par Philippe Van de Maele. Ayant pour cœur le plateau de Saclay et ses nombreuses infrastructures de recherche fondamentale et de formation, Paris-Saclay englobe également les communes de la vallée de l'Yvette (Palaiseau, Orsay, Gif-sur-Yvette principalement), ainsi que le territoire de Saint-Quentin-en-Yvelines, Massy, Vélizy-Villacoublay et Versailles.
Plume ! est un réseau national de vulgarisation scientifique à vocation multimédia et multidisciplinaire. Structurée autour d'antennes dans des villes universitaires, l'association rassemble également une communauté électronique d'acteurs concernés par (et impliqués dans) la diffusion des connaissances. L'association, créée en 2006, édite un trimestriel papier et une plateforme de publication web. Plume ! met également en place des formations universitaires, notamment à destination des étudiants en doctorat.
L’Institut français du Proche-Orient (Ifpo) fait partie du réseau des centres de recherches français à l'étranger (IFRE). Il est présent en Syrie, au Liban, en Jordanie, en Irak et dans les Territoires palestiniens.
Le programme de veille sur la radicalisation (Radicalization Watch Project, en anglais, RWP) est un programme de recherche tripartite (France-États-Unis-Canada), lancé en 2004 par le Laboratoire d’Analyse de l’Information Stratégique et de Veille Technologique au sein du CREC Saint-Cyr Coëtquidan (sous la direction de Mathieu Guidère alors professeur résident à Saint-Cyr). Ce programme se concentre sur le suivi informationnel multilingue des groupes radicaux dans le monde et sur l’analyse et la diffusion de l’information recoupée concernant les aspects idéologiques et psychologiques du terrorisme. Les thématiques principales du groupe de recherche sont axées sur la radicalisation, les droits de l’homme, la psychologie du terrorisme, la propagande terroriste, les psycho-traumatismes liés aux attentats terroristes et la gestion du stress post-traumatique. Basé sur un réseau d’experts spécialistes en terrorisme international, le programme RWP s’efforce de maintenir le contact entre des chercheurs et des universitaires travaillant sur la radicalisation en fournissant des recherches théoriques et pratiques, en facilitant l’échange d’expertise et en diffusant les résultats les plus pertinentes des recherches scientifiques sur ces thématiques. Le programme entretient un réseau de chercheurs sur le terrain localisés sur les cinq continents et en particulier en Afrique et en Asie, avec une focalisation sur la région MENA. Ce réseau alimente plusieurs bases de données à finalité heuristique et scientifique : 1) une base de données écrites (communiqués de revendications, documentation papier en langue source, livrets de propagande, etc.), une base de données audiovisuelles (vidéos et enregistrements audio issus des groupes radicaux), ainsi qu'une base de données cartographiques (cartes de localisation géographique des acteurs et des foyers de la radicalisation dans le monde). Grâce à un système de veille stratégique multilingue, le réseau se distingue par un travail de recherche mené exclusivement à partir de sources de première main et des documents en langue étrangère (arabe, ourdou, pachto, swahili, etc.), avec une offre de traduction pour les documents les plus importants.
Le projet Athena était un projet commun au MIT, DEC et IBM pour produire un environnement informatique distribué à l'échelle de plusieurs campus universitaires à des fins pédagogiques. Le projet Athena fut à l'origine de plusieurs technologies utilisées de nos jours comme l'interface utilisateur graphique X Window System ou les protocoles Kerberos et Zephyr. Le projet fut lancé en 1983 et les R&D se poursuivront pendant huit années jusqu'au 30 juin 1991. Le projet développa d'autres technologies, parmi lesquelles Xaw.  Portail de l’informatique  Portail du Massachusetts
Le protocole opératoire est l'ensemble des événements programmés et volontaires devant aboutir au parfait déroulement d'une expérience ou d'une opération particulière. Le protocole opératoire comporte éventuellement des éléments liés à la sécurité, et ce, dans tous les domaines où il y a danger pour l'environnement immédiat ou bien les opérateurs.
Le Recensement de la vie marine (Census of Marine Life ou CoML en anglais) est un vaste programme international de recherche en biologie marine. Démarrée officiellement le 23 octobre 2003 à Washington, cette étude, réalisée par 360 scientifiques du monde entier, a pour objectif de chercher, d'étudier et d'énumérer la biodiversité des fonds marins des mers et des océans du globe.
La recherche appliquée consiste en des travaux originaux entrepris en vue d'acquérir des connaissances nouvelles. Cependant, elle est surtout dirigée vers un but ou un objectif pratique déterminé, à la différence de la recherche fondamentale.
La recherche et développement (parfois abrégée en « R et D », « R & D » ou « R&D ») désigne l'ensemble des activités entreprises « de façon systématique en vue d’accroître la somme des connaissances, y compris la connaissance de l’homme, de la culture et de la société, ainsi que l’utilisation de cette somme de connaissances pour de nouvelles applications. ». L'expression désigne aussi par extension les catégories statistique, économique, comptable ou organisationnelle qui concernent ces activités.  Le concept de « recherche et développement » est une traduction littérale de l'anglais. On le trouve aux États-Unis dès 1953, dans des rapports d'ingénierie. Toutefois, ce n’est que dans les années 1960 qu’il se popularise, dans le jargon de l’ingénierie et des sciences, puis dans celui du management,,. En français, il fait son apparition vers la fin des années 1960, dans des traductions d’ouvrages américains, avant de se populariser avec l’enseignement du management d’entreprise,,. R&D est une tautologie propre à la stylistique anglaise, tout comme Q&A, S&D, R&B ou T&C, etc. En effet, tout développement nécessite une recherche préalable, appelée « étude » en français traditionnel. L’usage de la perluète est d'ailleurs courant en anglais, à cause de l'attrait visuel qu'elle présente du point de vue commercial. Son omniprésence dans les doublons commerciaux (C&A, B&Q, S&P, etc.) lui a d’ailleurs valu son surnom de « & commercial ». On distingue trois grandes composantes de la recherche et développement : la recherche fondamentale ; la recherche appliquée ; le développement expérimental. Les entreprises mènent des activités de recherche et développement afin d’améliorer leurs capacités de production, la qualité, notamment environnementale, de leur production, ou afin de créer de nouveaux biens et services, si possible en accord avec les orientations du marketing et, entre autres, en matière de développement durable. La « recherche et développement » est un élément majeur de la croissance et du développement par l'innovation, aussi bien pour une entreprise que pour un pays ou une vaste zone économique.
La recherche européenne est une notion qui désigne la recherche scientifique menée au niveau européen, et en particulier dans le cadre de la Communauté européenne. Elle ne doit pas être confondue avec la notion d'espace européen de la recherche, qui renvoie à l'idée d'une meilleure coordination et d'une plus forte intégration de la recherche des différents pays de l'Union européenne.
La recherche fondamentale consiste en des travaux expérimentaux ou théoriques entrepris principalement en vue d'acquérir de nouvelles connaissances sur les fondements des phénomènes et des faits observables, sans envisager une application ou une utilisation particulière. On oppose en général la recherche fondamentale à la recherche appliquée. Cette distinction est délicate à établir car de nombreux projets se situent à la frontière entre les deux. De nombreuses innovations majeures ont été développées dans une logique de recherche fondamentale, et n'auraient pas été développées dans un cadre de recherche appliquée ; par exemple, les technologies liées au laser, ou encore la théorie du chaos. Une distinction existe cependant qui relève avant tout de la démarche liée à l'objectif même de la production scientifique : la recherche fondamentale, comme sa définition le présente, cherche à acquérir de nouvelles connaissances qu'une ou des applications puissent exister ou non alors que la recherche appliquée part du besoin d'une application ou d'une amélioration de l'existant. La recherche fondamentale, n'ayant, par définition, pas de perspective économique, est presque exclusivement le fait de laboratoires de recherche ou de chercheurs indépendants et fait rarement l'objet de partenariats avec l'industrie ou les services. Cependant, il est fréquent que des recherches dans le cadre d'une application, parfois en partenariat avec des entreprises, puissent avoir des implications fondamentales — ou que les chercheurs impliqués dans le projet usent de leur liberté académique pour mener conjointement leur recherche en direction de l'application et une recherche plus fondamentale sur le même sujet ,. La frontière entre ces deux types de recherche est donc généralement assez floue.
La recherche technique, ou recherche technologique, est une branche de la recherche qui constitue la suite de la recherche scientifique, afin de valoriser dans l'industrie les découvertes scientifiques. L'expression « recherche technologique » correspond à un sens dérivé du mot technologie, qui signifie étymologiquement « étude des techniques » (du grec tekhnê et logos). Une découverte scientifique n'est généralement pas applicable telle quelle à un processus industriel. Il faut auparavant étudier, souvent par expérimentation : La faisabilité technique et les procédés techniques ; Les implications économiques et la rentabilité à terme ; Les impacts écologiques et sociaux d'une telle réalisation. Ces tâches sont réalisées par des ingénieurs, avec des équipes de techniciens, dans chacun des domaines concernés.
Le Research Assessment Exercise (RAE) était un cycle d'évaluation des activités de recherche de l'ensemble des départements des universités du Royaume-Uni. Il a été remplacé par le Research Excellence Framework (REF) dont le premier cycle s'est tenu en 2014. Il est organisé tous les quatre ou cinq ans par les quatre agences publiques de financement de l'enseignement supérieur (HEFCE, SHEFC, HEFCW, DELNI) pour évaluer la qualité des recherches menées dans chaque unité. Les résultats de cet exercice d'évaluation, exprimés dans une échelle de quatre points, déterminent le volume des fonds publics distribués à l'établissement l'année suivante. Les exercices précédents ont été effectués en 1986, 1989, 1992, 1996, 2001 et 2008. Plusieurs palmarès d'origine médiatique ont classé les universités et leurs départements sur la base des résultats du RAE.
Le Réseau Blaise Pascal (Sciences, Cultures et Foi), créé en avril 2001, est constitué de plus d’une vingtaine de groupes francophones d’inspiration chrétienne qui travaillent sur la question « sciences, cultures et foi. »
Une revue systématique est un travail de collecte, d'évaluation critique et de synthèse des connaissances existantes sur une question donnée. Cette question bien définie est issue de l'étude d'une problématique posée par un commanditaire, un gestionnaire, un praticien, un chercheur... Il s'agit, contrairement à de simples revues de littérature, de minimiser les biais pouvant être inhérents soit à la matière première (données, connaissances) soit à la conduite de la revue elle-même, afin d'atteindre la plus grande objectivité possible. Les biais ne pouvant pas être réduits à zéro, il s'agit donc de les mettre en lumière et de les prendre en compte dans le travail de synthèse afin que tout lecteur des résultats et conclusions de la revue puisse se les approprier en toute connaissance de cause, en en comprenant les limites et le niveau de confiance (incertitude) des résultats. La revue systématique peut notamment être utilisée pour produire une cartographie systématique. « Une revue systématique est la synthèse rigoureuse et reproductible des résultats de toutes les études originales existantes répondant à une même question de recherche. Une méta-analyse est la synthèse statistique des études incluses dans la revue systématique. »
Sciences, technologies et société (STS) est un domaine de recherche multidisciplinaire en sciences sociales (sociologie, économie, psychologie, sciences politiques, histoire, entre autres) portant sur les relations entre les sciences et les technologies et la société. L'expression recouvre à peu près le même programme que les Science and technology studies (STS) du monde anglo-saxon, bien que certaines orientations de recherche en fassent un domaine relativement autonome chez les chercheurs francophones. Le champ d'étude est vaste. Son noyau est la compréhension des interactions entre les modalités internes de la recherche et les caractéristiques spécifiques d`autres domaines de la société. Ainsi par exemple, pour étudier les relations entre la recherche fondamentale en génétique et les organisme financeurs de telles recherches, le chercheur en STS utilisera des résultats d'études de sociologie des sciences et de philosophie des sciences pour d'abord comprendre l'organisation de cette communauté de chercheurs, puis y discerner ce qui ne relève pas du cœur de la pratique scientifique mais qui pourtant dirige ou encadre le travail de recherche. Ces points sont autant de points d'ancrage possible pour les points de vue des financeurs, plus ou moins consciemment, plus ou moins fortement. Le chercheur en STS précise la nature de ces points d'ancrage. L'essentiel des travaux portent sur la sociologie des sciences, la recherche scientifique et l'innovation. Les politiques de science et technologie, politiques de recherche et les politiques d'innovation font également partie de ce domaine. Parmi les représentants des STS francophone, on peut nommer Jean-Jacques Salomon (Conservatoire national des arts et métiers), Bruno Latour (chaire Gabriel Tarde, Sciences Po), Michel Callon (École des mines de Paris), Dominique Pestre (EHESS), Dominique Vinck (Université de Lausanne), Benoît Godin (INRS) ou encore Camille Limoges (UQAM).
Sicmed - Surfaces et Interfaces Continentales Méditerranéennes - est un programme international de recherche sur le fonctionnement et les évolutions des anthropo-écosystèmes ruraux et sous les pressions climatique et anthropique. Il fait partie du Chantier méditerranéen Mistrals et est développé avec le soutien du CNRS-Insu, de l'IRD, de l'INRA et de l'IRSTEA
La Société de mathématiques appliquées et industrielles (SMAI) est une société savante française, régie par la loi de 1901, sans but lucratif et reconnue d'utilité publique depuis 2015. Fondée en 1983, la Société entend contribuer au développement des mathématiques appliquées à travers la recherche, les applications dans les entreprises, les publications, l’enseignement et la formation des chercheurs et ingénieurs. Elle comporte des membres individuels (personnes physiques) et des membres institutionnels (laboratoires de recherche universitaires ou industriels), dans tous les domaines des mathématiques appliquées (calcul scientifique, analyse numérique, équations aux dérivées partielles, contrôle, probabilités appliquées, statistiques, mathématiques financières, analyse d’image, modélisation du vivant...). L’association est dirigée par un conseil d'administration, élu par l’assemblée générale. Elle est également dotée d’un conseil scientifique. En 2009, elle comptait près de 1 300 membres.
La Société mathématique de France (SMF) a été fondée en novembre 1872, le premier président en a été Michel Chasles. C'est l'une des plus anciennes sociétés savantes de mathématiciens au monde. Association loi de 1901 reconnue d'utilité publique depuis 1988, elle a pour but « l'avancement et la propagation des études de Mathématiques pures et appliquées ». Cette société, formée de membres individuels (personnes physiques) et de membres institutionnels (laboratoires de recherches, bibliothèques, institutions, etc.), rassemble mathématiciens et mathématiciennes, qu'ils soient enseignants-chercheurs, chercheurs, enseignants, utilisateurs de mathématiques dans l'industrie ou les services, amateurs de mathématiques, etc., que leur sensibilité les porte vers les aspects fondamentaux de la discipline ou vers ses applications les plus variées. En 2010, elle comptait 2000 adhérents. L'association est administrée par un conseil d'administration de 24 membres et un bureau élus conformément à ses statuts et à son règlement intérieur. Elle est aussi dotée d'un conseil scientifique et d'un comité des publications.
Une station de recherche est une installation ou une structure construite dans le but de mener des recherches scientifiques. Les sites d'installation incluent l'espace (comme pour la station spatiale internationale) et les océans. De nombreux pays ont des stations de recherche en Antarctique : Showa (en), Halley et Troll sont des exemples.
La station scientifique des Hautes-Fagnes est une station de recherche de terrain de l'université de Liège, installée sur le plateau des Hautes Fagnes, au Mont Rigi sur la commune de Robertville (Waimes) depuis 1924.
Une topographie médicale est une pratique médicale très répandue au XIXe siècle, qui a pour but d'examiner les causes qui peuvent avoir une influence sur la santé, ou plus largement la vie, des habitants d'un lieu donné. Par extension, on appelle topographies médicales les ouvrages s'y rapportant. Le lieu étudié est généralement une ville, comme la Topographie médicale de Paris écrite par le médecin Claude Lachaise, parue en 1822. Ce type d'ouvrage s'inscrivait la plupart du temps dans des conceptions hygiénistes de la médecine, et proposait des précautions à ses lecteurs quant aux pratiques à avoir pour rester en bonne santé. Au moins 600 topographies médicales ont été rédigées en France, entre 1761 et 1895.
TranCYST est un Réseau de formation initiale Marie Curie (en) (ITN) de 4 ans qui a pour sujet la Polykystose Rénale Autosomique Dominante (PKD). Ce projet de recherche translationnel fait partie du Septième programme-cadre de la Commission européenne et a commencé le 1er décembre 2012 pour une fin fixée au 30 novembre 2016.
« Le transfert technologique est le processus désignant le transfert formel à l’industrie de découvertes résultant de la recherche universitaire ou privée dans le but de les commercialiser sous la forme de nouveaux produits et/ou services. »  Lorsque le concepteur est un laboratoire de recherche, c'est une activité de valorisation de la recherche. Le transfert peut donner lieu à une transaction financière, et se matérialiser de différentes façons (achat de brevet, coopération, recrutement ou méthodes « hostiles »). Les deux partenaires principaux sont généralement des organismes, sociétés commerciales ou organisations publiques. Mais l'on peut également considérer que ce sont deux domaines d'application distincts ; dans ce cas le transfert de technologie s'apparente à la transposition d'un concept, d'une idée, depuis son application typique vers un domaine comportant des similitudes, mais pour lequel cette mise en œuvre constitue une nouveauté. Dans tous les cas la technologie constitue une innovation pour l'acquéreur, le propriétaire la maîtrisant déjà. Pour des raisons normatives et politiques, ce sont encore souvent et uniquement les nouvelles technologies qui sont transférées. En effet, sachant que le transfert technologique s'est largement institutionnalisé pendant les années 1980-1990, il faut comprendre cette sphère d'activité dans le cadre des politiques d'innovation et du Manuel d'Oslo de l'OCDE permettant de mesurer cette dernière. Dans ce cadre, l'accent est fortement mis sur l'importance de la technologie et de l'entreprise comme unique moteur de l'économie et donc de la compétitivité. Benoît Godin a énormément travaillé sur ce sujet, notamment sur le lien existant entre ces politiques et leur impact au niveau des relations tissées entre université et industrie. On ne considère généralement pas que l'enseignement ou l'apprentissage d'un métier consiste en un transfert de technologie. On parle alors de formation professionnelle. « La diffusion et le transfert des technologies est un pilier majeur qui soutient la raison d’être du système des brevets ». Cette citation tirée d'un texte de l'OMPI (Organisation Mondiale de la Propriété Intellectuelle), précise l'importance que revêt le lien entre brevets et transferts de technologie. Les expressions de licence de technologie, de licensing, de contrat de transfert, expriment les diverses formes que peut revêtir le transfert de technologie, plus spécialement dans le domaine des affaires, du commerce et de l'industrie marchande. Si l'on prend en considération les significations du mot « technologie », il est très simplificateur de ne comprendre par transfert de technologie qu'accord commercial entre le propriétaire d'un brevet (ou de tout autre source de propriété de droit commercial) et un acquéreur de tout ou partie de ce brevet (ou de droits d'usage).
La valorisation de la recherche est une activité qui consiste à augmenter la valeur des résultats de recherche et développement. Elle se comprend dans le cadre des politiques d’innovation, la valorisation est aujourd’hui une fonction reconnue de l’université dans le cadre des systèmes d’innovation . Par ailleurs, il faut noter que le terme de valorisation est polysémique et que les définitions existantes varient en fonction de l’organisme ou du pays considéré.
En philosophie, la vérité par consensus est une représentation considérée comme fidèle à la réalité parce que faisant consensus. D'après le philosophe Nigel Warburton, ce n'est pas parce qu'il y a consensus sur une version des faits que cette version est fiable, d'autant que les individus sont crédules, manipulables et enclin à prendre leurs désirs pour des réalités.
Une vérité scientifique est une proposition construite par un raisonnement rigoureux, et vérifiée par l'expérience. Pour cette raison elle est réutilisable par d'autres scientifiques, qui pourront à partir d'elle énoncer d'autres propositions de ce type. La vision d'une vérité scientifique « pure » doit néanmoins être tempérée : les propositions reposent souvent sur des consensus établis par convention sur des questions pour lesquelles il n'y a pas assez d'éléments pour répondre. Différentes affirmations peuvent faire l'objet d'une controverse scientifique en attendant qu'un élément nouveau tranche définitivement. L'idée que la science permette d'accéder à une forme de vérité est présente aussi bien chez les philosophes que chez les scientifiques. Ainsi, le sous-titre du Discours de la méthode de René Descartes est « pour bien conduire sa raison, et chercher la vérité dans les sciences ». La vérité scientifique, pour mériter ce nom, ne doit pas dépendre d'une idéologie. (cf l'exemple emblématique de la controverse fameuse soulevée par les travaux de Lyssenko).
Une zone à régime restrictif (ZRR), en France, est une zone à accès réglementé dans le cadre de la protection du potentiel scientifique et technique national, lequel comporte cinq niveaux de protection imbriqués : une liste de secteurs scientifiques et techniques dits « protégés », objets d'un « annuaire national » recensant leurs laboratoires ; une liste de spécialités dont les savoir-faire sont susceptibles d'être détournés à des fins de terrorisme ou de prolifération d'armes de destruction massive et de leurs vecteurs, établie par un arrêté confidentiel Défense ; des « zones protégées », délimitées soit par des autorités militaires, soit par des autorités civiles ; dans les laboratoires relevant d'un secteur protégé, parmi les zones protégées, des ZRR, dont l'accès (physique ou électronique) est soumis à autorisation spéciale ; à l'intérieur des ZRR, éventuellement, des « locaux sensibles », à la protection renforcée.
La relation entre science et religion est un sujet abordé depuis l'Antiquité dans de nombreux champs d'investigation, dont la philosophie des sciences, la théologie, l'histoire des sciences et l'histoire des religions. Les points de vue sont variables, notamment en fonction des époques, des cultures et des régions ; certains voient une harmonie entre science et religion, d'autres des domaines séparés avec peu d'interactions et d'autres encore y voient une incompatibilité et un conflit fondamental entre la foi fondée sur des dogmes et la science fondée sur des faits et ouverte à la critique. Notons d'emblée la profonde difficulté d'assimiler des savoirs hétéroclites à des corpus de savoirs cohérents, ou encore de vastes conceptions systémiques et classificatoires à la science moderne que nous connaissons. C'est la méthode scientifique et la méthode expérimentale dont les prémisses sont définis en termes de « savoir » dans la philosophie des lumières avec la publication de la première Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers. Toutefois le savoir d'un temps peut être in extenso qualifié de science. La remarque s'applique aussi à la religion qui a profondément évolué tant dans ses rituels que dans ses conceptions et croyances. Par l'intermédiaire des traductions et des commentaires des philosophes arabes comme Avicenne et Averroès, l'œuvre d'Aristote est parvenue en Occident et a nourri la pensée médiévale. Ainsi, parce qu'au XIIIe siècle, les clercs ont adopté une partie de cet héritage antique et que les arts libéraux exploitent une fraction des savoirs pour justifier leurs rapides réalisations techniques, Thomas d'Aquin tente de concilier par une grande synthèse systémique la philosophie de la nature aristotélicienne et la foi révélée. La philosophie médiévale a transmis ces fondements intellectuels du savoir que sont les philosophies des grands auteurs de l'Antiquité.
La conception whig de l'Histoire (ou Historiographie Whig) est une approche de l'historiographie qui présente l'Histoire comme une progression inévitable vers une plus grande liberté dans la tradition des Lumières, culminant dans les formes modernes de démocratie libérale et de monarchie constitutionnelle. Le terme est employé largement au-delà du domaine relatif à l'histoire anglaise, par exemple dans l'histoire des sciences, pour désigner (et critiquer) des récits téléologiques (orientés vers une fin), basés sur des héros, mus par une vision transhistorique (non liée à une civilisation particulière) des choses. Le mot Whiggishness est parfois utilisé comme un terme générique pour désigner l'historiographie Whig. Dans l'histoire des sciences, le terme est utilisé pour désigner une historiographie qui se focalise sur l'aspect positif de la chaîne des théories et des expérimentations qui conduisent à la situation présente en ignorant les échecs et les voies sans issue. L'historiographie Whig a des similarités avec la théorie de l'histoire marxiste-léniniste qui pense que l'humanité se dirige à travers des étapes historiques vers une société communiste égalitaire et sans classe,. L'histoire whig est une forme de libéralisme qui met sa foi dans le pouvoir de la raison humaine pour réformer la société sans égard pour le passé historique et la tradition. Elle s'oppose à l'histoire conservative ou « Toryisme » qui, pour l'historien anglais A.J.P. Taylor, « doute de la nature humaine,... se méfie des améliorations, s'accroche aux institutions traditionnelles et préfère le passé au présent ».
La nature de la foi et de la raison et le conflit éventuel entre les deux sont des sujets de réflexion sur la religion et même en dehors du champ dit « religieux ». Les premiers textes cherchant à les concilier ou à en expliquer l'opposition datent de la pensée médiévale latine des XIIe et XIIIe siècles. Les développements ultérieurs peuvent être trouvés, par exemple, dans l'affaire Galilée, ou chez Luther, Descartes, Spinoza et Kant. Sur un plan épistémologique contemporain, pour un auteur comme Jacques Derrida, dans Foi et savoir ou Jacques Bouveresse, dans Peut-on ne pas croire ? il est question de déterminer si l'on peut se dispenser de croire pour savoir, s'il est légitime de croire au savoir, ou encore s'il est légitime de croire ce que l'on affirme par ailleurs ne pas pouvoir savoir. Pour Patrick Royannais, la réponse à cette dernière question est évidente dans le sens large du mot foi : « Croire est défini comme « pratique de la différence », et tout homme croit pour vivre, c’est-à-dire fait confiance à autrui qu’il lui demande une indication de direction dans la rue, qu’il mange le plat commandé au restaurant, etc. L’expression de « pratique de la différence » doit être comprise au même titre que celle de pratique chrétienne, non pas la pratique dominicale, mais celle de la parole de Dieu écoutée et pratiquée. L’homme pratique la différence parce qu'il ne peut faire autrement que de s’en remettre à l’autre, sans vérifier, sans savoir si l’autre dit vrai ou non. Il faut même dire que la preuve interdit la pratique de la différence et empêche de vivre. Que serait l’amour d’un homme qui ferait suivre sa femme pour être certain qu’elle lui est fidèle ? Que serait la vie de famille si père et enfants faisaient procéder à une analyse génétique pour s’assurer de la filiation, connue par la seule parole de la femme et mère ? ».
La position de l'Église catholique sur la théorie de l'évolution maintient l'inerrance de la Bible, tout en estimant nécessaire de distinguer les différents domaines ou hypothèses, et suivant[Quoi ?] les conclusions certaines des sciences profanes.  « En créant l'homme et la femme, Dieu leur avait donné une participation spéciale à sa vie divine, dans la sainteté et la justice. Dans le projet de Dieu, l'homme n'aurait dû ni souffrir ni mourir. En outre, il régnait une harmonie parfaite de l'homme en lui-même, entre la créature et le Créateur, entre l'homme et la femme, comme aussi entre le premier couple humain et toute la création. »  — CEC 374-379 L’Écriture montre les conséquences dramatiques de cette première désobéissance. Adam et Ève perdent immédiatement la grâce de la sainteté originelle (cf. Rm 3, 23). Ils ont peur de ce Dieu (cf. Gn 3, 9-10) dont ils ont conçu une fausse image, celle d’un Dieu jaloux de ses prérogatives (cf. Gn 3, 5).
James E. Alcock (né en 1942) enseigne la psychologie à l'Université d'York (Canada) depuis 1973. Alcock est principalement connu pour ses critiques de la parapsychologie et pour son implication au sein du Committee for Skeptical Inquiry (CSI). Il participe à l'édition du magazine The Skeptical Inquirer, dont il est aussi un contributeur régulier. Il a été chroniqueur pour le magazine Humanist Perspectives. Son ouvrage Parapsychologie : science ou magie ? a joué un rôle important dans la création du domaine de recherches de la psychologie anomalistique ou psychologie des expériences inhabituelles . En 1999, il a été nommé parmi les dix sceptiques les plus remarquables du XXe siècle par un jury composé d'acteurs du mouvement sceptique. En mai 2004 le CSI remet à Alcock sa plus haute distinction : In Praise of Reason Award. Alcock est également un illusionniste amateur et membre de l'International Brotherhood of Magicians (en).
Isaac Asimov, né vers le 2 janvier 1920 à Petrovitchi (en Russie) et mort le 6 avril 1992 à New York aux États-Unis, est un écrivain américano-russe, naturalisé en 1928, surtout connu pour ses œuvres de science-fiction et ses livres de vulgarisation scientifique.
L'Association brésilienne des sceptiques et rationalistes, en portugais Sociedade Brasileira de Céticos e Racionalistas est une association brésilienne qui vise à promouvoir le scepticisme rationnel et le rationalisme. L'association fut fondée en 2001 par le Dr. Renato Marcos Endrizzi Sabbatini, célèbre professeur à l'université publiant des articles scientifiques dans la presse. En 2013, l'association compte plus de 300 membres actifs. Le site web de l'association regroupe des écrits sur les pseudosciences et le scepticisme, ces derniers étant issus du Dr. Sabatini et d'autres membres de l'association, et dispose d'un forum de discussion.
L’Association française des biotechnologies végétales (AFBV) est une ONG régie par loi de 1901 sur les associations. Elle vise à développer une agriculture durable, notamment par le biais des biotechnologies végétales.
L'Association française pour l'information scientifique (AFIS) est une association loi de 1901, fondée par Michel Rouzé en novembre 1968 et dont le siège est à Paris. Composée de bénévoles, elle tire ses ressources du magazine qu'elle édite, Science et pseudo-sciences, et des cotisations de ses adhérents. Issue du courant rationaliste français, elle se donne pour objectif de « promouvoir la science » et de donner « un éclairage sur des sujets de société qui sont traités de manière pseudo-scientifique et font l'objet de désinformation ou polémiques. » Impliquée dans des controverses, elle fait notamment l'objet de critiques concernant ses prises de positions pro-OGM et un traitement jugé idéologique de certains sujets scientifiques.
L'Association indienne rationaliste est une organisation en Inde dont les 100 000 membres promeuvent le scepticisme scientifique et se montrent critiques envers les croyances au surnaturel. L'association publie des livres, des magazines, organise des séminaires, des lectures, et ses représentants apparaissent régulièrement à la télévision pour lutter contre les superstitions qui y sont relayées. L'Association indienne rationaliste fut fondée en 1949 à Chennai (nommée Madras jusqu'en 1996). Le président fondateur fut le docteur R. P. Paranjpye (futur vice-chancelier de l'université de Bombay). Sanal Edamaruku, auteur célèbre, est le président actuel de l'association. L'Association indienne rationaliste a des ramifications dans différents États de l'Inde, et ses quartiers principaux sont situés à New Delhi. Elle prit l'initiative de créer Rationalist International en 1995, et organisa trois conférences internationales rationalistes. L'écrivain australien Greg Egan évoque l'Association indienne rationaliste dans sa nouvelle Teranesia.
Normand Baillargeon, né le 6 juillet 1958 à Valleyfield (Québec), est un enseignant et universitaire canadien. Ancien professeur en sciences de l'éducation à l'université du Québec à Montréal (UQAM) de 1989 à 2015, il est aussi philosophe, essayiste, militant libertaire,, anarcho-syndicaliste, chroniqueur et collaborateur de différentes revues alternatives et à Radio-Canada de 2011 à 2016. Il s'identifie au mouvement sceptique contemporain, de même qu'à l'anarchisme.
Robert Allen Baker Jr. (né le 27 juin 1921 et décédé le 8 août 2005) est un professeur de psychologie américain, sceptique, écrivain et chercheur sur les fantômes, les enlèvements par des extraterrestres, les créatures lacustres et autres phénomènes paranormaux.
Banachek (de son vrai nom Steven Shaw, né le 30 novembre 1960) est un mentaliste américain. Il a écrit des ouvrages de mentalisme et inventé des tours de magie et de mentalisme, dont la version de Penn & Teller de capture d'une balle de revolver au vol (en) et son tour de l'« enterré vivant ». Il est directeur du défi paranormal à un million de dollars (en) de la James Randi Educational Foundation.
Susan Jane Blackmore, née le 29 juillet 1951 à Londres, est une parapsychologue, écrivain, conférencière et animatrice d'émission britannique. Elle est surtout connue pour son livre, The Meme Machine (traduction française : La théorie des mèmes. Elle est membre du Committee for Skeptical Inquiry, qui est une des organisations les plus importantes du mouvement sceptique contemporain.
Paul-Éric Blanrue, né le 18 avril 1967 à Metz, dans la Moselle, est un essayiste français.
Marcel Boll, né le 15 septembre 1886 à Paris et mort en 1971, est un ingénieur ESPCI, agrégé et docteur ès Sciences Physiques, professeur de chimie et d'électricité à l'École des hautes études commerciales de Paris (HEC), journaliste scientifique et membre fondateur de l'Union Rationaliste.
Jean Bricmont, né le 12 avril 1952 à Uccle, est un physicien et essayiste belge, professeur émérite de physique théorique à l'université catholique de Louvain et membre depuis 2004 de l'Académie royale de Belgique. Proche de Noam Chomsky, il milite d'une part contre les dérives postmodernistes et d'autre part contre les restrictions à la liberté d'expression en France, en demandant notamment l’abrogation de la loi Gayssot. Certains observateurs le présentent comme antisioniste et conspirationniste, et mettent en cause ses accointances avec les milieux et les thèses antisémites et négationnistes.
Henri Broch (8 novembre 1950, Nice) est docteur en sciences, professeur de biophysique théorique à l'Université de Nice Sophia-Antipolis et directeur du Laboratoire de zététique.
Bullshit! (nommé aussi : Penn and Teller: Bullshit!, Penn and Teller: BS) est une émission de télévision américaine diffusée entre 2003 et 2010, animée par le duo de prestidigitateurs Penn et Teller.
Camp Quest est un centre de vacances fondé en 1996 aux États-Unis d'Amérique, puis au Canada, en Grande-Bretagne, en Irlande et en Norvège. Il accueille des enfants de familles ne souhaitant pas affilier leurs enfants dans des centres religieux, et sont donc généralement des jeunes ayant des parents non théistes, libre-penseurs, athées, agnostiques ou encore sceptiques et rationalistes.
Robert Todd Caroll, né le 18 mai 1945 à Joliet (Illinois) et mort le 25 août 2016 à Davis (Californie), est un sceptique américain. Il est l'auteur du The Skeptic's Dictionary (Le dictionnaire sceptique). Il s'agit d'un encyclopédie de référence au sein du mouvement sceptique contemporain.
Dominique Caudron est un ufologue sceptique français, astronome amateur, graphiste d'images op-art, programmeur et humoriste. Il est l'un des « nouveaux ufologues », partisan d'une étude globale de la genèse d'un rapport d'observation d'ovnis.
Un charlatan est une personne qui pratique l'imposture, ou un jeu de dupes envers autrui, grâce à des trucages, des déformations de la réalité (par exemple via l'exploitation de biais cognitifs), ou des falsifications, en vue de gagner sa confiance, généralement pour obtenir de l'argent ou tout autre avantage. Le charlatanisme est la manière d'agir propre à un charlatan. Le terme est notamment utilisé dans le domaine médical.
Georges Charpak (Dąbrowica, 8 mars 1924 – Paris, 29 septembre 2010) est un physicien français lauréat du prix Nobel de physique en 1992.
Michel-Eugène Chevreul (1786–1889) est un chimiste français connu pour son travail sur les acides gras, la saponification, la découverte de la stéarine. Ces travaux lui valurent la médaille Copley en 1857. Nommé directeur de la manufacture des Gobelins, il appuya le travail de teinture sur des recherches sur la perception des couleurs. Il expose dans son ouvrage De la loi du contraste simultané des couleurs et de l'assortiment des objets colorés des principes qui influencèrent durablement les artistes peintres. Il est aussi le concepteur d'un atlas de couleurs indexé sur les raies de Fraunhofer.
Le Centre d'information et de prévention sur les psychothérapies abusives et déviantes (CIPPAD) est une association loi 1901 ayant pour objet d'alerter l’opinion publique sur le nombre croissant des dérives déontologiques et sectaires observées chez une minorité de psychothérapeutes. L'association entend accueillir et écouter les victimes d’escroquerie et de manipulation mentale dans le monde de la santé et entend proposer son aide. Le Cippad relaye les informations d'autres organismes luttant contre les sectes, les pseudo-sciences, etc, tels que la Ligue des Droits de l'Homme, l'ADFI (Association de défense des familles et de l'individu victime de secte) et la Miviludes.
Le Collectif de recherche transdisciplinaire esprit critique et sciences (ou CORTECS) est une association de loi 1901 fondée en 2013 entre Grenoble, Montpellier et Marseille. Son objectif est la transmission des divers aspects de l’esprit critique, par le biais notamment d'enseignements, de travaux de recherche et d'élaboration de ressources pédagogiques. Le collectif procède à la mise en réseau des professionneles de l'esprit critique.
Le Comité Para ou Comité belge pour l'analyse critique des parasciences est historiquement le groupe fondateur du scepticisme scientifique. Il a en effet été créé en 1949 en Belgique. La devise de ce groupe sceptique est : Ne rien nier a priori, ne rien affirmer sans preuve. (Robert Rendu). Avant 2014, le nom original était Comité belge pour l'investigation scientifique des phénomènes réputés paranormaux. Le Comité Para est membre du Conseil européen des organisations sceptiques (ou ECSO). L'organisation sœur néerlandophone du Comité Para (en Belgique, une organisation francophone a généralement une organisation similaire du côté néerlandophone) est SKEPP (Studiekring voor Kritische Evaluatie van Pseudowetenschap en het Paranormale, "Cercle érudite pour l'évaluation critique des pseudo-sciences et le paranormal").
Le Committee for Skeptical Inquiry anciennement Committee for the Scientific Investigation of Claims of the Paranormal (CSICOP), est une organisation américaine qui se consacre à la critique des phénomènes « paranormaux » ou de disciplines qu'il juge pseudo-scientifiques comme l'ufologie, la parapsychologie, la cryptozoologie ou encore l'homéopathie. Il s'agit d'une des organisations les plus importantes du mouvement sceptique contemporain, avec la The Skeptics Society. Le CSICOP a été fondé en 1976, par le philosophe Paul Kurtz et des membres aussi éminents que Carl Sagan, Isaac Asimov, James Randi, Martin Gardner. Il publie régulièrement un journal, le Skeptical Inquirer (L'Enquêteur sceptique). Outre l'étude du paranormal, le comité se donne aussi pour but l'éducation à la pensée critique, et la démarche scientifique d'une manière plus générale. Paul Kurtz eut l'idée de fonder le CSICOP à la suite du débat qui a opposé le Comité Belge pour l'Investigation Scientifique des Phénomènes Réputés Paranormaux (dit « Comité Para ») à Michel Gauquelin à propos de son étude sur l'effet Mars. En 2006, le CSICOP change de nom et devient le Committee for Skeptical Inquiry (CSI) afin de souligner que le groupe ne s'intéresse pas uniquement au paranormal, mais aussi aux pseudo-sciences, aux théories de la conspiration ou encore aux religions.
Le Conseil européen des organisations sceptiques (European Council of Skeptical Organisations) est une organisation parapluie des groupes sceptiques en Europe.
Frederick Campbell Crews (né en 1933 à Philadelphie, Pennsylvanie) est un professeur émérite d'anglais de l'Université de Californie (Berkeley) et écrivain américain.
La critique de la psychanalyse porte principalement sur le fait que cette théorie et sa pratique n'ont (et ne peuvent avoir, de par leur nature) aucune démonstration de ses fondements scientifiques. Les critiques de la psychanalyse présentent, schématiquement, deux temps majeurs : l'élaboration de la psychanalyse en tant que méthode d'exploration du fonctionnement psychique avec ses principaux concepts ; l'évolution ultérieure de la théorie et de la pratique ; et deux versants : l'un théorique comme connaissance du psychisme, centrée sur le déterminisme psychique inconscient ; l'autre pratique, en filiation directe avec la théorie, comme thérapie ou clinique. Corrélativement, la critique de la psychanalyse porte sur : le moment fondateur (contexte historique, épistémologique, scientifique, culturel, innovation, statuts des « découvertes freudiennes », méthode, prétentions scientifiques,…) qui recouvre le personnage même de Freud (intentions, ambitions, compétences…) ; la construction de la « légende Freud » à partir de la manipulation des sources et de la réécriture de l'histoire des origines, par Freud lui-même, et ses successeurs sans compter les « réhabillages » structuralistes ou herméneutiques ; les inflexions ultérieures de la psychanalyse ; le noyau conceptuel commun à l'ensemble des courants psychanalytiques ; l'efficacité de la cure analytique ; les modes de formation des psychanalystes (valeur d'une analyse didactique, réglementation, institutions). Cette démarche de réévaluation de la psychanalyse concilie donc un abord épistémologique et scientifique avec un abord historiographique (et aussi thérapeutique).
Le culte du cargo est un ensemble de rites qui apparaissent à la fin du XIXe siècle et dans la première moitié du XXe siècle chez les aborigènes, en réaction à la colonisation de Mélanésie (Océanie). Il consiste à imiter les opérateurs radios américains et japonais commandant du ravitaillement (distribués par avion-cargo) et plus généralement la technologie et la culture occidentale (moyens de transports, défilés militaire, habillement, etc.) en espérant déboucher sur les mêmes effets, selon ce qu'on a qualifié de croyances « millénaristes »,. En effet, les indigènes ignorent l'existence et les modalités de production occidentale ; dès lors, ils attribuent l'abondance et la sophistication des biens apportés par cargo à une faveur divine. Le culte a pris naissance en Mélanésie. Quasiment toute la Mélanésie, des îles Fidji à la Papouasie-Nouvelle-Guinée l'adopta simultanément (à l'exception de la Nouvelle-Calédonie) mais ce culte ne connaîtra une longévité exceptionnelle qu'à Tanna.
Richard Dawkins, né le 26 mars 1941 à Nairobi, est un biologiste et éthologiste britannique, vulgarisateur et théoricien de l'évolution, membre de la Royal Society. Professeur émérite au New College de l'université d'Oxford, Richard Dawkins est l'un des académiciens britanniques les plus célèbres. Il acquiert la consécration avec son livre de 1976 intitulé The Selfish Gene (en français : Le Gène égoïste), qui popularise la théorie de l'évolution centrée sur les gènes et introduit le terme de « mème ». En 1982, il développe cette théorie dans son ouvrage Phénotype étendu puis publie en 2006 The God Delusion (en français : Pour en finir avec Dieu), vendu à plus de deux millions d'exemplaires et traduit en trente et une langues. Vice-président de la British Humanist Association, il est reconnu comme un ardent défenseur du rationalisme, de la pensée scientifique et de l'athéisme. Il est résolument anticlérical et est aussi l'un des principaux critiques anglo-saxons du créationnisme, du dessein intelligent et des pseudo-sciences. Il s'est rendu célèbre aussi pour sa controverse amicale, mais ferme, avec son collègue Stephen Jay Gould sur la question des équilibres ponctués. En plus de ses nombreux ouvrages scientifiques, Dawkins promeut sa vision rationnelle au travers de films et documentaires, de conférences et de débats télévisés sur les grandes radios ou chaînes nationales du monde entier. Il complète son action sur le terrain associatif en créant et dirigeant la Fondation Richard Dawkins pour la Raison et la Science.
Le défi zététique international (du grec zêtêin, « chercher ») avait pour objet de mettre en évidence l'existence ou l'inexistence de phénomène(s) paranormal(aux). Lancé en 1987, il promettait un prix « pour la preuve d'un phénomène paranormal, quel qu'il soit, devant Henri Broch, Gérard Majax, Jacques Theodor ». Il s'agissait de la version francophone du One Million Dollar Challenge de James Randi. Initialement de 500 000 francs, le prix a été porté à 1 000 000 francs en 1992, puis à 200 000 euros en 1999. À l'époque de sa création en 1986 par le physicien Henri Broch, le service télématique Minitel 36.15 ZET (abréviation de Zététique) avait lancé un véritable défi aux soi-disant détenteurs de pouvoirs paranormaux avec la formule suivante : « Vous prétendez avoir des pouvoirs : … prouvez-le ! » Le prix était conçu sur le principe de raisonnement qui considère que c'est à la personne qui affirme détenir un pouvoir quelconque d'en faire la preuve, et non pas aux scientifiques de démontrer le contraire et que c'est une des bases de la méthode scientifique (et de la logique) que ce soit à celui qui prétend détenir des faits ou des théories nouvelles de les démontrer. En février 2002, soit au terme de quinze années, le défi a été clos, le prix restant non attribué, tous les candidats ayant échoué à apporter la preuve d'un phénomène paranormal. Il a été mis un terme au défi non en raison du prix (jamais attribué), mais du coût en temps et en énergie de l'expérimentation, face à de trop nombreuses candidatures fantaisistes. Les affirmations de phénomène paranormal étaient soumises à expérience, suivant un protocole agréé par les deux parties. L'expérience était contrôlée par deux scientifiques, le physicien Henri Broch et l'immunologue Jacques Theodor, ainsi que par le prestidigitateur Gérard Majax. Les tests et enquêtes étaient réalisés au laboratoire de zététique de Nice, à l'Université de Nice - Sophia Antipolis, en France. Le laboratoire de zététique de l'université conduirait toujours actuellement des recherches dans des domaines censés relever du paranormal. Il prétend qu'il « reste ouvert à toute proposition d'expérience sur un phénomène « paranormal » mais uniquement dans le cadre d'une proposition sérieuse, c'est-à-dire revendiquant une action « physiquement mesurable » ». Le One Million Dollar Challenge a été officiellement clos par la « James Randi Educational Fondation » (JREF) en 2015.
Dans la psychologie comportementale, le dénialisme est le choix de nier la réalité. Cela vient comme un moyen d'éviter une vérité psychologiquement inconfortable . Le dénialisme est alors une action irrationnelle qui vient comme un rejet de la validité d'une expérience historique ou d'un événement par une personne qui refuse d'accepter une réalité empiriquement vérifiable . Dans le domaine des sciences, le dénialisme est le rejet des faits et des concepts indiscutables et bien soutenus par le consensus scientifique en faveur des idées radicales et controversées  . Les formes de dénialisme présentent la caractéristique commune de rejeter les preuves accablantes et la génération de la controverse politique avec les tentatives de nier l'existence d'un consensus. Les motivations et les causes de dénialisme incluent la religion et l'intérêt personnel (économique, politique, financier, etc.) ainsi que les mécanismes de défense destinés à protéger le psychisme du dénialiste contre des faits et des idées mentalement troublants . En langue française, le terme dénialisme a une connotation scientifique, contrairement au terme négationnisme qui a une connotation politique. Mais tous les deux s'inscrivent dans la même logique de déni des faits et de la réalité objective. Selon le philosophe Normand Baillargeon «On pourra être tenté de traduire [le] denialism par "négationnisme". Cependant, ce serait oublier que ce mot désigne déjà, en français, le refus d’admettre la réalité de la Shoah, un fait historiquement prouvé. Cela constitue bien, sur le terrain de l’histoire, une forme de dénialisme, mais ce n’est pas la seule. Les personnes qui refusent d’admettre l’efficacité et l’innocuité quasi totale des vaccins sont aussi des dénialistes. Tout comme celles qui refusent d’admettre la réalité du réchauffement climatique anthropique; qui ne croient pas que le VIH cause le sida; ou qui refusent d’admettre la théorie de l’évolution».
Daniel Clement Dennett est un philosophe américain né le 28 mars 1942 à Boston. Il s'est spécialisé en philosophie de l'esprit et en philosophie des sciences et a largement contribué à la recherche fondamentale dans les sciences cognitives. Il est tout particulièrement reconnu pour son travail concernant les retombées de la théorie de l'évolution et son analyse critique de l'héritage du dualisme cartésien.
Devenez sorciers, devenez savants est un pamphlet et un essai de vulgarisation scientifique publié aux éditions Odile Jacob par Georges Charpak et Henri Broch en 2002. Les auteurs se livrent à une critique des disciplines considérées par les scientifiques comme des pseudo-sciences, et principalement des tenants de la parapsychologie et du paranormal. Un certain nombre de « trucs » de mystificateurs sont expliqués et diverses erreurs classiques de raisonnement sont démontées. Les auteurs s'en prennent notamment à Élizabeth Teissier, à TF1, mais aussi Arte (dont il est rappelé que pour sa première journée de diffusion hertzienne, sa première émission « scientifique » a été consacrée aux influences prétendues sur la matière de la forme pyramidale) et aux « antinucléaires ». Des données sociologiques sur la croyance des Français au paranormal sont citées en annexe.
Ann Druyan est une journaliste et un écrivain américain, dernière épouse et veuve de l'astronome et écrivain Carl Sagan. Elle est également impliquée dans des projets de vulgarisation scientifique. Elle est notamment coauteur de la série Cosmos.
Thomas C. Durand est un écrivain, dramaturge, vidéaste et vulgarisateur français.
Joseph Edamaruku (né le 7 septembre 1934 et décédé le 29 juin 2006) était un journaliste et rationaliste indien de Kerala.
Sanal Edamaruku est un rationaliste et écrivain indien. C’est le fondateur-président et éditeur de Rationalist International, président de l’Indian Rationalist Association et l’auteur de 25 livres et de nombreux articles.
L'effet Barnum, « effet Forer », « effet puits », « effet de validation subjective » ou « effet de validation personnelle », désigne un biais subjectif induisant toute personne à accepter une vague description de la personnalité comme s'appliquant spécifiquement à elle-même.
L’Enquête sur l’entendement humain (An Enquiry concerning Human Understanding en anglais), est une œuvre philosophique du philosophe empiriste écossais David Hume, publié en 1748. Cet ouvrage est la simplification d’une œuvre précédente, le Traité de la nature humaine publié sans nom à Londres en 1739-1740. Déçu par la réception du Traité de la nature humaine, l’Enquête sur l’entendement humain, plus courte et plus polémique que son précédent ouvrage, constitue une nouvelle tentative, de la part de Hume, de diffuser ses idées parmi le public. L’Enquête écarte une grande partie du matériel du Traité pour en éclaircir et en souligner les aspects plus importants. Par exemple Hume ne fait pas figurer son point de vue quant à l’identité personnelle ou sur la conscience mais à l’inverse ses arguments traitant du rôle de l’habitude sur la causalité, fondements de sa théorie de la connaissance, sont retenus.
Edzard Ernst, né à Wiesbaden le 30 janvier 1948, est un médecin naturalisé britannique, formé en Allemagne et en Autriche à la médecine physique et de réadaptation, à l'homéopathie et à la chiropraxie, professeur de médecine complémentaire à l'université d'Exeter (Royaume-Uni). Edzard Ernst a participé à de nombreuses publications médicales et scientifiques dans le domaine des médecines non conventionnelles : homéopathie, ostéopathie, phytothérapie, médecine chinoise, Reiki, etc. D'origine allemande, il est naturalisé britannique depuis 1999. Ernst a été accusé par le secrétaire privé du Prince Charles d'avoir rompu un accord de confidentialité concernant le rapport Smallwood de 2005. L'enquête réalisée par l'université d'Exeter a été très désagréable pour lui et bien qu'elle ait accepté son innocence, il dit être devenu persona non grata et ses financements ont été coupés. En 2011 il se retire 2 ans avant la fin de son poste. Ernst est entré en conflit avec le Prince Charles, l'accusant de vendre, via l'entreprise The Prince’s Trust qui commercialise des produits issus de son exploitation agricole biologique, des remèdes de détoxication douteux à base de plantes (pissenlit, artichaut), sans fondement scientifique et d'être un « snake oil salesman » (marchand de poudre de perlimpinpin),.
L'esprit critique (du grec κριτικός : qui discerne) est la disposition d'une personne à examiner attentivement une donnée avant d'en établir la validité.
Les Faiseurs de miracles est un livre de Gérard Majax publié en 1992 aux éditions Michel Lafon. Le prestidigitateur y traite de la mystification, ancienne ou moderne, qu'ont pratiqué de nombreux charlatans à travers l'Histoire, que ce soit sous couvert d'alchimie, de perception extrasensorielle ou de pouvoirs psychiques. Des recherches historiques ont été menées par Emmanuel Haymann pour étayer les explications des mystères les plus anciens.
Bertram R. Forer (24 octobre 1914 - 6 avril 2000) était un psychologue américain connu pour avoir décrit l'Effet Barnum, parfois appelée validation subjective. Né à Springfield (Massachusetts), Forer est diplômé de l'université de Massachusetts en 1936. Il a obtenu son doctorat en psychologie clinique de l'Université de Californie. Il a travaillé comme psychologue et administrateur dans un hôpital militaire en France pendant la Seconde Guerre mondiale. À son retour, il a travaillé dans une clinique de santé mentale du Département des Anciens combattants des États-Unis à Los Angeles et en pratique privée à Malibu. Dans son expérience de 1948, Forer a soumis ses élèves à un test de personnalité, puis comme résultat, il remit à chacun d'eux la même description construite à partir d'un recueil d'horoscopes et leur demanda de noter la pertinence de l'évaluation de sa personnalité sur une échelle de 0 (médiocre) à 5 (excellent). L'évaluation moyenne était de 4.26. L'expérience a été répétée des centaines de fois depuis 1948, et la moyenne reste à peu près 4.2. L'effet Barnum montre que les gens ont tendance à accepter des descriptions généralisées des personnalités sans se rendre compte que la même évaluation pourrait s'appliquer à presque n'importe qui d'autre, parce que les gens veulent que les résultats soient vrais. Cette expérience est souvent citée comme une critique des autres test de personnalités, comme le Myers-Briggs Type Indicator.
Barbara Carroll Forrest (née le 25 juin 1952) est un professeur de philosophie de la Southeastern Louisiana University (en). Elle est surtout connu pour ses critiques du dessein intelligent (DI) et du Discovery Institute. Forrest fait partie du conseil d'administration du National Center for Science Education (NCSE), de l'Americans United for Separation of Church and State et de la New Orleans Secular Humanist Association (NOSHA),,.
Chris French est un psychologue britannique né le 6 avril 1956. Il est un spécialiste de la psychologie des expériences inhabituelles, une branche de la parapsychologie. Il est aussi un membre actif du mouvement sceptique contemporain. Il est actuellement professeur à Goldsmiths College, University of London. Il est responsable de la "Anomalistic psychology Research Unit", qu'il fonda en 2000. Il est aussi le coéditeur du magazine "The Skeptic (UK)".
Martin Gardner (né le 21 octobre 1914 à Tulsa, Oklahoma et mort le 22 mai 2010 à Norman, Oklahoma) était un écrivain américain de vulgarisation mathématique et scientifique, aux intérêts portant aussi bien sur le scepticisme scientifique, la micromagie, la philosophie, la religion et la littérature - en particulier les écrits de Lewis Carroll, L. Frank Baum, et GK Chesterton,. Il était considéré comme une autorité majeure relativement à Lewis Carroll. Sa version annotée d'Alice, qui comprenait le texte des deux livres de L. Carroll, est son œuvre la plus accomplie et s'est vendue à plus d'un million d'exemplaires . Il s'est intéressé toute sa vie à la magie et l'illusion et était considéré comme l'un des plus importants magiciens du XXe siècle ainsi que le doyen des créateurs américains d'énigmes. C'était un auteur prolifique et polyvalent, publiant plus de 100 livres au cours de sa vie. Gardner était surtout connu pour la création et le maintien de l'intérêt pour les mathématiques récréatives - et par extension, les mathématiques en général - tout au long de la seconde moitié du XXe siècle, grâce à ses colonnes « Jeux mathématiques », qui parurent pendant vingt-ans dans Scientific American et les livres suivants qui les regroupaient,. Gardner fut l'un des principaux polémistes anti-pseudosciences du XXe siècle. Son livre Fads and Fallacies in the Name of Science, publié en 1957, est devenu une œuvre classique et fondatrice du mouvement sceptique. En 1976, il s'est joint à d'autres sceptiques pour fonder le CSICOP, organisme de promotion de la recherche scientifique et de l'utilisation de la raison dans l'examen des revendications sortant de l'ordinaire.
Nicolas Gauvrit, né le 25 septembre 1971, est un psychologue et mathématicien français spécialisé en science cognitive. Il enseigne les mathématiques à l'ESPE Lille-Nord-de-France et est membre institutionnel du laboratoire universitaire « Cognitions Humaine et Artificielle » (CHArt).
Prabir Ghosh (né le 1er mars 1945) est un citoyen indien, président de la Science and Rationalists' Association of India et de l'association humaniste, basée à Calcutta, principalement connu pour son militantisme contre les « devins et messies » en Inde orientale depuis le milieu des années 1980. Un documentaire, intitulé "Guru Busters", présente sa méthode et ses confrontations avec des escrocs et charlatans dans la région de Calcutta, dont l'astrologue Satyananda qui a demandé à ses disciples de le « lyncher » lors d'une émission de télévision. Il a fait une offre de 50 000 dollars pour toute personne « qui déclarerait avoir des pouvoirs surnaturels de toute sorte et capable de les démontrer sans avoir recours à des tours de prestidigitation et dans un lieu défini par Prabir Ghosh »
Michel Granger, né en 1943, est un scientifique de formation, ingénieur chimiste, docteur en chimie physique (Ph. D. Montréal, Canada),, passionné d'anomalies, de faits et de phénomènes inexplicables et de paranormal, auxquels il a consacré de nombreuses enquêtes et écrits depuis 1972.
Douglas James « D.J. » Grothe, né le 25 juin 1973 à Saint-Louis, Missouri, est un athée et un sceptique américain. Il fut jusqu'en 2009 vice-président du Center for Inquiry, un « think tank that advances science, reason and secular values in public affairs » (« un laboratoire d'idées ayant pour objectif d'avancer la science, la raison et les valeurs séculières dans les affaires publiques »). Il est un éditeur associé du magazine Free Inquiry et l'hôte des podcasts For Good Reason, le podcast officiel de la James Randi Educational Foundation (JREF). En décembre 2009, il a été nommé président de la James Randi Educational Foundation, remplaçant Philip Plait à ce poste le 1er janvier 2010. Avant de devenir le président du JREF, il était l'hôte du podcast Point of Inquiry (le podcast officiel du Center for Inquiry). À son départ, il fut remplacé par Chris Mooney, Karen Stollznow, et Robert M. Price.
Harry Houdini, de son vrai nom Ehrich Weisz, né le 24 mars 1874 à Budapest, alors en Autriche-Hongrie — mort le 31 octobre 1926 à Détroit, aux États-Unis, est un illusionniste américain d'origine hongroise.
David Hume (7 mai 1711 - 25 août 1776) est un philosophe, économiste et historien écossais. Il est considéré comme un des plus importants penseurs des Lumières écossaises (avec Adam Smith et Thomas Reid) et est un des plus grands philosophes et écrivains de langue anglaise. Fondateur de l'empirisme moderne (avec Locke et Berkeley), l'un des plus radicaux par son scepticisme, il s'opposa tout particulièrement à Descartes et aux philosophies considérant l'esprit humain d'un point de vue théologico-métaphysique : il ouvrit ainsi la voie à l'application de la méthode expérimentale aux phénomènes mentaux. Son importance dans le développement de la pensée contemporaine est considérable : Hume eut une influence profonde sur Kant, sur la philosophie analytique du début du XXe siècle et sur la phénoménologie. On ne retint pourtant longtemps de sa pensée que le scepticisme destructeur ; mais les commentateurs de la fin du XXe siècle se sont attachés à montrer le caractère positif et constructif de son projet philosophique.
En science et en philosophie, une hypothèse ad hoc est une hypothèse arbitraire ajoutée à une théorie afin d'empêcher de la voir falsifiée. Cet argument fallacieux est utilisé pour compenser les anomalies non prévues par la théorie dans sa forme initiale.
L'hypothèse du champ magnétique propose une explication rationnelle aux phénomènes d'objets volants non identifiés (ovnis) mais aussi à toute forme de vision mystique ou d'hallucination collective.
Impostures intellectuelles est un ouvrage d'Alan Sokal et Jean Bricmont publié en français en 1997. L'ouvrage constitue une critique assez dure envers ce que les auteurs regroupent sous le nom de « philosophie postmoderne ». Il vise en particulier des penseurs qui utilisent les concepts ou le vocabulaire des mathématiques ou de la physique, relève des erreurs, dénonce des pensées vides de sens et commente des extraits de livres de Jacques Lacan, Julia Kristeva, Bruno Latour, Gilles Deleuze, Luce Irigaray, Jean Baudrillard, et Félix Guattari. Cet ouvrage a été publié en anglais l'année suivante sous le titre Fashionable Nonsense: Postmodern Intellectuals' Abuse of Science. Une nouvelle édition française, revue et augmentée, est sortie en 1999.
La James Randi Educational Foundation (JREF) est un organisme à but non lucratif fondé en 1996 par l'illusionniste et sceptique James Randi. La mission du JREF inclut l'éducation du public et des médias sur les dangers de l'acception d'assertions non prouvées et soutient la recherche scientifique sur le paranormal. L'organisation offre un prix d'un million USD à toute personne en mesure de démontrer des pouvoirs surnaturels lors d'un test respectant les critères de l'expérimentation scientifique. Le JREF dispose aussi d'un fonds pour aider les personnes qui seraient attaquées en justice en raison de leurs enquêtes et critiques d'individus revendiquant des dons paranormaux. L'organisation est financée par les contributions des membres, les ventes de livres et de vidéos, ainsi que par des conférence. Chaque vendredi, le site web du JREF publie un commentaire intitulé Swift: Online Newsletter of the JREF. Celui-ci inclut les dernières nouvelles du JREF et des informations. Le directeur actuel de la James Randi Educational Foundation est, depuis janvier 2010, D.J. Grothe.
La jumbologie est une pseudo-science parodique créée par des physiciens[Lesquels ?] en réaction à l’engouement pour l’astrologie.
Philip J. Klass (Des Moines (Iowa), 8 novembre 1919 - Cocoa (Floride), 9 août 2005) est un journaliste et sceptique américain, membre de la cellule phénomène ovni du Committee for Skeptical Inquiry.
Paul Kurtz, né le 21 décembre 1925 à Newark, dans le New Jersey et mort le 20 octobre 2012, est un philosophe américain.
Le Laboratoire de zététique est une structure universitaire rattachée à l'université de Nice Sophia-Antipolis fondé par Henri Broch en 1998.
Gabrielle Hélène Joliot-Curie, devenue Hélène Langevin-Joliot, née le 19 septembre 1927 à Paris, est une physicienne française.
La lecture à froid est une technique pour récupérer des informations sur un individu par l'observation de ses réactions et une ligne de questionnement imprécis pour cibler rapidement ses besoins ou ses manques. Elle est utilisée par des vendeurs, des interrogateurs, des psychologues, des médecins (en particulier les psychiatres), des politiciens, des hypnotiseurs, des graphologues, des magiciens, des chiromanciens, des astrologues, des sectes et des escrocs. Même sans connaissance préalable d'une personne donnée, un lecteur froid peut rapidement obtenir beaucoup d'informations à son sujet en analysant avec attention ses habits ou sa mode, sa coiffure, son sexe, sa religion, son ethnie, son niveau d'éducation, sa manière de parler et son origine. La lecture à froid est un outil d'analyse et de communication entre individus de plus en plus utilisée dans la communication professionnelle[réf. nécessaire]. Elle permet de cibler les besoins d'une personne rapidement, de communiquer efficacement et d'enlever les doutes quand les échanges sont à clarifier.
Daniel Loxton, né en 1975, est un écrivain canadien, illustrateur et sceptique. Il a écrit ou co-écrit plusieurs ouvrages dont Tales of Prehistoric Life, une trilogie scientifique pour enfants et Abominable Science!, un aperçu scientifique sur la cryptozoologie. En tant que rédacteur en chef de Junior Skeptic, Loxton y écrit et illustre la plupart des questions, une section scientifique des enfants dans la société dans le magazine. Loxton a écrit des articles pour des publications de la pensée critique, y compris eSkeptic, Skeptic, Mémoires Sceptiques et le Skeptical Inquirer, ainsi que la contribution de l'art de couverture de Skeptic, Oui et Free Inquiry. Il contribue également régulièrement à Skepticblog, un blog de collaboration sur la science, la pensée critique et le scepticisme.  Portail d’Internet  Portail du scepticisme rationnel
The Magic of Reality: How We Know What's Really True, en français La magie de la réalité : comment nous savons ce qui est réellement vrai est un livre écrit en 2011 par le biologiste Richard Dawkins, et illustré par Dave McKean. Le livre parait le 15 septembre 2011 au Royaume-Uni, puis le 4 octobre 2011 aux États-Unis. Il s'agit d'un livre scientifique réalisé sous forme d'illustrations à destination des enfants et jeunes adultes. Richard Dawkins précise que le livre est accessible aux enfants à partir d'environ 12 ans, et que les enfants moins âgés peuvent parvenir à en comprendre le contenu avec l'assistance d'un adulte. Le livre est publié au Royaume-Uni par Bantam Press, et aux États-Unis d'Amérique par Free Press.
Gérard Majax de son vrai nom Maurice Faier, né le 28 avril 1943 à Nice, en France, est un artiste et homme de spectacle dont le savoir-faire est fondé sur la prestidigitation.
Le mentalisme est un art du spectacle qui consiste à créer l'illusion de facultés paranormales ou d'une spécialisation dans la maîtrise des capacités mentales humaines (télépathie, la psychokinésie, l'hypermnésie, clairvoyance, etc.), dans l'objectif de divertir un public.
Le modèle sociopsychologique du phénomène ovni est une thèse visant à expliquer la plupart des observations d'objet volant non identifié dans un cadre sociopsychologique, c'est-à-dire dans un milieu social dans lequel les individus peuvent s'influencer mutuellement (soit par communication directe, le bouche à oreille, soit par l'amplification qu'en donnent les médias directement implantés dans ce milieu). La théorie qui consiste à chercher une explication dans ce modèle repose en partie sur le principe du rasoir d'Occam, selon lequel l'explication la plus simple à un phénomène a priori inexplicable doit être privilégiée au détriment de thèses plus compliquées (particulièrement lorsqu'elles mettent en avant des éléments non prouvés comme des visites de la Terre par des extraterrestres). Cette hypothèse qui explique le phénomène OVNI par des méprises, des erreurs d'interprétation, des hallucinations ou de faux souvenirs, reçoit le soutien d'une partie de la communauté scientifique.
Michel Monnerie (né en 1940) est un ufologue français, membre de l'association Lumières dans la nuit (LDLN) et représentant de la « nouvelle ufologie ». Il a également été président de la Société Parisienne d'Etude des Phénomènes Spatiaux et Etranges (SPEPSE). En 1977, il lance un pavé dans la mare dans le monde de l'ufologie francophone en publiant l'ouvrage: Et si les ovnis n'existaient pas? (Paris, Les Humanoïdes Associés). Cet ouvrage fondateur marque le début du mouvement qui a été surnommé « nouvelle ufologie » en France. Deux années plus tard, Monnerie publie un second ouvrage, Le naufrage des extraterrestres (Paris, Nouvelles Editions Rationalistes, 1979). Ces deux publications ont été relativement mal accueillies par la communauté ufologique défendant l'hypothèse extraterrestre (HET). Cependant, ils ont influencé un certain nombre d'auteurs qui, à l'instar de Thierry Pinvidic, sont devenus sceptiques après les avoir lus.
Le Mouvement des brights regroupe des personnes qui portent sur le monde un regard « naturaliste », c'est-à-dire libre de tout élément surnaturel ou mystique ; les brights fondent leur éthique et leur comportement sur une compréhension naturaliste de l’univers. Le terme « bright » a été retenu par Paul Geisert et Mynga Futrell, de Sacramento, Californie. Ce néologisme a été présenté pour la première fois en public le 1er mars 2003 devant la Coalition for the Community of Reason, à Kansas City dans le Missouri. Le dictionnaire MacMillan de la langue anglaise lui consacre une page.
Wim Van Utrecht est un auteur sceptique belge néerlandophone s'étant intéressé au phénomène ovni. Van Utrecht a particulièrement travaillé sur la photo de Petit-Rechain qu'il a tenté de reproduire avec des moyens mécaniques.
Paul Zachary « PZ » Myers, né le 9 mars 1957 à Kent dans l'État de Washington, est un professeur américain de biologie à l'université du Minnesota à Morris (UMM) et l'auteur du blog scientifique Pharyngula. Il est actuellement professeur associé de biologie à l'UMM . Il travaille sur les poissons zèbres dans le domaine de la génétique évolutive du développement, et cultive un certain intérêt pour les céphalopodes. Il est un critique virulent contre l'idée d'Intelligent Design (ID) et du mouvement créationniste.
Le mythe de l’utilisation incomplète du cerveau est une légende urbaine selon laquelle la plupart des êtres humains n'utiliseraient que dix pour cent (pour la valeur la plus couramment citée) de leur cerveau.
Joe Nickell (né le 1er décembre 1944 à West Liberty) est un enquêteur sceptique du paranormal. Il travaille aussi en tant que consultant pour l'examen des documents historiques et a aidé à exposer des célèbres fraudes telle que par exemple le prétendu journal de Jack l'Éventreur. Nickell est senior Research Fellow au Center for Inquiry (CFI) et il est aussi un membre de son conseil exécutif. Il écrit régulièrement pour leur magazine, Skeptical Inquirer. Il a travaillé professionnellement en tant que détective privé, journaliste et instructeur universitaire. Il est l'auteur et l'éditeur d'une quarantaine d'ouvrages. L'astéroïde (31451) Joenickell a été nommé en son honneur.
Steven P. Novella, né le 29 juillet 1964, est un professeur américain de neurologie. Steven Novella est célèbre pour son implication dans le mouvement sceptique, et plus particulièrement parce qu'il est l'hôte du podcast The Skeptics' Guide to the Universe.
L'Observatoire zététique ou OZ est une association loi de 1901 française sceptique, fondée en 2003 et siégeant à Grenoble. Il est un membre du Conseil européen des organisations sceptiques (European Council of Skeptical Organisations, ECSO).
Penn et Teller est le nom d'un duo d'illusionnistes américains constitué de Penn Jillette (né en 1955) et de Teller (né en 1948).
Massimo Pigliucci, né le 16 janvier 1964 à Monrovia au Liberia, est un universitaire, vulgarisateur des sciences et bloggeur italien naturalisé américain. Il est généticien (doctorat), biologiste (PhD) et philosophe (PhD) de formation. Il est également une figure du mouvement sceptique contemporain et du mouvement des Brights ainsi que du stoïcisme moderne.
Philip Cary Plait (alias « The Bad Astronomer ») est un astronome et un sceptique américain. Il est l'auteur du Bad Astronomy Blog. Il a travaillé précédemment au département de physique et d'astronomie de l'université d'État de Sonoma. Début 2007, il démissionne de son poste pour écrire Death from the Skies. Le 4 août 2008, il devient président de la James Randi Educational Foundation. Il occupe ce poste jusqu'au 1er janvier 2010, il est remplacé par le sceptique D.J. Grothe. Il est aussi célèbre pour cette phrase « Enseignez à un humain comment raisonner et il pensera pendant toute une vie » soit en anglais Teach a man to reason and he'll think for a lifetime.
Point of Inquiry est une émission de radio et un podcast du Center for Inquiry, un think tank faisant la promotion des sciences, de la raison et des valeurs séculaires. Débuté en 2005, Point of Inquiry a rassemblé de plus en plus d'auditeurs, podcasts après podcasts. La radio a d'abord été animée par D.J. Grothe, qui a quitté l'animation en 2009 pour mener la James Randi Educational Foundation. Il a été remplacé par le journaliste Chris Mooney (en), Karen Stollznow, et Robert M. Price.
Massimo Polidoro (né le 10 mars 1969 à Voghera, dans la province de Pavie) est un écrivain, un journaliste, un universitaire et un vulgarisateur scientifique italien, qui se rattache au mouvement sceptique italien. Auteur et sceptique, Massimo Polidoro est directeur exécutif au Comitato Italiano per il Controllo delle Affermazioni sul Paranormale et a publié plus de trente livres, y compris des essais d'investigation, des biographies, des romans et des aventures pour jeunes adultes.
Pour en finir avec Dieu (The God Delusion) est un essai du biologiste britannique Richard Dawkins, détenteur de la chaire Charles Simonyi à la Public Understanding of Science de l'université d'Oxford, paru en 2006 dans sa version originale. La traduction française de l'ouvrage, réalisée par Marie-France Desjeux-Lefort est parue en 2008. Dawkins y soutient qu'un créateur surnaturel n'existe probablement pas et qualifie cette croyance en un dieu personnifié de délire qu'il définit comme une croyance fausse et persistante se maintenant face aux preuves qui la contredisent. Il reprend l'assertion du philosophe et écrivain américain Robert M. Pirsig disant que « quand une personne souffre de délire, on appelle cela de la folie. Quand un grand nombre de personnes souffrent de délire, on appelle cela une religion »,. Dawkins rappelle aussi que l'on n'a pas besoin de religion pour être moral (la Grèce et Rome nous ont laissé de grands modèles de vertu, alors que leur religion n'avait aucune exigence de cet ordre, qui relevait de la philosophie), et que les origines de la religion et de la moralité peuvent être expliquées de manière non religieuse. La version originale en anglais a été vendue à plus de deux millions d'exemplaires et traduite en plus de trente langues. Le livre a reçu un accueil critique contrasté. Il a suscité le débat et plusieurs ouvrages ont été publiés en réponse.
Michel de Pracontal, né à Cannes en 1954, est un journaliste scientifique français. Il travaille au Nouvel Observateur depuis 1990 ainsi qu'à Mediapart. Essayiste et romancier, il est l'auteur de plusieurs ouvrages. Il est titulaire d'une maîtrise de mathématiques et d'un doctorat en sciences de l'information sur la vulgarisation scientifique.
Robert McNair Price (né le 7 juillet 1954 dans l'État du Mississippi), surnommé « The Bible Geek », est professeur de théologie et d'études bibliques au Johnnie Colemon Theological Seminary, une école non agréée de Carol City, en Floride, dirigée par une organisation adepte de la Nouvelle Pensée, the Universal Foundation for Better Living. Sceptique de la religion, surtout par rapport aux croyances chrétiennes orthodoxes, et se décrivant à l'occasion comme un « athée chrétien » au cours des interviews, il est un ancien membre du Jesus Seminar et l'organisateur d'une communauté sur Internet destinée à ceux qui s'intéressent à l'histoire du christianisme. Il a édité le Journal of Higher Criticism, aujourd'hui disparu, et, outre de nombreux ouvrages et articles sur la religion, il a abondamment écrit à propos de Howard Phillips Lovecraft et du mythe de Cthulhu.
Le projet Alpha est le nom d’un célèbre canular qui eut lieu dans le Laboratoire de recherche parapsychique Mc Donnell, orchestré par l'illusionniste sceptique James Randi.
Pseudosciences & postmodernisme est un ouvrage écrit par le physicien américain Alan Sokal, connu pour son précédent livre Impostures intellectuelles. Ce nouvel ouvrage continue la critique du postmodernisme et du relativisme cognitif, déjà amorcée dans Impostures intellectuelles. Les liens supposés entre pensée postmoderne et pseudo-sciences sont ici analysés. Le relativisme cognitif considère tous les discours sur le monde comme des fictions équivalentes: sciences, traditions religieuses et pseudosciences sont ainsi considérées comme également vraies. Ce relativisme n'encourage-t-il pas l'essor des pseudosciences ? est une des questions posées par Alan Sokal dans ce livre.
Psiram.com, appelé Esowatch jusqu'en juillet 2012, est un projet internet fondé en 2007 comprenant des articles critiquant l'ésotérisme, les théories du complot et la pseudoscience. Sa plateforme offre un wiki basé sur MediaWiki, un blog collectif anonyme et un forum de discussion. En mars 2014, ce site est principalement rédigé en allemand avec 2 745 articles ; 304 articles existent en français et 98 articles en anglais.
Le Québec sceptique est le magazine officiel des Sceptiques du Québec. Il est publié trois fois par année depuis la fin des années 1980. Il traite des phénomènes paranormaux et pseudoscientifiques avec une approche critique propre au scepticisme scientifique. On y retrouve également le résumé des conférences mensuelles organisées par l'organisation et tenues le 13 de chaque mois à Montréal.
Benjamin Radford (né le 2 octobre 1970 à New York) est un sceptique américain. Il est le chef d'édition du magazine Skeptical Inquirer. Il est un enquêteur de terrain, particulièrement intéressé par les monstres de la cryptozoologie, les phénomènes de hantise et les voyants qui aident prétendument la police. Il a écrit plusieurs livres et de nombreux articles sur ces sujets. Il fut à une époque un des hôtes du balado Monster Talk (avec Blake Smith et Karen Stollznow), un des balados officiels de la Skeptics Society, qui est consacré exclusivement à la cryptozoologie. Il est enfin l'auteur d'un jeu de société : Playing Gods.
James Randi (né Randall James Hamilton Zwinge, le 7 août 1928 à Toronto, Ontario, Canada), plus connu sous l'appellation « Randi le stupéfiant » (« The Amazing Randi »), est un illusionniste professionnel. Il est plus particulièrement connu comme un démystificateur des pseudo-sciences et autres phénomènes paranormaux. Il est à travers sa James Randi Educational Foundation (JREF) un promoteur actif du scepticisme scientifique, et est en tant que tel régulièrement invité dans des émissions télévisées américaines.
Le rapport Condon est le nom usuel du rapport du projet OVNI de l'université du Colorado. Ce projet a étudié les objets volants non identifiés de 1966 à 1968 sous la direction du physicien Edward Condon et a rendu son rapport en 1968. Il avait été fait à la demande de l'US Air Force.
Le Rational Response Squad est un groupe d'athées américains qui s'opposent à tout ce qu'ils considèrent comme étant des revendications irrationnelles, particulièrement celles faite par les théistes. Les fondateurs de ce groupe sont Brian Sapient ainsi que Rook Hawkins. Avec le réalisateur Brian Flemming, le Rational Response Squad a fait les manchettes en décembre 2006 pour avoir mis sur YouTube un projet controversé, The Blasphemy Challenge, qui demande aux gens de faire une courte vidéo dans laquelle ils nient l'existence du Saint-Esprit.
Le rationalisme est la doctrine qui pose la raison discursive comme seule source possible de toute connaissance réelle. Autrement dit, le réel ne serait connaissable qu'en vertu d'une explication par la raison déterminante, suffisante et nécessaire. Ainsi, le rationalisme s'entend de toute doctrine qui attribue à la seule raison humaine la capacité de connaître et d'établir la vérité. Dans son acception classique, il s'agit de postuler que le raisonnement consiste à déterminer que certains effets résultent de certaines causes, uniquement à partir de principes logiques ; à la manière dont les théorèmes mathématiques résultent des hypothèses admises au départ. De plus, et en particulier, les principes logiques eux-mêmes utilisés dans le raisonnement ont été connus par déduction.
Jean-Bruno Renard (né à Paris en 1947) est un sociologue français, professeur à l’Université de Montpellier III (« Paul-Valéry »). Ses travaux portent sur la culture populaire (notamment la bande dessinée et le fantastique), sur les rumeurs et les légendes urbaines, ainsi que sur les croyances au paranormal et le phénomène ovni, qu’il aborde d’un point de vue sceptique. Il a créé le terme « néo-évhémérisme » pour qualifier la théorie des Anciens Astronautes.
Renverser la charge (ou le fardeau) de la preuve signifie qu'un ou plusieurs participants d'un débat heuristique inverse la charge de la preuve[tautologie]. Le renversement a une portée particulière en droit où on traite de charge de la preuve en droit ainsi qu'en sciences[pourquoi ?]. Normalement, la charge de la preuve repose sur celui qui procède à une affirmation. En ce sens, la question de la charge de la preuve est abordée par Christopher Hitchens, qui affirme que « ce qui est affirmé sans preuve peut aussi être rejeté sans preuve » (« What can be asserted without evidence can also be dismissed without evidence. »). Bertrand Russell a abordé le sujet dans son analogie de la théière. La formule originale est la locution latine : « Quod gratis asseritur gratis negatur. » (« Ce qui est affirmé sans preuve peut être nié sans preuve. »), formule régulièrement utilisée depuis au cours du XIXe siècle.
Pamela Reynolds Lowery (1956 – 22 mai 2010), originaire d'Atlanta en Géorgie, était une parolière et chanteuse américaine,. En 1991, à l’âge de 35 ans, elle déclare avoir eu une expérience de mort imminente (EMI) durant une opération d’un anévrisme géant au tronc basilaire par le chirurgien Robert F. Spetzler (en) au Barrow Neurological Institute (en) à Phoenix en Arizona. Son expérience figure parmi les études les plus largement documentées sur les expériences de mort imminente en raison des circonstances inhabituelles sous lesquelles elle s’est produite. Pamela Reynolds était sous surveillance médicale étroite pendant toute l'opération. Pendant une partie de l'opération, elle avait un électroencéphalogramme plat et son cerveau n’était plus irrigué par la circulation sanguine, ce qui la rendait cliniquement morte. Elle a fait plusieurs observations au sujet de la procédure qui ont ensuite été confirmées par le personnel médical comme exactes. Ce récit d'EMI a gagné une popularité internationale depuis sa publication en 1998 dans le livre Light and Death, du docteur Michaël Sabom, cardiologue et membre fondateur du groupe de pression IANDS. Les témoignages de Pamela Reynolds et de ses médecins ont donné lieu à un reportage diffusé mondialement. Cette expérience de mort imminente est considérée par beaucoup comme une preuve de la réalité de la survie de la conscience après la mort, et d'une vie après la mort. Toutefois, les critiques ont avancé plusieurs points contre cette interprétation. Voir aussi le paragraphe analyse critique. Pamela Reynolds est décédée d’une crise cardiaque le samedi 22 mai 2010 à Emory University Hospital (en), à Atlanta, Georgia à l’âge de 53 ans, 19 ans après son opération,.
Emily Rosa est une Américaine née le 6 février 1987  à Loveland dans le Colorado, qui est devenue à l'âge de onze ans la plus jeune personne à avoir publié dans une revue de médecine à comité de lecture — Le Journal of the American Medical Association. Fille d'une infirmière et d'un inventeur, en 1996 alors âgée de neuf ans Rosa conduisit une étude mettant en cause la validité du toucher thérapeutique (TT). Les 21 partisans n'arrivèrent pas à détecter un champ énergétique humain ou aura.
Michel Rouzé est un pseudonyme sous lequel travaillait le journaliste scientifique français Michel Kokoczynski (né le 17 août 1910 à Paris, mort le 18 février 2003). Il a notamment collaboré à la revue Science et Vie.
Carl Edward Sagan (prononcé [ˈseɪ.ɡən]), né le 9 novembre 1934 à Brooklyn, New York et mort le 20 décembre 1996 (à 62 ans) à Seattle, Washington, est un scientifique et astronome américain. Il est l'un des fondateurs de l'exobiologie. Il a soutenu le programme SETI de recherche d'intelligence extraterrestre et réalisé pour la télévision la série de vulgarisation scientifique Cosmos, diffusée sur plusieurs continents. Il est aussi connu pour son scepticisme.
Le scepticisme scientifique, nommé aussi scepticisme rationnel ou scepticisme contemporain, est, philosophiquement une position épistémologique, éthiquement une déontologie circonspecte et pratiquement une attitude de doute cartésien vis-à-vis des allégations non étayées par des preuves empiriques ou par la reproductibilité. Cette démarche cherche à promouvoir la science, la pensée critique et à soumettre à la méthode expérimentale (lorsque cela est possible) les phénomènes dits « paranormaux » (notamment ceux étudiés par l'ufologie, la parapsychologie et la cryptozoologie) ou surnaturels (réincarnation, résurrection). Les sceptiques soumettent ainsi au doute tant les théories du complot, les médecines non conventionnelles et, de manière plus générale, ce que la majeure partie de la communauté scientifique considère comme des pseudo-sciences, que les dérives idéologiques et méthodologiques consistant à transformer le doute cartésien en méthode hypercritique à des fins polémiques.
Les Sceptiques du Québec est une association à but non lucratif québécoise fondée en 1987. Son principal objectif est de promouvoir la pensée critique et la rigueur scientifique dans le cadre de l'étude d'allégations de nature pseudoscientifique, religieuse, ésotérique ou paranormale. Elle se rattache ainsi au scepticisme scientifique. La corporation compte près de 400 membres et abonnés à travers le Québec, dont une quarantaine de membres actifs, qui sont tous des bénévoles.
Scientific American est un magazine de vulgarisation scientifique américain à parution mensuelle (initialement hebdomadaire) existant depuis le 28 août 1845, ce qui en fait la plus ancienne revue des États-Unis parue de façon continue. Il appartient au groupe Holtzbrinck.
Le scientisme est une vision du monde, apparue au XIXe siècle, selon laquelle la science expérimentale a priorité sur les formes plus anciennes de référence — révélation religieuse, superstitions, tradition, et coutumes — pour interpréter le monde. Le scientisme veut, selon la formule d'Ernest Renan (1823-1892), « organiser scientifiquement l'humanité ». Il s'agit donc d'une confiance (le terme de foi ne s'applique pas, en principe, dans ce domaine) dans l'application des principes et méthodes de la science moderne dans tous les domaines. On peut résumer le cœur de cette croyance en : « La science décrit (vraiment) le monde tel qu'il est. » Le terme scientisme est aussi utilisé pour désigner l'idéologie selon laquelle tous les problèmes qui concernent l'humanité et le monde pourraient être réglés au mieux, si ce n'est parfaitement, suivant le paradigme de la méthode scientifique. Le scientisme croit que « l'esprit et les méthodes scientifiques doivent être étendues à tous les domaines de la vie intellectuelle et morale sans exception ». On peut distinguer pratique ou quête de la science et scientisme en tant que doctrine idéologique. Cette idéologie est liée à divers degrés à celles de la modernité, du rationalisme, à la « loi des trois états » d'Auguste Comte, mais aussi à bien des formes de réductionnisme, au matérialisme ou parfois au dualisme cartésien. Ses formes extrêmes font l'objet de critiques venant de divers horizons : philosophique, moral, politique, voire scientifique.
Jacques Scornaux (né en 1946) est un auteur sceptique belge. Ses publications portent principalement sur le phénomène ovni. À une époque où il n'était pas encore sceptique, il fut un membre actif de la Société belge d'étude des phénomènes spatiaux. Jacques Scornaux a beaucoup contribué à populariser et vulgariser les idées de Michel Monnerie à travers plusieurs articles. L'accès des textes de Monnerie était difficile car il ne maîtrisait pas le vocabulaire des sciences humaines et son style était jugé maladroit[réf. nécessaire]. Dans ses articles, Scornaux a présenté les idées de Monnerie dans un vocabulaire plus scientifique et d'une manière plus rigoureuse, ce qui a permis de convaincre plus de monde de l'intérêt de ses deux ouvrages qui sont à l'origine de la « nouvelle ufologie ».
Robert Sheaffer (né en 1949 à Chicago) est un astronome américain, sceptique, membre du Committee for Skeptical Inquiry. Il tient la chronique « Psychic Vibrations » du magazine Skeptical Inquirer. Robert Sheaffer s'intéresse particulièrement à l'étude critique du phénomène OVNI et a écrit un ouvrage sceptique sur cette question : UFO Sightings: The Evidence. Il est membre de Mensa.
Michael Brant Shermer (né le 8 septembre 1954 à Glendale) est un journaliste scientifique et un historien des sciences américain. Fondateur et président de The Skeptics Society, il est également rédacteur en chef du magazine Skeptic (en). Il tient une rubrique sceptique régulière dans Scientific American. Michael Shermer est l'auteur de nombreux ouvrages sur la science, le scepticisme scientifique et la pensée critique.
Seth Shostak, né le 20 juillet 1943 à Mountain View en Californie, est un astronome américain connu pour ses recherches dans le cadre du programme SETI,.
Khushwant Singh (2 février 1915 – 20 mars 2014) est un journaliste et romancier indien.
Simon Singh, né le 19 septembre 1964 à Wellington (comté de Somerset, Angleterre), est un écrivain et journaliste scientifique britannique. Il s'est spécialisé dans la vulgarisation de sujets mathématiques et scientifiques. Il a notamment écrit les livres Le Dernier Théorème de Fermat et Histoire des codes secrets. De l'Égypte des pharaons à l'ordinateur quantique.
Le Skeptical Inquirer the magazine for science and reason ("L'Enquêteur sceptique, le magazine pour la science et la raison") est un magazine bimestriel américain fondé et publié par le Committee for Skeptical Inquiry (CSI) depuis 1976. Le siège social du magazine est situé à Amherst (New York). Selon le CSI, la mission du magazine est « d'encourager des enquêtes scientifiques d'affirmations faites par diverses pseudo-sciences ou autres branches du paranormal et de diffuser les résultats auprès de la communauté scientifique et du public ». Le Skeptical Inquirer est distribué internationalement.
The Skeptics Society est une organisation à but non lucratif, soutenue par ses membres, elle est consacrée à la promotion du scepticisme scientifique et à la résistance à la propagation de la pseudoscience, de la superstition et des croyances irrationnelles . The Skeptics Society a été fondée par Michael Shermer en tant que groupe sceptique de Los Angeles pour remplacer les sceptiques de Southern California (la Californie du Sud). Après le succès de son magazine, Skeptic , introduit au début de 1992, il est devenu une organisation nationale et internationale. La mission déclarée de Skeptics Society et Skeptic magazine "est l'étude de la science et des controverses pseudoscientifiques.
Skeptoid est, depuis 2006, un site internet et un podcast. Le podcast Skeptoid hebdomadaire reprend tous les mythes les plus populaires et révèle la vraie science, l'histoire vraie et les leçons véritables que nous pouvons apprendre de chacun. Le flux Internet compte 228 000 auditeurs hebdomadaires, et beaucoup plus sur la radio syndiquée.
Alan Sokal, né le 24 janvier 1955, est un physicien et épistémologue américain. Il est professeur de mathématiques à l'University College de Londres et professeur de physique à l'Université de New York. Il a écrit et fait publier le canular à l'origine de ce qui est devenue « l'affaire Sokal » (1996).
Le syndrome du vrai croyant (en anglais : true-believer syndrome) est une expression utilisée dans le cadre du mouvement sceptique contemporain pour décrire une croyance irrationnelle dans des phénomènes paranormaux au sens large, comme la croyance inconditionnelle en l'hypothèse extraterrestre pour expliquer le phénomène OVNI ou encore en l'existence des animaux légendaires de la cryptozoologie.
The Amaz!ng Meeting (TAM) fut une conférence annuelle relative à la Science, au scepticisme, et à l'athéisme. La première d'entre elles fut organisée en 2003 par la James Randi Educational Foundation. Parmi les orateurs participant à ces conférences, on peut citer Christopher Hitchens, Penn & Teller, Phil Plait, Michael Shermer et Julia Sweeney.
The Skeptic's Dictionary est une collection d'essais sceptiques de Robert Todd Carroll, publié sur le site web skepdic.com puis par l'intermédiaire d'un livre,,. Le site skepdic.com site a été lancé en 1994 et le livre a été publié en 2003 avec environ 400 entrées. En janvier 2011, le site internet comprend plus de 500 entrées. Le site et le livre mènent à des informations sceptiques sur les pseudosciences, le paranormal, les sujets occultes. La bibliographie contient quelque 700 références pour plus d'informations détaillées. Selon la couverture arrière du livre, la version internet reçoit environ 500 000 visites par mois.
The Skeptics 'Guide to the Universe est un podcast hebdomadaire de 80 minutes organisé par Steven Novella, MD, et un panel de "rogues sceptiques ". C'est le podcast officiel de la New England Skeptical Society. Le spectacle présente des discussions sur les développements scientifiques récents en termes de profanes, et entretiens des auteurs, des personnes dans le domaine de la science et d'autres sceptiques célèbres. Le spectacle comprend également des discussions sur les mythes, les théories du complot , la pseudoscience , le paranormal, et nombreuses formes générales de superstition, du point de vue du scepticisme scientifique. Steven Novella , l'hôte du spectacle, a joué un rôle particulièrement actif dans le déboisement de la pseudoscience en médecine. Ses activités comprennent l'opposition aux revendications des activistes anti-vaccins , des praticiens de l'homéopathie et des individus qui refusent le lien entre le VIH et le sida.
Marcello Truzzi (né le 6 septembre 1935 - mort le 2 février 2003) était un professeur de sociologie au New College of Florida et à l'Université d'Eastern Michigan. Il est l'un des cofondateurs du Committee for the Scientific Investigation of Claims of the Paranormal (CSICOP) ainsi qu'un fondateur de la Society for Scientific Exploration. Il a été directeur du Center for Scientific Anomalies Research. Qualifié comme étant « le sceptique des sceptiques » (the skeptic's skeptic) par Paul Kurtz, Truzzi faisait des recherches sur plusieurs protosciences et pseudosciences.
Une chandelle dans les ténèbres est une collection d'ouvrages de zététique publiée par les éditions book-e-book et dirigée par Henri Broch, physicien et professeur à l'université de Nice Sophia Antipolis. Le titre de la collection est un hommage à de lointains prédécesseurs comme A candle in the dark de Thomas Ady (1656) et The Discoverie of witchcraft de Reginald Scot (1584). Elle balaie différents thèmes où le scepticisme scientifique permet d'éclairer diverses croyances modernes ou anciennes et d'y démêler les faits : numérologie, homéopathie, psychanalyse...
L'Union des athées est une association fondée en 1970 par Albert Beaughon qui a pour ambition la promotion de l'athéisme et le rassemblement de ceux qui y adhérent. L'athéisme est le fait de ne croire en aucune divinité supérieure et une attitude intellectuelle unissant un rationalisme kantien large et ouvert à une liberté de pensée.
L'Union rationaliste est une association française à but non lucratif fondée en 1930, sous l'impulsion, en particulier, du physicien Paul Langevin. Elle promeut le rôle fondamental de la raison dans les capacités d'adaptation, d'organisation, d'expérimentation et de critique propres à l'espèce humaine, et le fait de faire reconnaître que les avancées techniques, scientifiques, politiques et culturelles de l'homme sont principalement dues à la raison. Elle lutte contre les différentes formes de dogmatisme ainsi que contre le recours au surnaturel, et promeut une éducation laïque et républicaine. Elle anime des colloques, des conférences, une émission radio sur France Culture (un dimanche matin par mois) et publie deux revues (une bimestrielle et une trimestrielle), et distribue un prix annuel récompensant une œuvre d'inspiration rationaliste. L'Union rationaliste compte parmi ses membres des scientifiques de renom, professeurs au Collège de France ou prix Nobel, des membres de l'Institut de France, ainsi que des écrivains célèbres.
Jacques Van Rillaer, né à Leuven (Belgique) le 14 février 1944 de parents flamands, est un psychologue et psychothérapeute, spécialiste des thérapies cognitivo-comportementales. Il est professeur émérite à l’Université catholique de Louvain et à l’Université Saint-Louis (Bruxelles). Il est notamment connu pour ses recherches historiques et critiques sur la vie et de l'œuvre de Sigmund Freud.
Richard Wiseman (né en 1966) est un psychologue, un prestidigitateur et un sceptique britannique. Il a commencé sa carrière en tant que magicien, avant de s'orienter vers la psychologie.
Le wollfianisme (ou leibnizo-wolffianisme) est une doctrine issue de la philosophie de Christian Wolff et un des principaux courants des Lumières allemandes. Il est souvent identifié au courant rationaliste des Lumières. Mais cela ne doit pas faire oublier que la philosophie de Christian Wolff est aussi très attachée à l'expérience.
La zététique est définie comme « l'art du doute » par Henri Broch. La zététique est présentée comme « l'étude rationnelle des phénomènes présentés comme paranormaux, des pseudosciences et des thérapies étranges ». La zététique est destinée aux théories scientifiquement réfutables, c'est-à-dire respectant le critère de discrimination de Popper. De fait, contrairement aux autres mouvements sceptiques, elle ne pose pas la question des religions et des croyances non réfutables. Son objectif est la mise à l'épreuve d'énoncés pourvus de sens et de nature scientifique (c'est-à-dire réfutables selon Popper) dont les explications ne semblent pouvoir se rattacher à aucune théorie communément acceptée. La zététique se réclame aussi du scepticisme scientifique, et plus généralement de la démarche de doute cartésien qu'elle décrit comme nécessaire en science comme en philosophie. Elle se veut, pour reprendre le mot du biologiste Jean Rostand, une « hygiène préventive du jugement »,,.
Le projet interactif Artsciencefactory a pour objectif de susciter un dialogue entre artistes, scientifiques et citoyens. Il a été conçu par la Communauté d'agglomération du plateau de Saclay (CAPS), Scientipôle Savoirs et Société (S(cube)) et le Centre André Malraux de Sarajevo (CAM). Il est soutenu par le Conseil régional d'Île-de-France.
L'association science et bien commun (ASBC) est une association, créée en 2011 dont l'objet est la promotion d'une science ouverte.
Jean-Michel Cornu né le 13 février 1960 à Paris, France est un expert européen dans le domaine de la coopération et de l'intelligence collective ainsi que des nouvelles technologies et de la société de l'information. Il est également le secrétaire général de Imagination for People et un conférencier prisé sur l'impact des nouvelles technologies.
La crise est un phénomène qui se produit dans de nombreux domaines. Elle nécessite une gestion particulière. Dans certains cas, elle peut être bénéfique et entraîner un changement important. Actuellement de nouveaux type de crises apparaissent, entraînant de nouveaux risques plus ou moins importants. Une conjonction de facteurs peut ainsi donner naissance à une crise parfaite voire majeure. En belgique, le terme de 'crise' est utilisé lorsque les fus de bière sont vides.
Une “Exposciences” réunit un ensemble de projets scientifique, technique, industriel réalisés et présentés par des groupes d’enfants, d’adolescents ou de jeunes, avec ou sans l’appui d’adultes. Elle privilégie la coopération plutôt que l’aspect compétition. Autant que faire se peut, une “Exposciences” présente des projets issus de cadres éducatifs variés (extra scolaire – péri scolaire – scolaire, …). Il s’agit d’un événement ouvert au public qui permet la rencontre du public et du monde de la recherche. Par ailleurs, la mobilité des jeunes est favorisée par l'organisation d'échanges entre les Exposciences dans le monde et la participation de groupes de jeunes aux “Exposciences européennes et “Exposciences Internationales”. Les Exposciences participent de l'Éducation populaire aux sciences et par les sciences et occupent de ce fait une place singulière dans le champ de la Culture scientifique et technique. Ses acteurs se retrouvent dans la philosophie formalisée dans le manifeste de Montsouris. De telles expositions sont organisées dans plusieurs pays, notamment en France et au Québec.
Lydia Fairchild est une citoyenne américaine qui présente un cas de chimérisme, à savoir qu'elle est porteuse de deux ADN distincts.
La Fondation internet nouvelle génération (Fing) est une association loi de 1901 créée en 2000 par Daniel Kaplan, Jacques-François Marchandise et Jean-Michel Cornu. Sa mission s'articule autour de quatre grandes catégories d'objectifs : mobiliser autour des technologies à venir ; prendre part dans les nouveaux débats éthiques et sociétaux ; favoriser l'émergence d'idées et de projets innovants ; encourager l'appropriation de l'innovation et les partenariats.
La science forensique, ou la forensique, regroupe l'ensemble des différentes méthodes d'analyse fondées sur les sciences (chimie, physique, biologie, neurosciences, informatique, mathématique, imagerie, statistiques) afin de servir au travail d'investigation de manière large : une opération qui a pour but " la découverte de faits, l'amélioration des connaissances ou la résolution de doutes et de problèmes. Concrètement, il s'agit d'une recherche poussée d'informations, avec le but de l'exhaustivité dans la découverte des informations inconnues au début de l'enquête et parfois la volonté de publication des informations collectées." . Elles englobent les méthodes de police scientifique, de finance forensique, d'informatique forensique, de médecine légale (analyse physiologique et psychiatrique), d'intelligence stratégique et de renseignement.
Forum ATENA est une association loi de 1901 créée en janvier 2007 par Philippe Recouppé, Samir Koleilat, Jacques Heitzmann, Philippe Poux, Jacques Baudron, Florence de Courtenay, Gérard Peliks, Pauline Duffour, Gérard Dupin..., elle se définit comme « un lieu d'échange et de réflexion sur les nouvelles technologies de l'information et de la communication, en partenariat avec les entreprises et l'enseignement supérieur ». Forum ATENA a pour objectif de faciliter le développement des NTIC et de favoriser les usages pour le plus grand bénéfice de tous ses acteurs. Mission et moyens d'action : Communiquer avec sa lettre d'information ; Susciter les débats ; Mobiliser avec des événements ; Participer en groupes de travail avec des Ateliers ciblés.
La fringe science (littéralement « science marginale ») est un terme anglo-saxon désignant les recherches scientifiques, au sein de sciences académiques reconnues, qui s'éloignent significativement des théories généralement admises. La fringe science est donc la science à la frontière des disciplines scientifiques actuellement établies. Elle s'oppose à la mainstream science, c'est-à-dire la science conventionnelle (mainstream signifie « courant principal » en anglais). Les scientifiques mainstream considèrent typiquement que la fringe science repose sur des concepts très spéculatifs ou réfutés, contrairement aux protosciences qui représentent des pistes plausibles pour faire émerger des sciences nouvelles.
La géologie appliquée désigne l'ensemble des applications de la géologie au sein des activités humaines et de la société. En effet, la géologie et les études géologiques sont d'une importance essentielle dans de nombreux domaines touchant à l'économie, l'environnement et la société. Les exemples les plus emblématiques de l'application de la géologie sont : la recherche de matières premières (granulats, pétrole, gaz naturel, aluminium, terres rares…), la gestion des risques naturels liés à des facteurs géologiques (séismes, éruptions volcaniques, glissement de terrain, inondations…), les études géotechniques dans le cadre de constructions d'infrastructures ou encore la gestion et l'exploitation des ressources en eau.
Le terme junk science (« science poubelle ») est un anglicisme désignant la pseudo-science qui déforme les données scientifiques afin de servir des intérêts idéologiques ou commerciaux en donnant une valeur scientifique à ce qui n’en a pas.   Portail des sciences
Les Petits Débrouillards existent dans de nombreux pays à travers le monde. Ils ont été créés pour la première fois au Canada. Ce mouvement regroupe de nombreuses structures, la plupart du temps associatives, dont le but commun est de faire partager la curiosité scientifique au plus grand nombre. Ainsi, ces associations s'adressent à tous les publics (adultes et enfants). Historiquement, elles s'adressaient aux enfants de 6 à 12 ans en leur proposant de réaliser des expériences ludiques, façon science amusante. Aujourd'hui, ces associations produisent des expositions, des expositions interactives, des livres, des CD-ROM ; elles participent également à la formation professionnelle continue, proposent une expertise didactique et réalisent des outils pédagogiques.
Camille Limoges (né le 31 mai 1942 à Montréal) est un historien canadien, québécois, des sciences et de la technologie.
PARSEC est une association loi de 1901 fondée en 1986 par Jean-Louis Heudier, astronome à l'Observatoire de la Côte d'Azur. Son objet est la vulgarisation scientifique, essentiellement tournée vers l'Astronomie et l'Espace, effectuée essentiellement par des professionnels : scientifiques de l'observatoire de la Côte d'Azur ou ingénieurs du Centre spatial de Cannes - Mandelieu.
Planète Sciences est une association loi de 1901 qui depuis 1962 propose aux jeunes des activités scientifiques et techniques expérimentales, dans le cadre des loisirs et du temps scolaire, avec le soutien de grands organismes scientifiques et industriels tels que le CNRS, le CEA, ainsi que différents ministères. Le CNES représente le partenaire historique de l'association. Planète Sciences est agréée par le ministère de la Jeunesse comme association d'éducation populaire et comme association habilitée à dispenser les formations BAFA et par le ministère de l’Éducation nationale comme Association complémentaire à l'enseignement public.
La politisation de la science désigne la manipulation de la science pour en tirer un profit politique. Elle survient lorsqu'un gouvernement, une entreprise ou un groupe de défense d'intérêts utilise des pressions légales ou économiques pour influencer les résultats de recherches scientifiques ou la manière dont ils sont diffusés, dénoncés ou interprétés. La politisation de la science peut également affecter négativement les libertés scientifiques et académique. Historiquement, certains groupes ont conduit diverses campagnes pour promouvoir leurs intérêts au mépris du consensus scientifique et dans le but de manipuler la politique publique,,.
Le principe de symétrie, dans l’étude sur les sciences (STS - Science and Technology Studies), est une recommandation méthodologique portant sur la manière d’aborder la dynamique des sciences. [réf. nécessaire] En fait, plusieurs principes de symétrie ont été proposés au cours du temps.
La revue pour l'histoire du CNRS est une revue de recherche en sciences sociales publiée par CNRS Éditions.
Claire Salomon-Bayet, née en 1932, et morte à Paris le 9 octobre 2016, est philosophe, professeure émérite à la Sorbonne, spécialiste de Rousseau, Pasteur et Cournot.
Jean-Jacques Salomon (1929-2008) est un historien des sciences, économiste et philosophe français. Haut fonctionnaire à l'OCDE et professeur titulaire de chaire au Conservatoire national des arts et métiers. Spécialiste de politique scientifique, il est l'auteur de nombreux livres et articles.
Le terme positif renvoie à de nombreux usages en sciences humaines et sociales. Un de ces usages se rapporte à l'analyse des théories qui n'ont comme seul objectif que de tenter de décrire les choses telles qu'elles sont, par opposition à ce qu'elles devraient être. En ce sens, le contraire de positif est normatif. Un exemple serait l'analyse économique positive, en opposition à l'analyse économique normative. On dit aussi fréquemment que les énoncés positifs sont des énoncés descriptifs. Le mot positif est au cœur d'un des débats épistémologiques les plus importants en sciences humaines et sociales. Les défenseurs de la science positive (en sciences humaines et sociales) d'une part, sont porte-paroles d'une approche 'dénuée de valeurs' dans l'étude des sciences humaines, partageant ainsi de nombreuses méthodes avec les sciences naturelles. Les défenseurs du positivisme ne cherchent qu'à rendre objectives des descriptions de l'humanité et de la société sans apporter de jugements normatifs. En revanche, ceux qui s'opposent à la science positive rejettent la notion selon laquelle les méthodes des sciences naturelles seraient adaptées à l'explication et à la description de l'humanité et de la société (et ce principalement en raison du 'sens' que les êtres humains associent à leurs actes). Ils considèrent qu'il n'est pas possible de se séparer entièrement des valeurs dans leurs études, un individu ne pouvant pas totalement s'extirper de son temps ni de son espace. La sociologie humaniste est un exemple des approches post-positivistes en sciences sociales. Un autre sens du mot positif est employé pour décrire les choses qui sont définies par la construction, par opposition aux choses qui se définissent « négativement », c'est-à-dire par l'absence de quelque chose. En témoignent les exemples du droit positif ou négatif, ou de la liberté négative et de la liberté positive.
Sciences dures est une expression populaire désignant dans un même ensemble les sciences de la nature et les sciences formelles. Elle est cependant sensiblement plus problématique que cette dernière expression, en particulier du fait de son caractère normatif. Elle peut être perçue comme implicitement dévalorisante : à l'endroit des sciences humaines et des sciences sociales (parfois appelées « sciences molles » ou, moins connoté négativement, « sciences douces ») ; à l'encontre des sciences exactes qu'elle fait paraître comme inhumaines et rigides.
Sciences, technologies et société (STS) est un domaine de recherche multidisciplinaire en sciences sociales (sociologie, économie, psychologie, sciences politiques, histoire, entre autres) portant sur les relations entre les sciences et les technologies et la société. L'expression recouvre à peu près le même programme que les Science and technology studies (STS) du monde anglo-saxon, bien que certaines orientations de recherche en fassent un domaine relativement autonome chez les chercheurs francophones. Le champ d'étude est vaste. Son noyau est la compréhension des interactions entre les modalités internes de la recherche et les caractéristiques spécifiques d`autres domaines de la société. Ainsi par exemple, pour étudier les relations entre la recherche fondamentale en génétique et les organisme financeurs de telles recherches, le chercheur en STS utilisera des résultats d'études de sociologie des sciences et de philosophie des sciences pour d'abord comprendre l'organisation de cette communauté de chercheurs, puis y discerner ce qui ne relève pas du cœur de la pratique scientifique mais qui pourtant dirige ou encadre le travail de recherche. Ces points sont autant de points d'ancrage possible pour les points de vue des financeurs, plus ou moins consciemment, plus ou moins fortement. Le chercheur en STS précise la nature de ces points d'ancrage. L'essentiel des travaux portent sur la sociologie des sciences, la recherche scientifique et l'innovation. Les politiques de science et technologie, politiques de recherche et les politiques d'innovation font également partie de ce domaine. Parmi les représentants des STS francophone, on peut nommer Jean-Jacques Salomon (Conservatoire national des arts et métiers), Bruno Latour (chaire Gabriel Tarde, Sciences Po), Michel Callon (École des mines de Paris), Dominique Pestre (EHESS), Dominique Vinck (Université de Lausanne), Benoît Godin (INRS) ou encore Camille Limoges (UQAM).
Sur le modèle du serment d'Hippocrate, un groupe d'étudiants de l'École polytechnique fédérale de Lausanne a élaboré en 1990 un serment d'Archimède exprimant les responsabilités et les devoirs de l'ingénieur et du technicien. Il a été repris sous diverses versions par d'autres écoles d'ingénieurs européennes. Et, une autre version du serment d'Archimède :
Les Trois lois de la robotique, formulées par l'écrivain de science-fiction Isaac Asimov, sont des règles auxquelles tous les robots positroniques qui apparaissent dans ses romans doivent obéir.
Un ou une scientifique est une personne qui se consacre à l'étude de la science ou des sciences avec rigueur et des méthodes scientifiques. Bien que savant soit le terme pur, formée à partir de la racine savoir, il est plus ou moins tombé en désuétude et remplacé par scientifique ou chercheur. Il arrive que des personnes (ex. : charlatans) s'auto-qualifient de scientifiques, le scepticisme scientifique est une pratique qui remet en doute leurs allégations.
Un chercheur indépendant (en anglais independent scholar, en allemand Privatgelehrter) est une personne qui mène des recherches savantes en dehors des universités et des structures de recherches reconnues. Ces recherches peuvent être bénévoles ou financées par des groupements privés (entreprises, mécénats). Le chercheur indépendant peut être autodidacte ou, plus fréquemment, posséder une formation universitaire ou avoir déjà publié des travaux de recherches remarqués par la communauté officielle. Les résultats de ces travaux de recherche indépendante peuvent être diffusés ou non. Aux États-Unis ainsi qu'à l'Université Simon Fraser canadienne, il existe des associations qui se proposent de regrouper et d'aider les chercheurs indépendant.
Gérardine Mukeshimana, née le 10 décembre 1970 dans le district de Huye (Rwanda), est une scientifique et femme politique rwandaise. Elle est ministre de l'Agriculture et des Ressources animales depuis 2014.
Roger Gauthier, né le 3 avril 1906 à Gardner au Massachusetts et mort le 7 février 1982 à Montréal dans la province de Québec au Canada, est un professeur, botaniste, chercheur et scientifique. Il s’est distingué dans ses nombreuses responsabilités en tant que professeur et sous-directeur de l’Institut botanique, professeur titulaire de l’Université de Montréal, secrétaire et membre de la Faculté des sciences de l’Université de Montréal et premiers héritiers du Frère Marie-Victorin. Il est reconnu principalement comme spécialiste en anatomie végétale, en morphologie comparée et en paléobotanique. Intéressé plus particulièrement par la morphologie florale, il consacre sa vie à la recherche, à l’enseignement et à sa passion pour la botanique. Gauthier est également l’auteur de plusieurs articles de vulgarisation, notamment dans la chronique les Cercles des jeunes naturalistes (C.J.N.) du journal Le Devoir ainsi que dans le journal l’Article universitaire, la revue Sciences et Aventures, la revue Paysana et plusieurs autres.
Les sciences appliquées sont les sciences qui sont orientées vers l'application pratique des connaissances. Les sciences appliquées se distinguent des sciences fondamentales par leur objectif d'application concrète.
Une technique (du grec τέχνη ou technè) est une ou un ensemble de méthode(s) ; dans les métiers manuels (menuiserie, forgerie ...) , elle est souvent associée à un savoir-faire professionnel. La technique couvre l'ensemble des procédés de fabrication, de maintenance et de gestion, qui utilisent des méthodes issues de connaissances scientifiques ou simplement des méthodes dictées par la pratique de certains métiers. On peut alors parler d'art, dans son sens premier, et de science appliquée. La technique est l'une des grandes composantes du savoir-faire artisanal et industriel. Elle est le produit de l'ensemble de l'histoire de l'humanité, chaque peuple et chaque époque ayant apporté ses compétences.
L'aérospatiale (nom commun féminin singulier) est une discipline scientifique qui rassemble les techniques de l'aéronautique (déplacement dans l'atmosphère, utilisant des avions ou des hélicoptères par exemple) et spatiales (déplacements spatiaux, c'est-à-dire trajets hors atmosphère et interplanétaires, en utilisant des navettes spatiales ainsi que des fusées).
Une ailette est une petite aile ; son rôle n'est pas forcement de produire une sustentation ou un appui et donc n'a pas souvent un profil d'aile, plus couramment ce n'est qu'une feuille plate de métal ou d'un matériau approprié à sa fonction.
En moulage, la dépouille ou angle de dépouille est l'inclinaison des parois du moule nécessaire pour faciliter le démoulage de la pièce. On parle de contre-dépouille lorsque la forme de la pièce interdit un démoulage direct. Ces notions interviennent dans le moulage en moule rigide, notamment en plasturgie, staff, céramique technique et métallurgie. La notion d'angle de dépouille est utilisée également en usinage avec une définition analogue.
En sciences, particulièrement en physique, l'application numérique (souvent abrégé en a.n.) est l'obtention de la valeur numérique d'une grandeur physique à partir de celles d'autre grandeurs lorsque l'on connaît une formule analytique reliant ces grandeurs. En d'autres termes, il s'agit d'obtenir                         z         =         f         (         x         ,         y         )                 {\displaystyle z=f(x,y)}    à partir des valeurs numériques de                         x                 {\displaystyle x}    et                         y                 {\displaystyle y}   , et de la fonction                         f                 {\displaystyle f}   . En sciences expérimentales, deux difficultés s'ajoutent :                         x                 {\displaystyle x}    et                         y                 {\displaystyle y}    ont des unités, et il faut s'assurer qu'elles sont compatibles (voir Équation aux dimensions) ; les valeurs sont assorties d'une incertitude de mesure, et il faut évaluer l'incertitude qui en découle sur le résultat (voir Propagation des incertitudes).
Le broyage est une opération consistant à diviser un solide, pour augmenter sa surface spécifique (surface développée de la poudre par unité de masse) et donc sa réactivité. En minéralurgie, le broyage se fait jusqu'à la maille de libération. La maille de libération d'un minéral est la taille au-dessous de laquelle une particule de minéral est parfaitement libérée, c'est-à-dire constituée uniquement de l'espèce minérale à valoriser. En chimie, en pharmacie et en cuisine, le broyage manuel se fait à l'aide d'un mortier et d'un pilon. Les Japonais utilisent également un outil appelé yagen. On parle aussi de comminution ou d'attrition selon l'intensité de l'opération.
Le broyage mécanique consiste à réduire la taille des particules et des grains de différents types de matériaux. Il est très utilisé dans l’industrie pharmaceutique car il permet d’améliorer la biodisponibilité d’une substance. Par exemple dans le cas des composés peu solubles dans l’eau, la production de particules submicroniques peut être un moyen d’améliorer l’efficacité du produit lors d’une administration orale.
Les novices font traditionnellement l'objet de canulars dans les métiers techniques. Les victimes de ces farces ou blagues sont souvent des personnes qui ne connaissent pas encore tous les termes techniques de la spécialité : personnes extérieures à la discipline et débutants (stagiaires, apprentis, arpètes, etc.).
Une champignonnière est un lieu de culture de champignons. Les champignonnières sont des milieux sombres et humides, conditions idéales pour le développement des champignons. Le plus souvent on y cultive l'agaricus, plus connu sous le nom de champignon de Paris ou champignon de couche mais on sait aussi cultiver le pleurote, la truffe, la morille, le pied-bleu, le champignon noir et le shiitaké pour les plus courants. Les progrès en myciculture permettent de cultiver régulièrement de nouvelles espèces mais elles ne sont pas toutes rentables économiquement parlant.
La chronologie de la plongée sous-marine commence dès l'Antiquité et connaît de nombreux perfectionnements décisifs à partir du XVIIIe siècle.
Le cintrage de tubes consiste en une opération de déformation des tubes grâce à des outillages composés de diverses pièces mécaniques.
Une cintreuse permet le cintrage de pièces métalliques afin de leur donner des formes arrondies. Il existe plusieurs types de cintreuses suivant s'il s'agit de tôles ou de profilés.
Le Common European Research Information Format (CERIF) a été développé avec l'appui de la Commission européenne dans les deux grandes phases: 1987-1990 et 1997-1999. Il s'agit d'une norme recommandée par l’Union européenne à ses États membres. Depuis 2002, c’est EuroCRIS, organisation à un but non lucratif dédiée à la promotion des Current Research Information System (Système d'information des recherches en cours] encore appelés CRIS qui assure la maintenance et la gestion de ce standard. La plupart des États membres de l'Union européenne soutiennent publiquement des programmes de recherche. Il est connu de tous que la coordination de la recherche et du développement conduit à la création de richesses et au progrès social. Dans le même temps, le financement public de la recherche et de l’innovation est un enjeu clé de cette coordination. Dans cette articulation, il est important pour les bailleurs de fonds de recueillir des éléments d’informations qui leur permettent de renforcer leurs décisions de financement et d’orienter au besoin leur stratégie de recherche, d’où une gouvernance appropriée grâce à l’information mise à la disposition du public. De façon globale, les États membres ont le même processus de pilotage et coordination de recherche repris dans le modèle CERIF : la planification stratégique, la publication du programme, appel à propositions; évaluation des propositions et l'attribution des projets, le suivi des résultats et d'exploitation des résultats du projet.
La connaissance technique est l’ensemble des moyens cognitifs mis en œuvre pour conduire à son terme un projet technique. Notre époque, résolument technicienne, néglige souvent les techniques non strictement scientifiques qui utilisent des connaissances et savoirs à caractère empirique, heuristique ou expérimental. Pourtant l’histoire nous enseigne que la pratique professionnelle quotidienne, de l'artisan et de l'artiste comme de l'ouvrier ou de l'ingénieur, est souvent source de créativité et d'innovation. Cet article développe donc, dans une perspective historique, les modalités de la connaissance technique : origine et émergence, transmission, principes de formalisation, et cela qu’il s’agisse de techniques élémentaires (ex: « faire de la confiture ») ou plus élaborées (ex: « concevoir l'architecture du châssis d'un véhicule automobile de compétition »).
L’émaillage sur lave est une technique permettant de produire la lave émaillée, utilisée pour la réalisation de plaques de rues, de tables d'orientation, de plans de travail pour les cuisines, de revêtements muraux, de panneaux et ensembles décoratifs intérieurs et extérieurs, mais aussi d'objets très spécifiques comme des éléments de signalétique, les échelles limnimétriques pour les rivières, les voyants géodésiques ou encore les anciens panneaux de signalisation routière et bornes d'angle Michelin. Le métier d'émailleur sur lave est enseigné à l'IMAPEC-Traces de pierre (ex-EDAV, École départementale d'architecture de Volvic), à Volvic dans le Puy-de-Dôme. La formation est une certification professionnelle reconnue par l'État (inscrite au RNCP). C'est un métier aujourd'hui reconnu comme métier d'art, à mi-chemin entre les arts du travail de la pierre et les arts du feu.
Instrument formé de deux pièces ajustées à angle droit, l'équerre est utilisée soit pour vérifier des angles dièdres droits, soit pour tracer des angles plans droits.
Le façonnage de tubes consiste en l’usinage des extrémités des tubes par une ou plusieurs opérations de déformation et/ou d’enlèvement de matière grâce à des outils que l’on appelle poinçons.
La fluidisation est un processus semblable à la liquéfaction par lequel une substance granulaire est soumise à un fluide traitant dont la charge va se déposer sur ledit matériau. Ce processus se produit quand un fluide (liquide ou gaz) est passé vers le haut à travers la substance granulaire.
Le génie biomédical est une application des principes et des techniques de l'ingénierie dans le domaine médical visant au contrôle des systèmes biologiques ou au développement d’appareils servant au diagnostic et au traitement des patients. Ce domaine est un mélange de médecine, de biologie, d'ingénierie et de physique. Étant une discipline relativement récente, la plupart des travaux se situent dans le développement et la recherche couvrant un large secteur d'activité. Il comprend : La bioélectricité Le bioélectromagnétisme La bio-informatique La biomécanique L’étude des biomatériaux L’électrophysiologie L’évaluation des technologies médicales La régulation physiologique L’instrumentation biomédicale (dont l’imagerie médicale) La modélisation biomédicale Le traitement d'images et de signaux biomédicaux La radiothérapie Les exemples d'application sont le développement et la fabrication de prothèses biocompatibles, les dispositifs médicaux ainsi que les équipements d'imagerie et de diagnostic comme l'électroencéphalographe (EEG) et l'imagerie par résonance magnétique (IRM).
L'ingénierie tissulaire ou génie tissulaire (en anglais, tissue engineering) est l'ensemble des techniques faisant appel aux principes et aux méthodes de l'ingénierie, de la culture cellulaire, des sciences de la vie, des sciences des matériaux pour comprendre les relations entre les structures et les fonctions des tissus normaux et pathologiques des mammifères, afin de développer des substituts biologiques pouvant restaurer, maintenir ou améliorer les fonctions des tissus. Elle implique notamment d'identifier et maitriser les facteurs biochimiques et physico-chimiques de la croissance tissulaire maitrisée. Elle est souvent basée sur la construction ou l'utilisation d'un « échafaudage » qui servira de support à la croissance de nouveaux tissus viables, généralement à des fins médicales. Les définitions de l'ingénierie tissulaire couvrent une large gamme d'applications ; ce terme est associé à toutes les applications qui réparent ou remplacent des parties de tissus ou des tissus entiers (ex : os, cartilage, vaisseaux sanguins, vessie, peau, muscle, etc.). Souvent, les tissus à réparer doivent avoir des propriétés mécaniques et structurelles spécifiques (pour le bon fonctionnement de l'organisme). Cette science bien que souvent classé comme sous-domaine des biomatériaux, a une portée et une importance telle qu'elle peut être considérée comme un domaine à part entière. L'expression a aussi été appliqué aux efforts visant à effectuer des fonctions biochimiques spécifiques en (ré)utilisant des cellules naturelles dans un support artificiellement créé (ex : cœur, pancréas, rein ou foie artificiel). Le terme « médecine régénératrice » a été utilisé comme synonyme de l'ingénierie tissulaire, mais la médecine régénératrice évoque plus souvent l'utilisation de cellules souches ou de cellules progénitrices pour produire les tissus en question.
Un ingénieur maritime est un ingénieur génie civil spécialisé dans les aménagements côtiers. Principalement pour les plages, les estuaires et les ports, mais aussi pour les structures en mer (en particulier pour l’exploitation pétrolière « offshore »). L’ingénieur maritime est spécialisé en Génie Maritime (Coastal Engineering en anglais). Ces aménagements concernent une large gamme de structures : digues (ou brise lames) portuaires, quais, chenaux d’accès, dragages, épis de protection des plages et tous autres systèmes de protection contre l’érosion. Ces aménagements visent la mise en valeur des zones côtières urbaines, touristiques, industrielles, mais aussi les zones humides autour des estuaires et des lagunes. L’ingénieur maritime est soucieux des aspects environnementaux des ouvrages qu’il conçoit : les études d’impact sur l’environnement, et plus généralement les options d’aménagement du territoire, font partie de ses responsabilités (Coastal Management en anglais). L’ingénieur maritime est impliqué dans les projets d'ingénierie à partir de la phase de conception, jusqu’à la phase de réalisation, soit en tant que constructeur (dans une entreprise de BTP) soit en tant que maître d’œuvre (supervision des travaux effectuée par un consultant) pour le compte du maître d’ouvrage. Les principaux outils de l’ingénieur maritime sont le calcul de structure, la modélisation numérique et la modélisation physique (modèles réduits). Il s’appuie aussi largement sur les codes de dimensionnement des structures. Les ingénieurs maritimes français sont souvent formés dans les grandes écoles (Mines ParisTech, Centrale, Ponts et Chaussées ParisTech, ENSTA ParisTech, SeaTech, ESITC Caen, ENSIETA, Polytech Lille) et dans d’autres écoles plus spécialisées (Bac + 5). A l’étranger, ils sont formés dans les universités (Pays-Bas, Grande-Bretagne, Allemagne, Danemark, États-Unis, Canada, Japon).  Portail du travail et des métiers  Portail du monde maritime
Le lasertube est une technique de découpe laser, appliquée au travail du tube, qui a révolutionné les processus de production traditionnels ouvrant ainsi de nouveaux espaces pour plus d’efficacité et de valeur ajoutée dans l’usinage des tubes.
Layage: aspect strié d'un parement de pierres de taille laissé après dressage à l'aide d'une laye, marteau de tailleur de pierres à un ou deux tranchants.
La liste de vocabulaire pour les formes d'une pièce permet de décrire avec précision les formes d'une pièce. Alésage Arbre Arrondi Biseau Bossage Boutonnière Chambrage Chanfrein Collet Collerette Congé Décrochement Dégagement Dent Dépouille, contre-dépouille, angle de dépouille : formes facilitant ou non un démoulage Embase Embrèvement Encoche : petite entaille Entaille : partie d'une pièce enlevée par usinage Épaulement Ergot Évidement Extrados Fente Fraisure Gorge Goutte de suif : sur une pièce cylindrique, extrémité arrondi (calotte sphérique) éventuellement raccordée par un arrondi (portion de tore). Lamage Languette Locating (mot anglais) : pièce positionnant une autre pièce. Lumière Macaron : cylindre, dont le diamètre est beaucoup plus grand que la hauteur, qui assure (en général) le centrage. Méplat Mortaise Nervure Profilé Queue d'aronde Rainure Saignée : entaille profonde de faible épaisseur. Saillie : partie qui dépasse d'une surface. Semelle Tenon : saillie destinée à se loger dans une rainure ou une mortaise. Téton : petite saillie de forme cylindrique. Trou oblong (parfois boutonnière)
La lottinoplastie consiste à réaliser des moulages permettant de tirer un grand nombre de reproductions de précision. La technique consiste à appliquer avec soins plusieurs couche de feuilles de papier mouillées imprégnées de gélatine, sur l'original, en s'aidant d'une brosse afin que la papier aille dans la moindre anfractuosité. Une fois sec le moule en papier doit être cuit. Il peut être utilisé plusieurs fois pour réaliser un moulage fidèle en plâtre. Cette technique fut mise au point par Pierre-Victorien Lottin - dit Victor Lottin de Laval - qui naquit le 19 septembre 1810 à Orbec (Calvados). Pierre-Victorien Lottin fut aussi romancier, archéologue et peintre orientaliste (décès en 1903). Elle présente de nombreux avantages : non destruction de l’original, poids réduit et faible encombrement des moulages (il est possible de les découper) et reproduction à plusieurs exemplaires. La lottinoplastie est toujours utilisée en archéologie de nos jours.
La mécanographie regroupe un ensemble de techniques mécaniques ou électro-mécaniques permettant le calcul, le traitement et la publication de l'information. Elle consiste essentiellement à rechercher et à étudier les possibilités offertes par un système ou un moyen mécanique ou électromécanique afin d'exécuter rapidement un algorithme donné et d'en afficher le résultat. Avant l'émergence de l'informatique (terme apparu en 1962), la mécanographie avait pour but la réalisation de travaux de comptabilité, de gestion, de statistiques, en utilisant, comme support d'entrée des données, des cartes perforées, ou plus accessoirement des bandes perforées, voire directement des saisies sur clavier électromécanique. La mécanographie s'est développée de la fin du XIXe siècle jusqu'au milieu des années 1960 sous deux formes très différentes, concurrentes ou complémentaires : l'emploi d'ateliers de machines à cartes perforées ; l'emploi d'ateliers de machines comptables. À partir de 1960 environ, ces deux outils de calcul et de gestion ont été remplacés progressivement par l'emploi d'ordinateurs qui vont devenir essentiels au début des années 1970. La mécanographie fera dès lors partie intégrante de l'informatique en se limitant à la conception et l'étude des sous-ensembles électromécaniques de certains ordinateurs (disquette, disque dur, etc.) mais aussi des imprimantes. Malgré l'utilisation majoritaire des composants électroniques au sein des ordinateurs du début du XXIe siècle, les recherches sur la nano-mécanique pourraient rééquilibrer la place de la mécanographie au sein de certaines machines. Dans la mécanographie utilisant des appareils à cartes perforées, ces cartes servaient essentiellement de support des données en entrée et en sortie ; en fin de traitement elle servaient de mémoires de masses ainsi que de fichiers historiques, mais commencèrent également à servir de support des programmes à assembler durant l'apogée de la mécanographie (années 1960) et surtout à partir des débuts de l'informatique. À partir de 1965, la carte perforée fut progressivement remplacée par divers moyens de saisie et de mémorisation (disques, bandes, disquettes, cassettes, saisie sur terminal, sur micro-ordinateur), jusqu'à disparaître complètement au milieu des années 1980.
Un mélangeur statique est un dispositif de mélange en continu des fluides. Ce dispositif permet de mélanger des liquides mais il peut aussi être utilisé avec des gaz ou pour mélanger un gaz et un liquide. Les dépenses énergétiques permettant de favoriser le mélange provoquent une perte de pression lors du passage du fluide à travers le mélangeur statique. Il existe de nombreuses géométries de mélangeur statique. Le type à plaque est un type de géométrie commune. On rencontre aussi des mélangeurs insérés dans des conduites circulaires ou carré. Les dimensions des mélangeurs sont comprises entre 6 mm et 6 mètres de diamètre. Les mélangeurs statiques sont fabriqués à partir de nombreux types de matériaux en fonction de l'application visée comme l'acier inoxydable, le polypropylène, le Téflon, le PVDF, le PVC, le CPVC et le polyoxyméthylène.
Pour un système, le fonctionnement en mode nominal est le contraire d'un fonctionnement en mode dégradé. Le système n'est pas fautif (aucun capteur ou appareils en panne, aucun imprévu de fonctionnement).  Portail de la sécurité informatique
Un moule est un élément servant à la fabrication d'autres éléments par les techniques de moulage.  
Narguilé est le nom que les scaphandriers donnent au tube qui les relie à la surface et qui leur fournit l'air dont ils ont besoin pour respirer sous l'eau. Le tube est relié au casque du scaphandrier à l'un de ses extrêmes et à une pompe en surface à son autre extrême. Activée manuellement, comme c'était encore le cas au XIXe siècle, ou bien par le biais d'un mécanisme automatique, cette pompe est le dispositif destiné à faire descendre l'air vers le plongeur. Certains modèles de détendeur ont aussi été préparés, surtout dans les années 1950 et 60, pour s'adapter à un narguilé, en évitant ainsi au plongeur de porter les lourdes bouteilles métalliques qui normalement contiennent sa réserve d'air. Le nom de narguilé a été emprunté de celui d'une sorte de pipe orientale.
La navigation est la science et l'ensemble des techniques qui permettent de : connaître la position (ses coordonnées) d'un mobile par rapport à un système de référence, ou par rapport à un point fixe déterminé ; calculer ou mesurer la route à suivre pour rejoindre un autre point de coordonnées connues ; calculer toute autre information relative au déplacement de ce mobile (distances et durées, vitesse de déplacement, heure estimée d'arrivée, etc.).
Nicolas Fourneau est un maître charpentier originaire de Rouen et membre des compagnons du devoir. Il est né en 1726 et il est mort en 1792. Il est reconnu pour ses travaux sur l'art du trait de charpenterie, il a également été professeur à l'école nationale des ponts et chaussées.
Le noir de fumée est un résidu carboné obtenu par la combustion incomplète de diverses matières organiques riches en carbone. Il peut être utilisé comme pigment pour des peintures, de l’encre ou du cirage. Il est parfois appelé « noir de lampe ». Un ouvrage de chimie de 1906 en donne la définition suivante : « On appelle « noir de fumée » les particules de charbon que les flammes tiennent en suspension et auxquelles elles doivent leur pouvoir éclairant. Ce charbon se dépose en poudre très fine, sur les verres de lampe, les fumivores, etc., qu’il noircit ; il forme la suie des cheminées. On l’obtient toutes les fois que les substances combustibles riches en carbone, telles que les huiles, les résines, les essences brûlent incomplètement. Écrasons avec une soucoupe ou un papier fort la flamme d’une bougie, il se forme un dépôt abondant de noir de fumée. »
Un module est une unité de mesure conventionnelle adoptée pour régler les diverses parties d'un ensemble (construction, machine…). Il correspond à la plus petite commune mesure que doivent posséder les dimensions des éléments entrant dans la composition de cet ensemble pour qu’ils puissent se superposer, se combiner ou se juxtaposer sans retouches. En grec, le module est désigné par τόνος, le ton ; l’origine latine modulus, de modus indique la cadence, la mesure. Le terme peut également être employé dans le sens d'étalon, de gabarit ou encore de calibre. Enfin, par extension il désigne aussi un élément, une unité constitutive d'un ensemble. S’il est difficile de connaître précisément l’origine de cette notion qui n’est pas simple à concevoir, sa constance à travers les époques, sous des formes variables, et surtout l'utilisation remarquable qui en a été faite, invitent à l’étudier.
Les onggi sont des terres cuites d'origine coréenne, largement utilisées comme vaisselle ainsi que récipients de conservation alimentaire en Corée. Elles sont constituées à la fois de céramiques non émaillées cuites à près de 600 ~ 700 °C et de grès couvertes d'une glaçure brun foncé, cuites à plus de 1 100 °C. L'origine des onggi remonte approximativement à environ 4000 à 5000 av. J.-C.. Il y a deux types de terre cuite : une sans motif appelée céramique Mumun et une en noir et rouge. La première, terre cuite sans motif, est faite avec de morceaux d'argile comprenant beaucoup de sable fin; cependant, le prédécesseur du céladon de Goryeo et de la porcelaine blanche de la période Joseon, terre cuite en noir et rouge, est réalisé uniquement avec des morceaux d'argile. La couleur de la terre cuite est déterminée par le fer contenu dans la boue et la façon de cuire la poterie. La forme actuelle des onggi date de la période Joseon. Il existe de nombreuses mentions des onggi dans le Sejong Sillok Jiriji (세종실록지리지, « Traité de géographie du roi Sejong ») : « Il y a trois fours qui produisent du onggi jaune à Chogye-gun et Jinju-mok, dans la province de Gyeongsang » (Lee and Jeong, 16).
Un pancréas artificiel est une technologie pour aider les personnes atteintes de diabète à contrôler automatiquement leur taux de glycémie en délivrant une fonctionnalité endocrine de substitution comparable à celle d’un pancréas sain. Le pancréas présente deux fonctions importantes : une fonction exocrine qui sécrète des enzymes digestives et une fonction endocrine ou hormonale. C'est le manque de production d'insuline qui est la principale motivation pour développer un substitut à un pancréas défaillant. Les thérapies actuelles consistent à gérer manuellement le taux de glycémie avec l'insuline. Ce contrôle est difficile, pénible et insuffisant. L'objectif du pancréas artificiel est d'améliorer la thérapie de remplacement de l'insuline jusqu'à ce que le contrôle glycémique soit pratiquement normal en évitant les complications de l'hyperglycémie, mais également de faciliter le traitement de la dépendance à l'insuline. Les différentes approches envisagées comprennent : des équipements médicaux utilisant une pompe à insuline sous contrôle en boucle fermée en utilisant des données en temps réel à partir d'un capteur de glycémie continu dans le sang la bio-ingénierie - le développement d'un pancréas bio-artificiel constitué d'une feuille biocompatible de cellules bêta encapsulées. Implanté chirurgicalement, la feuille d'îlot de Langerhans se comporte comme un pancréas endocrinien et est viable pendant des années la thérapie génique - l'infection thérapeutique d'une personne diabétique par un virus génétiquement modifié pour provoquer un changement d'ADN des cellules intestinales afin qu’elles deviennent des cellules productrices d'insuline
La peinture est un revêtement appliqué en une ou plusieurs couches (couches minces de quelques dizaines de micromètres d’épaisseur, µm), sur différents matériaux dits subjectiles. Le résultat n’est obtenu qu’après application et séchage. En séchant, ou préfabriquée industriellement, la peinture forme un film solide, adhérant et durable. Le « film » de peinture est également dénommé « feuil ». Si le film est transparent ou translucide, c’est un vernis ; s’il est opaque, c’est une peinture.
Une pièce en fil métallique est une pièce formée par un procédé de pliage. On trouve ce type d’élément dans des systèmes mécaniques qui demandent une fonction de fixation, de ressort… Les matières qui constituent ces pièces cintrées sont des bobines d’acier, acier inoxydable, acier pré-galvanisé, acier ressort, cuivre, laiton et sont réalisées dans un fil tréfilé (calibré) à un diamètre précis. Les articles en fil métallique font partie intégrante de notre quotidien par exemple les chariots de supermarché, des crochets, les cintres, poignées, tringles, clips ou diverses goupilles…
Le terme poterie désigne des vases et récipients à usage essentiellement domestique ou culinaire réalisés en terre cuite poreuse qui peuvent demeurer bruts ou recevoir un revêtement glaçuré. Par métonymie, le terme poterie désigne également la technique de production et l'atelier du potier. Il est employé souvent à tort comme synonyme du terme plus large de céramique, qui inclut toutes les formes de terre cuite: objets architecturaux (tuiles, carreaux, etc.), lampes, figurines ainsi que des objets divers (pipes, tuyaux...). Bien que la faïence soit techniquement une poterie, avec sa terre poreuse rendue imperméable par un émail blanc à base d'étain, le public préfère réserver l'appellation poterie aux pièces de terre cuite brutes ou vernissées, à la facture populaire ou artisanale. Les grès et porcelaines, dont la pâte vitrifie à haute température, ne sont pas appelés poteries.
Le poteyage est l’action qui consiste à enduire une pièce d’un liquide protecteur qui facilitera son démoulage.
Le progrès technique représente l’amélioration des techniques, y compris organisationnelles, qui sont utilisées dans le processus de production des biens et des services. Le développement des « nouvelles technologies » est tel que l'on parle de révolution technique. Toutefois, le fait que le progrès technique tel qu'il a été mis en œuvre au cours de la révolution industrielle engendre des effets sur l'environnement et sur la société en général, et que d'autre part le modèle de développement qu'il induit ne peut être généralisé à l'ensemble des pays de la planète, font que, dans une optique de développement durable, il s'agit aujourd'hui d'un concept controversé, voire critiqué, de sorte que la notion même de progrès devrait être redéfinie.
La prospective est la démarche qui vise, par une approche rationnelle et holistique, à se préparer aujourd'hui pour demain. Elle ne consiste pas à prévoir l'avenir (ce qui relevait de la divination et relève aujourd'hui de la futurologie) mais à élaborer des scénarios possibles et impossibles dans leurs perceptions du moment sur la base de l'analyse des données disponibles (états des lieux, tendances lourdes, phénomènes d'émergences) et de la compréhension et prise en compte des processus sociopsychologiques, car comme le rappelle Michel Godet : « si l'histoire ne se répète pas, les comportements humains se reproduisent », la prospective doit donc aussi s'appuyer sur des intuitions liées aux signaux faibles, des analyses rétrospectives et la mémoire du passé humain et écologique (y compris et par exemple concernant les impacts environnementaux et humains des modifications géo-climatiques passées). Le prospectiviste se distingue ainsi du prolongateur de tendances comme du visionnaire qui élabore des scénarios à partir de révélations. Sa fonction première est de synthétiser les risques et d'offrir des visions (scénarios) temporelles en tant qu'aide à la décision stratégique, qui engage un individu ou un groupe et affecte des ressources (naturelles ou non) plus ou moins renouvelables ou coûteuses sur une longue durée. Elle acquiert ainsi après avoir pris les risques nécessaires à une double fonction de réduction des incertitudes (et donc éventuellement de certaines angoisses) face à l'avenir, et de priorisation ou légitimation des actions. La prospective est une démarche continue, car pour être efficace, elle doit être itérative et se fonder sur des successions d'ajustements et de corrections (en boucles rétroactives) dans le temps, notamment parce que la prise en compte de la prospective par les décideurs et différents acteurs de la société modifie elle-même sans cesse le futur (la prospective ne modifie pas le futur, elle se base sur le passé et le présent pour entrevoir le futur ; la prospective se nourrit d'elle-même et n'a aucune accroche de coïncidence avec des scénarios préétablis des acteurs politiques, elle n'est la propriété de personne, par contre la collecte, l'analyse et l'interprétation des données la font naître) qui est tout sauf prévisible. Elle s'appuie sur des horizons ou dates-butoir (ex 2010, 2020, 2030, 2040, 2050, 2100) qui sont aussi parfois des échéances légales, et qui permettent à différents acteurs de faire coïncider leurs scénarios ou calculs de tendance.
La psychanalyse appliquée (angewandte Psychoanalyse) désigne l'application de la théorie et de la clinique psychanalytiques à d'autres domaines de connaissance et des « sciences de l'esprit » (Geisteswissenschaften (en)) : art et littérature, mais aussi domaines scientifiques, éducation, ethnologie et anthropologie, notamment.
La qualification de matériel est un test effectué lors de l'installation d'un nouvel équipement industriel. Ce test est destiné à démontrer l’aptitude de l'équipement à satisfaire les exigences spécifiées dans un cahier des charges. Il existe trois types de qualification : élément 1 de conception ; élément 2 d'installation ; élément 3 opérationnelle. Par exemple, en centrale nucléaire, un matériel dit IPS (important pour la sûreté) installé(modifié) doit être (re)qualifié.
Une rainure est une entaille longue (et souvent débouchante) sur une pièce pour recevoir une languette, un lardon… L'action de réalisation d'une rainure se dit indifféremment rainer ou rainurer.
La recherche appliquée consiste en des travaux originaux entrepris en vue d'acquérir des connaissances nouvelles. Cependant, elle est surtout dirigée vers un but ou un objectif pratique déterminé, à la différence de la recherche fondamentale.
La règle de la main droite est un moyen de se rappeler comment sont liées diverses directions. Elle utilise les doigts de la main. Il y a deux règles (les plus connues) : celle qui imite un tire-bouchon avec une rotation et une translation et celle qui indique un repère direct. En électromagnétisme, ces règles permettent de déterminer le sens et la direction des forces de Laplace (machines électriques...), de Lorentz (particules chargées déviées), de la force électromotrice induite par la loi de Lenz-Faraday , de donner la forme et la direction des lignes de champ magnétique produites par un courant électrique, entre autres.
La règle de la main gauche est un moyen de se rappeler comment sont liées diverses directions. Elle utilise les doigts de la main. Il y a deux règles (les plus connues) : celle qui indique une rotation et une translation, et, celle qui indique trois translations.
Réponse linéaire se dit généralement d'un appareil qui répond à une excitation x sous la forme : réponse = a.x + b Nota : lorsque b est nul, la réponse est dite proportionnelle.
Une salle blanche (ou plus exactement salle propre selon la norme ISO 14644-1) est une pièce ou une série de pièces où la concentration particulaire est maîtrisée afin de minimiser l'introduction, la génération, la rétention de particules à l'intérieur, généralement dans un but spécifique industriel ou de recherche scientifique. Les paramètres tels que la température, l'humidité et la pression relative sont également maintenus à un niveau précis (définition selon la norme ISO 14644-1).
Un saut technologique est, dans un domaine technique, une innovation majeure dans la conception d'un produit. Il correspond à une révolution de l'état de l'art dans ce domaine.
Le scaphandre à casque, aussi appelé scaphandre pieds lourds, est un dispositif qui permet à un plongeur de déambuler sur le fond d'une masse d'eau (la mer, un lac, une rivière, une carrière immergée, un bassin, etc.) en respirant grâce à un tube relié à la surface, où d'autres hommes lui fournissent l'air nécessaire à sa survie grâce à un mécanisme de pompage. Toutefois quelques modèles de scaphandres à casque ont été autonomes et n'ont donc pas été alimentés en air de surface, comme, entre autres, les scaphandres Rouquayrol-Denayrouze (détendeurs alimentés par une réserve d'air comprimé et fabriqués en France à partir de 1864), ou les scaphandres Dräger (recycleurs alimentés en oxygène et fabriqués en Allemagne à partir de 1912).
Le scaphandre autonome est un dispositif individuel qui permet à un plongeur d'évoluer librement en plongée avec une réserve de gaz respirable comprimé. Un scaphandre autonome peut ainsi aussi bien fonctionner avec de l'air qu'avec d'autres mélanges respirables spécialement étudiés à cette fin (nitrox, trimix, hydreliox...) ou aussi avec un recycleur.
Les scaphandres rigides, aussi appelés scaphandres atmosphériques, sont une catégorie de scaphandres à casque. Ces derniers sont généralement souples et font subir au plongeur qui les utilise toute la pression environnante de la profondeur à laquelle il se trouve. Les scaphandres rigides, par contre, sont constitués de parties rigides reliées entre elles par un ensemble de joints étanches, à la manière d'une armure. Ces scaphandres sont destinés à explorer les grandes profondeurs marines et sont conçus pour résister à de gigantesques pressions, afin de protéger le scaphandrier qui les utilise. Un système intégré dans le scaphandre maintient artificiellement l'air ou le mélange de gaz intérieurs a une pression interne semblable à la pression atmosphérique de la surface.
Les sciences appliquées sont les sciences visant en premier lieu à la réalisation d'un objectif pratique, notamment par l'application des enseignements tirés des sciences fondamentales, qui elles visent en premier lieu à élucider certaines questions concernant le monde et à progresser dans sa connaissance.
Le terme de sciences du numérique désigne les sciences de l'information et de la communication sur leurs volets matériels et logiciels. Cette terminologie inclut les sciences informatiques (computer science) et les mathématiques appliquées liées à ces sujets et représente une nouvelle façon de parler des sciences de l'information et de la communication, à l'aube du XXIe siècle. Elle se nourrit de disciplines telles que l'automatique, le traitement du signal, ou la robotique. Cette dénomination ne représente donc pas un domaine scientifique, mais du travail scientifique à l'intersection de plusieurs domaines. Elle est utilisée en France dans la désignation d'un enseignement de spécialité dans le secondaire, et pour désigner globalement les sciences en amont des technologies numériques.
Les sciences numériques (traduction de l'anglais computational sciences) a pour objet la construction de modèles mathématiques et de méthodes d'analyse quantitative, en se basant sur l'utilisation des sciences du numérique, pour analyser et résoudre des problèmes scientifiques. Cette approche scientifique basée sur un recours massif aux modélisations informatiques et mathématiques et à la simulation se décline en : médecine numérique, biologie numérique, archéologie numérique, mécanique numérique, par exemple. Les modélisations numériques permettent aux sciences d'évoluer en introduisant des outils de simulation, c'est-à-dire d'expérimentation numérique. Ces outils d'investigation diffèrent de développements théoriques ou d'expériences de laboratoire qui sont les formes traditionnelles d'investigation de la science et de l'ingénierie. L'approche à partir de d'analyse de modèles mathématiques mis en œuvre à travers des simulations numériques permet de gagner en compréhension,. Des algorithmes modélisent les mécanismes étudiés et permettent de les représenter, et/ou de visualiser leurs développements futurs calculés. Ces techniques font l'objet de formations spécifiques. Ces techniques requièrent, pour garantir la reproductibilité des expériences numérique, d'archiver et documenter les données et le code utilisé pour obtenir les résultats,,.
La simulation est un outil utilisé par le chercheur, l'ingénieur, le militaire, etc. pour étudier les résultats d'une action sur un élément sans réaliser l'expérience sur l'élément réel. Lorsque l'outil de simulation utilise un ordinateur on parle de simulation numérique. Il a également existé des simulateurs analogiques et il a été envisagé dans les années 1970 d'en construire des stochastiques. Les chercheurs, les ingénieurs, les militaires et bien d'autres professionnels se posent souvent la question : quel est le résultat que j'obtiens si j'exerce telle action sur un élément ? Le moyen le plus simple serait de tenter l'expérience, c'est-à-dire d'exercer l'action souhaitée sur l'élément en cause pour pouvoir observer ou mesurer le résultat. Dans de nombreux cas l'expérience est irréalisable, trop chère ou contraire à l'éthique. On a alors recours à la simulation : rechercher un élément qui réagit d'une manière semblable à celui que l'on veut étudier et qui permettra de déduire les résultats.
STEM (acronyme de science, technology, engineering, and mathematics) ou STIM (science, technologie, ingénierie et mathématiques) en français canadien, est un américanisme désignant quatre disciplines : science, technologie, ingénierie et mathématiques. En 2011, selon l’United States National Research Council et le National Science Foundation, ces disciplines sont centrales aux sociétés technologiquement avancées. Dans plusieurs forums[Quoi ?] américains, qu'ils soient de nature politique, gouvernementale ou académique, l'expertise des travailleurs dans ces disciplines est un indice de la capacité d'un pays à soutenir son existence et sa croissance. MINT (mathematics, information sciences, natural sciences, and technology) est aussi utilisé dans le même principe.
Un swirler est un type de mélangeur statique qui génère un écoulement tournant. Il peut être installé dans des écoulements mono ou multi-phases pour des gaz, des liquides ou des particules solides.
Le système technique est un grand ensemble de cohérences qui se tissent à une époque donnée entre différentes techniques et qui constituent un stade plus ou moins durable de l’évolution des techniques. Ainsi chaque époque serait caractérisée par une synergie entre quelques techniques fondamentales créant ainsi une économie spécifique, avec un ensemble de techniques affluentes qui sont complémentaires et cohérentes les unes avec les autres. François Caron définit le système technique ainsi : il s'agit d'une notion qui « a pour ambition d'exprimer l'interdépendance étroite qui relie entre elles les différentes composantes de la technologie à un moment donné de l'histoire ». Les limites structurelles se font sentir à la fin de la période d'expansion du système : ce moment se caractérise soit par la difficulté d'accroître les quantités, soit par la difficulté de baisser les coûts de production, soit encore par l'impossibilité de diversifier les productions. Ce concept est développé par Bertrand Gille dans Histoire. des techniques. Il fournit des repères pour l’étude de l’histoire des techniques en évitant les interprétations simplistes du type : « Avec l'Éolipyle d'Héron d'Alexandrie, les Grecs sont passés à côté de la machine à vapeur » : ils ne disposaient pas des techniques affluentes nécessaires comme pour la production de la tôle ou encore la connaissance du vide et de la condensation. « Les Chinois étaient très en avance avec la boussole » : quelques techniques, fussent-elles de pointe, n’ont qu’une faible valeur si elles ne s’insèrent pas dans un ensemble de cohérences.
Une taraïette (tarraieto, en provençal) est une petite poterie représentant une pièce de dînette ou une petite cruche à eau dans laquelle les enfants soufflent pour imiter le chant du rossignol. Fabriquées initialement à Apt, Saint-Quentin-la-Poterie, Vallauris et Aubagne, seuls ces deux derniers centres, avec Dieulefit, continuent cette tradition. La popularité de ces petits objets en argile vernissée doit tout à la foire de la Saint-Jean à Marseille, dite foire à l'ail et aux taraïettes, qui se déroule de la mi-juin à la mi-juillet. C'est là où, pour la première fois, furent vendues des taraïettes mêlées aux herbes de la Saint-Jean.
La technique acoustique picoseconde permet la génération et la détection d'ondes sonores de hautes fréquences par des impulsions lumineuses ultracourtes. C’est une méthode non destructive et sans contacts, dans laquelle une impulsion acoustique picoseconde pénètre dans des films minces ou des nanostructures. Cette technique est parfois appelée technique optique picoseconde.
Techniques & Culture, revue semestrielle d’anthropologie, s’intéresse en particulier à l’ethnologie des techniques, aux techniques comme productions sociales à part entière au cœur des rapports entre les sociétés et leur environnement. La revue publie des numéros thématiques. Ses analyses émanent de disciplines diverses mais relèvent toujours de l'anthropologie au sens large, sans toutefois faire l'impasse sur une technologie plus ou moins détaillée. Car toutes les cultures, même très « exotiques », admettent de fait dans leurs conduites les plus banales quelques postulats d'efficience pratique... Y compris quand elles ne tracent pas dans le discours nos bornes à nous : « Faits techniques, et autres »; « Le travail, et le reste ». Mais, conjointement à nos propres classements, ne doit-on pas ranger les leurs parmi les données d'un vrai problème ? Les articles sont en ligne en « texte intégral » sur le site de la revue à partir du numéro 38 inclus. Les livraisons antérieures apparaissent sous la forme de sommaires. La revue est accessible en texte intégral sur OpenEdition Journals, avec un délai de restriction de deux ans. Cependant, les sommaires et résumés (anglais, français, espagnol), de ces numéros, ainsi que la présentation ou le premier article sont immédiatement consultables sur le site. Elle y est propulsée par le CMS libre Lodel. Elle applique les règles proposées par le Lexique des règles typographiques en usage à l'Imprimerie nationale.
Le terme technologies convergentes (Converging Technologies en anglais) est utilisé par différents groupes de pensées pour définir des choses différentes. De nombreuses opportunités de développement d'innovations ont été apportées dans divers domaines grâce aux progrès technologiques qui ont permis non seulement d'augmenter considérablement les performances techniques des systèmes électroniques, mais surtout d'intégrer dans un système technique dit "convergent" plusieurs éléments précédemment développés séparément. L'exemple le plus connu et le plus répandu est celui des offres multiples (multiple-play en anglais) des opérateurs de réseaux qui consistent à intégrer en un produit commercial unique délivré par une technologie convergente (une ligne téléphonique avec l'ADSL ou un téléphone mobile de type smartphone) un ensemble de services précédemment délivrés par des offres commerciales différentes et par des systèmes techniques différents et faisant l'objet de droits et d'obligations réglementaires différents. La fertilisation croisée des technogies convergentes est source d'innovations. C'est en intégrant des fonctions complémentaires issues de technologies différentes et autrefois séparées que de nouveaux types de services sont développés. Par exemple, les services de géolocalisation sont issus de la convergence de plusieurs technologies: la fourniture de coordonnées géographiques précises par des satellites, la connexion à des applications informatiques gérant différents types d'informations relatives à une zone donnée, et l'utilisation d'un smartphone connecté à un opérateur de réseau.  
La validation des simulations est un processus visant à s’assurer qu’une simulation et les modèles qu'elle met en œuvre représentent le monde réel (au sens où ce mot est employé en simulation) d’une façon suffisamment précise pour remplir les besoins d’une utilisation donnée. Il est très important de noter que la validation n’est valable que pour un domaine d’emploi donné et doit être remise en question pour toute nouvelle utilisation sortant de ce domaine. La validation peut également être appliquée aux données utilisées par les simulations. La validation est une partie d'un processus plus large, dit de vérification, validation et acceptation des données, modèles et simulations (VV&A). Ce processus est fondamental en simulation, car il détermine non seulement la qualité du produit final, mais également la crédibilité des résultats obtenus. La formatisation de ce processus fait l'objet de travaux de standardisation par le SISO et l'OTAN[réf. nécessaire].
Un épiphénomène désigne ce qui se surajoute à un phénomène sans exercer sur lui aucune influence. Autrement dit, c'est une manifestation pure, un aspect ou une apparence particulière d'un phénomène sous-jacent, et non une manifestation possédant une réalité indépendante capable d'exercer une rétroaction sur le phénomène qui lui a donné naissance. On qualifie d'épiphénomène un phénomène dont on suppose ne percevoir qu'une petite partie de ce qui est à l'œuvre réellement. Un épiphénomène est donc la manifestation de mécanismes dont on ne connaît qu'une partie ou que l'on ne connaît pas encore. D'où le fait que l'épiphénomène soit usuellement considéré comme mineur et sans importance. Cette notion d'épiphénomène est fondamentale en science et dans toute constitution du savoir : l'observation du monde donne une quantité très importante d'épiphénomènes qui sont autant de sujets d'investigations pour les chercheurs (au sens large du terme). L'activité de recherche vise à produire des explications cohérentes et valides des épiphénomènes. Sur ce plan, il faut faire la distinction entre deux modes de fonctionnement pour la production des explications : Le mode dit scientifique (ou plus généralement réaliste) qui produit du savoir (scienza : origine commune des mots science et savoir). Ce mode explicatif des phénomènes et épiphénomènes propose des hypothèses pour les mécanismes et mène des expériences et des observations afin de vérifier la pertinence des hypothèses explicatives proposées. Il s'agit de vérifier si les explications des épiphénomènes sont valides ou si elles sont fausses, et dans ce cas de corriger les hypothèses pour s'approcher de la validité. Cette conscience scientifique est souvent désignée par le terme probité. Les explications proposées sont souvent désignées par le terme théorie (par exemple : théorie de la gravitation, théorie de la cognition, etc. ). Le mode dit idéaliste qui produit des explications sans se soucier de proposer des vérifications. Les explications proposées sont souvent désignées par le terme théorie (par exemple : théorie de la résurrection). Exemples : le nombre maximum d'usagers simultanés d'un système informatique est un épiphénomène de ce système (qui dépend de la taille mémoire disponible, de la rapidité du CPU et de la liaison informatique etc.). la chute des corps, la déviation de la lumière par la masse sont des épiphénomènes de la gravitation. Le temps est probablement un épiphénomène des lois de la physique. Mauvais exemples : les émotions ont une influence directe sur l'organisme, elles ne sont donc pas des épiphénomènes du fonctionnement de l'organisme. Les rêves, plus particulièrement les cauchemars, peuvent écourter le sommeil, ils ne sont donc pas un épiphénomène du sommeil. Déformation : ce mot à la mode est souvent utilisé, à tort, comme synonyme de "phénomène sans importance / accessoire" ou même de "mode passagère". Le sens est proche de "phénomène secondaire", d'où le glissement sémantique, mais un phénomène secondaire n'est pas nécessairement sans importance ou passager, comme le montrent certains des exemples ci-dessus.
L'esprit critique (du grec κριτικός : qui discerne) est la disposition d'une personne à examiner attentivement une donnée avant d'en établir la validité.
Les méthodes expérimentales scientifiques consistent à tester la validité d'une hypothèse, en reproduisant un phénomène (souvent en laboratoire) et en faisant varier un paramètre. Le paramètre que l'on fait varier est impliqué dans l'hypothèse. Le résultat de l'expérience valide ou non l'hypothèse. La démarche expérimentale est appliquée dans les recherches en biologie, physique, chimie, psychologie, ou encore l'archéologie. Certains soutiennent que le savant Ibn Al Haytham (Alhazen),,, a été l'un des premiers à faire la promotion des méthodes expérimentales. Définies par le chimiste Michel-Eugène Chevreul en 1856, elles ont été développées par Claude Bernard en médecine et en biologie. Outil privilégié des sciences de la nature, les méthodes expérimentales sont également utilisées en sciences humaines et sociales.
La reproductibilité d'une expérience scientifique est une des conditions qui permettent d'inclure les observations réalisées durant cette expérience dans le processus d'amélioration perpétuelle des connaissances scientifiques. Cette condition part du principe qu'on ne peut tirer de conclusions que d'un événement bien décrit, qui est apparu plusieurs fois, provoqué par des personnes différentes. Cette condition permet de s'affranchir d'effets aléatoires venant fausser les résultats ainsi que des erreurs de jugement ou des manipulations de la part des scientifiques. Le critère de reproductibilité est une des conditions sur lesquelles le philosophe Karl Popper distingue le caractère scientifique d'une étude. Pour toutes les sciences expérimentales, les probabilités fournissent un modèle mathématique expliquant la variabilité des résultats.
La sensibilité aux conditions initiales est un phénomène découvert dès la fin du XIXe siècle par Poincaré dans des travaux concernant le problème à N corps en mécanique céleste, puis par Hadamard avec un modèle mathématique abstrait aujourd'hui baptisé « flot géodésique sur une surface à courbure négative ». Cette découverte a entrainé un grand nombre de travaux importants, principalement dans le domaine des mathématiques. Il a été redécouvert en 1963 par Lorenz lors de ses travaux en météorologie. Cette sensibilité explique le fait que, pour un système chaotique, une modification infime des conditions initiales peut entrainer des résultats imprévisibles sur le long terme. Ce résultat est souvent vulgarisé sous le nom « d'effet papillon ». La sensibilité aux conditions initiales se traduit mathématiquement par l'hyperbolicité d'une partie de l'espace des phases du système, hyperbolicité à laquelle est associée un ensemble d'exposants de Lyapounov positifs, ainsi qu'une entropie topologique également positive.
Dans une expérience scientifique, un témoin est un dispositif permettant d'isoler un facteur et de conclure sur l'action de ce facteur sur un phénomène physique ou biologique. Le témoin est nécessaire pour vérifier la probité de toute expérience scientifique. Dans une expérience, deux dispositifs sont mis en route. Dans l'un des deux, le facteur est présent, et dans l'autre non. Tous les autres paramètres sont en tout point identiques pour chaque manipulation. Ce facteur peut par exemple être une condition physique de l'expérience (pression, température, altitude...) ou la présence d'un élément chimique ou vivant supplémentaire. Par exemple, pour tester les effets d'un médicament, il est important de vérifier minutieusement que les effets sont dus uniquement à la substance du médicament. Pour cela les médecins effectuent une étude en double aveugle en milieu clinique : deux groupes de patients statistiquement identiques sont comparés, sachant qu'un des groupes a reçu le médicament et l'autre a reçu un placebo. Ni les patients ni les médecins ne savent qui a reçu la substance supposée active. Chaque groupe fournit des données, dont les différences vont permettre d'isoler les effets de l'unique molécule active du médicament.
Cet article présente les différentes utilisations des lettres de l'alphabet grec dans les sciences. Il faut noter que, en physique, les variables ou paramètres analogues sont notés en italique : la constante mathématique « pi » est ainsi notée π (symbole pi pas en italique) alors que la parallaxe sera notée π (symbole pi en italique). Note : l'ensemble de l'article se base sur les ouvrages référencés dans la section bibliographie, en particulier les deux ouvrages Formulaire technique et Tables numériques et formulaires.
Une atmosphère inerte est une atmosphère qui ne réagit pas avec le processus considéré. Elle est de nature différente selon les différents domaines, mais il s'agit le plus souvent d'air débarrassé de son oxygène (c'est-à-dire principalement de l'azote), l'oxygène étant l'espèce la plus réactive (ici, oxydante). En chimie, l'azote réagit peu avec la plupart des produits. Il peut également être déshydraté lorsque la réaction doit se dérouler en l'absence d'eau. Dans ce dernier cas, ou lorsque les réactifs sont susceptibles de réagir malgré tout avec l'azote, l'argon, plus inerte et plus facile à sécher, est utilisé, bien que plus cher. Dans le domaine électrique, les opérations à préserver de tout effet de l'atmosphère se déroulent dans du SF6 (hexafluorure de soufre), un gaz plus lourd que l'air qui a l'avantage d'avoir une tension de claquage supérieure à l'air, limitant les risques de décharges. Dans l'alimentaire, on parle plus souvent atmosphère protectrice, à la composition réglementée.
Le terme phénoménologie appliqué à la science est utilisé pour décrire un corps de connaissance reliant de nombreuses observations empiriques entre elles, de façon cohérente avec la théorie fondamentale, mais n'en étant pas issu. Une théorie phénoménologique exprime mathématiquement le résultat de l'observation d'un phénomène sans s'attarder à sa signification fondamentale. Le mot dérive de « phénomène » ( du grec φαινόμενoν, pl. φαινόμενα - phenomena, traduit par "chose manifeste, évidente, qui apparaît", et -λογία - -logia, traduit par « étude de » ou « recherche »), et s'applique à tout événement observable. Ainsi, des expressions algébriques peuvent être utilisées afin de modéliser des observations ou des résultats expérimentaux de différentes échelles de longueurs, masses, ou temps, et pour effectuer des prédictions sur les résultats d'autres observations ou expériences, bien que ces expressions ne soient pas issues d'approximations d'une théorie proposée pour ce domaine de connaissance. Une autre façon d'appréhender la phénoménologie en science est de considérer qu'elle constitue l'intermédiaire entre l'expérience et la théorie. Elle est plus logique et comprend plus d'étapes logiques que l'expérience, mais est plus directement liée à cette dernière qu'à la théorie. Les limites entre théorie et phénoménologie, et entre phénoménologie et expérience, sont parfois floues et dépendent en quelque sorte des préconceptions du scientifique les décrivant et du champ particulier de travail. La plupart des scientifiques diraient qu'une modélisation phénoménologique d'un phénomène ne constitue pas une compréhension du phénomène, mais agréeront la pertinence de son rôle dans les sciences. La philosophe des sciences Nancy Cartwright ne croit pas dans les lois fondamentales scientifiques, mais plutôt dans ses lois phénoménologiques.
La pré-découverte est en astronomie le fait de retrouver l'image d'un objet dans d'anciennes images archivées ou des plaques photographiques. Ceci arrive le plus souvent avec des planètes mineures, mais parfois une comète, une planète naine ou encore un satellite naturel est retrouvé dans d'anciennes images d'archives ; des observations pré-découverte d'exoplanètes ont même été obtenues.
La réfutabilité (également désignée à ses débuts par le recours à l'anglicisme falsifiabilité) a été introduite par Karl Popper et est considérée un concept important de l'épistémologie. Une affirmation est dite réfutable s'il est possible de consigner une observation ou de mener une expérience qui, si elle était positive, entrerait en contradiction avec cette affirmation. La réfutation résout à la fois le problème de la démarcation et celui de la validité: Une proposition réfutable est réputée être une hypothèse scientifique. Si elle est réfutée elle cesse d'être valide. Il suffirait ainsi de trouver un seul individu de Dodo encore en vie pour réfuter l'hypothèse de leur disparition. En revanche, une proposition non réfutable (irréfutable au sens logique) est catégorisée comme méta-physique (ce qui ne signifie pas qu'elle soit illégitime; ainsi en est-il des univers parallèles en 2016[réf. nécessaire]). Par exemple, l'affirmation « tous les corbeaux sont noirs » pourrait être réfutée en observant un corbeau blanc. Le cygne noir ne fut d'ailleurs connu que tardivement. (Voir Théorie du cygne noir.) Par opposition, « tous les humains sont mortels » est non réfutable, et donc non scientifique, parce qu'il faudrait attendre un temps infini pour conclure négativement (constater l'existence d'un humain immortel) et que l'observateur, un humain, même s'il observait la mort de tous ses semblables, ne pourrait conclure positivement qu'après sa propre mort. Le fait qu'aucun humain observé n'a vécu plus de 130 ans prouve seulement que « tous les humains actuellement morts étaient mortels ». Voir toutefois l'article Inférence bayésienne. Sur le plan mathématique la réfutabilité est puissante puisqu'un seul contre-exemple suffit à obtenir la négation (ou contradictoire) d'une proposition. Ainsi, la négation de « quel que soit l'objet A, A vérifie une propriété B » est « il existe au moins un objet A ne vérifiant pas B » (et non pas « Aucun objet A ne vérifie la propriété B » dont la négation est « au moins un objet A vérifie la propriété B »). Il suffit alors de trouver un seul objet A ne vérifiant pas la propriété B pour que la proposition faite initialement (« quel que soit l'objet A, A vérifie une propriété B ») soit fausse, et que la propriété contradictoire soit vraie (« il existe au moins un objet A ne vérifiant pas B »).
