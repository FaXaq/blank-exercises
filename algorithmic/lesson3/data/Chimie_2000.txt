La chimie alimentaire est la branche de la chimie qui étudie la composition des aliments et leur évolution au cours de la fabrication, du stockage, de la préparation et de la digestion. Elle s'intéresse aux réactions chimiques impliquant les substances présentes dans les aliments. Les réactions améliorant la conservation et certaines propriétés (organoleptiques, nutritives, etc.) du produit sont recherchées. La chimie alimentaire est similaire à la biochimie, les principales substances alimentaires étant les glucides, les protides et les lipides. L'analyse chimique est couramment utilisée en contrôle qualité, pour le contrôle et la surveillance de la contamination (chimique et biologique), et la détection de falsifications. L'objectif est de protéger et satisfaire le consommateur.
L'archéochimie est la discipline de la chimie appliquée au contexte archéologique.
La biologie chimique est une discipline scientifique qui étudie le vivant à l'aide d'outils chimiques. Elle se distingue de la biochimie qui s'attache à élucider les phénomènes chimiques intracellulaires. Alors que la chimie médicinale a pour but de mettre au point des médicaments, la biologie chimique construit des outils qui permettent de comprendre ou de manipuler des phénomènes biologiques. Cette discipline est en cours de structuration, comme le prouve la création récente de sociétés savantes et de revues scientifiques qui lui sont dédiées.
L'ambition de la chimie douce est de synthétiser des matériaux en s'inspirant des capacités des êtres vivants - plus ou moins élémentaires -, tels que les diatomées capables de produire du verre à partir de silicates dissous. Elle renouvelle la chimie du solide en substituant aux synthèses à haute température, qui conduisent au produit thermodynamique le plus stable, des synthèses à température ambiante (au plus quelques centaines de degrés Celsius) permettant d'obtenir divers produits cinétiques de la réaction. Cette spécialité émerge au cours des années 1980 par regroupement de diverses pratiques plus anciennes sous un label commun. Elle cristallise bientôt sous l'appellation « chimie douce », en reprenant une expression du chimiste français Jacques Livage, qui paraît dans un article du journal Le Monde du 26 octobre 1977. Succès de la francophonie, l'expression « chimie douce » est employée telle quelle au début du XXIe siècle dans les publications scientifiques, anglophones et autres. Son mode de synthèse s'apparente généralement aux réactions en jeu dans les polymérisations organiques et se base sur la constitution de solutions réactives sans apport énergétique essentiel (polycondensation). L'intérêt fondamental de ce type de polymérisation minérale obtenue à température ambiante est de préserver les molécules organiques ou micro-organismes que l'on souhaite y intégrer. Les produits obtenus par les voies de la chimie douce, dits procédés sol-gel, peuvent se ranger sous plusieurs types : des structures minérales de qualités diverses (finesse, uniformité, etc.) ; des structures mixtes combinant le minéral et les molécules organiques ; des structures minérales encapsulant des molécules complexes et même des micro-organismes en conservant ou optimisant leurs caractéristiques bénéfiques. Les premiers résultats ont consisté en la création de verres et de céramiques aux propriétés nouvelles. Ces différentes structures plus ou moins composites sont mobilisables dans une vaste plage d'applications, depuis la santé jusqu'aux besoins de la conquête de l'espace. Au-delà de son mode de synthèse, un composé au label « chimie douce » réunit les avantages du monde minéral (résistance, transparence, répétition de motifs, etc.) et le potentiel maintenant exploré de la biochimie et de la chimie organique (interface avec le monde organique, réactivité, capacité de synthèse, etc.). Selon les praticiens de la chimie douce, cette spécialité n'en serait qu'à ses premiers succès et pourrait ouvrir de vastes perspectives pour l'avenir.
La chimie fine, ou chimie de spécialité, est une division de l'industrie chimique qui synthétise des produits répondant à des besoins très spécifiques (exemples : pesticides, pigments, arômes et cosmétiques) et de faible volume de production. La chimie fine produit des molécules complexes comportant facilement une structure carbonée comptant des dizaines d'atomes, à partir d'autres molécules de synthèse. La synthèse implique souvent plusieurs étapes et une purification finale poussée. Le faible tonnage est compensé par une haute valeur ajoutée du produit final.
Le terme « chimie à haute pression » désigne l'ensemble des réactions chimiques qui se déroulent à une pression supérieure à la pression normale (101,3 kPa). Le procédé Haber-Bosch en est un exemple typique, la réaction : N2(g) + 3 H2(g) ⇌ 2 NH3(g) + ΔH peut se dérouler à TPN, mais le taux de rendement de l'ammoniac peut être augmenté de façon notable en élevant la pression du système. En milieu industriel, cette pression peut atteindre jusqu'à 1 000 MPa. Cette technologie est aussi à l'œuvre dans le procédé Fischer-Tropsch, la liquéfaction du charbon et la chimie de Reppe.
La chimie industrielle est l'activité économique qui produit des molécules et autres composés chimiques en grande quantité, dite industrielle, en exploitant les technologies du génie chimique. Les produits chimiques utilisés de manière massive proviennent soit de la commercialisation de matières premières brutes ou sommairement conditionnées, soit de traitements et autres procédés industriels exploitant ces matières premières. On aura par exemple des engrais pratiquement livrés sans traitement après extraction, ou au contraire plus ou moins substantiellement améliorés par la chimie industrielle pour en augmenter l'efficacité ou la valeur marchande. Au contraire, l'exploitation du pétrole suppose toujours le déploiement d'une technologie chimique spécifique au sein d'usines de raffinage.
La chimie macromoléculaire ou chimie des polymères est une branche de la chimie qui met en œuvre les macromolécules, c'est-à-dire de grosses molécules. La chimie macromoléculaire s’intéresse aux : réactions de polymérisation ; modifications chimiques des macromolécules ; dégradation des macromolécules. La physique des polymères et la chimie des polymères font partie de la science des polymères.
La mécanochimie est une branche de la chimie qui traite du comportement chimique des matériaux sous l’effet d’une action mécanique. Une branche importante de la mécanochimie est la tribochimie (voir tribologie) qui traite de la modification du comportement chimique des solides par action mécanique sur leurs surfaces. Le cisaillement, le frottement et le broyage sont quelques-unes des méthodes utilisées pour la génération de comportements mécanochimiques. Ces forces mécaniques conduisent à des changements structurels au niveau de la surface. L'abrasion peut induire l’augmentation de la surface, la diminution de la taille des particules, la formation de nouvelles surfaces et même des transitions de phase. Cette activation mécanique conduit à l'émission de photons, d’électrons et même à la formation de nouveaux composés par réaction mécanochimique.
La neurochimie ou neurobiochimie, est la science qui s'intéresse aux phénomènes biochimiques du système nerveux. Elle comprend principalement l'étude des neurotransmetteurs qui assurent la transmission de l'information électrique entre deux neurones. Elle comprend également l'étude des différentes molécules qui ont un effet pharmacologique sur les neurones.
L'oléochimie (du latin : oleum « huile d'olive ») est la science des transformations physico-chimiques appliquées aux huiles et aux graisses animales et végétales. La formation de substances oléochimiques basiques comme les acides gras, les esters méthyliques d'acides gras (FAME: fatty acid methyl esters ), les alcools gras, les amines grasses et les glycérols se fait par diverses réactions chimiques et enzymatiques. Les substances chimiques intermédiaires produites à partir de ces substances oléochimiques basiques comprennent les alcools éthoxylés, les alcools sulfates, les éther sulfates d'alcool, les sels d'ammonium quaternaires, les monoacylglycérols (MAG), les diacylglycérols (DAG), les triacylglycérols structurés (TAG), les esters de sucre et d'autres produits oléochimiques. Alors que le prix du pétrole brut augmentait à la fin des années 1970 , les fabricants passaient de la pétrochimie aux produits oléochimiques parce que les huiles lauriques transformées à partir d'huile de palmiste étaient moins chères. Depuis lors, l'huile de palmiste est principalement utilisée dans la production de détergent à lessive et d'articles de soins personnels comme le dentifrice, les barres de savon, la crème de douche et le shampoing. L'oléochimie a d'abord été utilisée pour la fabrication des savons. Aujourd'hui, on l'utilise dans plusieurs industries : agroalimentaire, cosmétique et pharmaceutique.
La parachimie ou chimie de formulation est un secteur industriel qui conditionne (ou « formule ») des produits issus de l’industrie chimique sous une forme utilisable par le consommateur final ou par une industrie spécifique. Les produits ainsi élaborés sont fonctionnels. Il existe une très grande variété de produits fabriqués par la parachimie, ce qui en fait un secteur très hétérogène. Tandis que certains produits parachimiques sont directement conditionnés pour l’utilisation finale et ne seront plus transformés (par exemple produits phytosanitaires, peintures décoratives, explosifs et colles), d’autres (huiles essentielles, encres d’imprimerie, additifs pour ciments ou bétons, huiles de lubrification, etc.) s’intègrent dans la fabrication en aval d’industries très diverses : agroalimentaire, emballage, etc. En France, selon la nomenclature officielle du ministère de l'Économie, des Finances et du Commerce extérieur, le secteur de la parachimie regroupe les industries qui fabriquent huit catégories de produits : produits agrochimiques ; peintures, vernis et encres ; produits explosifs ; colles et gélatines ; huiles essentielles ; produits chimiques pour la photographie ; supports de données ; produits chimiques à usage industriel. La fabrication de médicaments est une activité qui correspond à la définition de la parachimie, mais elle est en fait englobée dans l’industrie pharmaceutique.   Portail de la chimie  Portail de la production industrielle
La chimie pharmaceutique, encore appelée chimie médicinale, chimie médicale ou chimie thérapeutique, est une discipline scientifique placée à l'intersection de la chimie et de la pharmacologie. Son objet est l'étude des relations entre la structure des corps chimiques et leurs propriétés thérapeutiques. Elle recherche, identifie, analyse ou synthétise des substances médicalement actives et elle participe à leur développement.
La pharmacotechnie est une discipline pharmaceutique qui s’intéresse aux techniques de conception d'un médicament qui suivent l'extraction ou synthèse du principe actif et qui vont jusqu'à la forme finale la plus facilement administrable au patient. Elle est subdivisée en plusieurs sous-familles, comme la formulation pharmaceutique, la galénique ou encore la biopharmacie. Elle comprend notamment l'étude de ces différents procédés (par ordre approximatif dans la conception d'un médicament) : Purification Filtration Dessication Lyophilisation Dilution Pulvérisation Granulation Stérilisation Tamisage Compression Enrobage Préparation magistrale Homéopathie
La phytochimie, ou chimie des végétaux, est la science qui étudie la structure, le métabolisme et la fonction ainsi que les méthodes d'analyse, de purification et d'extraction des substances naturelles issues des plantes. Elle est indissociable d'autres disciplines telles que la pharmacognosie (du grec: pharmacon, drogue et gnôsis, connaissance) traitant des matières premières et des substances à potentialité médicamenteuse d’origine biologique. Les végétaux sont des organismes autotrophes qui peuvent synthétiser un grand nombre de molécules organiques complexes qui n'interviennent pas dans les grandes voies du métabolisme de base, c’est-à-dire le métabolisme énergétique et le métabolisme de carboné. Ces molécules sont toutefois utiles aux plantes elles-mêmes et aux consommateurs des chaînes alimentaires pour diverses raisons. Les plantes qui disposent d'énergie et de squelettes carbonés en quantité suffisante, grâce à la photosynthèse, s'avèrent être des producteurs polyvalents.
La thiochimie (du grec theion, le soufre) est l'activité de transformation chimique des produits contenant du soufre.
Un chimiste est un scientifique qui étudie la chimie, c’est-à-dire la science de la matière à l’échelle moléculaire ou atomique (« supra-atomique »). Le mot chimiste est dérivé d’alchimiste mais prend un sens bien différent. Si l’on considère les alchimistes pré-scientifiques, on peut dire que la profession de chimie constitue l’une des plus anciennes professions scientifiques de l’Histoire. Plusieurs des avancées en chimie furent effectuées par des physiciens, la physique et la chimie étant deux sciences qui étudient un même objet selon un point de vue et des méthodes différentes (Marie Curie, Niels Bohr et Linus Pauling).  
Aimé Argand, né à Genève le 5 juillet 1750 et mort à Genève le 14 octobre 1803, est un physicien et chimiste genevois.
Ameenah Gurib-Fakim, née le 17 octobre 1959 à Surinam (Maurice), est une chimiste et femme d'État mauricienne, présidente de la République du 5 juin 2015 au 23 mars 2018.
Miltiádis Kírkos (Μιλτιάδης Κύρκος / Miltiádis Kírkos), souvent appelé Míltos Kírkos (Μίλτος Κύρκος / Míltos Kírkos), né en 1959 à Athènes, est un homme politique grec, membre de La Rivière. Il est le fils de Leonídas Kýrkos. Le 25 mai 2014, il est élu député européen.
Lydia Phindile Makhubu (née le 1er juillet 1937) est une chimiste swazi et ancienne professeure de chimie, doyenne et vice-chancelière de l'Université du Swaziland.
Dicoh Mariam dit Konan est la 1re femme chimiste du Côte d’Ivoire. Elle est gravée sur la pièce de 25 francs CFA avec une burette de chimiste,.
Abû-l-Qâsim Maslama ibn Ahmad al-Faradi al-Hasib al-Qurtubi al-Majritî (arabe: أبو القاسم مسلمة بن أحمد المجريطي), mathématicien, chimiste et astronome d'Al-Andalus, né à Madrid en 950, mort en 1007 à Cordoue, connu comme le maître des mathématicien d'Al Andalus. Il vécut essentiellement à Cordoue.
Min Enze (chinois simplifié : 闵恩泽 ; chinois traditionnel : 閔恩澤 ; pinyin : Mǐn Ēnzé), né le 4 février 1924 à Chengdu et mort le 7 mars 2016 à Pékin, est un expert chinois renommé dans la catalyse pétrochimique, et un académicien de l'Académie chinoise des sciences (CAS) et l'Académie chinoise d'ingénierie (CAE).
Kyriákos Kóstas Nikoláou (grec moderne : Κυριάκος Κ. Νικολάου), connu aux États-Unis sous le nom de Kyriacos Costa Nicolaou, né le 5 juillet 1946, est un chimiste originaire de Chypre reconnu mondialement pour ses recherches concernant la synthèse de produits naturels. Il détient actuellement un poste comme professeur de chimie à l’Université Rice, au Texas. Plusieurs prix lui ont été décernés au cours de sa carrière.
Cyril Andrew Ponnamperuma (en cingalais : ආචාර්ය සිරිල් ඇන්ඩෘ පොන්නම්පෙරුම), né le 16 octobre 1923 à Galle, au Sri Lanka (alors Ceylan), et mort le 20 décembre 1994 à Takoma Park, Maryland, est un chimiste sri lankais spécialisé dans l'abiogenèse (étude de l'origine de la vie).
Bianca Tchoubar (née en 1910 en Ukraine et morte en 1990 à Paris) est une chimiste spécialiste des mécanismes de réaction .
La dénomination systématique est la règle de dénomination des éléments chimiques nouvellement synthétisés ou n'ayant pas encore été synthétisés, leur attribuant un nom et un symbole temporaires. En chimie, un élément transuranien (éléments dont le nombre atomique est supérieur à 92) reçoit un nom vernaculaire et un symbole seulement après que sa synthèse a été confirmée. Dans certains cas, cette confirmation peut être longue et entraîner de nombreuses controverses (en). Afin de parler de ces éléments sans ambiguïté, l'Union internationale de chimie pure et appliquée (UICPA) utilise plusieurs règles pour assigner un nom et un symbole systématiques et temporaires à chacun de ces éléments. Cette approche de dénomination prit ses origines dans la Nomenclature des molécules organiques.
On dénombre 118 éléments chimiques ayant été observés, dont seulement 90 qui se rencontrent dans le milieu naturel. Cette liste illustrée présente, sauf indication contraire, une forme courante des corps simples constitués de ces éléments dans des conditions normales de température et de pression, à l'exception toutefois des éléments dont le corps simple dans ces conditions est un gaz incolore, qui sont alors représentés dans leur état après ionisation dans une lampe à décharge ou à l'état liquide.
Cet article contient une liste des éléments chimiques du tableau périodique classés par numéro atomique.
Cet article contient une liste des éléments chimiques, par ordre alphabétique, avec indication de la température de fusion du corps simple (élémentaire ou moléculaire) correspondant. Sauf note contraire, les températures de fusion sont données à la pression atmosphérique.
Le tableau périodique des éléments, également appelé tableau ou table de Mendeleïev, classification périodique des éléments ou simplement tableau périodique, représente tous les éléments chimiques, ordonnés par numéro atomique croissant et organisés en fonction de leur configuration électronique, laquelle sous-tend leurs propriétés chimiques. La conception de ce tableau est généralement attribuée au chimiste russe Dmitri Mendeleïev, qui, en 1869, construisit une table, différente de celle qu'on utilise aujourd'hui mais semblable dans son principe, dont le grand intérêt était de proposer une classification systématique des éléments connus à l'époque en vue de souligner la périodicité de leurs propriétés chimiques, d'identifier les éléments qui restaient à découvrir, voire de prédire certaines propriétés d'éléments chimiques alors inconnus. Le tableau périodique a connu de nombreux réajustements depuis lors jusqu'à prendre la forme que nous lui connaissons aujourd'hui. Il est devenu un référentiel universel auquel peuvent être rapportés tous les types de comportements physique et chimique des éléments. Depuis la mise à jour de l'UICPA du 28 novembre 2016, sa forme standard comporte 118 éléments, allant de l'hydrogène 1H à l'oganesson 118Og.
Un tableau périodique étendu est un tableau périodique comportant des éléments chimiques au-delà de la 7e période, éléments hypothétiques de numéro atomique supérieur à 118 (correspondant à l'oganesson) classés en fonction de leurs configurations électroniques calculées. Le premier tableau périodique étendu a été théorisé par Glenn Seaborg en 1969 : il prévoyait une 8e période contenant 18 éléments du bloc g et une nouvelle famille d'éléments chimiques dite des « superactinides ». D'autres tableaux étendus ont été publiés par la suite, répartissant les éléments sur parfois 9 périodes, comme celui proposé en 1971 par Fricke et al. ou celui proposé en 2011 par Pekka Pyykkö.
Cette représentation alternative du tableau périodique des éléments, attribuée au physicien Timothy Stowe, permet de mettre en lumière les sous-couronnes électroniques. Pour cela on a choisi un format radial, de façon à représenter les couronnes électroniques séparément. Les correspondances chimiques entre les éléments se fait en transposant une position d'une couronne à l'autre, par exemple pour l'argent (Ag, 47) et l'or (Au, 79). Non-métaux Gaz nobles Métaux alcalins Métaux alcalino-terreux Métalloïdes Halogènes Métaux pauvres Métaux de transition Lanthanides Actinides Indéfinis Le remplissage des niveaux électroniques suit un schéma tel qu'il est possible d'ajouter ou enlever des éléments sans changer la forme du tableau :
En chimie, un élément synthétique est un élément chimique absent du milieu naturel et qui, pour être observé, doit être produit artificiellement par une réaction nucléaire. Il peut s'agir d'éléments qui étaient présents lors de la formation de la Terre mais se sont désintégrés depuis lors, ou bien d'éléments trop lourds pour avoir pu être formés par nucléosynthèse stellaire — hormis, dans certains cas, lors d'explosions de supernovae. À la première catégorie appartiennent tous les éléments synthétiques de numéro atomique allant jusqu'à 94 (qui correspond au plutonium) et dont aucun isotope n'a une durée de vie supérieure à 400 millions d'années (1 ⁄10 de l'âge de la Terre) : ces éléments ne sont plus présents sur Terre qu'à l'état de traces, hormis ceux qui résultent de la désintégration du thorium ou de l'uranium et qui sont continuellement reformés (comme le radium, le radon et le polonium). À la seconde catégorie appartiennent tous les éléments dont le numéro atomique est strictement supérieur à 94.
Albert Ghiorso (né le 15 juillet 1915 et mort le 26 décembre 2010) est un physicien nucléaire américain codécouvreur de 12 éléments chimiques du tableau périodique. Sa carrière scientifique s'étale sur cinq décennies, du début des années 1940 à la fin des années 1990.
Les métaux du groupe du platine dits MGP (ou PGM en anglais pour platinum group metals) regroupent six ou sept éléments chimiques appartenant à la famille des métaux de transition et apparentés dans le tableau périodique : ruthénium 44Ru, rhodium 45Rh, palladium 46Pd, osmium 76Os, iridium 77Ir, platine 78Pt, et, selon les sources, rhénium 75Re. Les métaux de ce groupe sont rares et caractérisés par des propriétés communes et inhabituelles chez les métaux. Ce sont notamment de puissants catalyseurs et sous certaines formes ils peuvent être toxiques. Pour ces raisons, en 2005, la valeur marchande respective du palladium était la moitié de celle de l'or, celle du platine le double de la valeur de l'or, et celle du rhodium le triple.
En chimie, on appelle élément du groupe principal un élément chimique appartenant au bloc s ou au bloc p, c'est-à-dire n'étant ni un métal de transition (bloc d), ni un métal de transition interne (actinide ou lanthanide, bloc f). Le groupe principal regroupe donc les groupes : I - colonne 1 à l'exception de l'hydrogène (métaux alcalins) II - colonne 2 (métaux alcalino-terreux) III - colonne 13 (groupe du bore) IV - colonne 14 (groupe du carbone) V - colonne 15 (pnictogènes) VI - colonne 16 (chalcogènes) VII - colonne 17 (halogènes) VIII - colonne 18 (gaz nobles)  
Remarque préliminaire : la notation qui va être décrite vaut pour la physique nucléaire où on s'intéresse à la structure du noyau de l'atome. En chimie on ne tiendra compte que de Z, qui devient redondant avec le symbole de l'élément chimique. Ce symbole va donc être utilisé tout seul dans la pratique, sans l'indication de Z. Pour chaque nucléide, on a : Z = nombre de protons (ou d'électrons), également appelé numéro atomique N = nombre de neutrons A = nombre de nucléons (protons + neutrons), également appelé nombre de masse (voir aussi masse atomique) On note ZX le nombre Z de protons de l'élément nommé X, ainsi que AX le nombre total A de nucléons dans l'élément nommé X et enfin XN le nombre N de neutrons dans l'élément nommé X. Deux nucléides qui possèdent le même nombre de nucléons sont des isobares (Z différents, A identiques, N différents), par exemple le carbone 14 et l'azote 14 (notation:                                                                                      6                                   14                                        C                        8                                     {\displaystyle {}_{\ 6}^{14}\operatorname {C} _{8}}    et                                                                                      7                                   14                                        N                        7                                     {\displaystyle {}_{\ 7}^{14}\operatorname {N} _{7}}   ). Lors d'une désintégration bêta, l'élément père est un isobare de l'élément fils. On appelle isotopes deux variétés d'un même élément chimique qui se différencient par leur nombre de neutrons (Z identiques, A différents, N différents), par exemple le carbone 12 et le carbone 14 (notation:                                                                                      6                                   12                                        C                        6                                     {\displaystyle {}_{\ 6}^{12}\operatorname {C} _{6}}    et                                                                                      6                                   14                                        C                        8                                     {\displaystyle {}_{\ 6}^{14}\operatorname {C} _{8}}   ). Des nucléides qui ont le même nombre de neutrons sont dits isotones (Z différents, A différents, N identiques), par exemple le carbone 13 et l'azote 14 (notation:                                                                                      6                                   13                                        C                        7                                     {\displaystyle {}_{\ 6}^{13}\operatorname {C} _{7}}    et                                                                                      7                                   14                                        N                        7                                     {\displaystyle {}_{\ 7}^{14}\operatorname {N} _{7}}   ).
On appelle isotopes (d'un certain élément chimique) les nucléides partageant le même nombre de protons (caractéristique de cet élément), mais ayant un nombre de neutrons différent. Autrement dit, si l'on considère deux nucléides dont les nombres de protons sont Z et Z', et les nombres de neutrons N et N', ces nucléides sont dits isotopes si Z = Z' et N ≠ N'. Par extension, on appelle souvent isotope un nucléide caractérisé par son nombre de protons Z et son nombre de neutrons N (ou son nombre de masse A = Z + N), mais sans distinction concernant son spin ou son état énergétique.  Les isotopes ne doivent pas être confondus avec : les isotones, nucléides ayant le même nombre de neutrons mais un nombre de protons différent (Z ≠ Z' mais N = N') ; les isobares, nucléides ayant des nombres de protons différents, des nombres de neutrons différents, mais des nombres de masse identiques (Z ≠ Z', N ≠ N', mais Z + N = A = A' = Z' + N') ; les isomères, nucléides ayant le même nombre de protons Z et le même nombre de neutrons N (donc aussi le même nombre de masse A), mais pas le même spin ni le même niveau énergétique.
Charles Janet, né à Paris le 15 juin 1849 et mort le 7 février 1932 à Voisinlieu dans l'actuelle commune de Beauvais en France, est un ingénieur, industriel et inventeur français tombé dans l'oubli et redécouvert au début du XXIe siècle pour sa présentation novatrice du tableau périodique des éléments.
Dmitri Ivanovitch Mendeleïev (en russe : Дмитрий Иванович Менделеев, [ˈdmʲitrʲɪj ɪˈvanəvʲɪtɕ mʲɪndʲɪˈlʲejɪf] ), né le 27 janvier 1834 (8 février 1834 dans le calendrier grégorien) à Tobolsk et mort le 20 janvier 1907 (2 février 1907 dans le calendrier grégorien) à Saint-Pétersbourg, est un chimiste russe. Il est principalement connu pour son travail sur la classification périodique des éléments, publiée en 1869 et également appelée « tableau de Mendeleïev ». Il déclara que les éléments chimiques pouvaient être arrangés selon un modèle qui permettait de prévoir les propriétés des éléments encore non découverts.
Le musée Mendeleïev est installé dans un appartement occupé à Saint-Pétersbourg par le chimiste russe Dmitri Mendeleïev, inventeur en 1869 du tableau périodique des éléments chimiques. Il présente des souvenirs liés à sa vie et à son travail.
Une relation diagonale existe entre certains éléments chimiques voisins des deuxième et troisième périodes du tableau périodique. Il s'agit des paires : lithium - magnésium béryllium - aluminium bore - silicium carbone - phosphore azote - soufre oxygène - chlore (du point de vue de l'électronégativité) Par exemple, la solubilité des sels de lithium ressemble plus à celle des sels de magnésium correspondants qu'à celle des sels des autres métaux alcalins. Le bore et le silicium sont des semiconducteurs, ont des oxydes acides et forment des hydrures volatils très réactifs alors que l'aluminium est un métal formant un hydrure solide. Ces relations diagonales proviennent du fait que parcourir une période de gauche à droite et descendre une colonne du tableau périodique ont des effets opposés sur le rayon atomique et l'électronégativité des éléments. Le rapport taille/charge des atomes est à peu près constant sur ces diagonales, d'où certaines similitudes dans les propriétés des éléments concernés.
Voir aussi la table divisée des isotopes pour une lecture plus facile. Cette table répertorie tous les isotopes connus des éléments chimiques, ordonnés selon leur numéro atomique croissant de gauche à droite et avec un nombre croissant de neutrons de haut en bas. Les demi-vies sont indiquées par la couleur de la cellule de chaque isotope ; les isotopes ayant deux modes de désintégration qui diffèrent par leur demi-vie ont des couleurs de fond et de bordure différentes. Les éléments colorés en rouge (éléments stables) forment une suite qualifiée de « vallée de stabilité ».   Portail de la chimie  Portail de la physique
Ce tableau périodique des éléments détaillé contient le nom, le numéro atomique, le symbole et la masse atomique moyenne des isotopes naturels de chaque élément. La légende apparaissant sous le tableau explique l'encodage des couleurs et la mise en page de chaque entrée.  †. Une valeur entre crochets, comme [259,1011], est la masse atomique de l'isotope le plus stable, à moins que ce ne soit un entier, dans ce cas, c'est le nombre de masse du plus stable des isotopes. Dans tous les autres cas, il s'agit de la valeur de la masse atomique relative calculée à partir des éléments chimiques contenus dans la Terre, tel que publiée par Atomic Weights of the Elements (2001), avec l'incertitude entre parenthèses. Par exemple, la valeur 1,00794(7) pour l'hydrogène signifie que sa composition isotopique terrestre a une masse atomique relative de 1,00794 (u) avec une incertitude de 0,00007u, ce qui réflète les variations géologiques autour de la Terre.
La classification TAS (pour Total Alkali Silica) est un système qui permet de définir une roche volcanique par sa composition chimique à partir du rapport entre le taux pondéral de silice (SiO2) et le taux pondéral de minéraux alcalins (Na 2O et K2O). Cette classification permet de distinguer plusieurs séries magmatiques, selon leur teneur en Na et K dont: La série sub-alcaline. La série alcaline. La série hyper-alcaline   Portail de la chimie  Portail des minéraux et roches  Portail du volcanisme
L'American Chemical Society Award in Pure Chemistry est un récompense décernée chaque année par l'American Chemical Society (ACS) « afin de reconnaitre et d'encourager des travaux de recherche fondamentale en chimie pure menés sur le continent nord américain par des jeunes hommes et des jeunes femmes (de moins de 35 ans lors de la cérémonie de réception de la récompense qui a lieu lors du congrès de printemps de l'ACS). » l'obtention de ce prix nécessite "d'avoir accompli des travaux de recherche exceptionnels lors du début de sa carrière. Une attention particulière est portée à l'indépendance et à l'originalité de ceux-ci…" Cette récompense a été décernée pour la première fois en 1931 à Linus Pauling. Elle est actuellement financée par la Alpha Chi Sigma Fraternity et la Alpha Chi Sigma Educational Foundation.
Le Prix Alfred-Bader est remis annuellement par la Société canadienne de chimie. Il est remis pour des recherches exceptionnelles en chimie organique.
Le prix Anselme-Payen est remis annuellement par l'American Chemical Society. Le prix est remis en l'honneur de Anselme Payen, chimiste français qui a découvert la cellulose. Le prix est remis au scientifique qui a fait des recherches dans le domaine de la cellulose.
L'Arthur C. Cope Award est un prix qui récompense des travaux dans le domaine de la chimie organique. Il est généralement considéré comme l'une des plus hautes distinctions dans ce domaine. Ce prix, sponsorisé par le fond Arthur C. Cope (en), est décerné depuis 1973 par l'American Chemical Society. Liste des lauréats :
Le Prix Shanti Swarup Bhatnagar de Sciences et de Technologie (SSB) est une distinction scientifique en Inde remise chaque année par le Conseil de Recherche Scientifique et Industrielle (en) (CSIR) pour les recherches notables et remarquables, appliquées ou fondamentales, en biologie, en chimie, en sciences de l'environnement, en ingénierie, en mathématiques, en médecine et en physique.
Depuis 1877, la médaille Davy est une distinction scientifique décernée annuellement par la Royal Society. Cette médaille de bronze frappée à l'effigie de Humphry Davy vise à récompenser des scientifiques pour des travaux exceptionnels dans le domaine de la chimie.
Le Faraday Lectureship est une récompense accordée par la Royal Society of Chemistry tous les trois ans depuis 1867 pour commémorer le nom de Michael Faraday. Cette récompense est remise à des chimistes de préférence physiciens ou théoriciens.
Le Grand prix Félix Trombe créé en 2004 par la Société chimique de France,récompense toute personne physique ayant réalisé un développement remarquable au service de la chimie. Ceci peut concerner une innovation technologique concrétisée par une réalisation industrielle, mais aussi un parcours exceptionnel. Le prix est doté de 3000 euros. Il est remis en l'honneur du chimiste français Félix Trombe.
La Médaille Hector (anciennement connu sous le nom de Médaille commémorative Hector) est une distinction scientifique qui est décernée par la Société royale de Nouvelle-Zélande à la mémoire de Sir James Hector à des chercheurs travaillant en Nouvelle-Zélande.
Le Claude S. Hudson Award in Carbohydrate Chemistry, appelé parfois simplement prix Hudson, est un prix remis par l'American Chemical Society pour reconnaître une contribution exceptionnelle à la chimie des hydrocarbures.
Le prix Ignacio-Ribas-Marqués (Premio Ignacio Ribas Marqués) était destiné à récompenser un premier travail de recherche et de synthèse dans les domaines de la science et de la technologie de la chimie.
Le prix Jecker de l'Académie des sciences est un prix décerné tous les quatre ans pour encourager de jeunes scientifiques. Le prix fut créé après le legs de Louis Jecker à l'Académie des sciences en 1851. Il était à l'origine attribué tous les ans et resta longtemps un des prix les plus importants récompensant les chimistes en France.
Le prix scientifique Klung-Wilhelmy (1973 à 2001 : prix Otto-Klung, 2001 à 2007 : prix Otto-Klung-Weberbank, 2007 à 2013 : prix Klung-Wilhelmy-Weberbank) est un prix scientifique, qui chaque année, est remis à de jeunes scientifiques allemands spécialisés en chimie et en physique et âgés de moins de 40 ans. Le choix des lauréats est fait au terme de la tenue de plusieurs commissions à l'Institut de chimie et biochimie et à l'Institut de physique à l'université libre de Berlin, impulsé également par des professeurs d'autres grandes écoles, en Allemagne ou à l'étranger, et issu de dossiers de chercheurs autant nationaux qu'internationaux de pointe. Les candidatures personnelles ne sont pas prises en considération. La décision de choix est réservée aux fondations : la Fondation Otto-Klung (université libre de Berlin) et la Fondation Dr Wilhelmy. Ensemble, elles ont défini un but commun : intensifier l'extrême qualité de recherche scientifique de pointe et promouvoir de tels travaux en vue de l'obtention future de la reconnaissance internationale. Cinq lauréats ont plus tard reçu le prix Nobel. Grâce à la coopération des fondations, le prix, décerné pour la première fois en 1973 par la Fondation Otto-Klung, est devenu depuis 2007 l'une des plus hautes dotations financières privées de prix scientifiques en Allemagne. La remise du prix, qui se fait depuis de nombreuses années en novembre, est publique.
Le prix Irving-Langmuir en chimie physique est remis annuellement par l'American Chemical Society les années paires et, les années impaires, par la Société américaine de physique. Il récompense l'auteur d'une contribution majeure dans les dix années précédentes en chimie ou en physique dans l'esprit d'Irving Langmuir. Il est réservé aux résidents des États-Unis. Le prix a été créé en 1931 par A. C. Langmuir, frère d'Irving. Le premier bénéficiaire fut Linus Pauling. Il s'accompagne d'une récompense de 10000$ d'abord donnée par l'American Chemical Society puis par la fondation General Electric en 1964, GE Global Research en 2006, rejoint depuis 2009 par ACS Division of Physical Chemistry.
La médaille Lavoisier est une récompense décernée aux auteurs de travaux renommés dans le domaine de la chimie. Son nom vient d'Antoine Lavoisier, considéré comme le père de la chimie moderne. La médaille Lavoisier est décernée par trois organismes dans le monde : la Société chimique de France, l'International Society for Biological Calorimetry et la firme DuPont.
Le grand prix Achille-Le-Bel est remis annuellement par la Société chimique de France et récompense des travaux dans le domaine de la chimie reconnus au niveau international. Le prix, créé en 1976, est remis en l'honneur du chimiste français Joseph Achille Le Bel (21 janvier 1847 - 6 août 1930) considéré comme le précurseur de la stéréochimie moderne.
La Médaille Liebig est une distinction décernée par la Société des chimistes allemands.
La médaille Carl-Friedrich-Gauß est une récompense créée en 1949 par la Braunschweigische Wissenschaftliche Gesellschaft qui distingue des scientifiques pour leurs travaux. Elle est décernée chaque année, en l'honneur du mathématicien et physicien Carl Friedrich Gauß (1777–1855).
La médaille d'or Othmer reconnaît les réalisations des personnes qui ont contribué au progrès de la chimie et de la science à travers leurs activités dans des domaines tels que l'innovation, l'entrepreneuriat, la recherche, l'éducation, la diffusion auprès du public, de la législation et la philanthropie. La médaille est décernée chaque année sous le parrainage de la Chemical Heritage Foundation (en) et quatre organisations affiliées: l'American Chemical Society (ACS), l'American Institute of Chemical Engineers (AIChE), le Club des chimistes, et la section américaine de la Société de chimie industrielle, lors de la journée du patrimoine de la Chemical Heritage Foundation.
La Médaille Garvan–Olin est un prix annuel fondé en 1936 qui récompense les accomplissements et les services rendus à la chimie par des femmes chimistes américaines. Il est décerné par l'American Chemical Society et consiste en une récompense de 5000$US et d'une médaille.
La médaille Perkin est un prix décerné chaque année par la section américaine de la Society of Chemical Industry (en) à un ou une scientifique résidant aux États-Unis pour une innovation en chimie appliquée conduisant à un développement commercial. Elle est considérée comme la plus haute distinction accordée dans l'industrie chimique américaine. La Médaille Perkin a été décernée la première fois en 1906 pour célébrer le 50e anniversaire de la découverte de la mauvéine, premier colorant synthétique, lequel a été obtenu par William Henry Perkin, un chimiste anglais. Le prix lui fut décerné en 1906 à l'occasion de sa visite aux États-Unis, l'année avant sa mort. Depuis 1908, la médaille Perkin a été décernée chaque année.
La médaille William-H.-Nichols est décernée chaque année pour un travail de recherche original en chimie. Les candidats doivent avoir fait une "contribution significative et originale dans l'un des domaines de la chimie" au cours des cinq années précédant la date de présentation. Le médaillé reçoit une médaille d'or, une réplique en bronze et $5000.
Le National Academy of Sciences Award in Chemical Sciences est un prix qui récompense des recherches innovantes dans le domaine des sciences chimiques. Les travaux récompensés doivent contribuer à une meilleure compréhension des sciences naturelles et être bénéfiques pour l'humanité.
Le prix Nobel de chimie est une récompense décernée une fois par an, depuis 1901, par l'Académie royale des sciences de Suède à un scientifique dont l'œuvre et les travaux ont rendu de grands services à l'humanité par une contribution exceptionnelle en chimie. Après la publication du nom du lauréat au début octobre, le prix (une médaille et un diplôme) est officiellement remis par le roi de Suède, le 10 décembre, jour anniversaire de la mort du fondateur du prix. Depuis 2001, il est doté d'un montant de dix millions de couronnes suédoises.
Les Olympiades internationales de chimie (IChO en anglais) sont une compétition internationale de chimie qui se déroule chaque année depuis 1968 (sauf en 1971) au mois de juillet dans un pays organisateur. Environ 250 étudiants non spécialisés en chimie issus des enseignements secondaires d'environ 70 pays y participent chaque année. La 49e Olympiade a eu lieu à Nakhon Pathom (Thaïlande) du 6 au 15 juillet 2017. La 50e Olympiade aura lieu à Prague (République tchèque) et à Bratislava (Slovaquie) du 19 au 29 juillet 2018.
La médaille Priestley est un prix scientifique décerné annuellement par l'American Chemical Society (société américaine de chimie) à des scientifiques de toutes nationalités pour récompenser des travaux dans le domaine de la chimie. Cette médaille a été créée en 1922 en mémoire de Joseph Priestley, l'un des chimistes anglais ayant découvert l'oxygène, et qui émigra aux États-Unis en 1793.
Le Ernest Guenther Award est une distinction remise par l'American Chemical Society. Son but est de reconnaître et d'encourager les réalisations exceptionnelles dans l'analyse, la détermination de la structure et la synthèse chimique de produits naturels, avec une attention particulière accordée à l'indépendance de la pensée et l'originalité. La dotation de cette distinction est de six mille dollars. Cette distinction a été créée en 1948 par la société Fritzsche Dodge and Olcott Inc. à l'occasion de ses soixante-quinze ans. Elle a été nommée ainsi en 1969 en hommage à Ernest Guenther, un ancien directeur, connu pour ses travaux sur les huiles essentielles. Depuis 1992, elle est parrainée par la firme suisse Givaudan qui a racheté Fritzsche Dodge and Olcott Inc. en 1990.
Le prix Procter & Gamble est remis par l'American Chemical Society, il s'appelait autrefois le prix Kendall Company.
La Médaille commémorative Rutherford est remise par la Société royale du Canada depuis 1980. Cette récompense est remise à la mémoire d'Ernest Rutherford éminent scientifique et chef de file en recherche nucléaire. La médaille est remise annuellement à deux lauréats pour des recherches éminentes en physique et en chimie.
Le Prix de science macromoléculaire et de génie est remis par l'Institut de chimie du Canada. Le prix récompense une contribution exceptionnelle à la science macromoléculaire ou bien au génie. Le prix est commandité par NOVA Chemicals Ltd., le prix a porté plusieurs noms depuis sa création en 1971.
La Médaille Shorland est une distinction instituée en 1999 par l'Association des scientifiques de Nouvelle-Zélande et décernée annuellement en reconnaissance d'une « grande et constante contribution à la recherche fondamentale ou appliquée qui a ajouté considérablement à la compréhension scientifique ou a entraîné d'importants avantages pour la société. ». Elle porte le nom du chimiste organique néo-zélandais Francis Shorland (1909–1999).
Les Bourses Sloan de recherche sont accordées chaque année par la Fondation Alfred P. Sloan, depuis 1955 afin « de fournir le soutien et la reconnaissance à des scientifiques et des savants en début de carrière ». Les lauréats sont appelés Sloan Fellows ou boursiers Sloan. Les boursiers Sloan de recherche sont distinctes des Sloan Fellows (en) dans le domaine des affaires. Les bourses ont été initialement attribuées en physique, en chimie et en mathématiques. Des récompenses ont été ajoutées plus tard dans les neurosciences (1972), les sciences économiques (1980), l'informatique (1993) et la biologie moléculaire computationnelle et évolutionnaire (2002). Ces bourses de deux ans sont attribuées à 126 chercheurs chaque année. Depuis le début du programme en 1955, 43 boursiers ont gagné un Prix Nobel et 16 ont obtenu la médaille Fields en mathématiques.
Le Grand Prix Pierre Süe est remis annuellement par la Société chimique de France et récompense des travaux dans le domaine de la chimie reconnus au niveau international. Le prix, créé en 1974, est remis en l'honneur du chimiste français Pierre Süe.
Le Willard Gibbs Award a été fondé en 1910 par William A. Converse. Cette récompense est décernée "à un éminent chimiste qui [...] a apporté au monde des avancées permettant à chacun de vivre plus confortablement ainsi que d'avoir une meilleure compréhension du monde." Les lauréats de ce prix reçoivent une médaille en or de 18 carats, la Willard Gibbs Medal. Ce prix est nommé en l'honneur de J. Willard Gibbs, un chimiste américain du XIXe siècle. Actuellement, ce prix est financé par la section de Chicago de l'American Chemical Society.
Le prix Wolf de chimie est remis annuellement par la fondation Wolf, en Israël. C'est l'un des six prix Wolf remis depuis 1978, les autres étant ceux en agriculture, mathématiques, médecine, physique et art.
Les prix Wolf sont officiellement attribués tous les ans en Israël par la Fondation Wolf depuis 1978. Cinq ou six prix sont décernés à des artistes et scientifiques vivants d'envergure exceptionnelle, sans considération de nationalité, ethnie, couleur, religion, sexe ou opinion politique, pour des réalisations dans l'intérêt de l'humanité et des relations pacifiques entre les peuples.
Une boîte de chimie est un jeu éducatif permettant aux enfants de faire des expériences de chimie. Les boîtes se distinguent par le nombre de réactifs et donc d'expériences réalisables. Les produits utilisés sont en principe peu toxiques. Les marques anglo-saxonnes les plus connues sont Chemcraft et Gilbert. Une boîte typique contient des réactifs sous forme solide, de la verrerie de laboratoire, un brûleur à alcool,... Les expériences sont essentiellement visuelles (changement de couleurs) (?). Parmi les réactifs, on trouve : du chlorure de cobalt, du sulfate de cuivre, un ruban de magnésium, de la limaille de fer, du soufre, du sulfate de cuivre du sulfate ferreux un indicateur de pH du carbonate de sodium de l'acide tartrique du bicarbonate de sodium de l'acide citrique de l'hydroxyde de calcium du thiosulfate de sodium du sulfate de magnésium du chlorure de sodium de la teinture d'iode du permanganate de potassium de l'iodure de potassium du chlorure d'ammonium du sulfate de sodium du sulfate de nickel ammoniacal
Le BTS Chimiste forme des techniciens supérieurs en chimie.
Marc Laffitte est un chimiste français né à Nancy le 11 décembre 1929, spécialisé dans la thermodynamique. Il a beaucoup œuvré pour l'enseignement de la chimie. Il est décédé à Cassis le 14 septembre 2016.
La Faculté de chimie de l'université de Belgrade (en serbe cyrillique : Хемијски факултет Универзитета у Београду ; en serbe latin : Hemijski fakultet Univerziteta u Beogradu) est l'une des 31 facultés de l'université de Belgrade, la capitale de la Serbie. Elle a été fondée dans les années 1980. En 2013, son doyen est le professeur Branimir Jovančićević.
La Société chimique du Japon est une association à but non lucratif dont l'objectif est l'avancement de la recherche en chimie et de sa promotion dans la science et l'industrie.
Entre 1921 et 1927, lors de la troisième guerre du Rif au protectorat espagnol au Maroc, l'armée espagnole d'Afrique a utilisé des armes chimiques afin d'écraser la rébellion berbère rifaine menée par Abdelkrim al-Khattabi, chef de la guérilla. Lors de ce conflit, du gaz moutarde a été largué par avions en 1924, un an avant la signature du protocole de Genève « concernant la prohibition d'emploi à la guerre de gaz asphyxiants, toxiques ou similaires et de moyens bactériologiques ». Le gaz a été produit à La Marañosa, près de Madrid, par la Fabrica Nacional de Productos Quimicos, une entreprise grandement mise sur pied par le chimiste allemand Hugo Stoltzenberg, impliqué dans les activités d'armement chimique du gouvernement allemand au début des années 1920.
Le masque à gaz pour cheval,, est un appareil de protection pour chevaux utilisé notamment dans l’Armée française depuis la Première Guerre mondiale pour éviter aux animaux d’inhaler des gaz toxiques sur les champs de bataille.
Le Projet 112 était un programme expérimental d'armes chimiques et biologiques mené sous la conduite du Département de la Défense des États-Unis de 1962 à 1973. Ce projet est né sous l'administration de John Fitzgerald Kennedy et a été autorisé par son secrétaire de la défense Robert McNamara dans le cadre d'un réexamen complet des systèmes de défense des États-Unis. Le nom de Projet 112 fait référence au numéro de ce programme parmi les 150 propositions de ce réexamen autorisé par McNamara. Toutes les composantes de l'armée des États-Unis ainsi que les agences de renseignement (un euphémisme pour la Direction de la science et technologie du Bureau des Services Techniques de la CIA) y ont participé, aussi bien financièrement que directement avec leur personnel. Le Canada et le Royaume-Uni ont également participé à certaines activités du Projet 112. Le Projet 112 s'est d'abord penché sur  l'utilisation d'aérosols pour répandre des produits chimiques et biologiques qui pourraient provoquer des incapacités temporaires contrôlées. Le programme de test devait être mené à grande échelle hors des 48 états des USA continentaux, dans le Pacifique Sud, le Pacifique Central  et l'Alaska ceci de concert avec la Grande-Bretagne, le Canada et l'Australie. Au moins 50 essais ont été réalisés dont au moins 18 concernaient des "imitations" d'agents biologiques (comme le Bacillus subtilis) et au moins 14 concernaient des produits chimiques y compris le gaz sarin et le gaz VX, mais aussi des gaz lacrymogènes et d'autres produits semblables. Les lieux des tests ont été Porton Down (Grande-Bretagne), Ralston (Canada) et au moins 13 navires de guerre de l'US Navy ; les essais embarqués ont été nommés SHAD (pour Shipboard Hazard And Defense soit Défense et Danger Embarqués). Le projet a été coordonné depuis le Deseret Test Center dans l'Utah, à 100 km au sud-ouest de Salt Lake City. Jusqu'en 2005, les informations disponibles sur le Projet 112 sont restées incomplètes.
Hugo Gustav Adolf Stoltzenberg (né le 27 avril 1883 à Strengen – mort le 14 janvier 1974) est un chimiste allemand. Il a notamment développé des armes chimiques pour la branche clandestine liée à la guerre chimique du gouvernement allemand du début des années 1920. Stoltzenberg a été un proche collaborateur de Fritz Haber. Ils ont travaillé ensemble sur l'importation de matériel et la construction d'installations permettant la fabrication d'armes chimiques. Ils ont ainsi participé notamment à la Fabrica Nacional de Productos Quimicos à La Marañosa, près de Madrid.
L'histoire de la chimie est intrinsèquement liée à la volonté de l'Homme de comprendre la nature et les propriétés de la matière, plus particulièrement la façon dont celle-ci existe ou se transforme. L'histoire de la chimie débute avec la découverte du feu qui est la première source d'énergie utilisée par l'homme pour améliorer son quotidien ; éclairage, chauffage, cuisson des aliments etc. La maîtrise du feu a permis de réaliser les premières transformations contrôlées de la matière, notamment la fabrication du verre et de la céramique mais également d'alliages métalliques. L'histoire de la chimie est également marquée par les nombreuses tentatives pour développer une théorie cohérente de la matière parmi lesquels on peut citer les théories atomique de Démocrite et des éléments d'Aristote pendant la période antique ou le développement de l'alchimie au Moyen Âge. La chimie ne se distinguera de cette dernière que vers le XVIIe siècle, notamment par les travaux de Robert Boyle qui applique la méthode scientifique à ses expériences. La publication de son célèbre Sceptical Chymist en 1661 est d'ailleurs parfois considéré dans le monde anglo-saxon comme le point de départ de la chimie moderne. Plus tard, les travaux de Lavoisier sur les lois de la conservation de la masse contribueront à placer définitivement la chimie au rang de science. Actuellement l'interdisciplinarité dans le monde scientifique fait qu'il est parfois difficile de différencier l'histoire de la chimie de celle de la physique ou des sciences de la vie telle que la biochimie.
A Boy and His Atom (« Un garçon et son atome ») est un court-métrage d'animation de l'entreprise américaine IBM, présenté le 1er mai 2013 sur YouTube. Les images, réalisées en positionnant des molécules de monoxyde de carbone sur une plaque de cuivre, représentent un garçon jouant avec un atome pendant 1 minute. Le film est précédé de 30 secondes de mise en situation sur la méthode de production. Afin d'être visibles, les images ont été agrandies par un facteur de 100 millions. Ces images ont été réalisées à l'aide d'un microscope à effet tunnel, appareil permettant d'enregistrer la position des atomes ainsi que de déplacer ceux-ci en formant une interaction entre une pointe métallique et l'atome à déplacer, puis en brisant cette interaction une fois l'atome en place. Ce film, créé entre autres dans le but d'intéresser les enfants à la science, a été certifié comme étant le « film d'animation le plus petit du monde » par le livre Guinness des records.
L'acétarsol est un dérivé organoarsénié pentavalent. C'est un médicament employé au cours du XXe siècle, principalement dans le traitement de la syphilis.
Un alcali, écrit autrefois alkali à la fin du XVIIIe siècle pour marquer l'origine arabe via le latin médiéval, est un terme de l'alchimie puis de la chimie décrivant différents composés chimiques, parfois en mélange, à propriétés dites alcalines ou basiques. Depuis le XVIIe siècle, le terme est employé de manière générique pour désigner des bases, des sels ou des solutions basiques concentrées. En chimie industrielle moderne, un alcali désigne une base forte dans l'eau. L'adjectif associé alcalin, qui indiquait une saveur amère ou agressive typique, la présence d'une base forte ou une fonction basique réduisant l'acidité d'un milieu, a fini par désigner les ions ou atomes de lithium, sodium ou potassium, plus rarement rubidium, césium ou francium de la première colonne du tableau périodique. Ces métaux alcalins ont la propriété de former avec l'oxygène ou l'eau des bases fortes, nommées alcalis. Ainsi une pile alcaline, faisant mouvoir des ions Li+ ou Na+... D'un point de vue pratique et commercial, le terme alcali dénomme encore des produits de nettoyage, agents puissants autorisés pour déboucher les éviers comme toutes autres canalisations résistantes aux bases chimiques ou encore lessive liquide, pour renforcer l'action des lessives ordinaires. Depuis des temps immémoriaux, les alcalis se caractérisent notamment par la propriété de "ramollir les matières organiques" et favoriser leurs dissolutions ultérieures dans l'eau.
L’année internationale de la chimie 2011 (en anglais International Year of Chemistry ou IYC 2011) a été proclamée par l’Assemblée générale des Nations unies en décembre 2008. Le thème est « Chimie - notre vie, notre avenir ». L’AIC 2011 est initiée par l’UICPA et l’UNESCO. Elle a été déclarée pour célébrer les réalisations de la chimie et sa contribution à l’amélioration des conditions de vie de l’humanité. Tout au long de l’année, diverses manifestations sont organisées à travers le monde. L’évènement de clôture de cette célébration a eu lieu à Bruxelles le 1er décembre 2011, marquant le 100e anniversaire de l’attribution du prix Nobel de chimie à Marie Curie, et le 100e anniversaire des congrès Solvay.
Un atome (grec ancien ἄτομος [átomos], « insécable ») est la plus petite partie d'un corps simple pouvant se combiner chimiquement avec un autre. Les atomes sont les constituants élémentaires de toutes les substances solides, liquides ou gazeuses. Les propriétés physiques et chimiques de ces substances sont déterminées par les atomes qui les constituent ainsi que par l'arrangement tridimensionnel de ces atomes. Contrairement à ce que leur étymologie suggère, les atomes ne sont pas indivisibles, mais sont eux-mêmes constitués de particules subatomiques. Les atomes comprennent un noyau, qui concentre plus de 99,9 % de leur masse, autour duquel se distribuent des électrons, qui forment un nuage 10 000 à 100 000 fois plus étendu que le noyau lui-même,, de sorte que le volume d'un atome, grossièrement sphérique, est presque entièrement vide. Le noyau est formé de protons, porteurs d'une charge électrique positive, et de neutrons, électriquement neutres ; l'hydrogène fait exception, car le noyau de son isotope 1H, appelé protium, ne contient aucun neutron. Protons et neutrons, également appelés nucléons, sont maintenus ensemble dans le noyau par la liaison nucléaire, qui est une manifestation de l'interaction forte. Les électrons occupent des orbitales atomiques en interaction avec le noyau via la force électromagnétique. Le nuage électronique est stratifié en niveaux d'énergie quantifiés autour du noyau, niveaux qui définissent des couches et des sous-couches électroniques ; les nucléons se distribuent également selon des couches nucléaires, bien qu'un modèle approché assez commode popularise la structure nucléaire d'après le modèle de la goutte liquide. Plusieurs atomes peuvent établir des liaisons chimiques entre eux grâce à leurs électrons. D'une manière générale, les propriétés chimiques des atomes sont déterminées par leur configuration électronique, laquelle découle du nombre de protons de leur noyau. Ce nombre, appelé numéro atomique, définit un élément chimique. 118 éléments chimiques sont reconnus par l'Union internationale de chimie pure et appliquée (IUPAC) depuis le 18 novembre 2016. Les atomes d'éléments différents ont des tailles différentes, ainsi généralement que des masses différentes, bien que les atomes d'un élément chimique donné puissent avoir des masses différentes selon les isotopes considérés. Les atomes les plus lourds, ou dont le noyau présente un déséquilibre trop important entre les deux types de nucléons, tendent à devenir plus instables, et sont alors radioactifs ; le plomb 208 est l'isotope stable le plus lourd. La théorie atomiste, qui soutient l'idée d'une matière composée de « grains » indivisibles (contre l'idée d'une matière indéfiniment sécable), est connue depuis l'Antiquité, et fut notamment défendue par Leucippe et son disciple Démocrite, philosophes de la Grèce antique, ainsi qu'en Inde, plus antérieurement, par l'une des six écoles de philosophie hindoue, le vaisheshika, fondé par Kanada. Elle fut disputée jusqu'à la fin du XIXe siècle et n'a plus été remise en cause depuis lors. L'observation directe d'atomes n'est devenue possible qu'au milieu du XXe siècle avec la microscopie électronique en transmission et l'invention du microscope à effet tunnel. C'est ainsi sur les propriétés des atomes que reposent toutes les sciences des matériaux modernes, tandis que l'élucidation de la nature et de la structure des atomes a contribué de manière décisive au développement de la physique moderne, et notamment de la mécanique quantique.
Hieronymus Brunschwig, Jérôme Brunschwig (né vers 1450 à Strasbourg, mort vers 1512 à Strasbourg) est un apothicaire et un chirurgien alsacien, germanophone. Il est l'auteur d'ouvrages pratiques sur les techniques chirurgicales et les instruments et procédés de distillation de matière médicale. Écrit en allemand (plutôt qu'en latin) pour se mettre à la portée du plus grand nombre, il s'adresse directement à ses lecteurs et leur indique comment procéder pas à pas par eux-mêmes. Les deux tomes du Livre de la Distillation (publiés en 1500 et 1512) sont probablement les premiers manuels de distillation destinés aux apothicaires publiés en Europe. Ouvrages à succès, ils eurent une influence profonde à travers de nombreuses rééditions, traductions et imitations. Leur célébrité tient aussi à la qualité du travail de l'imprimeur-éditeur Johann Grüninger qui employait une équipe de graveurs de planches xylographiques afin de pourvoir ses publications de nombreuses illustrations de belles factures. En tant qu'apothicaire, Brunschwig concevait la distillation comme une technique de purification des substances permettant d'en extraire la partie pure, thérapeutiquement efficace, de la partie impure, toxique. Il prôna le remplacement des formes galéniques traditionnelles de la matière médicale par une forme distillée. Sa notion de la distillation influencée par Roquetaillade et l'usage qu'il en fit en pharmacologie, marque le début du passage de l'usage des herbes médicinales aux produits chimiques.
Le Liber de arte distillandi de simplicibus ou Le Petit Livre de la Distillation (Kleines Destillierbuch), est un manuel de distillation de matières médicales, écrit pour les apothicaires, par Hieronymus Brunschwig. C'est le premier manuel de ce genre à être imprimé en Europe en 1500. Brunschwig, un apothicaire et chirurgien alsacien, germanophone, concevait la distillation comme une technique de purification des substances végétales ou animales permettant d'en extraire la partie pure, thérapeutiquement efficace, de la partie impure, toxique. Cet ouvrage donne dans une première partie, une série d'instructions précises pour construire et utiliser tous les ustensiles et les appareils de distillation. Dans une seconde partie, il offre une liste de 305 notices sur les distillats de substances végétales et animales, avec leurs indications thérapeutiques. L'ouvrage est agréablement illustré de planches de xylogravures instructives. L'œuvre traduite en néerlandais en 1517, en anglais en 1527, en tchèque en 1559, et de multiples fois rééditées, fit autorité pendant tout le XVIe siècle.
Le carbone 14,, est un isotope radioactif du carbone,,. De nombre de masse 14, il est noté 14C. Sur Terre, il est formé lors de l'absorption de neutrons par les atomes d'azote de la stratosphère et des couches hautes de la troposphère. Les neutrons proviennent de la collision des rayons cosmiques avec les atomes de l'atmosphère, notamment l'oxygène. Son unique mode de désintégration se fait par émission d'une particule bêta de 156 keV en se transmutant en azote 14N ; avec une période radioactive de 5 730 ± 40 ans. Le carbone 14 a longtemps été le seul radioisotope du carbone à avoir des applications. Pour cette raison, il était appelé radiocarbone. Un gramme de carbone 14 pur présente une activité de 164,9 GBq.
Cette chronologie de la chimie recense les travaux, idées, inventions et expériences qui ont changé de manière significative la compréhension de la composition de la matière et de ses interactions, c'est-à-dire le domaine scientifique de la chimie. On considère généralement que la chimie en tant que science moderne débute avec Robert Boyle ; cependant ses racines sont beaucoup plus anciennes. Les idées les plus anciennes qui seront plus tard incorporées dans la chimie moderne proviennent essentiellement de deux sources : les philosophes de la Nature tels que Aristote et Démocrite qui ont utilisé un raisonnement déductif pour tenter d'expliquer les phénomènes naturels et les alchimistes qui ont utilisé des techniques expérimentales dont le but principal était la transmutation des métaux vils en or et argent. C'est au XVIIe siècle que se fera la synthèse des idées issues de ces deux disciplines ; la déduction et l'expérimentation amenant au développement d'une méthode de pensée appelée méthode scientifique. Avec l'introduction de la méthode scientifique, la chimie en tant que science moderne était née. Parfois appelée la science centrale , l'étude de la chimie est fortement influencée par les autres domaines scientifiques tout en ayant elle-même une forte influence dans ceux-ci. C'est ainsi que des évènements et découvertes considérés comme fondamentaux pour la compréhension de la chimie sont également considérés comme des découvertes clés dans de nombreux autres domaines tels que la physique, la biologie, l'astronomie, la science des matériaux...
Étienne de Clave (1587-1645) est un chimiste médecin, auteur d'ouvrages de chimie et de philosophie chimique, célèbre par le scandale provoqué par les thèses chimiques contre Aristote qu'il soutint à Paris en 1624. Il annonçait alors vouloir réfuter la physique d’Aristote par les moyens de la chimie. Dans ses ouvrages, il élabore une théorie des cinq éléments (ou principes) constitutifs de tous les corps mixtes. Ce sont des corps simples, matériels, extractables des corps mixtes, par divers procédés chimiques de séparation. Il les nomme: le phlegme, l'esprit, l'huile, le sel et la terre. En opposition frontale avec la doctrine philosophique des Quatre éléments d'Aristote, il entend donner les procédures expérimentales permettant d'extraire ces éléments à l'état pur « par la résolution des mixtes en ses parties intégrantes et constituantes » plutôt que « par une milliasse de raisonnements fantasmatiques et chimériques de Péripatéticiens » (Nouvelle Lumière philosophique, p. 470). Cette théorie novatrice sera ensuite reprise par les chimistes français jusqu'à la fin du XVIIe siècle. Sa philosophie chimique s'inscrit explicitement dans le courant anti-aristotélicien représenté à ses yeux par Patricius, Sébastien Basson, ou encore Campanella et Gassendi, qu'il rencontra certainement à Paris. Il reste toutefois fidèle à la tradition alchimique, notamment lorsqu'il oppose les paracelsiens aux vrais chimistes que sont pour lui, les « herméticiens ». Face aux disciples ignorants et pédants de Paracelse se trouvent les « vrais et légitimes disciples et imitateurs de ce grand Hermès Trismégiste ». Il partage avec Descartes son refus de la scolastique et sa défense du principe de la libre critique, mais à la différence de ce dernier, il entend édifier une théorie de la matière sur des principes strictement matériels en s'appuyant sur des expériences de chimie.
Le Congrès de Karlsruhe de 1860 est un congrès scientifique international de chimie qui s'est tenu dans la ville de Karlsruhe du 3 au 5 septembre 1860. Initié par August Kékulé, ce congrès réuni plus de 120 chimistes dont Stanislao Cannizzaro, Jean-Baptiste Dumas, Dimitri Mendeleïev pour discuter des notions d'atome, molécule, équivalence et s'accorder sur une harmonisation des notations employés. Ce Congrès est le premier de cette envergure dans le domaine de la chimie et est parfois également considéré comme la première conférence scientifique internationale moderne.
Les congrès Solvay (aussi appelés conseils Solvay et conférences Solvay, ou encore comités Solvay) sont des conférences scientifiques en physique et en chimie qui se sont tenues depuis 1911. Au début du XXe siècle, ces conseils, réunissant les plus grands scientifiques de l'époque, permirent des avancées importantes en mécanique quantique. Ils furent organisés grâce au mécénat d'Ernest Solvay, un chimiste et industriel belge. Les Conseils Solvay sont organisés depuis la Seconde Guerre mondiale selon un cycle de trois ans : conseil de physique la première année, aucune conférence la deuxième et conseil de chimie la troisième. Ce cycle a cependant été parfois perturbé.
La cornue est un récipient utilisé dans un laboratoire de chimie pour la distillation ou la distillation sèche de substances. Il est généralement en verre, parfois en terre ou en métal. Il comprend un vase sphérique contenant la substance à chauffer et un long col étroit, courbé vers le bas. Celui-ci joue le rôle de condenseur ; un récipient placé sous son extrémité recueille les vapeurs condensées. La cornue a été inventée au IXe siècle par l'alchimiste Geber (Jâbir ibn Hayyân). Les cornues étaient largement utilisées par les alchimistes et beaucoup de ces récipients apparaissent dans des dessins de leur laboratoire. De nombreux chimistes célèbres en firent usage, comme Antoine Lavoisier et Jöns Jacob Berzelius. La cornue n'est pratiquement plus utilisée, du fait du développement de ballons et réfrigérants modernes.
Le Cours de chimie d'Étienne de Clave fut publié en 1646, probablement après le décès de son auteur, à partir de notes manuscrites retrouvées par l'éditeur. L'auteur est un chimiste médecin, connu pour ses compétences en matière de travaux de laboratoire, qui donna des cours privés de chimie à Paris pour les médecins et apothicaires. Il publia aussi de son vivant, deux ouvrages de réflexions théoriques sur les fondements de la chimie. Le présent cours expose sa doctrine des cinq éléments, qui deviennent avec lui, des corps simples, qui en principe, peuvent être extraits des corps mixtes par des opérations de laboratoire. La majeure partie de l'ouvrage est consacrée à la présentation de recettes chimiques : préparation de médicaments tirés de végétaux par distillation, préparation de sels à partir de minéraux et préparation des métaux.
Le Cours de chymie est l’œuvre majeure du chimiste apothicaire Nicolas Lémery, ayant vécu lors du règne de Louis XIV. La première édition de 1675, distribuée dans son laboratoire, alors qu'il n'avait pas encore trente ans et venait de finir sa formation à l'Université de Montpellier, sera durant les quarante années qui lui resteront à vivre, en permanence révisée, complétée et rééditée. L'ouvrage est un manuel de cours accompagnant l'enseignement que Lémery donnait dans le laboratoire de son apothicairerie du Quartier latin de Paris. Il connut un succès inégalé pour ce genre de sujet, en se vendant selon Fontenelle, « comme un ouvrage de Galanterie ou de Satire ». Se sont succédé onze éditions officielles du vivant de l'auteur, avec certaines éditions « contrefaites » (Fontenelle) imprimées par plusieurs éditeurs-imprimeurs différents, plusieurs éditions commentées posthumes et de multiples éditions en anglais, allemand, italien, espagnol, néerlandais et même latin. Le célèbre éloge de Lémery fait à l'Académie royale des sciences par Fontenelle donne la mesure de la perception que certains savants de son époque pouvaient avoir de son œuvre. Il écrit « La chimie avait été jusque-là une science, où, pour emprunter ses propres termes, un peu de vrai était tellement dissous dans une grande quantité de faux, qu'il en était devenu invisible et tous deux presque inséparables.[…] M. Lémery fut le premier qui dissipa les ténèbres naturelles ou affectées de la Chimie, qui la réduisit à des idées plus nettes et plus simples, qui abolit la barbarie inutile de son langage, qui ne promit de sa part que ce qu'elle pouvait et ce qu'il la connaissait capable d'exécuter, et de là vint la grand succès ».  Le Cours de chymie de Lémery se présente comme une synthèse des connaissances empiriques popularisées par la série de « cours de chimie » publiés au cours du XVIIe siècle. Mais à l'encontre de ses prédécesseurs, le cours de Lémery est immédiatement accessible aux lecteurs contemporains (Metzger). Pour comprendre sa théorie, il n'est point utile de s'initier aux philosophies chimiques plus ou moins étranges, que partageaient les apothicaires chimistes qui l'ont précédé. Lémery évacue tout semblant de spéculations philosophiques et s'en tient à la seule expérience. À l'instar d'Étienne de Clave, il entend appréhender la structure de la matière sur les seules bases de l'expérience chimique. D'emblée, dès les premières pages, il assure de « ne recevoir pour fondement que celui qui est palpable et démonstratif…Les belles imaginations des autres Philosophes touchant leurs principes physiques élèvent l'esprit par de grandes idées, mais elles ne leur prouvent rien démonstrativement » (Cours, p. 5). Pour interpréter les expériences de laboratoire, il adopte la théorie des Cinq principes - une tentative pour comprendre la structure de la matière par les seuls moyens de la chimie, totalement à l'opposé de l'approche philosophique qui prévalait depuis l'Antiquité et même pourrait-on dire sans exagération, qui a prévalu de tout temps et en tout lieu. Lémery est un héritier de la démarche d'Étienne de Clave qui visait à réfuter la physique d'Aristote par les seuls moyens de la chimie expérimentale. Toutefois, tous les efforts des chimistes du XVIIe siècle montrent que les techniques de laboratoire de l'époque ne permettaient pas d'atteindre l'objectif fixé de déterminer les principes (ou éléments indécomposables) composant les mixtes. Il fallut une réorientation des interprétations des observations de laboratoire par des modèles corpusculaires mécanistes, alliés à des pesées précises des réactifs, pour suivre correctement le devenir des corps simples et ouvrir la voie à une meilleure compréhension des constituants de la matière. Lémery utilise a minima la théorie des cinq Principes qui avait montré ses insuffisances. À défaut de lui fournir une terminologie cohérente, elle lui offrait une hypothèse heuristique qui pouvait guider l'analyse des mixtes en éléments plus simples. L'originalité de la pensée de Lémery et une des clés de son succès, réside dans son modèle explicatif corpusculaire et mécaniste des « pointes et pores » , qu'il met en œuvre pour donner une compréhension imagée de ce qu'est une réaction chimique, et particulièrement de la réaction acide-base. Il commence à dégager la notion de réaction chimique et de rapport pondéral à respecter entre réactifs pour obtenir un résultat optimum. Lémery est un empiriste qui toujours essaya de s'en tenir au mieux à l'expérience de laboratoire telle qu'elle pouvait être interprétée avec les hypothèses de la théorie des Cinq principes ou les modèles corpusculaires. Il rejeta aussi la doctrine astrologique comme mal fondée et la quête de la pierre philosophale comme vaine.
L'hypothèse du cyclol est le premier modèle structurel (en) du repliement des protéines globulaires.  Portail de la chimie  Portail de la biochimie
Dmitri Mendeleïev publie le premier tableau périodique des éléments en 1869, basé sur des propriétés qui apparaissent avec régularité en fonction de la masse des éléments. Les éléments prédits par Mendeleïev sont un ensemble d'éléments chimiques dont l'existence a été prédite par Dmitri Mendeleïev alors qu'ils n'étaient pas découverts.
La Faraday Society était une société britannique chargée de l'étude de la chimie physique, fondée en 1903 et nommée en l'honneur du chimiste britannique Michael Faraday. Elle fusionna avec des organisations similaires en 1980 afin de former la Royal Society of Chemistry. Elle publia les Faraday Transactions (en) de 1905 à 1971, lorsque la Royal Society of Chemistry en prit le contrôle. Les conférences appelées Faraday Discussions étaient organisées par la Faraday Society : leurs contenus étaient publiés sous le même nom. La publication incluait le débat autour d'un article en plus de l'article lui-même. Lors de la réunion, le temps imparti au débat était plus important que celui imparti à la présentation de l'article, les articles étant distribués avant la conférence. Ces conférences existent toujours, organisées par la Royal Society of Chemistry.
Avant 1828, la force vitale, en latin vis vitalis, est une cause mystérieuse et unique censée être capable d'édifier in vivo des composés comme l'acide acétique ou l'éthanol. En 1828, cette théorie est battue en brèche par Friedrich Wöhler, qui parvient à synthétiser l'urée, considérée comme une molécule organique, à partir d'une solution de cyanate d'ammonium qui est un réactif minéral. Mais la théorie subsiste jusqu'à la fin du siècle sous une forme plus faible : même si la chimie ordinaire explique la formation des molécules organiques, c'est la force vitale qui expliquerait leur agencement complexe caractéristique des êtres vivants. Une telle théorie implique la possibilité d'une véritable génération spontanée, par application de la « force vitale » à un milieu propice. Les expériences de Louis Pasteur, permettant de relier tous les phénomènes mystérieux à l'existence de microbes, et l'impossibilité de mettre en évidence une génération spontanée une fois prise toutes les précautions utiles (élimination des « germes ») font définitivement abandonner cette théorie par la science.
Magia naturalis (la Magie naturelle) est l'ouvrage principal de l'érudit napolitain Giambattista Della Porta, publié d'abord en 1558 en quatre livres puis en 1586 en vingt livres. C'est la compilation de tous les phénomènes fantastiques et des croyances les plus merveilleuses qu'il a récoltés au cours de sa vie et qu'il a tenté d'arracher à la magie divinatoire en leur donnant une justification naturaliste ou en les légitimant par des références littéraires classiques.
Jean Grosse (?-1744), en allemand Johann Gross, est un chimiste d'origine allemande entré à l'Académie des sciences en 1731. Ses travaux, peu nombreux mais de valeur, ont été publiés dans les Mémoires de l'Académie royale des sciences .
L'électrochimie est une discipline de la chimie qui a connu de nombreux changements au cours de son évolution depuis les premiers principes des aimants au début du XVIe siècle et du XVIIe siècle pour ensuite aboutir à des théories utilisant les notions de conductivité, des charges électriques ainsi que des méthodes mathématiques. Le terme électrochimie fut utilisé pour décrire les phénomènes électriques au XIXe siècle et au XXe siècle. Au cours des dernières décennies, l'électrochimie a été un large domaine de la recherche dans lequel ont fait partie l'étude des batteries et des piles à combustible et le développement de techniques pour éviter la corrosion des métaux et améliorer les techniques de raffinage grâce à des électrolyses et les électrophorèses.
Cet article présente, dans l'ordre chronologique, les dates auxquelles ont été découverts les éléments chimiques, ainsi que les auteurs de ces découvertes. Certains éléments sont connus depuis des temps immémoriaux, mais la plupart ont été découverts au cours de l'époque contemporaine.
L'histoire de la mécanique quantique commence essentiellement par les évènements suivants : en 1838 avec la découverte des rayons cathodiques par Michael Faraday, durant l'hiver 1859-1860 avec l'énoncé du problème du corps noir par Gustav Kirchhoff, la suggestion en 1877 de Ludwig Boltzmann que les états d'énergie d'un système physique puissent être discrets, et l'hypothèse quantique de Max Planck stipulant que tout système atomique irradiant de l'énergie peut être divisé en « éléments d'énergie » discrets ε tels que chacun de ces éléments est proportionnel à la fréquence                         ν                 {\displaystyle \nu }    avec laquelle chacun irradie individuellement son énergie, comme défini dans la formule suivante :                         ϵ         =         h         ν                          {\displaystyle \epsilon =h\nu \,}    où h est une valeur numérique appelée constante de Planck. En 1905, afin d'expliquer l'effet photoélectrique (connu depuis 1839), c'est-à-dire que l'irradiation lumineuse de certains matériaux puissent en éjecter des électrons, Albert Einstein postula, en se basant sur l'hypothèse quantique de Planck, que la lumière elle-même se composait de particules individuelles quantifiées, appelées par la suite photons (à partir de 1926). L'expression de « mécanique quantique » fut utilisée pour la première fois en 1924 par Max Born dans son article Zur Quantenmechanik. Dans les années qui suivirent, cette base théorique commença à être appliquée à la structure, à la réactivité et à la liaison chimiques.
La poudre à canon est le premier explosif chimique découvert, et le seul connu jusqu'à l'invention de la nitrocellulose, de la nitroglycérine et de la TNT au XIXe siècle. Cependant, avant l'invention de la poudre à canon, de nombreuses munitions incendiaires ont été utilisées, dont le feu grégeois.
L’histoire de la production d'hydrogène débute avec les expériences de Cavendish en 1766. L'alchimiste Paracelse, qui vivait au XVIe siècle, a entrevu le gaz ; un siècle plus tard, Robert Boyle parvint à le recueillir, mais ne le distingua pas de l’air ordinaire. En 1603, Théodore de Mayerne l’enflamma, et John Mayow, vers la fin du XVIIe siècle, le distingua de l’air. Enfin, au commencement du XVIIIe siècle, Nicolas Lémery en constata aussi l’inflammabilité. Ce n’est qu’en 1766 que ce gaz fut étudié par Cavendish. En 1783, Antoine Lavoisier découvre que l’« air inflammable » de Cavendish, qu’il baptise hydrogène (du grec « formeur d’eau »), réagit avec l’oxygène pour former de l’eau. La découverte de l’« air inflammable » comme on l’appelait est donc ancienne. Théodore de Mayerne et Paracelse l’obtenaient par réaction entre l’« huile de vitriol » (de l’acide sulfurique) diluée et versée sur du fer ou du zinc. En 1870, le gaz produit pour les besoins des ballons à gaz n’utilise pas d’autre moyen. Au XXIe siècle, le gros du dihydrogène requis est produit à partir du méthane présent dans le gaz naturel, par catalyse hétérogène.
L'Histoire du gaz manufacturé se rapporte à la période qui a précédé l'avènement de l'électricité, où les villes furent éclairées et ensuite chauffées par des gaz manufacturés, c'est-à-dire fabriqués par « distillation » dans des usines à gaz ou des cokeries. Cette période a vu naître les premiers grands groupes énergétiques. Apparus avec la chimie moderne, et les découvertes d'Antoine Lavoisier, les recherches sur les gaz manufacturés initiées par Philippe Lebon, et William Murdoch permirent par la suite l'essor de la carbochimie. Les gaz de synthèse et les opérations modernes de gazéification sont les descendants des opérations réalisées au XIXe siècle pour obtenir les gaz manufacturés. L'électricité devient la principale source d'énergie pour l'éclairage à partir de 1880. À partir de 1920 aux États-Unis, et 1960 en Europe, le gaz naturel remplace le gaz manufacturé, dans la plupart de ses applications fermant de facto l'ère des gaz manufacturés : en France, la dernière usine à gaz, celle de Belfort en Franche-Comté, fermera en 1971.
En chimie, l'histoire du concept de molécule retrace les origines du concept ou de l'idée de l'existence, dans la nature, d'une structure formée par la liaison de deux atomes ou plus, selon lesquels les structures de l'Univers sont établies. En ce sens, le concept d'objets fondamentaux semblable aux concepts modernes de molécule et d'atome trouve ses origines au Ve siècle av. J.-C. avec le philosophe grec Leucippe qui soutenait le fait que tout l'Univers se compose d'atomes et de vides. Cette idée sera développée plus tard, vers 450 av. J.-C. par le philosophe grec Empédocle, qui fonde la théorie des quatre éléments : le feu, l'air, l'eau, la terre, ainsi que les forces qui leur sont associées : l'attraction et la répulsion. Sur la base de ce raisonnement (alors purement philosophique), de nombreux scientifiques spéculeront tout au long de l'histoire sur la manière dont les éléments ou les atomes peuvent interagir entre eux en un système cohérent.
Fritz Hofmann (né Friedrich Carl Albert Hofmann le 2 novembre 1866 à Kölleda, Allemagne - 22 octobre 1956 à Hanovre, Allemagne) est un chimiste allemand. Il est surtout connu pour ses travaux sur le caoutchouc synthétique.
L’iatrochimie (mot dérivé du grec ιατρός, iatrós, « médecin » et χημεία, chemeia, « chimie » pouvant se traduire par médico-fonderie) ou chimiatrie, désigne l’école de pensée de Paracelse, célèbre médecin du XVIe siècle. Doctrine empreinte d'hermétisme s'opposant dès ses débuts au galénisme, elle professe une pathologie fondée sur le désaccord entre le corps humain (microcosme) et son environnement (macrocosme). Elle est à l'origine de la pharmacopée chimique, et l'une des sources de la pharmacopée moderne.
L'Institut d'histoire des sciences est une institution américaine qui promeut la compréhension de l'histoire des sciences. Situé à Philadelphie, en Pennsylvanie, l'organisme regroupe une bibliothèque, un musée, des archives, un centre de recherche et un centre de conférences. L'Institut d'histoire des sciences a été fondé en 1982 par le biais d'une coentreprise formée de l'American Chemical Society et de l'Université de Pennsylvanie en tant que Centre d'histoire de la chimie. L'American Institute of Chemical Engineers en est devenu membre cofondateur en 1984. Le Centre a été rebaptisé Chemical Heritage Foundation en 1992 et a déménagé deux ans plus tard à l'emplacement actuel de l'institution, sis au 315 Chestnut Street dans la vieille ville de Philadelphie (en). Le 1er décembre 2015, la Chemical Heritage Foundation a fusionné avec la Fondation des sciences de la vie (en), créant une organisation qui couvre "l'histoire des sciences de la vie et de la biotechnologie ainsi que l'histoire des sciences de la chimie et de l'ingénierie",. À compter du 1er février 2018, l'organisme a été rebaptisé "Institut d'histoire des sciences", afin de refléter ses intérêts historiques plus vastes, allant de la chimie et du génie aux sciences de la vie et à la biotechnologie. L'Institut se concentre non seulement sur l'histoire de la chimie, mais aussi sur l'histoire des sciences, l'histoire de la technologie, les tendances de la recherche et du développement, l'impact de la science sur la société et les relations entre la science et l'art, entre autres sujets. Il soutient une communauté de chercheurs et un programme d'histoire orale. En 2012, c'était le plus grand donateur américain de bourses de recherche pour l'histoire de la science,.
En thermodynamique, la loi de Dulong et Petit est une loi empirique selon laquelle « la capacité thermique isobare molaire                                    C                        p                                     {\displaystyle C_{p}}    des éléments solides est voisine de                         3         R                 {\displaystyle 3R}    », soit 25 J K‑1 mol−1. Pour la plupart des éléments elle est assez bien vérifiée à température ambiante.
La loi de Kopp ou loi de Kopp-Neumann a pour énoncé : « La capacité calorifique d'un composé chimique à l'état solide est la somme des capacités calorifiques des éléments qui le composent ». Elle donne une estimation de la capacité calorifique des composés chimiques des métaux, ainsi que de celle des alliages, à partir de celle des éléments. D'après la loi de Dulong et Petit, la contribution de chaque atome à la capacité calorifique molaire Cp est environ égale à 3R (R est la constante des gaz parfaits) d'où, pour un composé chimique comportant m atomes, la loi de Kopp : Cp = 3 m R. En complément de la loi de Dulong et Petit, elle a été utilisée par Cannizzaro pour étayer la théorie atomique.
La loi des proportions définies est une loi pondérale énoncée par Louis Proust, selon laquelle lorsque deux ou plusieurs corps simples s'unissent pour former un composé défini, leur combinaison s'effectue toujours selon un même rapport pondéral. Cette loi constitue, avec la loi des proportions multiples, la base de la stœchiométrie en chimie. On peut, par exemple, mesurer que, quelle que soit la masse d'eau considérée, le rapport entre la masse d'hydrogène et la masse d'oxygène entrant dans sa composition est toujours de 1 pour 8. Cette loi affirme l'invariabilité des proportions massiques des éléments combinés au sein d'une espèce chimique donnée. L'énoncé de cette loi, qui met sur la voie de la notion de masse atomique, caractéristique de chaque élément, constitue l'une des étapes de la construction de la théorie atomique. La loi des proportions multiples est une loi énoncée par John Dalton : si deux éléments peuvent se combiner en donnant plusieurs substances différentes, les rapports de masse du premier élément qui se lie à une masse constante de l'autre ont entre eux un rapport de nombres entiers. Exemple : l'azote et l'oxygène donnent les substances : NO, N2O, NO2, N2O3 et N2O5. La loi des proportions multiples indique que dans ce cas, les différentes proportions d'oxygène par rapport à l'azote sont dans des rapports de nombres entiers. On constate que la proportion d'oxygène dans NO2 est le double de ce qu'elle est dans NO. N2O3 est le triple de ce que l'on trouve dans N2O.  Portail de la chimie
La loi des proportions multiples précise la loi des proportions définies. Quelques notations la rendent plus facile à comprendre qu'un énoncé abstrait. Étant donné un composé chimique constitué de deux éléments A et B, par exemple l’oxygène et l'azote, r est le rapport m(A)/m(B) des masses de A et B mesurées pour une certaine quantité de ce composé. Considérons deux composés chimiques différents de A et B, avec les rapports respectifs r1 et r2. La loi des proportions multiples s'énonce : Le rapport r1/r2 est égal au rapport de deux nombres entiers petits. En 1803 John Dalton a établi cette loi et il en a tiré l'hypothèse atomique : dans une molécule donnée chaque type d'atome apporte une ou plusieurs fois sa masse et le rapport r2/r2 pour deux molécules est le rapport entre les nombres d'atomes dans chacune. Soit par exemple une certaine quantité de deux oxydes d'azote. On y mesure les valeurs respectives des rapports oxygène /azote r1 = 2,29 et r2 = 2,86 d'où r1/r2 = 0,80 = 4/5. Il s'agit ici de NO2 et N2O5 avec le rapport r1/r2 = 2/2,5 = 0,8. En 1808,William Hyde Wollaston, surtout connu pour ses découvertes du palladium et du rhodium, a vérifié la loi des proportions multiples pour les oxalates, les sulfates et les carbonates alcalins. Wollaston a aussi prévu que, autour d'un atome central, quatre autres atomes devraient former un arrangement tétraédrique et il a ainsi anticipé la stéréochimie créée en 1874 par van 't Hoff avec le carbone tétraédrique et par Le Bel avec l'explication de l'isomérie optique.
La mauvéine fut le premier colorant industriel synthétique. Son nom chimique est l'acétate de 3-amino-2,±9-diméthyl-5-phényl-7-(p-tolylamino)phénazinium. Sa formule est C26H23N4+X− (pour la mauvéine A) et C27H25N4+X− (pour la mauvéine B).
Un modèle atomique est une représentation théorique des propriétés de l'atome. En effet, l'atome est très petit et, même s'il existe au XXIe siècle des appareils qui permettent de l'observer et de le manipuler directement, il est difficile de cerner l'ensemble de ses propriétés.
Le modèle atomique de Rutherford ou modèle planétaire de l'atome est un modèle physique proposé en 1911 par Ernest Rutherford pour décrire la structure d'un atome. Ce modèle fait suite au modèle atomique de Thomson (ou « modèle du plum pudding »), proposé en 1904 par Joseph John Thomson (dont Rutherford était l'élève), et qui fut invalidé à la suite de l'expérience de Rutherford ou « expérience de la feuille d'or » en 1909. Le modèle de Rutherford est dans la continuité du modèle « saturnien » de l'atome développé par le physicien japonais Hantarō Nagaoka en 1904, mais en diffère quelque peu.
Le musée de la chimie, situé à Jarrie, en Isère, est un musée communal retraçant l'histoire, les techniques et les applications de la chimie, cœur du patrimoine industriel local. Le musée se trouve dans le même bâtiment qui abrite la mairie de Jarrie, autrefois propriété de la famille de gantiers Jouvin, en particulier dans le parc du Clos Jouvin, créé vers 1880 par l'architecte paysagiste Gabriel Luizet, réalisateur de plusieurs parcs reconnus monuments historiques.
Le National Chemical Laboratory (ou NCL) est un laboratoire de recherche situé à Pune (État du Maharashtra, Inde) fondé en 1950. Mi-privé mi-public, cet établissement très bien équipé regroupe environ 800 personnes (chercheurs, docteurs, étudiants en thèse) dans les différents domaines de la chimie (chimie macromoléculaire, chimie organique, biochimie, catalyse, chimie à haute pression, etc.). Comme beaucoup de laboratoires et autres organismes en Inde, toute une zone de Pune lui est réservée où l'on trouve poste, banques, logements, commerces, etc.  Portail de la chimie  Portail du monde indien
Les National Historic Chemical Landmarks (en français : Sites nationaux sur l'histoire de la chimie) sont un projet initié par l'American Chemical Society, en 1992, et qui en a attribué le statut a plus de 60 sites depuis lors. Les lieux ou réalisations retenus sont marquants de l'histoire de la chimie et des hommes qui contribuèrent à son évolution aux États-Unis et dans le monde.
En chimie et en physique, le nombre d'Avogadro, (ou constante d'Avogadro), nommé en l'honneur du physicien et chimiste Amedeo Avogadro, noté NA, est défini comme le nombre d'entités élémentaires (atomes, molécules, ou ions en général) qui se trouvent dans une mole de matière. Sa valeur correspond par convention au nombre d'atomes de carbone dans 12 grammes de carbone 12. Elle est mesurée à : NA = 6,022 140 857(74) × 1023 mol−1. Jean Perrin a décrit dans son livre de vulgarisation important Les atomes (1913) les expériences concordantes qui ont permis d'approcher le nombre supposé par Avogadro et ainsi d'asseoir la théorie atomique. Néanmoins, la valeur de ce nombre est encore très imprécise et les chimistes d'aujourd'hui travaillent pour améliorer l'incertitude de mesure. Si N(X) désigne le nombre d'entités X d'un échantillon donné d'un corps pur, et si n(X) désigne la quantité de matière d'entités X du même échantillon, on obtient la relation :                         n         (         X         )         =                                                N               (               X               )                                         N                                                   A                                                                                  {\displaystyle n(X)={\frac {N(X)}{N_{\mathrm {A} }}}}   . Le nombre d'Avogadro correspond également au facteur de conversion entre le gramme et l'unité de masse atomique (u) :                         1                             g                  =                    N                                       A                                          ⋅         u                 {\displaystyle 1\;\mathrm {g} =N_{\mathrm {A} }\cdot u}
Marius Paul Antoine Gabriel Otto (Nice, 1870 - 1939) est un chimiste et chef d'entreprise français.
Papyrus de Leyde et papyrus de Stockholm  Le papyrus de Leyde et le papyrus de Stockholm sont des compilations de recettes techniques relatives à l'argent, l'or, les pierres et les étoffes. Ils ont été écrits en grec aux environs du IIIe siècle, probablement en Haute-Égypte à l'époque romaine. Témoins du processus mal connu qui a conduit de l'artisanat vers l'alchimie, ces recettes de procédés d'imitation de métaux précieux et de pierres précieuses nous apportent un grand nombre d'informations sur les techniques de l'Antiquité, d'un intérêt majeur pour l'histoire de la chimie.
Paracelse, né Philippus Theophrastus Aureolus Bombastus von Hohenheim en 1493 à Einsiedeln (en Suisse centrale) et mort le 24 septembre 1541 à Salzbourg (aujourd'hui en Autriche) est un médecin, philosophe mais aussi théologien laïque suisse, d’expression allemande (de dialecte alémanique). Ce fut un médecin-chirurgien innovateur en thérapeutique, un philosophe de la nature concevant les phénomènes naturels comme des processus (al)chimiques de transformation, un théoricien des forces surnaturelles et un rebelle s'en prenant parfois avec virulence aux institutions et aux traditions. Paracelse, philosophe, est un théoricien du Grand Tout, toujours animé par le désir de pénétrer la nature profonde des choses, attiré aussi bien par la Nature que par le royaume de Dieu. Sa pensée foisonnante, exubérante, est à l'image de l'homme rebelle, truculent, profondément croyant, se pensant sur la fin de sa vie, comme le médecin-prophète du dernier âge (Pierre Deghaye). Paradoxalement, sa philosophie de la nature d'inspiration chrétienne et alchimiste, centrée sur Dieu, allait dans les siècles suivants, fournir un cadre intellectuel plus fructueux au développement de la médecine chimique moderne que la philosophie de la nature, rationaliste et naturaliste de la médecine galéniste, dominante à l'époque, mais qui était devenue dogmatique et sclérosée. Toutefois le paradoxe n'est qu'apparent, car le système de pensée de Paracelse n'était pas à prendre ou à laisser en totalité, seuls quelques éléments provenant de la pratique médicale pouvaient être gardés. Dans son œuvre immense, toute imprégnée de la magie naturelle propre à la Renaissance, se trouvaient quelques idées fortes et innovantes qui semblent avoir impulsé (ou parfois seulement préfiguré) les recherches ultérieures des médecins paracelsiens sur la voie d'une analyse réductionniste des maladies, de l'extraction des principes actifs des substances, de l'usage interne des médicaments chimiques ou des remèdes psychoactifs. En somme, Paracelse initie le tournant de la médecine galéniste vers la médecine moderne basée sur la biochimie, en déstabilisant les édifices galénique et aristotélicien et en ouvrant la voie à la physiologie expérimentale. Ce travail de sape de l'orthodoxie médicale scolastique fut aussi mené par l'autre grande figure médicale du XVIe siècle, l'anatomiste André Vésale (1514-1564) qui osa s'écarter du modèle anatomique galéniste en s'appuyant sur l'observation directe des corps disséqués. La médecine moderne s'est ainsi construite en dénigrant complètement la « médecine traditionnelle européenne » galéniste, contrairement aux médecines traditionnelles chinoise et indienne qui résistèrent beaucoup mieux au rouleau compresseur de la médecine moderne, car celle-ci fut reçue en Asie comme une médecine occidentale menaçant l'identité des grandes cultures locales. On peut aussi considérer que la pensée de Paracelse est le point de départ du long processus de séparation de la chimie de l'alchimie. Les travaux de nombreux savants sur deux siècles et demi permirent de se libérer des excès métaphysiques de Paracelse et en s'appuyant sur les expériences de laboratoire d'aboutir à la révolution chimique de Lavoisier des années 1787-1789.
La Pharmacopée des dogmatiques réformée est un ouvrage publié en 1607, par Joseph du Chesne un médecin français, célèbre pour avoir défendu une forme conciliante du paracelsisme face à l'orthodoxie intransigeante de la Faculté de médecine de Paris. Deux ans avant son décès, Joseph du Chesne publie un ouvrage de pharmacopée rassemblant les principales recettes médicamenteuses hippocrato-galéniques qu'il avait sélectionnées au cours de sa vie. Sous le couvert d'un appel aux autorités de l'Antiquité, à Hippocrate et à Hermès, il entend tenir compte de toutes les innovations apportées par la médecine byzantine et arabe aux Xe – XIIIe siècles et des remèdes distillés, issus de la vogue des techniques de distillation auprès des apothicaires et médecins du XVIe siècle. Du Chesne propose donc un ouvrage d'une grande orthodoxie scolastique tout en insistant sur l'idée que la doctrine médicale ne doit pas rester figée mais doit progresser et s'enrichir des innovations venant aussi bien des médecins alchimistes arabes que des apothicaires distillateurs du siècle passé. Bien qu'il se soit interdit dans la très grande majorité des cas de proposer des remèdes chimiques ou spagyriques qu'il réserve pour un autre ouvrage à venir, la Pharmacopée spagyrique, il se permet dans les derniers chapitres quelques exceptions.
Philipp Ulsted(ou Philipp Ulstad, latin: Philippus Ulstadius), (14..?-15..?) est un médecin de Nuremberg, issu d'une famille noble de cette ville. Il enseigna la médecine à l'université de Fribourg en Suisse durant la première moitié du XVIe siècle. C'était un médecin qui avait une bonne connaissance de l'alchimie mais qui était plus intéressé par l'extraction de la quintessence que par la production de transmutations.
La théorie du phlogistique est une théorie chimique qui expliquait la combustion en postulant l'existence d'un « élément-flamme », fluide nommé φλογιστόν (phlogistón) (du grec φλόξ phlóx, flamme), présent au sein des corps combustibles. Elle a été conçue par J.J. Becher à la fin du XVIIe siècle, et développée par Georg Ernst Stahl. Cette théorie a été réfutée par la découverte du rôle de l'oxygène de l'air dans le processus de combustion, mis en évidence par Lavoisier au XVIIIe siècle, et a été supplantée par la théorie du calorique. L'idée de base est que, puisque les flammes sont évidemment réelles, il doit nécessairement exister un élément qui participe à la constitution des corps combustibles, fluide comme les flammes le sont manifestement et qui, à la suite de la combustion, serait le constituant concret principal de ces flammes. L'idée se rattache aux quatre éléments d’Empédocle censés constituer toute chose selon la philosophie naturelle.
Le Prontosil est un médicament antibactérien. La démonstration par Gerhard Domagk de l'efficacité de ce composé du soufre contre certaines infections à streptocoque et la découverte par Daniel Bovet de son principe actif, le sulfanilamide, ont ouvert en 1935 à la chimie pharmaceutique la voie nouvelle de la sulfamidothérapie.
Joseph Louis Proust, né le 26 septembre 1754 à Angers et mort le 5 juillet 1826 à Angers, est un chimiste français. Il a établi expérimentalement la loi des proportions définies, premier pas vers la théorie atomique de Dalton. Français mais aussi Européen puisque, de 1778 à 1806, il a développé la chimie en Espagne dans les domaines de la recherche et de l'enseignement.
Quatre livres de secrets de médecine et de philosophie chymique est un ouvrage de remèdes distillés du médecin Jean Liébault publié en 1573. Il se place dans la filiation du Livre de la Distillation de Hieronymus Brunschwig de 1500, puis de ceux de Philipp Ulstad (1525) et de Conrad Gesner (1552) et marque le début de l'influence de la médecine chimique paracelsienne en France.
Établie en 1858 par Cannizzaro, la règle de Cannizzaro a contribué à l'acceptation de la théorie atomique par les chimistes. Cette règle s'énonce ainsi : « Étant données différentes substances chimiques les contributions d'un même élément à leur masse molaire sont des multiples entiers de la masse molaire atomique de l'élément ». Dans cet énoncé, traduit  et mis aux normes actuelles, le terme « substance chimique » désigne un corps pur ou un composé chimique. La règle de Cannizzaro a d'abord été diffusée sous la forme d'un fascicule destiné aux étudiants puis publiée dans la revue scientifique italienne Il nuovo cimento. En complément Cannizzaro a utilisé les capacités calorifiques pour vérifier la théorie atomique dans le cas de composés inorganiques non volatils.
La révolution chimique est une transformation de la chimie qui eut lieu à la fin du XVIIIe siècle à la suite des travaux du chimiste français Antoine Lavoisier, considéré comme le « père » de cette discipline. Ses principaux fondements sont la loi de conservation de la masse (loi de Lavoisier : « rien ne se perd, rien ne se crée, tout se transforme. ») et la théorie de la combustion par l'oxygène. Plusieurs facteurs menèrent à cette révolution, notamment la preuve que l'air n'est pas un élément mais est composé de différents gaz : Henry Cavendish et Joseph Priestley menèrent des expériences importantes pour l'établir. Antoine Lavoisier traduisit le langage et jargon archaïque de la chimie dans un vocabulaire plus accessible aux masses non éduquées, augmentant l'intérêt du public dans l'apprentissage et la pratique de la discipline. La publication de Lavoisier Traité élémentaire de chimie (1789) marque le début de la révolution. Lavoisier découvrit alors la composition de l'air et de l'eau et inventa le terme « oxygène ». Il explicite aussi la théorie de la combustion et relégua la théorie du phlogistique au statut d'hypothèse obsolète. Son contemporain Jöns Jakob Berzelius simplifia la description des composés chimiques en se fondant sur la théorie des masses atomiques de John Dalton.
La rhodoquine, ou 710 F, est un médicament antipaludéen utilisé dans les années 1930. C'est un dérivé quinoléique, qui ne diffère de la plasmoquine que par sa chaîne diaminée.
Werner Rolfinck ou Guernerus Rolfincius (né le 15 novembre 1599 à Hambourg, mort le 6 mai 1673 à Iéna) était un médecin, naturaliste, chimiste et botaniste allemand.
Les sept métaux sont les métaux connus, et reconnus comme tels, de l'Antiquité jusqu'à la Renaissance. Les astrologues de l'Antiquité les ont mis en correspondance avec les sept « planètes » (Le Soleil, la Lune, et les cinq planètes observables à l'œil nu), elles-mêmes associées aux dieux du panthéon gréco-romain. L'histoire des procédés d'extraction des métaux commence 6 000 ans avant notre ère. Au Moyen orient, six métaux furent utilisés durant la préhistoire et l'Antiquité : l'or, l'argent, le cuivre, l'étain, le plomb et le fer. L'or et l'argent étaient trop mous pour être utilisés en dehors des bijoux. Le cuivre pouvait être durci en le martelant mais c'est la découverte de l'alliage cuivre-étain, nommé bronze, qui permit de faire des outils. Le plomb, facile à travailler, servit à faire des récipients et des canalisations d'eau sous l'Empire romain. La maîtrise du fer fut si importante qu'elle marque la transition de l'Âge du bronze à l'Âge du fer vers 1 000 ans avant notre ère. Le premier métal de l'époque historique est le mercure. Il est utilisé sous forme d'amalgames au troisième siècle avant notre ère. Ces sept métaux de l'Antiquité sont apparus à peu près aux périodes suivantes (selon Cramb) : Ces métaux étaient connus des Mésopotamiens, Égyptiens, Grecs et Romains de l'Antiquité. De la découverte des premiers métaux, l'or et le cuivre, jusqu'à la fin du XVIIe siècle, seulement douze métaux furent découverts. Quatre d'entre-eux, l'arsenic, l'antimoine, le zinc, et le bismuth, furent découverts aux XIIIe – XIVe siècles. Le douzième métal découvert est le platine au XVIe siècle. Actuellement, on en compte 82.
Vegetable Staticks de Stephen Hales La Statique des végétaux est un ouvrage de Stephen Hales publié en 1727, sous le nom de Vegetable Staticks. Hales est un ecclésiastique qui s'est adonné avec passion à la botanique, la biologie animale et à la construction de machines ingénieuses. Il est surtout connu pour ses Statical Essays (Essais statiques) dont le premier volume Vegetable staticks (1727), traduit par Buffon en Statique des végétaux (1753) contient le compte rendu d'un grand nombre d'expériences sur la physiologie des plantes. Le chapitre VI représentant environ la moitié du livre traite d'un sujet crucial pour la chimie : le dégagement de gaz issus de diverses transformations chimiques et du chauffage de divers végétaux, animaux et substances variées. Son dispositif expérimental de la cuve à eau (fig. 38) permettant de récupérer l’air (ie. le gaz) produit dans une réaction va permettre le développement de la chimie pneumatique, c’est-à-dire de l’étude des transformations chimiques au sein de mélanges gazeux. Par ses inventions d'appareils ingénieux, Hales permit d'ouvrir de nouveaux champs d'études en physiologie végétale et en chimie.
Table des rapports de Geoffroy Le 27 août 1718, le chimiste Étienne-François Geoffroy présente à l'Académie royale des sciences sa table des différents rapports observés en Chimie entre différentes substances où il propose une classification des substances chimiques suivant leur plus ou moins grande « disposition à s'unir » à une substance de référence. L'idée que certaines substances puissent s'unir plus facilement que d'autres n'était pas nouvelle, mais le crédit de réunir toutes les informations disponibles dans un grand tableau général, appelé plus tard table d'affinité, en revient à Geoffroy. Cette table qui ne ressemblait à rien d'autre publié auparavant en chimie, incita de nombreux chimistes à publier leurs propres tables d'affinité dans les décennies suivantes. À la fin du siècle, Fourcroy un des premiers chimistes convertis aux vues de Lavoisier, écrira que la table de Geoffroy fit office de prototype pour les futures tables d'affinité et servit de « flambeau pour guider les chimistes » du XVIIIe siècle: « Mais aucune découverte n'est plus éclatante dans cette époque des grands travaux...aucune enfin n'a conduit à des résultats plus importants que celle qui est relative à la détermination des affinités entre corps et à l'exposition des degrés de cette force entre les différentes substances naturelles. C'est à Geoffroy l'aîné, ...qu'on doit cette belle idée de la table des rapports ou des affinités chimiques. ...Cette idée lumineuse a servi de flambeau pour guider les pas des chimistes... » Encyclopédie méthodique. Chymie, pharmacie et métallurgie, Fourcroy, 1786, p. 333 Le rapport-affinité entre deux substances se présente non comme un objet de pensée spéculative mais comme le produit d'observations empiriques. Après l'échec du modèle corpusculaire « des pointes et des pores » de Lémery, Geoffroy propose une approche opératoire de l'union de deux substances qui pourrait avoir mis les chimistes de la fin du siècle sur la piste d'une notion correcte des corps composés et d'une nouvelle théorie des éléments, enfin satisfaisante. Car pour classer hiérarchiquement les rapports-affinités entre substances, il utilise un test de déplacement d’une entité chimique par une autre, un phénomène chimique bien réel qui ne sera expliqué que beaucoup plus tard, aux XIXe – XXe siècles. La chimie délaisse alors le substantialisme des principes, et va définir les substances par leur circulation d’une combinaison à l’autre. Le paradoxe de Geoffroy, c'est de combiner des conceptions alchimiques anciennes (fabriquer du fer par transmutation) et l'idée moderne de rapport-affinité entre substances, basée sur la seule observation. Le savant de la continuité n'était-il pas aussi porteur de la rupture lavoisienne qui s'annonçait ?
La Tabula Affinitatum est une table d'affinités en bois, conservée au Musée Galilée de Florence. La table illustre les affinités chimiques entre différentes substances. Commissionée en 1766 par le pharmacien Franz Huber Hoefer pour l'apothicaire du grand-duché de Florence, elle devait servir à aider le préparateur de remèdes pharmaceutiques à identifier les réactifs susceptibles de se combiner. La table est modelée sur la Table des différents rapports observés en Chimie entre différentes substances (Paris 1718) d'Étienne-François Geoffroy, dont elle se différencie par l'ajout d'une dix-septième colonne. Les substances sont indiquées par les symboles alchimiques traditionnels et par la symbologie employée au XVIIe et au début du XVIIIe siècle. Il est à noter que le symbole de l'air n'apparaît pas sur la table. Cela indique qu'elle fut dessinée à une période pendant laquelle la fonction de l'air en tant que substance active, capable de se combiner avec des solides et des liquides, n'avait pas été pleinement établie. Une table semblable à celle florentine se trouve aussi parmi les Planches de la grande Encyclopédie de Diderot et d'Alembert.
Tapputi ou Tapputi-Belatekallim (Belatekallim est le titre donné à une surintendante d'un palais) est considérée comme la première chimiste. C'est une parfumeuse mentionnée sur une tablette babylonienne en cunéiforme vers 1200 av. J.-C.. On y trouve aussi la plus vieille mention d'un alambic. Elle était aussi surintendante du palais royal et travaillait avec une chercheuse nommée (—)-ninu, dont le prénom n'a pas été conservé par l'histoire.
Tyrocinium chymicum est constitué d'une série de cours de chimie publiée en latin par Jean Béguin en 1610. En ce début du XVIIe siècle, l'apothicaire Jean Béguin, protégé du roi Henri IV, donna les premiers cours de ce qu'il appela « chimie » mais était en fait, un état intermédiaire entre l'alchimie et ce qui allait devenir la science chimique. Sous l'impulsion donnée par le médecin germanophone Paracelse, la partie de l'alchimie non orientée vers la recherche de la pierre philosophale était en train de se transformer en science chimique autonome. À cette époque, les termes d'alchimie et de chimie étaient interchangeables, aussi Didier Kahn a proposé de désigner la doctrine de cet entre-deux par le terme d'« (al)chimie ». Les cours de Béguin furent fréquentés par la noblesse de robe et d'épée et même par des médecins, tous attirés par la nouveauté de l'entreprise. Il y eut en 1610 une édition pirate. Jean Béguin décida d'y remédier en publiant une version correcte de ses cours sous le titre de Tyrocinium chymicium à savoir « L'apprentissage de la chimie ». En 1615, parait une traduction en français sous le titre de Les Elemens de chymie. Cet ouvrage connut d'innombrables rééditions, une cinquantaine d'éditions en tout, en latin et en français entre 1610 et 1690, avec d'importantes augmentations qui ne sont pas toutes de la main de Béguin. Son œuvre continua à se développer après sa mort (en 1620), passant de 70 pages pour la première édition latine à 290 pages pour la première édition française et à plus de 500 pages pour la dernières, par ajout de nouvelles recettes. Il est écrit dans une langue claire, sans aucune obscurité, ce qui était rare à l'époque pour les ouvrages d'(al)chimie. Il marqua l'essor d'un genre littéraire nouveau, dit « des cours de chimie ». Il sera suivi en France, par les cours de W. Davidson en 1635, d'Étienne de Clave en 1646, Arnaud en 1656, de Barlet en 1657, de Nicaise Le Febvre en 1660, Christophe Glaser en 1663, Jacques Thibaut le Lorrain en 1667, Malbec de Tressel en 1671, jusqu'au célèbre Cours de chymie de Nicolas Lemery publié en 1675. Malgré ces nouveaux ouvrages, celui de Béguin demeura le guide classique du praticien.
Un verre ardent (en latin : lens caustica) est une grande lentille convexe qui permet de concentrer les rayons du soleil sur une petite surface, la chauffant et provoquant sa combustion. Les verres ardents étaient utilisés au XVIIIe siècle pour étudier la combustion de certains matériaux et analyser les gaz qui étaient émis. On utilisait également des miroirs ardents pour obtenir un résultat similaire au moyen de surfaces réfléchissantes afin de concentrer la lumière du soleil sur un point.
Mary Elvira Weeks (10 avril 1892 - 20 juin 1975) est une chimiste et historienne des sciences américaine. Elle est l'auteur d'une histoire de la découverte des éléments.
L'industrie chimique est le secteur industriel dont l'activité consiste à fabriquer des produits par synthèse chimique contrôlée. Ce secteur regroupe entre autres la pétrochimie, la chimie phytosanitaire, l'industrie pharmaceutique, la fabrication de polymères, de peintures et l'oléochimie. Cette industrie est de type process. L'industrie chimique fabrique à la fois des produits chimiques de base, des produits chimiques intermédiaires et des produits finis.
Adisseo est un groupe industriel, filiale du groupe international BlueStar, spécialisé dans la nutrition animale. Il est en 2010 numéro trois mondial dans le domaine des solutions nutritionnelles pour animaux. La responsabilité de l'entreprise a été mise en cause en 2003 à la suite de plusieurs cas de cancers parmi ses salariés.
L'agrochimie est le secteur industriel qui développe la chimie à destination du secteur agricole. Les principaux produits de cette branche de l'industrie chimique sont les biocides qui regroupent les produits phytosanitaires (ou pesticides) tels que les herbicides, les fongicides, les insecticides et de nombreux autres produits issus de synthèse chimique pour de nombreuses utilisations en agriculture. Depuis plus d'une trentaine d'années, plusieurs entreprises de l'agrochimie ont également investi de manière importante dans le secteur des biotechnologies végétales et des semences. C'est le cas notamment de DuPont, Bayer, Monsanto, Dow AgroScience, Syngenta, etc.
Un alcali, écrit autrefois alkali à la fin du XVIIIe siècle pour marquer l'origine arabe via le latin médiéval, est un terme de l'alchimie puis de la chimie décrivant différents composés chimiques, parfois en mélange, à propriétés dites alcalines ou basiques. Depuis le XVIIe siècle, le terme est employé de manière générique pour désigner des bases, des sels ou des solutions basiques concentrées. En chimie industrielle moderne, un alcali désigne une base forte dans l'eau. L'adjectif associé alcalin, qui indiquait une saveur amère ou agressive typique, la présence d'une base forte ou une fonction basique réduisant l'acidité d'un milieu, a fini par désigner les ions ou atomes de lithium, sodium ou potassium, plus rarement rubidium, césium ou francium de la première colonne du tableau périodique. Ces métaux alcalins ont la propriété de former avec l'oxygène ou l'eau des bases fortes, nommées alcalis. Ainsi une pile alcaline, faisant mouvoir des ions Li+ ou Na+... D'un point de vue pratique et commercial, le terme alcali dénomme encore des produits de nettoyage, agents puissants autorisés pour déboucher les éviers comme toutes autres canalisations résistantes aux bases chimiques ou encore lessive liquide, pour renforcer l'action des lessives ordinaires. Depuis des temps immémoriaux, les alcalis se caractérisent notamment par la propriété de "ramollir les matières organiques" et favoriser leurs dissolutions ultérieures dans l'eau.
Le biais de financement désigne en science le fait que les résultats des recherches ont tendance à être plus favorables au financeur direct ou indirect. Ce biais a été mis en évidence dans de nombreux domaines, où des industries ont un intérêt fort (pour des raisons de régulation) à ce que les résultats aillent plutôt dans son sens : recherche sur les effets du tabac, de la nourriture, des pesticides, des perturbateurs endocriniens, des OGM, recherches dans le secteur biomédical. Il ne s'agit pas d'un biais unique à proprement parler mais d'un ensemble de biais dont les contours ne sont pas bien identifiés.
Le Conseil Européen de l'Industrie Chimique ou CEFIC (acronyme de son ancien nom Conseil Européen des Fédérations de l'Industrie Chimique) est la principale association professionnelle de l'industrie chimique. Il a été fondé en 1959, et son histoire suit celle de l'Union européenne. Son siège et ses bureaux sont situés à Bruxelles, avenue Edmond Van Nieuwenhuyse à Auderghem.
Le Centre technique de la chimie (CTC) est un centre technique tunisien d'intérêt public et économique sous la tutelle du ministère de l'Industrie. Le CTC est créé par la loi no 94-123 du 28 novembre 1994 pour développer et promouvoir le secteur des industries de la chimie en Tunisie. Il est actif aussi bien sur le plan national qu'international et entretient d'étroites relations de coopération avec diverses institutions et organisations.
La chimie fine, ou chimie de spécialité, est une division de l'industrie chimique qui synthétise des produits répondant à des besoins très spécifiques (exemples : pesticides, pigments, arômes et cosmétiques) et de faible volume de production. La chimie fine produit des molécules complexes comportant facilement une structure carbonée comptant des dizaines d'atomes, à partir d'autres molécules de synthèse. La synthèse implique souvent plusieurs étapes et une purification finale poussée. Le faible tonnage est compensé par une haute valeur ajoutée du produit final.
La chimie industrielle est l'activité économique qui produit des molécules et autres composés chimiques en grande quantité, dite industrielle, en exploitant les technologies du génie chimique. Les produits chimiques utilisés de manière massive proviennent soit de la commercialisation de matières premières brutes ou sommairement conditionnées, soit de traitements et autres procédés industriels exploitant ces matières premières. On aura par exemple des engrais pratiquement livrés sans traitement après extraction, ou au contraire plus ou moins substantiellement améliorés par la chimie industrielle pour en augmenter l'efficacité ou la valeur marchande. Au contraire, l'exploitation du pétrole suppose toujours le déploiement d'une technologie chimique spécifique au sein d'usines de raffinage.
Anciennement appelé couloir de la chimie, mais de nos jours vallée de la chimie, est le nom donné à une zone au sud du Grand Lyon, située sur plusieurs communes et qui comporte une grande concentration d'industries chimiques avec, pour une bonne partie d'entre elles, un classement Seveso 2. Sur une dizaine de kilomètres, le long de l'autoroute A7, se succèdent des établissements de l’industrie chimique et pétrochimique : Solvay, Bluestar Silicones, Novacyl, raffinerie Total, Air liquide, etc.
Dzerjinsk (en russe : Дзержинск) est une ville industrielle de l’oblast de Nijni Novgorod, en Russie. Avec une population de 237 668 habitants en 2013, Dzerjinsk est la deuxième ville de l'oblast. Elle fut nommée en l'honneur de Félix Dzerjinski, fondateur et chef de la Tchéka. Dzerjinsk fut l'un des principaux centres de l'industrie chimique soviétique et elle est encore considérée comme la « capitale russe de l'industrie chimique ». Pour cette raison, c'est une des villes les plus polluées du pays.
Essenscia (anciennement Fédération des industries chimiques de Belgique, puis Fedichem) est la fédération belge de l'industrie chimique et des sciences de la vie, une organisation professionnelle belge défendant les intérêts des industriels actifs dans ces domaines. Elle compte près de 800 membres, en grande partie des PME, et est active dans 17 secteurs. Essenscia est structuré selon les réalités institutionnelles de la Belgique: les trois sections régionales de Essenscia, sont les interlocuteurs des entreprises auprès des autorités bruxelloises, flamandes et wallonnes pour les matières régionales. Yves Verschueren est administrateur délégué d'essenscia, Frank Beckx d'essenscia Vlaanderen, Frédéric Druck d'essenscia Wallonie/Bruxelles. La plupart des membres sont actifs à l'exportation. Ensemble, ils réalisent un chiffre d'affaires de 62,7 milliards d'euros (2013). Ils occupent directement 88 700 personnes, ce qui équivaut à 18,1 % de l'emploi dans l'industrie belge. À cela, il faut ajouter 145 000 emplois indirects. Les sujets spécifiques aux secteurs sont traités par Essenscia en collaboration avec la FEB, Voka (en Flandre), l'UWE (en Wallonie), Beci (à Bruxelles) et avec le CEFIC au niveau européen.
Euro Chlor est l'organisation professionnelle des opérateurs des procédés chlore-alcali (producteurs de chlore et de soude caustique) en Europe. Elle a été fondée en 1969 sous le nom de Bureau International Technique du Chlore et est basée à Bruxelles. Euro Chlor réunit 97 % des producteurs de chlore de l'Union européenne et de l'AELE. Il compte 37 membres, employant 39 000 personnes sur 70 sites de production dans 20 pays. De plus, des membres associés et des correspondants techniques représentent les fournisseurs d'équipement, de matériel et de services, ainsi que les utilisateurs en aval et les producteurs hors d'Europe. Euro Chlor compte aussi des groupes sectoriels représentant des produits dérivés du chlore, tels que les paraffines chlorées, les solvants chlorés, les chloroisocyanurates, l'hydroxyde de potassium. Euro Chlor est un membre du Conseil Mondial du Chlore et est une des divisions du Conseil Européen de l'Industrie Chimique (CEFIC).
Maurice de Keghel est un chimiste et industriel français d'origine belge né à Gand le 9 août 1883 et décédé à Montmorency le 17 juin 1965, auteur de nombreux ouvrages de chimie appliquée consacrés essentiellement aux produits à usage ménager, industriel ou artisanal.
Primo Levi, né le 31 juillet 1919 à Turin et mort le 11 avril 1987 à Turin, est un docteur en chimie italien rendu célèbre par son livre Si c'est un homme, dans lequel il relate son emprisonnement au cours de l'année 1944 dans le camp de concentration et d'extermination d'Auschwitz-Monowitz. Juif italien de naissance, chimiste de profession et de vocation, il entre tardivement dans une carrière d'écrivain orientée par l'analyse scientifique de cette expérience de survivant de la Shoah, dans le but de montrer, retranscrire, transmettre, expliciter. Il est l'auteur d'histoires courtes, de poèmes et de romans.
Une nitrière est une salpêtrière, c'est-à-dire un lieu de production de nitrate de potassium ou salpêtre utilisé notamment pour la fabrication de la poudre à canon. Elle peut être naturelle ou artificielle.
Un oxyduc ou oxygénoduc est une canalisation transportant de l'oxygène sous forme liquide ou gazeuse. Les oxyducs sont généralement associés aux azoducs, aussi appelés nitrogénoducs. Ces deux types de canalisations spécialisées représentent un réseau dont la longueur mondiale totalise environ 8 500 km. Selon une estimation de 1996, 5 500 km sont exploités par Air liquide, 1 200 km par Air Products, 1 120 km par Praxair et 500 km par Messer. Ces ouvrages se trouvent en en Allemagne, dans le Benelux, aux États-Unis, en France (dans le Nord et en Lorraine), au Royaume-Uni et en Italie. En France, environ 45 % de la production industrielle de dioxygène est acheminée par canalisation.
La parachimie ou chimie de formulation est un secteur industriel qui conditionne (ou « formule ») des produits issus de l’industrie chimique sous une forme utilisable par le consommateur final ou par une industrie spécifique. Les produits ainsi élaborés sont fonctionnels. Il existe une très grande variété de produits fabriqués par la parachimie, ce qui en fait un secteur très hétérogène. Tandis que certains produits parachimiques sont directement conditionnés pour l’utilisation finale et ne seront plus transformés (par exemple produits phytosanitaires, peintures décoratives, explosifs et colles), d’autres (huiles essentielles, encres d’imprimerie, additifs pour ciments ou bétons, huiles de lubrification, etc.) s’intègrent dans la fabrication en aval d’industries très diverses : agroalimentaire, emballage, etc. En France, selon la nomenclature officielle du ministère de l'Économie, des Finances et du Commerce extérieur, le secteur de la parachimie regroupe les industries qui fabriquent huit catégories de produits : produits agrochimiques ; peintures, vernis et encres ; produits explosifs ; colles et gélatines ; huiles essentielles ; produits chimiques pour la photographie ; supports de données ; produits chimiques à usage industriel. La fabrication de médicaments est une activité qui correspond à la définition de la parachimie, mais elle est en fait englobée dans l’industrie pharmaceutique.   Portail de la chimie  Portail de la production industrielle
Une plateforme chimique est, le plus souvent, un ensemble d'entreprises du domaine de la chimie ainsi que des prestataires de services intégrés sur un même site ou à une même adresse. Elles sont souvent sous la tutelle d'une seule société qui gère les utilités (azote, vapeur, électricité, etc.), les déchets, la sécurité incendie et le secours à personne ainsi que la sécurité intrusion, mais également l'entretien des bâtiments, voiries et canalisations. Elles peuvent êtres de tailles différentes : « modérées » (Sobegi à Lacq) ou « grandes » (BASF à Ludwigshafen).
Les « substances préoccupantes » (et au sein de ce groupe les « substances extrêmement préoccupantes » sont un ensemble de plusieurs milliers de produits chimiques dont les effets sont préoccupants pour la santé publique, la santé au travail et/ou l'environnement, par exemple parce qu'elles sont cancérigènes, mutagènes ou reprotoxiques (elles sont alors classées dans le groupe dit des CMR) ou parce qu'elles sont des perturbateurs endocriniens. En Europe, au regard du Règlement Reach (sur l'enregistrement, l'évaluation, l'autorisation et les restrictions des substances chimiques, en vigueur depuis le 1er juin 2007), on comptait environ 1500 substances préoccupantes au début des années 2000. REACH vise a améliorer le contrôle de ces substances, impose des restrictions « conçues pour faire face aux risques qui ne sont pas correctement maîtrisés par l’industrie ». Il impose aussi un processus d’autorisation visant « à garantir que les risques émanant de substances extrêmement préoccupantes sont maîtrisés et que ces substances seront progressivement remplacées par des produits de substitution appropriés lorsque ceux-ci seront économiquement et techniquement viables ». En 2013, la Commission, les États-membres et l’Agence européenne des produits chimiques (AEPC) devraient intensifier leurs efforts pour identifier les substances extrêmement préoccupantes, en s’appuyant sur les options de gestion des risques (OGR)
Le Système périodique (italien : il Sistema periodico) est un recueil d'histoires courtes de Primo Levi, publié en 1975. À la veille de sa retraite du monde de la chimie afin de se consacrer à sa carrière d'écrivain, Primo Levi revient, à travers 21 chapitres portant chacun le nom d'un élément chimique, sur cette science sous le signe de laquelle il a placé sa vie et tenté de trouver une réponse aux questions de l'univers, en se confrontant directement à la matière, amicale, indifférente ou hostile. La chimie a, en retour influencé son écriture, en y introduisant l'analyse et la description objective, le besoin d'élucider les sources des problèmes afin de les résoudre et la remise en question permanente des acquis. Malgré la référence au tableau périodique de Mendeleïev, le Système périodique n'est pas tant un livre de chimie que d'un chimiste, Juif italien du Piémont, combattant anti-fasciste, déporté puis témoin et écrivain, racontant « sa » chimie. Primo Levi joue avec les éléments, qui peuvent être cités au sens propre lorsqu'ils interviennent directement dans le récit de la nouvelle, ou figuré voire étymologique. Les histoires couvrent la vie de l'auteur, de sa naissance à la rédaction du livre, en s'arrêtant sur les moments essentiels ou formateurs : les origines, la vocation, les années de formation, la montée du fascisme et des lois raciales, la vie en clandestinité, l'arrestation de Primo Levi, l'expérience concentrationnaire, la réadaptation à la vie, la confrontation avec d'anciens acteurs du passé et des expériences professionnelles, le tout sous le prisme de la chimie ; la plupart des histoires sont vécues, quelques-unes sont des fictions de l'auteur et signalées comme telles. Chacune des histoires a le nom d'un élément qui lui est associé d'une façon ou d'une autre. Le livre a été intronisé best science book ever par la Royal Institution d'Angleterre en 2006.
La Ullmann's Encyclopedia of Industrial Chemistry (« Encyclopédie Ullmann de chimie industrielle ») est un ouvrage de référence majeur en chimie. Cette encyclopédie propose des articles en chimie appliquée, chimie industrielle et génie chimique. Elle a été fondée par le chimiste allemand Fritz Ullmann (1875–1939) en 1914.
L'union des industries chimiques (UIC) est une organisation professionnelle française dont la mission est de représenter les entreprises du secteur de la chimie en France et de défendre leurs intérêts devant les différents niveaux de l'administration. Elle négocie la convention collective nationale de la chimie. L'UIC est le chef de file de la négociation de branche, à ce titre elle prépare les projets d'accords soumis aux partenaires sociaux. L'UIC comprend depuis décembre 1995 un Collège National d'Experts en Environnement de l'Industrie Chimique (CNEEIC). L'UIC forme des inspecteurs chargés du suivi des équipements, capables d'établir des plans d'inspection pour tous les modèles d'appareils des sites chimiques et pétrochimiques. Cette formation diplomante donne droit à l'attribution de 2 titres Inspecteur UIC 1 puis Inspecteur UIC 2, les sites employant des inspecteurs UIC2 pourront demander à la DRIRE[sigle à expliciter] la reconnaissance du service inspection (devenant SIR Service Inspection Reconnu); les sites disposant d'un SIR pourront aménager les périodicités de visites, sous contrôle des DREAL[réf. nécessaire]. L'UIC est chargé du suivi de la mise en application du règlement REACH (Registration, Evaluation and Authorisation of Chemicals) dans l'industrie chimique française. Elle travaille régulièrement avec les différents ministères impliqués sur la problématique REACH.
William Ziegler (Beaver County (Pennsylvanie), 1re septembre 1843 - Darien (Connecticut), 25 mai 1905) est un industriel et homme d'affaires américain, fondateur de la Royal Baking Powder Company et sponsor d’expéditions arctiques.
Le manuel Gmelin de chimie minérale, ex-« manuel de chimie théorique », est publié à l'origine par Leopold Gmelin (1788-1853), l'objectif étant de rassembler toutes les données pertinentes concernant la chimie. La première édition est publiée en 1817 par l'éditeur Franz Varrentrapp, établi à Francfort-sur-le-Main.
Méthode de nomenclature chimique est un traité scientifique de nomenclature chimique d'Antoine Lavoisier, écrit en collaboration avec Louis-Bernard Guyton-Morveau, Claude-Louis Berthollet, Antoine-François Fourcroy, Jean Henri Hassenfratz et Pierre Auguste Adet, et publié à Paris en août 1787, chez l'éditeur Cuchet. Ouvrage majeur de l'histoire de la chimie, il constitue la première tentative de classification méthodique des éléments, dont est presque entièrement hérité le système actuel, et servira de base aux travaux de Mendeleïev, près de 80 ans plus tard. Il constitue l'œuvre majeure de Lavoisier avec le Traité élémentaire de chimie, publié deux ans plus tard, qui s'appuiera sur ses découvertes. L'œuvre se compose de cinq mémoires exposant la démarche et la thèse des quatre chimistes, dont les deux premiers prononcés à l'Académie des sciences en avril-mai 1787, de sept tableaux dépliants classant les éléments et synthétisant les recherches, et de cinq parties guidant l'utilisation des tableaux.
La Notice sur les travaux scientifiques de Jean Perrin a été écrite par Jean Perrin en 1923. Elle regroupe les fonctions et titres de l'auteur, la liste de ses publications jusqu'à cette période ainsi qu'un résumé de ses travaux en physique et en chimie jusqu'à cette période. Elle comprend 97 pages et a été éditée par l'Imprimerie et Librairie Édouard Privat. Elle n'a pas été rééditée mais peut être trouvée en format numérique. Voici son contenu :
Œuvres scientifiques de Jean Perrin est le nom du recueil de 408 pages publié en 1950 par les Éditions du CNRS et regroupant un certain nombre des travaux scientifiques du physicien Jean Perrin. Il fait partie d'une collection dans laquelle on peut trouver aussi entre autres les œuvres de Paul Langevin et celles d'Émile Borel. Le recueil étant assez dense, on peut y retrouver un grand nombre des publications de l'auteur mais ce livre n'étant pas exhaustif, on trouvera au début et à la fin de l'ouvrage une liste des œuvres de l'auteur qui ne s'y trouvent pas. Le recueil est relié et peut encore se trouver à l'état neuf actuellement mais peut être trouvé plus facilement en livre d'occasion. Il n'a pas été réédité récemment. Ci-dessous se trouvent une description de la table des matières du livre ainsi que le parcours de l'auteur et l'avertissement qui fait office d'introduction, publié par son fils, le physicien Francis Perrin.
L'Université européenne des saveurs et des senteurs est un centre de formation, situé à Forcalquier, au Couvent des Cordeliers. Elle dispense trois types de formations sur les thématiques des saveurs et des senteurs. La formation professionnelle est son activité majeure. Elle s'adresse aux personnes souhaitant valider ou acquérir des compétences sur la fabrication, la réglementation et la commercialisation des produits issus de la filière senteurs - saveurs. S'y adjoignent un diplôme universitaire, une licence professionnelle et un master professionnel décernés par l'Académie d'Aix-Marseille. Le troisième volet concerne une formation grand public qui propose des ateliers pédagogiques consacrés à l'éveil des sens pour s'initier aux domaines du goût et des senteurs.
L’institut de chimie et biochimie moléculaires et supramoléculaires (ICBMS) est une unité de recherche spécialisée dans la chimie et la biochimie moléculaire dont le siège se situe à Villeurbanne, dans le département du Rhône. Il est placé sous plusieurs tutelles : le CNRS (UMR 5246), l'université Claude Bernard Lyon 1, l'INSA de Lyon et l'École supérieure chimie physique électronique de Lyon. Il est né de la fusion en janvier 2007 des laboratoire de génie enzymatique et moléculaire (UMR 5013) et du laboratoire de catalyse et synthèse organique (UMR 5181). Les études sont effectuées autour de trois thèmes : Synthèse, méthodologie et catalyse ; biomolécules : synthèse, propriétés et assemblage ; membranes - biocatalyse.
La maison de la Chimie est un centre de congrès et de conférences situé au 28, rue Saint-Dominique dans le 7e arrondissement de Paris. Depuis les années 1930, cette demeure située à proximité de l'Assemblée nationale et de la plupart des ministères est le théâtre d‘événements scientifiques, culturels, politiques et économiques.
Le site Kuhlmann-Rhodia désigne un vaste complexe chimique situé au nord de Lille (France), en activité de 1847 jusqu'à son démantèlement complet en 2005. Il s'étendait de part et d'autre de la Deûle, à cheval sur les communes de La Madeleine, Marquette et Saint-André.
Un accélérateur est un adjuvant pour matériaux cimentaires tels que les coulis, les mortiers de ciment et les bétons de ciment. Il est utilisé pour ralentir la réaction d’hydratation du ciment. L’hydratation du ciment est la réaction chimique qui a lieu entre le ciment et l’eau. Deux types de accélérateurs existent : les accélérateurs de prise et les accélérateurs de durcissement.
Les adjuvants pour matériaux cimentaires sont des produits chimiques ajoutés au matériaux cimentaires tel que les coulis, les mortiers de ciment et les bétons de ciment pour modifier leurs caractéristiques. Les ajouts de ces adjuvants, réalisés lors du malaxage, sont le plus souvent inférieurs à 5 % en masse de ciment.
Un adoucissant  ou assouplissant  est un produit ménager utilisé pour adoucir (parfumer le plus souvent ) les vêtements et limiter leur électricité statique. Ils se retrouvent sous forme liquide, cristalline, ou en feuilles[réf. nécessaire]. Les adoucissants sont apparus au début du XXe siècle, ils étaient alors composés de savon et d'huile d'olive, d'huile de maïs, ou de suif.
Un agent tampon est un produit chimique permettant de maintenir constant le pH d'une substance. Par exemple, dans l'industrie alimentaire, des acides acétique, lactique ou citrique sont ajoutés aux aliments transformés afin de maintenir leur pH.  Portail de la chimie
Pour le géologue, les asphaltes naturels (ou bitumes naturels) sont des pétroles extra-lourds de consistance très visqueuse à solide. Ces pétroles ultra-lourds proviennent de kérogène dégradé par la chaleur et ayant migré à partir d'une roche-mère vers une roche-réservoir plus poreuse. On les trouve piégés dans des matrices sableuses (sables bitumineux) ou des matrices calcaires (calcaires asphaltiques ou bitumineux créés à la suite de la compression conjointe de ces deux matériaux au cours du temps par des couches supérieures ; le calcaire est alors imprégné à cœur par le bitume (environ 12 % de bitume). Cette roche est actuellement exploitée sous forme de poudre que l'on incorpore à d'autres formules, à partir de mines ou d'affleurements de surface. Près de la surface, au contact de l'eau et de l'oxygène qu'elle apporte et via les colonies bactériennes qui s'y développent, le pétrole subit une lente dégradation chimique et biochimique, plus rapide sur les zones d'affleurements de réservoirs, ou en présence de failles ou de zones d'érosion qui auraient exposé une roche riche en pétrole à des conditions permettant sa dégradation. Par rapport au pétrole, les asphaltes contiennent moins d'alcanes, ont perdu une partie des cyclanes et des hydrocarbures aromatiques de bas poids moléculaire. Ils sont donc proportionnellement plus riches en résines et asphaltènes. Ils sont souvent enrichis en soufre, azote et métaux lourds (dont nickel et vanadium) et donc source de pollution en cas de combustion. En tant qu'hydrocarbures, les asphaltes sont trop pâteux pour être extraits par les techniques pétrolières classiques. Des techniques minières classiques (carrière) sont utilisées ou des procédés (polluants et coûteux en énergie) visant à liquéfier les hydrocarbures par traitement thermique (eau chaude ou vapeur surchauffée avec produits chimiques). Dans les travaux publics, l’asphalte désigne un mélange de bitume et de granulats. C'est un matériau « fermé » ne comportant pas ou peu de vide contrairement à l'enrobé bitumineux. L'enrobé constitue la plupart des couches supérieures des chaussées.
Calgon est une marque déposée de différentes sociétés. Le produit original consiste en une poudre d'hexamétaphosphate de sodium qui, dans l'eau, réagit avec les ions calcium et certains cations, prévenant la formation de sels non souhaités et des interférences entre ces cations et le savon et les autres détergents. Ce nom est dérivé des mots « calcium gone » (« plus de calcium »). À l'origine promu pour le bain et le nettoyage, il a permis la création de produits dérivés qui ont divergé de la composition originale. Aujourd'hui, les adoucissants Calgon contiennent des ingrédients actifs de zéolithe et d'acide carboxylique, qui sont moins problématiques dans le traitement des eaux usées.
Un conservateur est défini comme toute substance capable de s’opposer aux altérations d’origines physique (photodégradation, dissociation, biodégradation, etc.) chimiques (transformation par réaction d'oxydoréduction avec l'air ou un composant, ou un contenant) ou microbiologiques d’un produit (altération par les bactéries en général).
Un convertisseur de rouille est un produit destiné à être appliqué sur des éléments en fer ou en alliage ferreux et qui transforme la rouille dont ils sont attaqués en un composé plus résistant et formant une barrière protectrice. Les convertisseurs de rouille sont notamment basés sur l'action des tanins sur l'oxyde de fer. L'acide tannique, et le pyrogallol (ou des dérivés), qui transforment l'oxyde de fer en tannate de fer bleu-noir, sont utilisés comme ingrédients principaux. Les convertisseurs de rouille peuvent aussi contenir de l'acide phosphorique, qui transforme l'oxyde de fer(III) Fe2O3 en phosphate de fer FePO4.
L'eau sèche est une substance se présentant comme une fine poudre blanche formée de gouttelettes d'eau enrobées d'une pellicule de silice. Elle a été nommée ainsi car elle est constituée d'eau à 95 % mais ne mouille pas les surfaces avec lesquelles elle est en contact. Elle peut être produite en dispersant des nanoparticules de silice hydrophobe dans de l'eau à l'aide d'une tige d'agitation à 19 000 tours par minute pendant 90 secondes, ce qui enrobe complètement les gouttelettes. Le concept a été breveté en 1968 et a été étudié en particulier pour son potentiel d'applications dans l'industrie des cosmétiques. Deux chercheurs du Collège de France, Pascale Aussilous et David Quéré, ont notamment travaillé sur le sujet au début du siècle et publié leurs travaux dans la revue Nature en été 2001, tandis que d'autres équipes reprenaient ces travaux au Royaume-Uni, à l'université de Hull en 2006, et, plus récemment, à l'université de Liverpool, afin d'en explorer les applications potentielles. Andrew Cooper et ses équipes ont ainsi montré en 2008 que le méthane est efficacement absorbé par l'eau sèche en formant un clathrate stable jusqu'à -70 °C à pression atmosphérique. Ceci permet d'envisager des modes de transport alternatifs du gaz naturel ainsi qu'un moyen de véhiculer des gaz explosifs en limitant les risques de détonations. Ben Carter, travaillant avec Andrew Cooper, a par la suite présenté des travaux complémentaires lors du 240e colloque national de l'American Chemical Society qui s'est tenu à Boston, Massachusetts, du 22 au 26 août 2010. Très médiatisées, les perspectives présentées à cette occasion sont en effet prometteuses, notamment du point de vue de la protection de l'environnement. L'eau sèche pourrait ainsi permettre de développer des technologies efficaces de séquestration du dioxyde de carbone, le CO2 se liant trois fois plus abondamment à l'eau sèche qu'à l'eau naturelle ; étendu à l'ensemble des gaz à effet de serre, ce type de technologies pourrait jouer un rôle dans la lutte contre le réchauffement climatique. L'eau sèche constituerait également un catalyseur efficace de la conversion, en présence d'hydrogène, de l'acide maléique en acide succinique, un composé organique au carrefour de nombreux procédés industriels.
Un entraîneur d’air est un adjuvant pour matériaux cimentaires tels que les coulis, les mortiers de ciment et les bétons de ciment. Il est utilisé pour protéger ces matériaux contre les cycles de gel/dégel. Les entraîneurs d’air sont ajoutés aux matériaux cimentaires à des ratios entre 0,05 et 0,2 % de la masse de ciment utilisé. La teneur en air dans le matériau durci serait entre 4 et 6 %.
Les goudrons (de l'arabe قَطْران — qaṭrān) sont des produits issus de poix modifiée de résineux, produits par distillation destructive sous pyrolyse. Ils sont composés de centaines de substances chimiques, dont plusieurs sont considérées comme cancérigènes ou potentiellement dangereuses. Parmi celles-ci, on trouve notamment des hydrocarbures aromatiques polycycliques (HAP), des amines aromatiques et des composés inorganiques.
Un hydrofuge de masse est un adjuvant pour matériaux cimentaires tels que les coulis, les mortiers de ciment et les bétons de ciment. Il est utilisé pour réduire l'absorption capillaire et donc la perméabilité à l'eau de ces matériaux. Les hydrofuges de masse sont ajoutés aux matériaux cimentaires à des ratios entre 0,5 et 2 % de la masse de ciment utilisé. Il ne faut pas confondre hydrofuge de masse et hydrofuge de surface. Le premier entre dans la composition du béton alors que le second est appliqué uniquement à sa surface.
La lanoline, autrement appelée graisse de laine ou cire de laine, est une graisse obtenue par purification et raffinage du suint (partie grasse absorbée sur la laine). Elle comprend de l'oléine et de la stéarine. Chimiquement, la lanoline est une cire, un mélange d'esters et d'acides gras avec des alcools à haute masse moléculaire. On a identifié plus de 180 acides gras et 80 alcools différents. Elle est amphiphile et forme des émulsions très stables avec l'eau, de plus il s'agit d'une espèce très hygroscopique. La lanoline est utilisée en pharmacie, par exemple pour la supplémentation en vitamine D,, et dans la fabrication des produits de beauté. C'est l'un des rares produits d'origine animale qui soit autorisé par les labels de cosmétiques biologiques. (Nature et Progrès, BDIH) Son code FAO est 0994 [1]
L'oxygène actif est l'appellation commerciale d'un produit écologique qui tue les micro-organismes par oxydation sans dégager de sous-produit toxique. Il est utilisé pour la purification de l'eau de piscines. Il contient soit de l'eau oxygénée (H2O2, forme liquide), du permanganate de potassium (KMnO4) ou du sel triple du monopersulfate de potassium (H3K5O18S4, forme solide : poudre ou pastille blanches). Il n'irrite pas la peau, les yeux ni les muqueuses et ne modifie pas le pH de l'eau, et est apprécié pour son action contre les micro-organismes tels que les algues, mais son efficacité est limitée dans le temps, obligeant à renouveler l'opération régulièrement, et la quantité de produit à utiliser est plus importante que pour le chlore, meilleur marché mais toxique. Il est donc surtout utilisé pour les petits bassins ou pour les traitements chocs en cas de prolifération d'algues. Son efficacité est réduite pour les eaux chaudes (telles que celles des spas) ou lorsque le pH n'est pas compris entre 7 et 7,6.
Un plastifiant ou réducteur d'eau est un adjuvant pour matériaux cimentaires tels que les coulis, les mortiers de ciment et les bétons de ciment. Il est utilisé pour diminuer la quantité d’eau présente à l’état frais et ainsi augmenter leur résistance mécanique à l’état durci. Ces adjuvants interviennent par la réduction du rapport E/C (eau/ciment). Les plastifiants sont ajoutés aux matériaux cimentaires à des ratios entre 0,3 et 0,6 % de la masse de ciment utilisé. Les plastifiants ont un mode d'action similaire à celui des superplastifiants mais il se produit avec une intensité bien moins importante.
Un plastifiant est une molécule ou un oligomère qui rend un plastique souple et flexible. Il abaisse la température de transition vitreuse du matériau final en s'insérant entre les chaînes macromoléculaires et en remplaçant une partie des interactions polymère-polymère par des interactions polymère-plastifiant. Par exemple, le polychlorure de vinyle (PVC) brut est rigide ; par ajout de plastifiants tels les phtalates d'alkyle ou les phosphates, il devient souple. La proportion massique peut atteindre cinquante parties de plastifiant pour cent parties de polymère. Les matériaux obtenus sont totalement différents (obtention par exemple d'un plastisol).
Le marché des produits d'entretien pour véhicules automobiles en France se partage en deux parties : le premier secteur sont les grandes surfaces, éventuellement spécialisées, le second secteur est internet et les e-boutiques. Le terme « produits d'entretien pour véhicules automobiles » reprend en fait deux catégories de produits, les produits esthétiques pour intérieur et/ou extérieur et les produits d'entretien mécanique.
Un retardateur de prise est un adjuvant pour matériaux cimentaires tels que les coulis, les mortiers de ciment et les bétons de ciment. Il est utilisé pour ralentir la réaction d’hydratation du ciment et donc sa prise. Ceci permet d'augmenter son temps de travail. L’hydratation du ciment est la réaction chimique qui a lieu entre le ciment et l’eau. Les retardateurs de prise sont ajoutés aux matériaux cimentaires à des ratios entre 0,2 et 0,5% de la masse de ciment utilisé.
Un rétenteur d'eau est un adjuvant pour matériaux cimentaires tels que les coulis, les mortiers de ciment et les bétons de ciment. Il est utilisé pour réduire la tendance au ressuage de ces matériaux. Ceci augmente alors leur stabilité.
La sandaraque (du grec ancien σανδαράκη / sandarákê, réalgar) ou sandarac est une résine végétale extraite du cyprès de l'Atlas (Tetraclinis reticulata), originaire d'Afrique du Nord. Elle se présente sous forme de larmes ou de grains. Elle s'utilise fréquemment dans la composition des vernis à alcool dont elle assure la transparence. Elle est en partie dissoute à chaud dans l'huile. Son point de fusion est de 140 °C. Elle fond en se boursouflant. Connue depuis l’Antiquité, elle est très fréquemment utilisée. Dès le VIIIe siècle, elle sert à la confection de vernis gras du Moyen Âge. C’était, avec la résine mastic, la résine de choix pour la confection des vernis. Elle se rapproche beaucoup de la colophane mais elle est moins sensible à l’oxydation. Insoluble dans l’eau, partiellement soluble dans l’essence de térébenthine, elle est soluble dans l’éthanol, l’éther et l’acétone (contrairement au mastic, elle donne une solution claire dans l’alcool). Pour la dissoudre dans l’huile, il faut préalablement la fondre, de même si l’on veut la dissoudre dans l’essence de térébenthine, sans cela elle se ramollit en une masse plastique sans se dissoudre. La sandaraque entre dans la composition des vernis durs, incolores et brillants, applicables au pinceau. Elle est utilisée en lutherie, en ébénisterie ou en entomologie en formulation dans l'Euparal.  Portail de la chimie
De la même manière que les herbicides sont des pesticides utilisés pour détruire des plantes indésirables, les silvicides sont des produits spécialement conçus et homologués pour détruire par dévitalisation des buissons, souches, plants, arbres, arbres forestiers, ou certaines espèces forestières ou une forêt entière ou le processus de régénération naturelle.
Une sonde fluorescente est une molécule fluorescente que l'on ajoute à un milieu (cellule ou monocouche, par exemple) pour mettre en évidence certaines zones et/ou pour étudier les propriétés physiques d'un milieu.  Portail de la chimie  Portail de la physique  Portail de la biologie
Une substance chimique, ou produit chimique (parfois appelée substance pure), est tout échantillon de matière de composition chimique définie et présentant des propriétés caractéristiques (couleur, odeur, densité, point de fusion, etc.), indépendamment de son origine.
Un superplastifiant ou haut réducteur d’eau est un adjuvant pour matériaux cimentaires tels que les coulis, les mortiers de ciment et les bétons de ciment. Il est utilisé pour diminuer la quantité d’eau qu’ils contiennent à l’état frais et ainsi augmenter leur résistance mécanique à l’état durci. Ces adjuvants interviennent par la réduction du rapport E/C (eau/ciment). Les surperplastifiants sont ajoutés aux matériaux cimentaires à des ratios entre 0,8 et 3 % de la masse de ciment utilisé. Les superplastifiants ont un mode d'action similaire à celui des plastifiants mais il se produit avec une intensité bien plus importante.
Un arrête-flammes est un dispositif de sécurité autonome permettant d’éviter la propagation d’une explosion entre l’amont et l’aval d’un équipement sans influencer la marche normale du procédé ou de l’installation en permettant notamment une libre circulation des fluides.
Les bonnes pratiques de laboratoire ou BPL (en anglais : good laboratory practices ou GLP) sont un ensemble de règles à respecter lors d'essais non-cliniques (pré-cliniques), c'est-à-dire sur l'animal, afin de garantir la qualité, la reproductibilité et l’intégrité des résultats obtenus,. Ces études concernent l'efficacité et la sécurité des substances chimiques ou biologiques dans différents domaines comme la santé (humaine ou animale) ou l'environnement. Elles ont été énoncées et recommandées pour la première fois par l'OCDE à ses membres en 1981. Elles ont ensuite été traduites dans le droit local des membres. En Europe, les principes des BPL sont définis par la directive européenne 2004/10/CE qui s'applique après transposition dans le droit national de chaque État.
La cellule mobile d’intervention chimique, abrégée CMIC, est un véhicule d'intervention spécialisé des pompiers en Belgique et en France.
Les conseils de prudence (« phrases S ») sont des indications présentes sur les étiquettes de produits chimiques, qui conseillent l’utilisateur quant aux précautions à prendre lors de leur manipulation ou utilisation. Elles se présentent sous la forme d'un S suivi d’un ou de plusieurs nombres, chacun correspondant à un conseil particulier. Elles sont définies dans l'annexe IV de la directive européenne 67/548/CEE : Conseils de prudence concernant les substances et préparations dangereuses. La liste a été complétée et publiée à nouveau dans la directive 2001/59/CE. Cette législation a été abrogée au 1er juin 2015 et les phrases S sont devenues des phrases P suivant les directives du système général harmonisé de classification et d'étiquetage des produits chimiques.
Danger immédiat pour la vie ou la santé, connu sous le sigle DIVS, est une valeur représentant la concentration maximale d'un produit présent dans un milieu et duquel un individu peut s'échapper dans un délai de 30 minutes, sans présenter de symptômes pouvant l'empêcher de fuir et sans produire des effets irréversibles sur sa santé. Cette concentration a été définie dans le but de sélectionner un appareil de protection respiratoire approprié. Cette désignation provient de l'organisme NIOSH, National Institute for Occupational Safety and Health, l'acronyme anglais est IDLH.  Portail de la chimie
Une douche portative de sécurité est un appareil ayant souvent l'apparence d'un extincteur, mais dont le corps est de couleur verte, et qui sert à secourir les victimes de brûlures thermiques ou chimiques.
Enregistrement, évaluation, autorisation et restriction des substances chimiques — en anglais : Registration, Evaluation, Authorization and restriction of CHemicals (REACH) — est un règlement du Parlement européen et du Conseil de l'Union européenne, adopté le 18 décembre 2006, qui modernise la législation européenne en matière de substances chimiques, et met en place un système intégré unique d'enregistrement, d'évaluation et d'autorisation des substances chimiques dans l'Union européenne. Son objectif est d'améliorer la protection de la santé humaine et de l’environnement, tout en maintenant la compétitivité et en renforçant l'esprit d’innovation de l'industrie chimique européenne. Dans plusieurs domaines concurrentiels, durant sa négociation (de 2001 à 2006), ce règlement a suscité contre lui une « bataille des lobbies », équilibrée selon L. Bu par le contre-lobbying de quelques grandes ONG impliquées dans le processus.
Les étiquettes de danger des substances chimiques sont des dessins conventionnels conçus pour signaler des substances ou des dépôts de matières dangereuses. L'emploi de ces pictogrammes est régi par la loi dans la plupart des pays et doit se conformer aux normes et recommandations internationales. La couleur du dessin et du fond du pictogramme, la forme du cadre et les mentions littérales éventuelles sont spécifiques de chaque type de risque.
L'European chemical Substances Information System (ESIS, Système d'information européen sur les substances chimiques) est une base de données chémoinformatique qui fournit des informations sur les substances chimiques du marché européen, par exemple, les données physico-chimiques, la classification et l'étiquetage des substances, les informations PBT (persistant, bioaccumulable, toxique) ou vPvB (très persistant, très bioaccumulable), les numéros EINECS, ELINCS et « No-Longer Polymers » (de) (NLP, ex-polymères). ESIS donne accès aux fiches techniques de la base IUCLID (en) (International Uniform Chemical Information Database). En février 2013, ESIS contenait 14 897 noms de substance.
La fiche de données de sécurité ou FDS (en anglais : Safety Data Sheet ou SDS) est un formulaire contenant des données relatives aux propriétés d'une substance chimique. Le format utilisé en Amérique du Nord est appelé MSDS (pour Material Safety data Sheet). Ces fiches sont un élément important de la santé et sécurité au travail, sur le lieu de travail pour les utilisateurs des produits et pour ceux qui traitent leurs restes, résidus, ou des déchets souillés par ces produits toxiques et/ou dangereux mais aussi en amont pour ceux qui travaillent à l'ergonomie des postes de travail ou à la conception des process, et en aval pour les soignants, en informant les travailleurs et le personnel d'urgence (dont les centres anti-poison) sur les risques liés à ces produits et les moyens de les réduire. Ces fiches sont très utilisées pour cataloguer l'information sur les produits chimiques. On doit pouvoir les trouver partout où une substance est utilisée. En Europe, elles doivent notamment être distribuées par le fabricant ou le distributeur du produit au client et dans la langue de ce dernier. Elles sont également transmises au médecin du travail.
Les fiches internationales de sécurité chimique (fiches ICSC) fournissent sous une forme claire et concise l’essentiel des données relatives à la sécurité et à la protection de la santé dans l’utilisation des produits chimiques. Le but principal des fiches ICSC est la promotion de la sécurité dans l’utilisation des produits chimiques sur le lieu de travail et les destinataires principaux sont les travailleurs. Le projet ICSC est un projet conjoint de l’Organisation mondiale de la Santé (OMS) et du Bureau international du Travail (BIT), mené en coopération avec la Commission européenne.
Un flegmatisant, ou stabilisant d'explosif, (phlegmatizer en anglais) est une substance qui, ajoutée à un explosif, tend à le stabiliser ou à le désensibiliser. Le flegmatisant diminue la sensibilité aux chocs et aux frictions accidentels de l'explosif. Le mélange obtenu, rendu moins sensible à la détonation, est plus sûr. Parfois, la stabilisation est souhaitable pour permettre la manipulation ou réduire le taux de combustion. Normalement, seuls des explosifs pouvant être moulés sont stabilisés. Certains composés explosifs ne peuvent exister que dans des états qui limitent leur application. Des précautions doivent toujours être observées lors du transport, du stockage et de la manipulation du produit flegmatisé.
La sécurité en laboratoire renvoie à de multiples aspects (prévention technique, respect d'un minimum de consignes, formation du personnel, organisation du travail, qualité des relations). Lors de son travail, le personnel de laboratoire s'expose à des dangers chimiques, physiques, biologiques et radiologiques. Ces dangers peuvent être évités ou limités, si les règles élémentaires de sécurité sont respectées. Dans un laboratoire, il faut avoir une attitude réfléchie pour ne pas mettre sa vie en danger ni celle d'autrui.
Le NFPA 704 est un standard créé par l'organisation américaine National Fire Protection Association (en). Plus familièrement connu sous le nom de « diamant du feu », cet étiquetage définit les principaux risques liés à chaque produit. Cette norme est surtout utilisée dans le monde anglo-saxon, mais il n'est pas rare de le retrouver dans d'autres pays que les États-Unis. Cet étiquetage permet à la personne qui manipule un produit d'en connaître rapidement le niveau de dangerosité grâce à un code couleur et un échelonnage du risque en 5 niveaux. Dans le cas où des produits se trouvent stockés dans un lieu en proie à un incendie par exemple, cet étiquetage permet également aux équipes d'intervention d'identifier rapidement les risques propres à chaque substance, et de choisir l'équipement d'intervention en conséquence et ainsi éviter des risques supplémentaires.
Les phrases de risque (« phrases R ») sont des annotations présentes sur les étiquettes de produits chimiques qui indiquent les risques encourus lors de leur utilisation, de leur contact, de leur ingestion, de leur inhalation, de leur manipulation ou de leur rejet dans la nature ou l'environnement. Elles se présentent sous la forme d'un R suivi d’un ou de plusieurs nombres, chacun correspondant à un risque particulier. Elles sont définies dans l'annexe III de la directive européenne 67/548/CEE : Nature des risques particuliers attribués aux substances et préparations dangereuses. La liste a été complétée et publiée à nouveau dans la directive 2001/59/CE. Cette législation a été abrogée au 1er juin 2015 et les phrases R vont devenir des phrases H suivant les directives du système général harmonisé de classification et d'étiquetage des produits chimiques.
La réaction chimique, en vertu des énergies considérables qu'elle implique, peut mener, si elle est conduite de manière non contrôlée, à des accidents extrêmement graves, comme des explosions, qui peuvent être mortelles. Parmi les cas les plus marquants, citons le cas de la catastrophe de Seveso, où une pollution majeure à la dioxine fut provoquée par l'emballement d'un réacteur. En raison de tels accidents survenus par le passé, le public perçoit bien souvent l'industrie chimique comme une menace importante pour son environnement. La sécurité des procédés chimiques, et en particulier celle des réacteurs est de nos jours une préoccupation majeure de cette industrie. De manière générale, l'accident survient lorsque la chaleur produite par la réaction chimique ne peut plus être intégralement évacuée par le système de refroidissement, ce qui provoque une accumulation d'énergie, sous forme d'augmentation de température et de pression, qui peut mener à l'explosion. Une évaluation du danger est nécessaire pour chaque étape du procédé, de l'arrivée des réactifs jusqu'au transport des produits, en passant par leur conditionnement. Certains accidents majeurs ont par exemple été provoqués par des stockages mal dimensionnées ou défectueux, comme la catastrophe de Bhopal ou celle de l'usine AZF à Toulouse. Les concepts développés ci-dessous concernent essentiellement la sécurité des réacteurs, mais ils sont cependant largement applicables aux sites de stockages.
Le règlement CLP (en anglais : Classification, Labelling, Packaging) désigne le règlement (CE) n° 1272/2008 du Parlement européen relatif à la classification, à l’étiquetage et à l’emballage des substances chimiques et des mélanges. Texte officiel de référence en Europe, il permet la mise en application du système général harmonisé (SGH) dans les secteurs du travail et de la consommation. Publié au JOUE n° L 353 le 31 décembre 2008, il est entré en vigueur le 20 janvier 2009. Le règlement (CE) n° 790/2009 de la Commission du 10 août 2009, publié le 5 septembre 2009 adapte le CLP au progrès technique et scientifique. L'application du CLP prévoit la modification puis l'abrogation en 2015, des directives : 67/548/CEE modifiée relative à la classification, l’étiquetage et l’emballage des substances dangereuses. On parle également de cette directive sous l’appellation DSD : « Dangerous Substances Directive ». 1999/45/CE modifiée relative à la classification, l’étiquetage et l’emballage des préparations dangereuses, plus connue sous le nom de « Dangerous Preparations Directive » (DPD).
Le risque chimique est le résultat de l'exposition (professionnelle ou non) à un agent chimique dangereux, généralement (hors situation accidentelles) à l'occasion d'activités de production, manutention, stockage, transport, élimination ou traitement, ou à la diffusion volontaire dans l'environnement de produits chimiques dangereux. Cet article (mis à part les problèmes de toxicité traités ailleurs) présente les risques suivants : asphyxie, risques liés au pH, incendie, explosion, fluides cryogéniques, rayonnement ionisant), avec un résumé des conséquences sur les biens et les personnes ou l’environnement ; et les moyens de prévention.
Le système de signalisation des substances dangereuses se fonde sur l'usage de symboles standardisés en forme de pictogrammes qui identifient les risques entraînés par la mise en marché ou l'usage de certaines substances chimiques. Le système européen d'étiquetage des substances dangereuses a été défini dans l'annexe II de la directive 67/548/CEE (27 juin 1967). Il est maintenant remplacé par le système général harmonisé (SGH) suivant le règlement CLP (CE n° 1272/2008 « classification, étiquetage et emballage ») depuis le 1er décembre 2010 pour les substances et depuis le 1er juin 2015 pour les mélanges.
Le système général harmonisé de classification et d'étiquetage des produits chimiques (abrégé « système général harmonisé », SGH ; en anglais : Globally Harmonised System of Classification and Labelling of Chemicals, GHS) est un système international d'étiquetage des matières dangereuses. Il est destiné à unifier les différents systèmes nationaux en vigueur. Son développement a commencé au Sommet de la Terre de Rio en 1992, lorsque l'OIT, l'OCDE et différents gouvernements et parties prenantes se sont réunis à l'ONU. Il remplace le précédent étiquetage européen avec le règlement CLP.
La température maximale de stockage en sécurité (MSST, Maximum safe storage temperature en anglais) est la température la plus élevée pour stocker une substance réactive (telle un peroxyde organique) ; au-dessus de cette température, une lente décomposition ou une explosion peut se produire. L'ajout d'un flegmatisant à la substance la rend moins sensible à la détonation et donc plus stable et plus sûre à transporter, stocker et manipuler.
On peut définir la thermodynamique de deux façons simples : la science de la chaleur et des machines thermiques ou la science des grands systèmes en équilibre. La première définition est aussi la première dans l'histoire. La seconde est venue ensuite, grâce aux travaux pionniers de Ludwig Boltzmann. Avec la physique statistique, dont elle est désormais une partie, la thermodynamique est l'une des grandes théories sur lesquelles se fonde la compréhension actuelle de la matière.
En physique nucléaire, l'activité physique ou l'activité d'une source radioactive est la vitesse de désintégration du matériau radioactif la constituant. Elle correspond au nombre d'atomes radioactifs qui se désintègrent par unité de temps. Le Système international d'unités définit le becquerel (Bq) comme unité pour l'activité d'une source (bien qu'en toute rigueur l'unité aurait dû être un nombre de moles par seconde, homogène à une activité catalytique). Un becquerel correspond à une désintégration par seconde. Ce nom a été donné en hommage au physicien français Henri Becquerel (1852-1908). Cette unité ne concerne en aucun cas les sources de rayonnement X. Elle concerne les rayonnements α, β et γ. Il convient de ne plus utiliser l'ancienne unité : le curie (Ci). Un curie correspond à 37 milliards de désintégrations par seconde, donc à 37 milliards de becquerels dans le SI. Comme l'activité                         A         (         t         )                 {\displaystyle A(t)}    à l'instant                         t                 {\displaystyle t}    est un nombre de noyaux désintégrés en une seconde, on peut établir la relation :                         A         (         t         )         =         −                                                                 d                              N                                                          d                              t                                                            {\displaystyle A(t)=-{\frac {\mathrm {d} N}{\mathrm {d} t}}~}    (le signe moins dénote la décroissance du nombre d'atomes au cours du temps) soit :                         A         (         t         )         =         λ         N                           {\displaystyle A(t)=\lambda N~}    or par résolution d'équation différentielle :                         N         (         t         )         =                    N                        0                                                                    e                                −                 λ                 t                                                                               {\displaystyle N(t)=N_{0}{\rm {e^{-\lambda t}~}}}    alors :                         A         (         t         )         =         λ                    N                        0                                                                    e                                −                 λ                 t                                                                               {\displaystyle A(t)=\lambda N_{0}{\rm {e^{-\lambda t}~}}}    on remarque que :                                    A                        0                             =         λ                    N                        0                                               {\displaystyle A_{0}=\lambda N_{0}~}    on trouve finalement (formule de décroissance) :                         A         (         t         )         =                    A                        0                                                                    e                                −                 λ                 t                                                                               {\displaystyle A(t)=A_{0}{\rm {e^{-\lambda t}~}}}    On peut aussi définir l'activité comme cela : c'est l'activité au bout de n périodes radioactives, une période étant égale à T1/2, le temps au bout duquel l'activité radioactive est divisée par 2 :                         A         (         n         T         )         =                                                A                                0                                                        2                                n                                                                           {\displaystyle A(nT)={\frac {A_{0}}{2^{n}}}~}    Note : une première définition historique de l'activité était le « nombre de particules émises par seconde » par une matière radioactive. Cette définition faisant référence à des paramètres variables du fait des conditions locales de mesure, elle a été abandonnée.
L'activité de l'eau (symbole                                    a                                       w                                                  {\displaystyle a_{\mathrm {w} }}    pour activity of water) représente la pression de vapeur d'eau p d'un produit humide divisée par la pression de vapeur saturante                                    p                        0                                     {\displaystyle p_{0}}    à la même température.                                    a                                       w                                          ≡                                 p                            p                                0                                                                 {\displaystyle a_{\mathrm {w} }\equiv {\frac {p}{p_{0}}}}    Ce paramètre traduit les interactions de l'eau avec la matrice de l'aliment. L'activité de l'eau est l'un des principaux paramètres influençant la conservation des aliments ou des produits pharmaceutiques. Les micro-organismes ont besoin d'eau « libre » (libre pour les réactions biochimiques) pour se développer. L'activité de l'eau ne représente pas la teneur en eau (ou humidité) mais bien la disponibilité de cette eau. Plus l'activité de l'eau est élevée, plus la quantité d'eau libre est grande (1 étant le maximum) et plus les micro-organismes se développeront. Les champignons ont habituellement besoin d'une                                    a                                       w                                                  {\displaystyle a_{\mathrm {w} }}    d'au moins 0,7 et les bactéries d'au moins 0,91. L'eau a aussi un impact sur la texture de l'aliment. Le contrôle de l'eau est maintenant un paramètre défini validé par l'USP 1112. Afin de diminuer cette activité, on peut : sécher le produit ; ajouter un soluté qui va fixer l'eau et la rendre non-utilisable par les micro-organismes : c'est la salaison des produits de charcuterie, par exemple, ou le sucrage des confitures.
L'affinité chimique en thermodynamique est une fonction d'état permettant de prévoir le sens de l'évolution d'une réaction chimique. La notion d'affinité chimique, héritée de l'alchimie (elle est mentionnée au XIIIe siècle dans les travaux d'Albert le Grand), est longtemps évoquée sans être formellement définie par les savants étudiant les réactions chimiques. En 1718 et 1720, Geoffroy présente à l'Académie des sciences sa table des rapports, listes d'affinités chimiques obtenues par l'observation des réactions des substances les unes avec les autres. En 1733 pour Boerhaave, médecin de Leyde, l'affinité chimique est la force en vertu de laquelle les particules des corps se recherchent, s'unissent et se retiennent,. Berthollet, qui le premier évoque en 1803 la notion d'équilibre chimique, utilise la notion d'affinité chimique sans lui donner une définition claire, tout en la rapprochant de la loi universelle de la gravitation,. Peu après, suite aux récentes découvertes dans le domaine de l'électricité, et notamment l'électrolyse de certains sels par Davy, Berzelius exprime la notion d'affinité chimique en termes d'attraction et de répulsion de pôles chargés sur ce que l'on ne nommait pas encore les molécules. En 1879, Berthelot énonce son principe de travail maximum, tentant de relier l'affinité à la chaleur dégagée par la réaction, qui selon lui ne pouvait être que positive. La définition formelle actuelle de l'affinité chimique par de Donder, qui la relie au deuxième principe de la thermodynamique, date de 1922.
L'air humide est le nom donné à l'air par les scientifiques utilisant l'air comme vecteur de vapeur d'eau tels que les climatologues, thermiciens et ingénieurs du génie des procédés. L'air humide sera caractérisé par plusieurs grandeurs dont on gardera traditionnellement: La température (dite température sèche) La température humide ou température de thermomètre humide La température de rosée ou point de rosée L'humidité absolue ou teneur en eau L'humidité relative L'enthalpie Toutes ces grandeurs ne sont pas indépendantes. Connaître deux grandeurs permet de calculer toutes les autres. Ainsi, ces différents paramètres ont été regroupés dans des diagrammes appelés diagrammes enthalpique de l'air humide différents suivant les variables indépendantes choisis et sur lesquelles figurent quatre types de courbes (Enthalpie, Température, Teneur en eau et Humidité relative): diagrammes de Carrier (la teneur en eau en fonction de la température) diagrammes de Mollier (l'enthalpie en fonction de la teneur en eau en fonction de la température en coordonnées rectangulaires) diagrammes de Mollier-Ramzin (l'enthalpie en fonction de la teneur en eau en fonction de la température en coordonnées obliques) diagrammes de Mahn (l'enthalpie en fonction de l'humidité relative) diagrammes de Höhn (l'enthalpie en fonction de la température)  Portail de la physique  Portail de la chimie
L'anneau de 's Gravesande est une expérience de physique des matériaux et de thermodynamique conçue par le physicien néerlandais Willem 's Gravesande au XVIIIe siècle, qui a pour but d'illustrer la dilatation thermique, en observant le passage d'une boule de métal au travers d'un anneau. En effet, ceux-ci sont choisis de sorte que la différence entre le diamètre intérieur de l'anneau, et celui de la boule, soit suffisamment faible pour qu'elle puisse y passer dans les conditions normales, mais ne le puisse plus lorsqu'elle est chauffée, mettant ainsi en évidence une augmentation de son volume sous l'effet de la chaleur.
Un bain de sable est un équipement commun de laboratoire fabriqué à partir d'un récipient rempli de sable chauffé. Il est utilisé pour fournir un chauffage uniforme pour un autre conteneur tel qu'un réacteur (dans lequel se déroule une réaction chimique) ou un erlenmeyer. Le bain de sable permet d'atteindre 400 °C.
La caléfaction (du latin calefacere : chauffer) est un phénomène d'isolation thermique d'un liquide par rapport à une surface chauffante ayant atteint une température seuil Ts supérieure à la température d'ébullition du liquide Te.
Herbert Bernard Callen (1919 – 22 mai 1993) est un physicien américain renommé pour ses travaux en thermodynamique.
En thermodynamique, les relations de Maxwell sont utilisées pour le calcul des grandeurs thermodynamiques inconnues en fonction de grandeurs connues. Ces relations sont nombreuses et très similaires. Un moyen mnémotechnique peu connu en France mais pourtant très pratique est l'usage d'un carré constitué de 8 cases reprenant les variables et potentiels thermodynamiques. Cette méthode a été développée par Max Born sous une forme un peu différente.Il existe des variantes qui généralisent en volume ce procédé  et . Les côtés du carré comportent les potentiels thermodynamiques et les coins opposent les variables conjuguées. Les variables situées sur le côté gauche du carré sont affectées d'un signe négatif. Les anglais utilisent des phrases pour retenir l'ordre des lettres, comme "Good Physicists Have Studied Under Very Fine Teachers" (en français :"Les bons physiciens ont eu de très bons professeurs").
Une cellule thermophotovoltaïque est une cellule photovoltaïque optimisée pour la conversion en électricité d'un rayonnement électromagnétique infrarouge. Cette technologie, conceptualisée par Pierre Aigrain et réalisée pour la première fois par Henry Kolm au MIT en 1956, est à présent activement investiguée de par le monde car elle permettrait d'élargir sensiblement le spectre de longueurs d'onde susceptibles d'être converties en électricité, et donc d'étendre les domaines d'application de l'énergie photovoltaïque. Elle se heurte néanmoins à des difficultés techniques qui limitent fortement le rapport de son efficacité énergétique à son coût de revient.
La chaleur sensible est la quantité de chaleur qui est échangée, sans transition de phase physique, entre plusieurs corps formant un système isolé. Elle est qualifiée de « sensible » parce que cet échange de chaleur sans changement de phase change la température du corps, effet qui peut être ressenti ou mesuré par un observateur. En cela, la chaleur sensible s'oppose à la « chaleur latente », qui, elle, est absorbée lors d'un changement de phase, sans changement de température.
Cet article présente une frise chronologique des évènements notables de l'histoire de la thermodynamique, de la physique statistique et des processus stochastiques.
En physique, et plus particulièrement en thermodynamique, les coefficients calorimétriques sont des coefficients permettant d'exprimer la chaleur absorbée par un système thermodynamique subissant une transformation sans changement de composition chimique ni changement de phase en fonction des variables pression, température et volume. Les coefficients thermoélastiques permettent d'exprimer la variation de volume ou de pression subie par le système en fonction de ces mêmes variables et ainsi d'établir l'équation d'état d'un corps pur ou d'un mélange. Tous ces coefficients sont liés aux potentiels thermodynamiques qui permettent d'établir diverses relations entre eux, notamment les relations de Clapeyron, la relation de Mayer et la relation de Reech.
Le compresseur axial est un compresseur dont le flux de vapeur suit l'axe de rotation, et dont le fluide de sortie a un mouvement radial. Le compresseur axial génère un flux continu de vapeur comprimé et fournit un rendement élevé pour une masse volumique donnée et une section donnée du compresseur[réf. nécessaire]. Il est nécessaire d'avoir plusieurs étages de pales pour obtenir des pressions élevées et des taux de compression équivalent a ceux d'un compresseur centrifuge.
Le terme « compresseur centrifuge » (aussi appelé « compresseur radial ») désigne un type de turbomachines à circulation radiale et à absorption de travail qui comprend des pompes, des ventilateurs, des soufflantes et des compresseurs. Les premières machines de ce type étaient composées de pompes, de ventilateurs et de soufflantes.
La compression est le contraire de la détente. Un gaz diminue son volume par abaissement de température et/ou augmentation de pression.
La comptabilité énergétique est un système qui permet de mesurer, analyser et rendre compte régulièrement de la consommation d'énergie liée à différentes activités dans le but d'améliorer l'efficacité énergétique, et de surveiller l'impact environnemental de la consommation d'énergie.
La condensation est le phénomène physique de changement d'état de la matière d'un état gazeux à un état condensé (solide ou liquide). Le passage de l'état gazeux à l'état liquide est aussi appelé liquéfaction. La cinétique de ce phénomène est décrite par la relation de Hertz-Knudsen. Dans la nature la condensation de la vapeur d'eau est une étape importante du cycle de l'eau, à l'origine notamment de la rosée, des nuages et de la pluie, de la neige, du givre ou de certaines formes de verglas (brouillard givrant, qui est une forme de condensation solide). On peut expérimenter ce changement d'état lors d'une douche où, au contact du miroir froid, la vapeur d'eau présente dans l'air se condense en gouttelettes.
La condensation, dans le cas des aérosols, se produit lorsque de la vapeur se condense sur une particule, ou s’en évapore. Le phénomène de la condensation peut être résumé par la transition de phase de gazeux à liquide lorsque la vapeur se condense sur la particule, ou, dans le cas de l’évaporation, de liquide vers gazeux. Lorsque la vapeur se condense, la particule hôte grossit alors que dans le cas de l’évaporation, elle rapetisse. Cet effet est relié à la distribution de la taille des aérosols. Si le bilan est positif, on a condensation et donc la distribution des tailles tend vers les plus grosses tailles. Si le bilan est négatif, on a évaporation et la distribution des tailles tend vers les plus petites tailles. Si les particules sont plus grosses, la concentration massique dans l’air augmente. Le contraire se produit lorsque les particules rapetissent.  Les conditions ambiantes telles que la pression de vapeur et la température autour de la particule décide à savoir s’il y aura condensation ou évaporation. À partir de l’effet de Kelvin (basé sur le fait que les gouttelettes sont courbes et non plates telles les surfaces liquides habituelles) on déduit que les plus petites gouttelettes demandent une humidité relative supérieure à celle que demandent les plus grosses gouttelettes pour se maintenir à l’équilibre. L’humidité relative (exprimée en %) pour les conditions d’équilibre s’exprime de la façon suivante :                         R         H         =                    (                                                        p                                    s                                                                p                                    0                                                                   )                  ×         100         =         S         ×         100                 {\displaystyle RH=\left({\frac {p_{s}}{p_{0}}}\right)\times 100=S\times 100}    où ps est la pression de vapeur de saturation au-dessus d’une particule à l’équilibre (autour d’une surface courbe), p0 est la pression de vapeur de saturation pour une surface plane pour le même liquide et S est le rapport de saturation. L’équation de Kelvin pour la pression de vapeur de saturation autour d’une surface courbe est :                                    p                        s                             =                    p                        0                             exp         ⁡                    (                                                        2                 σ                 M                                               R                 T                 ρ                                    r                                        p                                                                                      )                          {\displaystyle p_{s}=p_{0}\exp \left({\frac {2\sigma M}{RT\rho r_{p}}}\right)}    où p0 est la pression de vapeur de saturation pour une surface plane, rp est le rayon de la gouttelette, σ est la tension de surface de la gouttelette, ρ est la densité du liquide, M est la masse molaire, T est la température, et R est la constante des gaz parfaits.  Portail de la physique
Un condenseur est un appareil dont la fonction principale est de liquéfier (ou condenser, transformation d'un gaz en liquide) de la vapeur sur une surface froide, ou via un échangeur thermique maintenu froid par la circulation d'un fluide réfrigérant. La chaleur latente du corps est transférée dans le fluide réfrigérant, ce qui consiste en un changement de phase à température constante. Le fluide réfrigérant varie en fonction du débit de gaz ou vapeur à condenser et de la température de condensation du gaz : air, eau, saumure.
Les conditions normales de température et de pression (parfois abrégé CNTP) sont des conditions pratiques, en partie arbitraires, d'expérimentation et de mesure en laboratoire en physique et en chimie. Elles permettent des comparaisons commodes entre résultats expérimentaux. Les conditions les plus usuelles fixent la température normale à 0 °C (273,15 K) et la pression normale à 1 atm (1,013 bar = 101 325 Pa), soit la pression atmosphérique au sol. Il existe toutefois d'autres définitions.
La conservation de l'énergie est un principe physique, selon lequel l'énergie totale d'un système isolé est invariante au cours du temps. Ce principe, largement vérifié expérimentalement, est de première importance en physique, et impose que pour tout phénomène physique l'énergie totale initiale du système isolé soit égale à l'énergie totale finale, donc que de l'énergie passe d'une forme à une autre durant le déroulement du phénomène, sans création ni disparition d'énergie. Alors qu'il est postulé en mécanique newtonienne, ce principe est démontrable en mécanique lagrangienne par le biais d'un théorème de Noether. On peut ainsi étudier les transformations d'énergie durant une combustion (où n'interviennent que l'énergie thermique et l'énergie des liaisons chimiques) ou une réaction nucléaire (où intervient principalement l'énergie présente dans les noyaux des atomes et l'énergie thermique). Ce principe rend impossible un mouvement perpétuel car aucun système physique réel n'étant parfaitement isolé de son environnement, son mouvement perd de l'énergie sous une forme ou une autre (frottement, lumière, chaleur, etc).
La constante universelle des gaz parfaits (notée                         R                 {\displaystyle R}   ,                                    R                        m                                     {\displaystyle R_{m}}    ou                                    R                        n                                     {\displaystyle R^{n}}   ) est le produit du nombre d'Avogadro (                                   N                                       A                                                  {\displaystyle N_{\mathrm {A} }}   ) et de la constante de Boltzmann (                                   k                                       B                                                  {\displaystyle k_{\mathrm {B} }}   ). Ce produit vaut 8,314 462 1 J mol−1 K−1 avec une incertitude de 7,5×10−6 J mol−1 K−1.
Lorsque l'on trace une courbe d'analyse thermique, on étudie la température (T), en fonction du temps (x) lors du chauffage ou du refroidissement isobare d'un corps pur. On peut alors constater qu'il existe différents paliers de changements de phases.  Portail de la physique  Portail de la chimie
La courbe d'ébullition représente la progression selon une variable – par exemple le temps ou la concentration – du point de liquide saturé. On suit donc l'endroit où, du liquide, vont s'échapper les premières molécules de gaz de cette même espèce.
Un cube de Leslie est un dispositif utilisé dans la démonstration et la mesure des variations de l'énergie de radiation selon la surface des corps. Dans une expérience décrite par Tyndall, les quatre faces verticales d'un cube étaient recouvertes par une matière différente: or, argent, cuivre et un vernis d'Ichtyocolle. Lorsque le cube est rempli d'eau, le détecteur thermique montrera des valeurs différentes selon l'émissivité de chaque surface (voir aussi les descriptions de Poynting et Thomson). L'original du cube de Leslie a été conçu en 1804 par John Leslie (1766-1832), mathématicien et physicien écossais. À l'origine, il s'agissait d'un cube de laiton avec deux côtés bruts, un côté poli et le quatrième peint en noir. La mesure du rayonnement de chaque surface obéit à la loi de Stefan-Boltzmann et à la loi de Kirchhoff. Cette expérience a été un pas majeur dans la discipline de la thermographie dans la mesure où elle démontrait l'existence de l'émissivité variable des surfaces et a permis également de composer les tables d'émissivité des matériaux. Cependant le choix d'origine des matériaux était impropre dans la mesure où la majorité de ces surfaces avaient une émissivité si basse qu'elle était presque immensurable mais, en tant qu'expérience, a permis de démontrer des effets d'écart si forts qu'ils ont incité à creuser le sujet et donc à aboutir à des tables de correction des températures perçues selon leur nature et leur état de surface.
Le cycle transcritique est un cycle thermodynamique dans lequel le fluide utilisé peut se trouver en dessous et au-dessus de son point critique. La réfrigération utilisant le CO2 (R-744) comme fluide réfrigérant est un exemple d’utilisation d’un cycle transcritique. La température critique assez basse du CO2 (31 °C) impose l’utilisation d’un cycle supercritique, et donc de très fortes pressions, pour pouvoir dissiper la chaleur dans un environnement extérieur où règnent des températures supérieures à 15 ou 20 °C. Le cycle transcritique moderne a été développé entre 1988 et 1991 par le chercheur norvégien Gustav Lorentzen (1915-1995) et son équipe.
Le démon de Maxwell est une expérience de pensée imaginée par James Clerk Maxwell en 1867, pour suggérer que la seconde loi de la thermodynamique n'est vraie que de manière statistique. Cette loi établit l'irréversibilité de phénomènes de physique statistique et notamment des transferts thermiques, se traduisant par une augmentation continue de l'entropie. Par exemple, si on laisse ouverte la porte d'un réfrigérateur éteint, la température du réfrigérateur et de la pièce vont s'équilibrer, et cela de manière irréversible sans apport d'énergie. Or, l'expérience du démon de Maxwell propose un processus permettant de revenir à un état de température inégal, sans dépenser d'énergie, et en diminuant l'entropie, ce qui est en principe impossible selon la seconde loi de la thermodynamique. Ce paradoxe a suscité, et suscite encore, un grand nombre d'études et de débats depuis son énoncé en 1871. Pendant soixante-deux ans, son étude n'a pas tellement progressé, jusqu'à ce que Leó Szilárd propose en 1929 un modèle physique du démon de Maxwell permettant d'étudier précisément et formellement le processus. Vingt ans plus tard, en 1949, Léon Brillouin propose une solution du paradoxe mettant l'accent sur la nécessité pour le démon d'acquérir de l'information, et mettant en évidence que cette acquisition augmente l'entropie du système et sauve la seconde loi. Après avoir été adoptée par la plus grande partie de la communauté scientifique, cette solution a de plus en plus été remise en question, notamment par l'établissement de modèles de "démons" automatiques, où l'acquisition d'information ne joue pas un rôle déterminant. Le lien fait par Brillouin entre l'entropie et la théorie de l'information a également été critiqué. Un nouveau tournant a lieu en 1961, quand Rolf Landauer — suivi de Charles Bennett — met en évidence l'importance de la mémorisation de l'information et surtout de la nécessité d'effacer cette mémoire pour réaliser un cycle thermodynamique complet. L'effacement de la mémoire ayant un coût en entropie, cela rétablit le second principe de la thermodynamique. Des études plus modernes mettant en jeu des versions quantiques du démon de Maxwell, effectuées notamment par Wojciech Hubert Zurek dans les années 1980 confirment le principe de Landauer. Toutefois, de nouveaux modèles d'expériences de pensée remettant en cause le second principe continuent à être proposés dans les années 2000, soit ne nécessitant pas d'effacement d'information, soit n'utilisant pas du tout le concept d'information, ni même de démon, mais tirant parti de conditions spécifiques comme des géométries non euclidiennes, l'intrication quantique ou des champs de force.
Pour une surface frontière définie, en un point de cette surface, la densité de flux thermique                         φ                 {\displaystyle \varphi }    – ou flux thermique surfacique ou densité surfacique de flux thermique, ou encore densité de flux de chaleur – est le flux thermique par unité de surface. Cette grandeur est donc définie pour ce qui traverse un point d'une surface donnée, lequel est à la frontière du système physique considéré. Elle s'exprime en watt par mètre carré (W/m2 ou W·m-2).                         φ         =                                                                 d                              Φ                                                          d                              S                                                  {\displaystyle \varphi ={\frac {\mathrm {d} \Phi }{\mathrm {d} S}}}    Le vecteur densité de courant thermique                                                 φ             →                                     {\displaystyle {\overrightarrow {\varphi }}}    ou                                                                j                                                   t                   h                                                          →                                     {\displaystyle {\overrightarrow {j_{\mathrm {th} }}}}    – ou vecteur densité de flux thermique – est est lié au flux thermique surfacique sur une frontière particulière par                         φ         =                                 φ             →                             ⋅                                 n             →                                     {\displaystyle \varphi ={\overrightarrow {\varphi }}\cdot {\overrightarrow {n}}}   , où                                                 n             →                                     {\displaystyle {\overrightarrow {n}}}    est le vecteur unitaire normal à l'élément de surface                                    d                  S                 {\displaystyle \mathrm {d} S}   . En notant                                                                                 d                              S                          →                             =                                 n             →                             ⋅                    d                  S                 {\displaystyle {\overrightarrow {\mathrm {d} S}}={\overrightarrow {n}}\cdot \mathrm {d} S}   , le flux thermique élémentaire s'exprime :                                    d                  Φ         =                                 φ             →                             ⋅                                                                 d                              S                          →                                     {\displaystyle \mathrm {d} \Phi ={\overrightarrow {\varphi }}\cdot {\overrightarrow {\mathrm {d} S}}}   . Macroscopiquement, le flux thermique global à travers une surface (grandeur extensive) se définit comme le flux (au sens mathématique du terme) du champ vectoriel                                                                j                                                   t                   h                                                          →                                     {\displaystyle {\overrightarrow {j_{\mathrm {th} }}}}    à travers cette surface :                         Φ         =                    ∬                        S                                                     φ             →                             ⋅                                                                 d                              S                          →                                     {\displaystyle \Phi =\iint _{S}{\overrightarrow {\varphi }}\cdot {\overrightarrow {\mathrm {d} S}}}   .
La densité de puissance (ou densité volumique de puissance, ou simplement puissance volumique) est la quantité de puissance (débit d'échange d'énergie) enmagasinable par un système physique donné, divisé par le volume de ce système. Cette grandeur physique traduit la capacité du système à transmettre de l'énergie, à volume donné. On l'exprime en W/m3. La densité de puissance peut être un critère important lorsqu'il existe une contrainte sur le volume disponible.
La densité surfacique de puissance décrit la manière dont un flux de puissance se répartit sur une surface donnée. On l'exprime en watt par mètre carré (W m-2 ou W/m2) ou, plus rarement, en kilowatts-heures par mètre carré par an (kW h m−2 a−1, sachant que 1 W m−2 = 8,760 kW h m−2 a−1). Ce terme est utilisé dans de nombreux domaines.
Les bassins des piscines perdent beaucoup de chaleur. La déperdition thermique associée aux piscines extérieures a lieu surtout à la surface de l’eau par évaporation, par rayonnement et par convection. C’est surtout par évaporation que les piscines intérieures subissent des pertes thermiques. Les couvre-piscines éliminent pratiquement toutes les pertes par évaporation; elles réduisent aussi celles par rayonnement et par convection.
En physique, le mot dépression désigne : en général, une diminution de la pression, par rapport à une pression de référence ; dans un cas particulier, la baisse de niveau d'un liquide dans un tube capillaire en forme de U. L'origine du mot vient d'un moyen de mesure : était branché par un tuyau le volume de gaz dont on veut mesurer la dépression à l'une des extrémités d'un tube de verre en U rempli à moitié d'eau colorée : les niveaux de liquide dans les deux branches sont égaux. Si la pression du volume baisse, il aspire le liquide dans la colonne du U à laquelle il est joint, et corrélativement baisse le niveau du liquide coloré dans la colonne du U restée en relation avec la pression atmosphérique par son extrémité ouverte, dans laquelle il y a dépression du niveau du liquide coloré. Plus tard, le liquide coloré sera remplacé par du mercure, d'une densité 13,6 fois plus importante, pour mesurer, donc, des dépressions d'autant plus importantes. Ce système sera perfectionné pour, lui-même, devenir l'outil à créer des dépressions dans un arrangement différent du tube en U par l'adjonction d'un goutoir, inventé en 1865 par Hermann Sprengel qui lui donne le nom de trompe à mercure. L'unité de mesure des pressions a donc été d'abord le millimètre de mercure (mmHg) correspondant à la mesure de la différence des hauteurs des niveaux dans les branches du U, pour ensuite être comparée à la pression atmosphérique. Aujourd'hui l'unité de pression est le pascal (Pa).
La détente est le contraire de la compression. Un gaz augmente son volume en se chauffant et / ou en baissant sa pression.
La dilatation thermique est l'expansion à pression constante du volume d'un corps occasionné par son réchauffement, généralement imperceptible. Dans le cas d'un gaz, il y a dilatation à pression constante ou maintien du volume et augmentation de la pression lorsque la température augmente.
L'ébulliométrie est l'étude de la vaporisation d'un mélange de N1 molécules d'un liquide 1 et de N2 molécules d'un liquide 2: par exemple le diazote et le dioxygène : on appelle X2 := N2/(N1+N2) la fraction molaire du deuxième liquide.
L’ébullition est la formation de bulles lors du changement violent d’un corps de l’état liquide vers l’état gazeux (une vaporisation rapide). Il se produit lorsque la pression de vapeur saturante est égale ou supérieure à la pression du liquide : les bulles de vapeur formées au fond du récipient deviennent stables et peuvent donc remonter à la surface.
Un aérocondenseur est un type d'échangeur thermique à contact indirect, destiné à refroidir et condenser de la vapeur avec de l'air frais.
Un échangeur de chaleur est un dispositif permettant de transférer de l'énergie thermique d'un fluide vers un autre sans les mélanger. Le flux thermique traverse la surface d'échange qui sépare les fluides. L'intérêt du dispositif réside dans la séparation des deux circuits et dans l'absence d'autres échanges que la chaleur, gardant à chaque fluide ses caractéristiques physico-chimiques inchangées (pression, concentration en éléments chimiques, ...) hormis bien sûr leur température et/ou leur état. Un échangeur se caractérise par les fluides en présence, le but recherché et la puissance à mettre en œuvre ; ces critères déterminent sa forme et ses dimensions optimales.
Un échangeur de chaleur rotatif, encore appelé roue thermique, est un type d'échangeur de chaleur entre deux flux de gaz (généralement de l'air).
L'effet Dufour est une composante du flux de chaleur lié à la diffusion dans les milieux multiespèces.
En physique-chimie, l'effet Gibbs-Thomson décrit la relation entre la tension de surface et la pression de vapeur saturante d'un système composé de deux phases. Elle est nommée d'après les physiciens Josiah Willard Gibbs, et Joseph John Thomson.
En physique, l'effet Joule-Thomson, également appelé effet Joule-Kelvin, est un phénomène lors duquel la température d'un gaz diminue lorsque ce gaz subit une expansion adiabatique. L'effet a été baptisé d'après les physiciens James Prescott Joule et William Thomson (Lord Kelvin). Ce dernier établit l'existence de cet effet en 1852 en poursuivant les travaux de Joule sur l'expansion de Joule, lors de laquelle un gaz subit une expansion à énergie interne constante.
En physique et ingénierie mécanique, l'efficacité énergétique (ou efficacité thermodynamique) est un nombre sans dimension, qui est le rapport entre ce qui peut être récupéré utilement de la machine sur ce qui a été dépensé pour la faire fonctionner. Cette notion est souvent confondue avec une définition du rendement thermodynamique, pour des systèmes dont l'efficacité énergétique théorique maximale est inférieure à un, comme les moteurs dithermes ou les moteurs électriques. Toutefois, il est déconseillé d'utiliser le terme de rendement à la place de l'expression efficacité énergétique pour des machines dont l'efficacité énergétique théorique maximale est supérieure à un, comme les systèmes disposant d'un cycle récepteur de la chaleur ambiante, telle une pompe à chaleur. En économie, le terme d’efficacité énergétique est utilisé de manière synonyme de l’efficience énergétique, qui consiste à réduire les consommations d’énergie, à service rendu égal.
L'énergétisme, ou énergétique, est une théorie physique adossée à la philosophie selon laquelle toute réalité est énergie. Les processus physiques et mentaux y sont interprétés comme des échanges d'énergie.
En physique statistique, l'ensemble microcanonique est un ensemble statistique,  constitué des répliques fictives d'un système réel pouvant être considéré comme isolé, par suite dont l'énergie (E), le volume (V) et le nombre de particules (N) sont fixés. Cet ensemble statistique a une importance particulière, car c'est à partir de celui-ci que le postulat de la physique statistique est défini. Cet ensemble permet aussi de déterminer les ensembles canonique et grand-canonique, à l'aide d'échanges d'énergie ou de particules avec un réservoir. Le terme d'ensemble microcanonique a été introduit par Gibbs dans son traité fondateur de la mécanique statistique. Toutefois Boltzmann avait déjà considéré une telle situation, qu'il avait nommé ergode dans un article de 1884, .
Un équilibre chimique est le résultat de deux réactions chimiques antagonistes simultanées dont les effets s'annulent mutuellement. Une réaction telle que la combustion du propane avec l'oxygène, s'arrêtant lorsque l'un des réactifs est totalement épuisé, est qualifiée de réaction totale, complète ou irréversible. À contrario, une réaction comme l'estérification, aboutissant à un mélange stable dans le temps de réactifs et de produits, sans disparition totale de l'une des espèces, est qualifiée de réaction partielle, incomplète, réversible ou inversible : ce type de réaction aboutit à un équilibre chimique. Au cours d'un processus de transformation chimique deux réactions peuvent s'opposer, l'une consommant des réactifs, l'autre consommant les produits de la première réaction pour recréer les réactifs initiaux. Une réaction est totale lorsqu'elle l'emporte sur sa réaction antagoniste. Un équilibre chimique apparaît lorsque la première réaction consomme les réactifs aussi vite que la seconde les recrée. De plus, une modification des conditions opératoires d'un équilibre chimique (modification de la pression ou de la température, ajout ou extraction de l'un des constituants du mélange réactionnel, etc.) pourra favoriser l'une ou l'autre réaction, impliquant un déplacement de l'équilibre, c'est-à-dire l'obtention d'un nouvel état d'équilibre à une composition différente de celle de l'équilibre initial ; un retour aux conditions opératoires initiales induira un retour à l'équilibre initial. Dans certains cas, la modification des conditions opératoires pourra conduire à une rupture d'équilibre, c'est-à-dire l'obtention d'une réaction totale. La notion d'équilibre chimique fut évoquée pour la première fois en 1803 par Berthollet à la suite de ses observations sur les rives du lac Natron lors de la campagne d'Égypte. Avant lui, les réactions chimiques étaient supposées être toujours totales,. Il fallut cependant attendre la deuxième moitié du XIXe siècle pour que des progrès significatifs soient faits dans la compréhension des réactions et équilibres chimiques. En 1858 Kirchhoff énonçait ses relations liant variation de la chaleur d'une réaction chimique et différence des capacités calorifiques des produits et des réactifs. En 1865 Guldberg et Waage, en s'inspirant des propositions de Berthollet, montrèrent expérimentalement qu'il existait une relation entre les concentrations des espèces présentes à l'équilibre en solution ; la constante d'équilibre                         K                 {\displaystyle K}    qu'ils définirent a été appelée constante de Guldberg et Waage ou constante de la loi d'action de masse. Un premier formalisme mathématique des équilibres chimiques fut développé par van 't Hoff, qui énonça la relation portant son nom donnant la variation de                         K                 {\displaystyle K}    en fonction de la chaleur de la réaction. En 1884, Le Chatelier, toujours sur base d'expérimentations, énonçait son « principe de modération », dit « principe de Le Chatelier », selon lequel un équilibre s'oppose aux changements extérieurs qui tentent de le modifier. Il existe cependant des cas dans lesquels ce principe n'est pas vérifié, notamment pour l'ajout ou l'extraction d'une espèce du mélange réactionnel. Le développement ultérieur du formalisme mathématique de la thermodynamique par Gibbs (qui introduisit la fonction enthalpie libre                         G                 {\displaystyle G}    et la notion de potentiel chimique                                    μ                        i                                     {\displaystyle \mu _{i}}   ), de Donder (qui définit l'avancement de réaction                         ξ                 {\displaystyle \xi }    et l'affinité chimique                                                 A                                     {\displaystyle {\mathcal {A}}}   , reliant l'évolution des réactions chimiques au deuxième principe de la thermodynamique, donnant la condition d'évolution spontanée de toute réaction chimique) et Lewis (qui introduisit la notion d'activité chimique                                    a                        i                                     {\displaystyle a_{i}}   ) a permis en 1950 à Prigogine et Defay de démontrer rigoureusement les relations et principes formulés par leurs prédécesseurs. Ce formalisme est développé dans cet article qui définit les grandeurs permettant : de positionner l'équilibre : enthalpie libre standard de réaction :                                    Δ                                       r                                                     G                        ∘                                     {\displaystyle \Delta _{\mathrm {r} }G^{\circ }}    ; constante d'équilibre :                         K                 {\displaystyle K}    ;  de prédire l'évolution de la réaction hors équilibre : enthalpie libre de réaction :                                    Δ                                       r                                          G                 {\displaystyle \Delta _{\mathrm {r} }G}    ; quotient de réaction :                                    Q                                       r                                                  {\displaystyle Q_{\mathrm {r} }}   .
Un équilibre dynamique peut avoir différentes significations : Mécanique  l'équilibre dynamique d'une pièce en rotation est atteint lorsque aucune vibration ne se produit quelle que soit sa vitesse de rotation. Chimie  équilibre chimique dans lequel une réaction réversible se produit, mais sans modifier globalement le ratio réactif/produit, ces composés étant détruits aussi rapidement qu'ils sont produits, ce qui signifie qu'il n'y a pas de changement net dans la composition du système chimique. C'est un cas particulier d'état stationnaire. Thermodynamique  En thermodynamique un système fermé est en équilibre thermodynamique quand les réactions qui s'y produisent le font à des taux tels que la composition du mélange ne varie pas avec le temps. Des réactions se produisent bien, parfois même de façon violente, mais de telle façon que les variations de la composition du système ne peuvent pas être observées. Dynamique des populations  En dynamique des populations, peut s'établir un équilibre dynamique entre des populations de proies et de leurs prédateurs, ou autres sous-populations, tels que les individus sains, infectés et résistants dans les modèles compartimentaux en épidémiologie.  Portail de la chimie  Portail de la physique  Portail des technologies
L'équilibre liquide-gaz est un état dans lequel un liquide et sa vapeur (phase gazeuse) sont en équilibre, c'est-à-dire qu'il y a autant de vaporisation (transformation du liquide au gaz) que de liquéfaction (transformation du gaz au liquide) à l'échelle moléculaire. À l'échelle macromoléculaire, on n'observe donc pas d'interconversion liquide-gaz. Un composé à l'équilibre liquide-gaz est généralement qualifié de fluide saturé. Pour un composé chimique pur, cela implique qu'il est à son point d'ébullition. La notion de « fluide saturé » inclut le liquide saturé (sur le point de se vaporiser), le mélange liquide-gaz saturé et le gaz saturé (sur le point de se liquéfier).
En thermodynamique, un système thermodynamique est en équilibre thermodynamique quand il est à la fois en équilibre thermique, mécanique et chimique. L'état local d'un système en équilibre thermodynamique est déterminé par les valeurs de ses paramètres intensifs, comme la pression ou la température. Plus spécifiquement, l'équilibre thermodynamique est caractérisé par le minimum d'un potentiel thermodynamique, comme l'énergie libre de Helmholtz pour les systèmes à température et volume constants, ou l'enthalpie libre de Gibbs pour les systèmes à pression et température constantes. Le processus conduisant à l'équilibre thermodynamique est appelé thermalisation. Un exemple en est un système de particules en interaction isolé de toute influence extérieure. En interagissant, les particules vont échanger de l'énergie et du moment entre elles, et atteindre un état où la statistique globale du système restera invariante dans le temps.
En thermodynamique on appelle état défini d'un système, un état caractérisé par un petit nombre de paramètres pertinents ou coordonnées thermodynamiques. Par exemple, une mole de gaz prise à l'équilibre dans les conditions normales de température et de pression est dans un état défini. Un état d'équilibre d'un système simple est toujours un état défini, tandis que la réciproque est fausse. Un oscillateur constitué par une masse suspendue à un ressort évolue adiabatiquement, la longueur, la température du ressort sont définis à tout instant, mais le système n'est pas en équilibre dans le champ de pesanteur. Un système dans un état défini peut être représenté par un point figuratif dans un diagramme d'état.  Portail de la physique
Un évaporateur est un appareillage réalisant dans une de ses parties un changement d'état de liquide au gazeux. En génie chimique, l'évaporateur est un appareil dessiné pour concentrer une solution par apport d'énergie, la solution étant composée par un solvant volatil en mélange avec une phase non volatile.
L'évaporation est un passage progressif de l'état liquide à l'état gazeux. Ce phénomène est donc une vaporisation progressive qui a pour effet d'absorber des calories et donc de réduire la température de l’environnement.
L’évaporation par faisceau d'électrons est une technique qui consiste à évaporer un matériau placé dans un creuset porté à haute température, soit par un courant traversant une nacelle résistive (effet Joule), soit par le bombardement d'un faisceau d'électrons.
L'expérience de Clément-Desormes permet d'estimer le coefficient de Laplace                         γ         =                                                C                                P                                                        C                                V                                                                 {\displaystyle \gamma ={\frac {C_{P}}{C_{V}}}}   , d'un gaz parfait, où :                                    C                        P                                     {\displaystyle C_{P}}    est la capacité thermique isobare,                                    C                        V                                     {\displaystyle C_{V}}    est la capacité thermique isochore.
L'expérience de Joule a permis d'établir l'équivalence entre travail (énergie mécanique) et chaleur (énergie thermique). Cette expérience consiste à agiter un fluide en lui fournissant un travail connu et à mesurer l'élévation de température qui en résulte. Une toute première expérience, moins convaincante, utilisait un frottement solide.
L'expérience de Rüchardt permet de déterminer un ordre de grandeur du coefficient                         γ         =                                                C                                P                                                        C                                V                                                                 {\displaystyle \gamma ={\frac {C_{P}}{C_{V}}}}    d'un gaz parfait, où :                                    C                        P                                     {\displaystyle C_{P}}    est la capacité thermique isobare                                    C                        V                                     {\displaystyle C_{V}}    est la capacité thermique isochore Pour faire cette expérience, on place le gaz à étudier dans un récipient étanche muni d'un tube. On y lâche ensuite une bille de diamètre exactement ajusté à celui du tube. Cette bille va alors osciller de haut en bas dans le tube. Un calcul à l'ordre 1 permet (en faisant l'hypothèse que le gaz subit une transformation adiabatique réversible), de relier                         γ                 {\displaystyle \gamma }    à la période T des oscillations par                         γ         =                                                4                                π                                    2                                               m                                V                                    0                                                                                          p                                    0                                                                S                                    2                                                                T                                    2                                                                                  {\displaystyle \gamma ={\frac {4\pi ^{2}mV_{0}}{p_{0}S^{2}T^{2}}}}      Portail de la physique  Portail de la chimie
L'expérience de contournement du point critique consiste à transformer un liquide en vapeur, puis en une phase fluide dite "critique" (sans qu'il ait cette fois de transition visible), puis à nouveau en liquide. L'expérience est décrite de façon très détaillée dans le lien externe ci-dessous.
Les grandeurs extensives et intensives sont des catégories de grandeurs physiques d'un système physique : une propriété est « intensive » si sa valeur ne dépend pas de la taille du système (en particulier, si sa valeur est la même en tout point d'un système homogène) : par exemple, la température ou la pression ; une propriété est « extensive » si elle est proportionnelle à la quantité de matière du système : par exemple, la masse ou le volume. Si deux chevaux courent côte à côte et chacun à 60 km/h, à eux deux, ils font un ensemble allant aussi à 60 km/h (la vitesse est intensive) ; par contre, à eux deux, ils font un passage deux fois plus imposant qu'un cheval seul (débit, puissance et masse sont doublés : ce sont des grandeurs extensives). Le rapport entre deux propriétés extensives d'un même objet est une grandeur physique intensive. Ainsi, par exemple, le rapport entre la masse et le volume d'un objet est sa masse volumique moyenne, ce qui permet de mesurer la masse volumique intrinsèque de ce corps s'il est considéré comme homogène.
En chimie, le facteur de séparation est l'enrichissement obtenu par une certaine opération (physique, chimique...) qui fait varier la concentration d'un mélange.
Supercritique, en physique-chimie, est l'état de la matière lorsqu'elle est soumise à une forte pression et à une forte température. On parle de fluide supercritique lorsqu'un fluide est chauffé au-delà de sa température critique et lorsqu'il est comprimé au-dessus de sa pression critique. Cet état de la matière a été découvert en 1822 par Charles Cagniard de Latour. Les propriétés physiques d'un fluide supercritique (densité, viscosité, diffusivité) sont intermédiaires entre celles des liquides et celles des gaz. Le dioxyde de carbone supercritique est assez courant en raison de sa facilité d'obtention (température critique 31 °C, pression critique 73,8 bars) et de ses propriétés économiques et écologiques intéressantes (non inflammable, non toxique, relativement peu cher et sans coût d'élimination comparativement aux solvants organiques).
Le flux thermique ou flux de chaleur, souvent noté                         Φ                 {\displaystyle \Phi }   , entre deux milieux de températures                                    T                        i                                     {\displaystyle T_{i}}    différentes correspond au transfert thermique                         Q                 {\displaystyle Q}    qui s'écoule par unité de temps entre les deux milieux :                         Φ         =                                 Q                            Δ               t                                          .                 {\displaystyle \Phi ={\frac {Q}{\Delta t}}.}
Une fonction d'état est une fonction des variables d'état qui définissent l'état d'équilibre d'un système thermodynamique. Il ne s'agit là que d'une simple fonction, comme celles que l'on rencontre en mathématique. Sa valeur est calculable à partir de variables d'état : par exemple la température                         T                 {\displaystyle T}   ,la pression                         p                 {\displaystyle p}   , le volume                         V                 {\displaystyle V}   , variables importantes en thermodynamique. Une telle fonction possède donc la propriété de ne dépendre que de l'état d'équilibre dans lequel se trouve le système, quel que soit le chemin emprunté par le système pour arriver à cet état. En particulier, au cours d'une transformation entre deux états d'équilibre, la variation d'une fonction d'état ne dépend pas du chemin suivi par le système pendant la transformation, mais uniquement des états d'équilibre initial et final. Ceci permet de déterminer cette variation pour n'importe quelle transformation, y compris une transformation irréversible, en imaginant un chemin réversible pour lequel le calcul est possible. L’énergie interne                         U         (         T         ,         p         ,         V         )                           {\displaystyle U(T,p,V)~}    et l’entropie                         S         (         T         ,         p         ,         V         )                           {\displaystyle S(T,p,V)~}    sont des exemples de fonctions d'état. D'une façon générale, les potentiels thermodynamiques sont également des fonctions d'état particulièrement importantes en physique. Quant au travail et à la chaleur échangés pendant une transformation, ils ne peuvent pas être assimilés à la variation d'une fonction d'état car ils dépendent de la nature de la transformation. Au cas général, le travail et la chaleur sont plutôt les fonctions de parcours parce que leurs valeurs dépendent du chemin parcouru entre l'état initial et l'état final du système. Néanmoins, il existe des cas particuliers où la chaleur et le travail ne dépendent plus du chemin suivi, qu'il soit réversible ou irréversible, lorsque les transformations s’effectuent soit à pression constante (voir enthalpie) ou à volume constant. Mathématiquement, la possibilité de représenter une grandeur physique par une fonction d'état est liée aux propriétés de sa forme différentielle, c'est-à-dire sa variation dans une transformation infinitésimale autour d'un état d'équilibre.
En mécanique statistique, la fonction de distribution radiale (ou fonction de corrélation de paires )                         g         (         r         )                 {\displaystyle g(r)}    dans un système de particules (atomes, molécules, colloïdes, etc.), décrit comment la densité varie en fonction de la distance par rapport à une particule de référence.
En physique, une force entropique est une force dont la cause n'est pas une interaction fondamentale, mais un phénomène de nature thermodynamique. Un exemple simple de force entropique est la force qui résulte de l'étirement d'un élastique, comme l'explique ainsi le physicien Erik Verlinde : « Peut-être que l'exemple le plus connu est celui d'un polymère. Une seule molécule polymère peut être modélisée en joignant ensemble de nombreux monomères de longueur fixe, où chaque monomère peut librement tourner autour de points d'attache et s'orienter dans n'importe quelle direction. Chacune de ces configurations a la même énergie. Quand la molécule polymère est immergée dans un réservoir thermique, elle tend à se mettre dans un état enroulé de manière aléatoire car ces configurations sont favorisées d'un point de vue entropique. Il y a bien plus de configurations quand le système est court que lorsque le système est étiré. La tendance statistique à retourner vers un état d'entropie maximale se traduit alors en une force macroscopique, en l'occurrence la force élastique. »
Formulaire de thermodynamique :
Les formules d'Ehrenfest sont des formules permettant de définir l'évolution en fonction de la température de la pression de transition de phase d'un corps pur. Ces formules ne sont valables que pour une transition de phase d'ordre deux selon la classification d'Ehrenfest des changements d'état, c'est-à-dire, selon la classification actuelle, pour une transition de phase n'impliquant pas une enthalpie de changement d'état. Si tel n'est pas le cas, la transition est d'ordre un et il faut se rapporter à la formule de Clapeyron.
Le froid Stirling consiste à la production de froid en utilisant le cycle thermodynamique Stirling. Bien que les machines Stirling soient peu connues du grand public, les installations de réfrigération utilisant ce cycle thermodynamique sont nombreuses dans l’industrie, allant du domaine de la cryogénie à l’aérospatial en passant par la climatisation, de même que dans le secteur militaire où la performance spécifique est primordiale et où les surcoûts provenant d’exigence en matériaux et en processus de fabrication ne constituent pas un obstacle majeur.
En physique et en métallurgie, la fusion est le passage d'un corps de l'état solide vers l'état liquide. Pour un corps pur, c’est-à-dire pour une substance constituée d'atomes ou molécules tous identiques, la fusion s'effectue à température constante dite point de fusion. La température de fusion ou de solidification d'un corps pur, appelée « point de fusion », est une constante qui dépend très peu de la pression (contrairement à la température d'ébullition, voir diagramme de phase). Si la température de fusion d'un corps pur est une constante physique, sa température de solidification peut, pour sa part, varier. L'eau par exemple peut geler à des températures de l'ordre de -39°C dans l'atmosphère (surfusion de l'eau). Par contre, la température de fusion d'un mélange de corps purs (voir Diagramme de phase > Diagramme binaire et ternaire ) s'effectue sur une plage de température (sauf pour un eutectique), et dépend de la proportion de chaque constituant du mélange. Dans la pratique, le corps est placé dans un récipient, afin de le contenir une fois qu'il sera liquide. On chauffe, en général, le récipient par dessous, à l'aide d'une flamme ou d'une résistance électrique ; la fusion commence donc par la partie en contact avec le récipient. Dans un four, la chaleur est apportée par l'air et par radiation des parois, c'est donc en priorité le dessus qui fond. En métallurgie, on pratique parfois la fusion par un arc électrique (soudure à l'arc, acier électrique) : l'énergie de chauffage est apportée par le passage du courant dans l'air entre les électrodes. Lorsque le récipient est dans un métal ferromagnétique et que le point de fusion du solide est inférieur au point de Curie du récipient, on peut chauffer par induction. Les corps non-conducteurs peuvent être chauffés par micro-ondes.
Le « gaz de photons » est le nom donné à une collection de photons dont on souhaite étudier les propriétés globales comme pour un gaz constitué de molécules.
La loi de répartition d'un gaz parfait globalement immobile dans le référentiel lié à un cylindre vertical de hauteur H et de rayon R en rotation par rapport à un référentiel galiléen est stratifiée en distance axiale, en fonction de l'énergie potentielle centrifuge : -1/2 m                                    Ω                        2                                        r                        2                                     {\displaystyle \Omega ^{2}r^{2}}   . On s'attend donc, conformément à la distribution de Boltzmann, à une pression P(r) = P(0). exp - (-1/2 m                                    Ω                        2                                        r                        2                                     {\displaystyle \Omega ^{2}r^{2}}   )/kT, ce que confirme le calcul via l'équation dP/dr =                         ρ                    Ω                        2                             r                 {\displaystyle \rho \Omega ^{2}r}   .
Un gaz monoatomique est un gaz dont les constituants sont des atomes isolés. Dans un sens plus restreint, l'expression gaz monoatomique désigne un corps simple élémentaire à l'état gazeux monoatomique, c'est-à-dire un gaz dont les atomes, isolés, sont tous du même élément chimique. Ils se répartissent en trois types : les gaz nobles (groupe 18 du tableau périodique) : He, Ne, Ar, etc. Ces éléments sont gazeux et monoatomiques dans les conditions normales de température et de pression (CNTP), et ne se liquéfient qu'à très basse température ; les non-métaux (H, C, N, P, O, S, Se) et les halogènes (F, Cl, Br, I) à très basse pression et très haute température. Dans les CNTP ces éléments sont à l'état solide (C, P, S, Se, I), liquide (Br) ou gazeux mais moléculaires (H2, N2, O2, Cl2, etc.) ; les métalloïdes (B, Si, Ge, As, Sb, Te, As) et les métaux (tous les autres éléments). Dans les CNTP ces éléments sont presque tous à l'état solide. Quand ils se vaporisent (souvent seulement à très haute température), le gaz formé est d'emblée monoatomique.
Le gaz parfait est un modèle thermodynamique décrivant le comportement des gaz réels à basse pression. Ce modèle a été développé du milieu du XVIIe siècle au milieu du XVIIIe siècle et formalisé au XIXe siècle. Il est fondé sur l'observation expérimentale selon laquelle tous les gaz tendent vers ce comportement à pression suffisamment basse, quelle que soit la nature chimique du gaz, ce qu'exprime la loi d'Avogadro, énoncée en 1811 : la relation entre la pression, le volume et la température est, dans ces conditions, indépendante de la nature du gaz. Cette propriété s'explique par le fait que lorsque la pression est faible, les molécules de gaz sont suffisamment éloignées les unes des autres pour que l'on puisse négliger les interactions électrostatiques qui dépendent, elles, de la nature du gaz (molécules plus ou moins polaires). De nombreux gaz réels vérifient avec une excellente approximation le modèle du gaz parfait dans les conditions normales. C'est le cas des gaz principaux de l'air, le diazote et le dioxygène.
Herbert (Bert) Sydney Green (17 décembre 1920 – 16 février 1999) est un physicien anglo-australien. Il est connu pour ses travaux en physique statistique. Son nom figure dans le titre « Hiérarchie BBGKY » pour Nikolaï Bogolioubov, Max Born, Herbert Green, John Kirkwood et Jacques Yvon pour ses travaux publiés avec Born dans les Proceedings of the Royal Society.
Melville Saul Green (1922 - 27 mars 1979) est un physicien spécialiste de la physique statistique. Il a travaillé sur la hiérarchie BBGKY (Bogolioubov, Born, Green, Kirkwood et Yvon) et est surtout connu pour ses résultats en physique statistique hors d'équilibre avec la relation de Green-Kubo. Ultérieurement il a travaillé sur les phénomènes de changement de phase et les groupes de renormalisation avec Anneke Sengers.
Hazime Mori (transcription du japonais 森 肇 dans le système Kunrei) est un physicien ayant travaillé dans le domaine de la physique statistique hors d'équilibre.
En thermodynamique, un hohlraum (lit. chambre creuse en allemand), est un dispositif de laboratoire destiné à produire un rayonnement de corps noir. Dans la pratique, il est constitué d'un tube fermé opaque, percé d'une fente étroite sur l'une de ses faces, et souvent réalisé en or.
L’humidité absolue est définie pour l'air humide (ou d'autres gaz) comme sa teneur en vapeur d'eau. Elle est limitée par la quantité maximale que le gaz peut absorber avant qu'il y ait saturation à la température de celui-ci.
L'humidité relative de l'air, ou degré hygrométrique, couramment notée φ, correspond au rapport de la pression partielle de la vapeur d'eau contenue dans l'air sur la pression de vapeur saturante (ou tension de vapeur) à la même température. Elle est donc une mesure du rapport entre le contenu en vapeur d'eau de l'air et sa capacité maximale à en contenir dans ces conditions. Ce rapport changera si on change la température ou la pression bien que l'humidité absolue de l'air n'ait pas changé. Elle est mesurée à l'aide d'un hygromètre.
L'humidité spécifique (HS ou q), ou teneur en eau (Y), est le rapport de la masse d'eau dans l'air sur la masse d'air humide. À ne pas confondre avec le rapport de mélange, qui est le rapport de la masse d'eau dans l'air sur la masse d'air sec. L'humidité spécifique maximale en état de saturation, appelée humidité spécifique à saturation, est notée HSs ou qs. Sur un diagramme d'air humide q = f(T), la courbe d'humidité spécifique constante est une droite horizontale. Contrairement à l'humidité relative ou absolue, l'humidité spécifique se conserve lors d'un changement d'altitude ou de température de la masse d'air, tant qu'il n'y a ni condensation ni évaporation. La raison est qu'un kilogramme d'air ou de vapeur reste un kilogramme, indépendamment de la pression ou de la température de l'air. Ainsi lorsque l'on donne l'humidité spécifique, l'altitude de la parcelle d'air n'a pas d'importance. Ceci reste valable tant que la quantité de vapeur reste constante, il ne doit donc pas y avoir de changement d'état physique. Malgré cet avantage, la mesure de l'humidité spécifique est difficile et doit en général être effectuée par un laboratoire.
Un hygromètre (parfois appelé « humidimètre ») est un appareil qui sert à mesurer l'humidité relative de l'air (l'hygrométrie relative).
L’Institut International du Froid (IIF) (International Institute of Refrigeration (IIR)) est une organisation indépendante et intergouvernementale qui s’engage à promouvoir la connaissance du froid au niveau mondial dans toutes ses applications et toutes ses technologies afin d’améliorer la qualité de la vie en respectant l’environnement et en accord avec les impératifs économiques ; la qualité et la sécurité des produits alimentaires de la fourche à la fourchette le confort dans les immeubles résidentiels et commerciaux les produits et les services de santé les technologies des très basses températures et de liquéfaction des gaz l’efficacité énergétique l’utilisation de frigorigènes à faible impact sur la couche d’ozone et sur le réchauffement planétaire, sans négliger la sécurité. En savoir plus sur l'IIF à www.iifiir.org.
John « Jack » Gamble Kirkwood (né le 30 mai 1907 à Gotebo, Oklahoma – décédé le 9 août 1959 à New Haven, Connecticut) est un chimiste et un physicien qui a enseigné à l'Université Cornell, à l'Université de Chicago, au California Institute of Technology et à l'Université Yale. Il est connu pour les travaux connus sous le nom de hiérarchie BBGKY où il est le K de la chaîne caractères.
kT est le produit de la constante de Boltzmann (k) et de la température absolue (T, en kelvins). Du fait de l'unité de k (des joules par kelvin), le produit de ces deux éléments est une énergie, qui dépend donc de la température. Cette énergie est utilisée en physique comme facteur d'échelle pour les systèmes moléculaires, beaucoup de phénomènes ou réactions ayant des taux et des fréquences dépendant non seulement de l'énergie, mais du ratio entre l'énergie et ce facteur kT. On retrouve cette dépendance notamment dans la loi d'Arrhenius ou encore dans la distribution de Boltzmann et par conséquent dans la statistique de Maxwell-Boltzmann, mais aussi dans les statistiques de Bose-Einstein et de Fermi-Dirac en mécanique quantique. Pour un système en équilibre d'un ensemble canonique, la probabilité du système d'être dans un état d'énergie E est proportionnelle à e−ΔE / kT. On retrouve cette propriété appliquée dans de nombreux domaines de la physique ou de la chimie, notamment en thermodynamique (théorie cinétique des gaz), en biophysique, ou encore en électronique (théorie des bandes). Dans ce dernier cas, il est courant de comparer des niveaux d'énergie, que ce soit la bande interdite (gap) quand elle est suffisamment étroite ou des niveaux d'énergie quantifiés dans des nanostructures (puits, fils ou boîtes quantiques), à kT, qui est alors assimilée à l'énergie apportée par l'agitation thermique aux électrons, leur permettant (ou non) de franchir des barrières énergétiques. En thermodynamique, kT est la quantité de chaleur requise pour augmenter l'entropie d'un système d'un nat. Dans les systèmes macroscopiques, avec donc un grand nombre de particules (molécules) présentes, on utilise plutôt RT, où R est la constante universelle des gaz parfaits, qui est le produit de la constante de Boltzmann (k) et du nombre d'Avogadro (NA),ce qui fait que le produit RT est une énergie par mole.  À température ambiante (25 °C), kT vaut 4,11 × 10−21 J, 4,114 pN nm, 9,83 × 10−22 cal, 25,7 meV ou encore 200 cm−1.
La loi de Kopp ou loi de Kopp-Neumann a pour énoncé : « La capacité calorifique d'un composé chimique à l'état solide est la somme des capacités calorifiques des éléments qui le composent ». Elle donne une estimation de la capacité calorifique des composés chimiques des métaux, ainsi que de celle des alliages, à partir de celle des éléments. D'après la loi de Dulong et Petit, la contribution de chaque atome à la capacité calorifique molaire Cp est environ égale à 3R (R est la constante des gaz parfaits) d'où, pour un composé chimique comportant m atomes, la loi de Kopp : Cp = 3 m R. En complément de la loi de Dulong et Petit, elle a été utilisée par Cannizzaro pour étayer la théorie atomique.
Une machine thermique est un mécanisme qui fait subir à un fluide des transformations cycliques. Au cours de ces transformations, le fluide échange avec l'extérieur de l'énergie sous forme de travail avec des sources froides et de l'énergie sous forme de chaleur avec des sources chaudes. La théorie des machines thermiques s'attache à la description et à l'étude physique de certains systèmes thermodynamiques qui permettent de transformer l'énergie thermique en énergie mécanique, et vice versa. Fondée au milieu du XIXe siècle, elle s'appuie sur la thermodynamique, et en particulier sur ses deux premiers principes. De nombreuses machines thermiques sont d'un usage courant : le moteur thermique (sous la forme d'un moteur à combustion externe tel que la machine à vapeur et la turbine à vapeur, ou sous la forme d'un moteur à combustion interne tel que le moteur à essence, le moteur Diesel, le moteur à réaction et la turbine à gaz), le réfrigérateur et la pompe à chaleur.
Peter Mazur (né à Vienne, Autriche le 11 décembre 1922, mort à Lausanne, Suisse, le 15 aout 2001) est un physicien néerlandais. Il est l'un des fondateurs de la thermodynamique hors équilibre. Il est le père de Eric Mazur.
Un mélange est une association de deux ou plusieurs substances solides, liquides ou gazeuses qui n'interagissent pas chimiquement. Le résultat de l'opération est une préparation aussi appelée mélange. Les substances mélangées sont étroitement juxtaposées dans un même espace, chacune gardant ses propriétés physiques et chimiques. Les éléments mélangés peuvent être séparés de nouveau par l’action d'un procédé physique. Un mélange est différent d'un corps pur qui ne comporte qu'une seule substance.
Selon le théorème de Gibbs un mélange de gaz parfaits est une solution idéale.
L'échelle mésoscopique (du grec μέσος (« milieu ») et σκοπέω (« observer »)) est une échelle spatiale intermédiaire entre une échelle dite microscopique et une autre dite macroscopique. Mais les notions de microscopique et macroscopique diffèrent selon le contexte : en physique quantique, la physique mésoscopique concerne les systèmes de dimensions intermédiaires entre celles de la physique quantique et de la physique classique. L'échelle mésoscopique s'étend des dimensions de l'atome (entre 60 et 600 pm) jusqu'au micromètre ; en thermodynamique, l'échelle mésoscopique est suffisamment étendue pour inclure un grand nombre de particules (de telle sorte que leurs propriétés statistiques ne fluctuent pas significativement), tout en restant suffisamment fine pour que les grandeurs thermodynamiques (pression, température, etc.) restent locales (ponctuelles à l'échelle macroscopique). L'échelle mésoscopique dépend donc de la densité des corps étudiés : de l'ordre du nanomètre pour les corps condensés, jusqu'à plusieurs mètres pour des gaz raréfiés ; en météorologie et en océanographie, l'échelle mésoscopique, ou méso-échelle, est intermédiaire entre les systèmes à micro-échelle (moins de 2 km de diamètre) et la circulation planétaire d'échelle synoptique (dépressions et anticyclones sur tout un continent, courants marins, etc.) et . Pour l'atmosphère il s'agit donc de couches s'étendant horizontalement de quelques kilomètres à moins de 2 000 km.  Portail de la physique
La méthode du pincement (en anglais : Pinch analysis) est un outil d’analyse ou de planification d’une installation thermique dans laquelle circulent de manière stationnaire des fluides soumis à des échanges thermiques. L’objectif est de maximiser l’efficacité énergétique de l’installation en lui incorporant des éléments de récupération de chaleur (échangeurs et/ou pompes à chaleur). Cette méthode a été proposée en 1979 par l’ingénieur chimiste Bodo Linnhoff (en) dans le cadre de son Ph. D.. Le terme de pincement se réfère à l’écart minimum entre deux courbes composites d’un diagramme Enthalpie - Température, l’une étant associée aux flux à chauffer, l’autre aux flux à refroidir. Tant que cet écart minimum est positif, il existe théoriquement un moyen de réduire la consommation énergétique. Sous une forme plus élaborée, des algorithmes numériques exploitent cette méthode pour chercher les adaptations les plus économiques, tout en tenant compte des investissements nécessaires, du prix de l’énergie, etc.
Le modèle d'Ising est un modèle de physique statistique. Il a été utilisé pour modéliser différents phénomènes dans lesquels des effets collectifs sont produits par des interactions locales entre particules à deux états. L'exemple principal est le ferromagnétisme pour lequel le modèle d'Ising est un modèle sur réseau de moments magnétiques, dans lequel les particules sont toujours orientées suivant le même axe spatial et ne peuvent prendre que deux valeurs, +M et -M. Ce modèle est parfois appelé modèle de Lenz-Ising. Il doit son nom aux physiciens Wilhelm Lenz et Ernst Ising.
Le mouvement perpétuel désigne l'idée d'un mouvement (généralement périodique), au sein d'un système, capable de durer indéfiniment sans apport extérieur d'énergie ou de matière, ni transformation irréversible du système. Depuis la Renaissance, des inventeurs ignorant les principes de la mécanique ont tenté de construire des systèmes mécaniques aptes à perpétuer leur mouvement, pensant qu'ils pourraient constituer une source illimitée de travail. Leurs mécanismes ne pouvaient fonctionner conformément à leurs espérances, car les connaissances techniques de l'époque ne permettaient guère de réduire de façon significative les phénomènes de frottement entre les pièces fixes et pièces mobiles. On sait que si un mouvement perpétuel peut exister (en théorie) sans aucune énergie apportée, il ne peut devenir une source d'énergie : en effet, cela revient à en consommer sans en avoir ajouté, alors que d'après le premier principe de la thermodynamique elle ne peut ni être créée ni être détruite mais uniquement être transformée. L'obtention d'un « moteur perpétuel », source d'énergie utilisant un mouvement perpétuel, est donc impossible. Pour créer de l’énergie il faudrait qu’un mouvement perpétuel produise également une accélération perpétuelle de son mouvement, ce sans quoi il ne peut être source d’autre énergie que celle de son propre mouvement.
Le nombre de configuration est un entier naturel utilisé en thermodynamique pour caractériser l'entropie d'un système donné, notamment par la formule de Boltzmann.
Un oiseau buveur est un jouet thermodynamique à bascule. Il est animé par l'évaporation de l'eau qu'il « boit » ; il s'agit donc d'un moteur à eau.
L'opalescence critique est un phénomène que l'on peut observer à l'œil nu lorsqu'on refroidit un gaz à une température très proche (quelques millikelvin) de sa température critique.
Le paradoxe de Gibbs est un pseudo-paradoxe apparaissant lorsqu'on cherche à concilier la thermodynamique et la physique statistique. Il intervient lors du calcul de l'entropie de mélange de deux gaz parfaits. Il a été nommé d'après le physicien Willard Gibbs qui l'a découvert en 1861 dans l'application du théorème qui porte son nom. On retrouve la mention de ce paradoxe au chapitre 16 de son ouvrage paru en 1902.
Un paramètre d'ordre est une quantité qui caractérise l'état d'un système physique au cours d'une transition de phase. Dans la théorie de Landau des transitions de phase, la phase désordonnée est invariante par un groupe de transformation                         G                 {\displaystyle G}   , tandis que la phase ordonnée est seulement invariante sous l'action d'un sous-groupe                                    G           ′                          {\displaystyle G'}    du groupe                         G                 {\displaystyle G}   . Le paramètre d'ordre est une quantité invariante sous l'action du sous groupe                                    G           ′                          {\displaystyle G'}    mais pas du groupe                         G                 {\displaystyle G}    tout entier. La phase désordonnée étant invariante sous l'action du groupe                         G                 {\displaystyle G}    tout entier, le paramètre d'ordre doit nécessairement avoir une valeur nulle dans cette phase. Lors d'une transition de phase (par exemple la transition liquide-solide), cette quantité passe d'une valeur nulle dans la phase désordonnée (ex: liquide) à une valeur non nulle dans la phase dite ordonnée (ex: solide). Dans l'exemple de la transition liquide solide, la phase liquide est invariante sous l'action du groupe des isométries de l'espace euclidien, alors que la phase solide n'est invariante que sous l'action d'un des 230 groupes d'espace. Une propriété importante des transitions avec paramètre d'ordre, dans la théorie de Landau, est que comme la symétrie est soit présente soit absente il n'est pas possible de passer continument de la phase désordonnée à la phase ordonnée. En particulier, une ligne de transition solide-liquide ne peut pas se terminer par un point critique. Dans le cas d'une transition liquide-gaz, au contraire, la phase de haute température (le gaz) et la phase de basse température (le liquide) possèdent toutes les deux l'invariance par le groupe des isométries. Strictement parlant, il n'existe donc pas de paramètre d'ordre pour la transition liquide-gaz. Du fait de l'absence de brisure de symétrie dans la transition liquide-gaz, il est possible de passer continument de l'un de ces états de la matière à l'autre. C'est pourquoi la ligne de transition liquide gaz se termine par un point critique. Il est possible de passer continument de l'état liquide à l'état gazeux par un chemin thermodynamique qui contourne ce point critique. On considère donc l'état gazeux et l'état liquide du point de vue de la symétrie comme un même état de la matière, l'état fluide. Toutefois, la transition liquide-gaz étant à suffisamment basse pression une transition du premier ordre, la masse volumique du fluide varie de façon discontinue à travers le point de transition. On peut donc, mathématiquement parlant, associer à cette transition de phase un paramètre d'ordre égal à la différence entre la masse volumique du fluide et celle du gaz. Ce paramètre d'ordre est par définition nul dans le gaz, non nul dans le liquide. Il est alors possible de développer une théorie de la transition liquide gaz qui mathématiquement parlant est analogue à la théorie de la transition entre l'état ferromagnétique et l'état paramagnétique dans un système uniaxe sous champ. C'est l'analogie bien connue entre le modèle du gaz sur réseau et le modèle d'Ising. Pour le système magnétique, l'absence de différence de symétrie entre la phase ferromagnétique et la phase paramagnétique vient de la présence d'un champ magnétique qui crée une aimantation non nulle dans la phase paramagnétique. Dans le cas de la supraconductivité ou de la superfluidité, les paramètres d'ordre correspondent à des brisures de symétries internes (au lieu de symétries d'espace) telles que l'invariance de jauge                         U         (         1         )                 {\displaystyle U(1)}   . Plus récemment, dans le cadre de la théorie de l'effet Hall Quantique Fractionnaire ou du gap de Haldane, les physiciens ont été amenés à étendre la définition du paramètre d'ordre pour prendre en compte la brisure de symétries non locales.
Un péritectique est un mélange de deux corps purs dans des proportions définies, et qui fond de manière particulière : il se décompose en un liquide et en un solide, le nouveau solide étant d'une phase différente de celle du péritectique : liquide + Asolide → Psolide Sur le diagramme de phase, il se reconnaît aisément : la droite verticale correspondant au péritectique s'arrête sur une frontière de phases horizontale, faisant comme un T. Le solide dont la composition correspond au péritectique ne peut généralement pas être obtenu par simple refroidissement d'un liquide de même composition.
La physique statistique a pour but d'expliquer le comportement et l'évolution de systèmes physiques comportant un grand nombre de particules (on parle de systèmes macroscopiques), à partir des caractéristiques de leurs constituants microscopiques (les particules). Ces constituants peuvent être des atomes, des molécules, des ions, des électrons, des photons, des neutrinos, ou des particules élémentaires. Ces constituants et les interactions qu'ils peuvent avoir entre eux sont en général décrits par la mécanique quantique, mais la description macroscopique d'un ensemble de tels constituants ne fait, elle, pas directement appel (ou en tout cas pas toujours) à la mécanique quantique. De fait, cette description macroscopique, en particulier la thermodynamique, a été obtenue pour partie avant le développement de la mécanique quantique en tant que théorie de la physique, essentiellement dans la seconde moitié du XIXe siècle. On distingue la physique statistique d'équilibre (au sens d'équilibre thermodynamique), auquel cet article est consacré, de la physique statistique hors d'équilibre.
La physique statistique hors d'équilibre étudie les phénomènes de relaxation et de transport au voisinage de l'équilibre thermodynamique. Il s'agit là de phénomènes dissipatifs donc irréversibles, liés à une augmentation de l'entropie. À la différence de la thermodynamique hors équilibre les méthodes utilisées relèvent du domaine de la physique statistique et permettent de donner les lois caractérisant un phénomène ainsi que les coefficients présents dans ces lois par l'étude des fluctuations dans le milieu au voisinage de l'équilibre thermodynamique. Dans cet article on adopte la convention de sommation d'Einstein.
L'énergie du point zéro, ou énergie du point zéro du vide quantique, est la plus faible énergie possible qu'un système physique quantique puisse avoir ; cela correspond à son énergie quand il est dans son état fondamental, c'est-à-dire lorsque toute autre forme d'énergie a été retirée,. Tous les systèmes mécaniques quantiques subissent des fluctuations même quand ils sont à leur état fondamental (auquel est associée une énergie du point zéro), une conséquence de leur nature ondulatoire. Le principe d'incertitude implique que chaque système physique possède un point zéro pour son énergie, supérieure au minimum de son puits de potentiel classique. Cela entraîne du mouvement même au zéro absolu. Par exemple, l'hélium liquide ne gèle pas sous la pression atmosphérique, quelle que soit la température, à cause de son énergie du point zéro. Le concept d'énergie du point zéro a été développé par Max Planck en Allemagne en 1911 comme terme correcteur ajouté à l'équation de sa théorie quantique originale datant de 1900. Le terme énergie du point zéro est une traduction du mot allemand « Nullpunktsenergie ». L'énergie du vide correspond à l'énergie du point zéro de tous les champs de l'espace, ce qui, pour le modèle standard, inclut le champ électromagnétique, les champs de jauge et les champs fermioniques, ainsi que le champ de Higgs électrofaible. C'est l'énergie du vide qui dans la théorie quantique des champs est définie non comme un espace vide mais comme l'état fondamental des champs. En cosmologie, l'énergie du vide est une explication possible pour la constante cosmologique. Le système quantique le plus simple possédant une énergie du point zéro est un oscillateur harmonique quantique, dont l'énergie minimale est                         ϵ         =                                                h               ν                          2                                     {\displaystyle \epsilon ={\frac {h\nu }{2}}}    où                         ν                 {\displaystyle \nu }    est la fréquence propre de l'oscillateur. L'énergie quantifiée d'une onde électromagnétique possède une expression similaire à celle d'un oscillateur harmonique, et il s'avère qu’on peut développer l'analogie pour quantifier le champ électromagnétique, qui possède également une énergie de point zéro moyenne égale à                                                 E                                   2                             =                                                ℏ               ω                                         2                                ϵ                                    0                                               V                                                  {\displaystyle \mathrm {E} ^{2}={\frac {\hbar \omega }{2\epsilon _{0}V}}}    (                                   ϵ                        0                                     {\displaystyle \epsilon _{0}}    est la permittivité du vide et V le volume occupé par le champ). Ceci implique que, même en l'absence de toute matière, le vide possède une énergie de point zéro, fluctuante, d'autant plus grande que le volume considéré est petit. Aux échelles macroscopiques, cette énergie est négligeable car les fluctuations s'annulent sur de grands volumes. Cette énergie possède cependant des effets physiques microscopiques comme l'effet Casimir, l'émission spontanée de photons par des atomes, la création de paires de particules/antiparticules, ou une agitation minimale des molécules. Ceci implique notamment que la température du zéro absolu ne peut être atteinte microscopiquement, à cause de l'agitation minimale de la matière ou l'existence d'une énergie de point zéro.
Une pompe à chaleur (PAC), aussi appelée thermopompe, est un dispositif permettant de transférer de l'énergie thermique (calories) d'un milieu à basse température (source froide) vers un milieu à haute température (source chaude). Ce dispositif permet donc d'inverser le "sens naturel" du transfert spontané de l'énergie thermique. Selon le sens du dispositif de pompage, une pompe à chaleur peut soit être considéré comme un système : de chauffage si l'on souhaite augmenter la température de la source chaude, de réfrigération si l'on souhaite abaisser la température de la source froide. Lorsque le but du dispositif de pompage est, à la fois, de chauffer et de refroidir, le système est alors considéré comme une thermo-frigo-pompe. Des pompes à chaleur se retrouvent ainsi dans de nombreuses installations telles que des réfrigérateurs, des climatiseurs et divers systèmes de chauffage.
Tout comme n'importe quelle autre pompe à chaleur, une pompe à chaleur au gaz naturel est un dispositif thermodynamique permettant de transférer la chaleur d'un milieu froid (et donc le refroidir encore) vers un milieu chaud (et donc de le chauffer davantage). Il existe deux types de pompes à chaleur gaz naturel qui ont toutes deux des applications dans le cadre de la thermique du bâtiment pour le chauffage et la climatisation de ceux-ci : les pompes à chaleur à compression gaz naturel (ou pompes à chaleur moteur gaz naturel), les pompes à chaleur à absorption gaz naturel.
En thermodynamique, le potentiel chimique d'une espèce chimique correspond à la variation d'énergie d'un système thermodynamique liée à la variation de la quantité (nombre de moles) de cette espèce dans ce système. Le potentiel chimique est une notion introduite entre 1875 et 1878 par Willard Gibbs et Pierre Duhem. Étroitement lié au deuxième principe de la thermodynamique, le potentiel chimique permet notamment l'étude du phénomène de diffusion de la matière, des systèmes thermodynamiques ouverts et des systèmes réactionnels et de définir les conditions d'équilibre d'une réaction chimique et d'équilibre de phases. Toutefois, la notion de potentiel chimique se révéla difficile à manipuler, car un potentiel chimique ne peut être calculé qu'à une constante additive près et non de façon absolue ; de plus, le potentiel chimique de toute espèce tend vers moins l'infini à dilution infinie. En 1900 et 1901, Gilbert Lewis introduisit la notion de fugacité qui décrit l'écart de comportement d'un corps réel, pur ou en mélange, par rapport au même corps à l'état de gaz parfait pur. Cette notion se révéla efficace dans son application pour les gaz, mais les équations d'état représentant assez mal les phases liquides, Lewis introduisit en 1923 la notion d'activité chimique plus spécialement employée pour les phases condensées (liquide ou solide). Les notions de fugacité et d'activité chimique, définies à l'aide du potentiel chimique, sont plus faciles à manipuler que le potentiel chimique lui-même, notamment pour exprimer des vitesses de réaction et des constantes d'équilibre dans l'étude des réactions et équilibres chimiques et dans le calcul des coefficients de partage dans l'étude des équilibres de phases.
Le potentiel de Lennard-Jones, qui est plus précisément une énergie potentielle, décrit l'interaction entre deux atomes au sein d'un gaz monoatomique de type gaz rare. Bien que ne faisant appel qu'à la seule distance entre deux centres il est également utilisé pour l'interaction de molécules. On peut alors l'interpréter comme potentiel moyen. Son expression en fonction de la distance r entre les deux atomes est :                                    E                                       p                                          (         r         )         =         4                             E                        0                                                 [                                                        (                                                         d                     r                                                     )                                               12                                         −                                             (                                                         d                     r                                                     )                                               6                                                  ]                          {\displaystyle E_{\mathrm {p} }(r)=4\,E_{0}\,\left[\left({\frac {d}{r}}\right)^{12}-\left({\frac {d}{r}}\right)^{6}\right]}    Le potentiel s'annule pour                         r         =         d                 {\displaystyle r=d}    et possède un minimum en                                                 r             d                             =                    2                                       1               6                                          =                 {\displaystyle {\frac {r}{d}}=2^{\frac {1}{6}}=}    1,12246.... Le terme à la puissance 6, terme attractif dominant à grande distance, porte le nom d'interaction de Van der Waals. En revanche, l'exposant 12 du terme répulsif, dominant à courte distance, est empirique : il s'agit là de rendre compte de façon ad hoc de la répulsion de Pauli entre les électrons, qui empêche l'interpénétration mutuelle des nuages électroniques de deux atomes. Quelques valeurs (k est la constante de Boltzmann) : Lorsque l'on ne connaît pas les caractéristiques du potentiel pour une interaction hétérogène entre les atomes i et j, on utilise les lois empiriques suivantes :                                    d                        i             j                             =                                                                 d                                    i                   i                                               +                                d                                    j                   j                                                          2                                     {\displaystyle d_{ij}={\frac {d_{ii}+d_{jj}}{2}}}                                                                   E                                0                                                               i             j                             =                                                                                    E                                        0                                                                                   i                 i                                                                                            E                                        0                                                                                   j                 j                                                                 {\displaystyle {E_{0}}_{ij}={\sqrt {{E_{0}}_{ii}{E_{0}}_{jj}}}}    Ce potentiel a été très largement utilisé en utilisant la procédure suivante pour les gaz : Mesure (en général la viscosité)                         →                 {\displaystyle \rightarrow }    Calcul des paramètres de la loi                        →                 {\displaystyle \rightarrow }    utilisation pour calculer les autres coefficients de transfert (coefficients de diffusion, conductivité). On utilise plutôt aujourd'hui le potentiel SSH (Schwartz, Slawsky, Herzfeld), plus précis, recalé par tranches par comparaison à des expériences de spectroscopie. Le problème de Lennard-Jones à N atomes (connu aussi sous le nom de problème LJ cluster dans la littérature anglo-saxonne) consiste à trouver la configuration spatiale d'une molécule à N atomes minimisant l'énergie potentielle totale de la molécule. Cette configuration est en principe celle que prendra physiquement la molécule, puisque la plus stable. Ce problème a des applications pratiques dans de nombreux domaines (chimie, biologie), et se ramène à un problème de minimisation non linéaire à 3N variables. Ces problèmes ont été traités, jusqu'à N = 1000 en utilisant des algorithmes stochastiques (recuit simulé, algorithmes génétiques), mais la preuve d'optimalité des solutions est extrêmement difficile à établir, le record actuel étant de N = 5 .
La pression de rayonnement ou pression radiative est l'analogue pour le rayonnement de la pression gazeuse et, comme elle, associée au transfert de quantité de mouvement volumique dans une direction de propagation donnée, plus précisément au flux de cette quantité. Il s'agit donc d'une quantité thermodynamique, même si elle est intimement liée à la description donnée par l'électromagnétisme. C'est à cause de ce lien que l'on parle par extension de pression exercée sur une particule de faible dimension (du même ordre de grandeur que la longueur d'onde), phénomène accessible seulement à l'électromagnétisme. Cette notion est utilisée dans de nombreux domaines liés à la physique des plasmas, l'astrophysique et la physique stellaire. L'aspect électromagnétique est présent dans la manipulation de particules.
La pression normale est une valeur pratique, en partie arbitraire, de la pression absolue d'expérimentation et de mesure en laboratoire en physique et en chimie. Le choix d'une pression normalisée permet des comparaisons commodes entre résultats expérimentaux. Les conditions les plus usuelles, issues des méthodes expérimentales du XIXe siècle, fixent la pression à une atmosphère. Toutefois, il existe d'autres définitions de la pression normale.
En génie chimique, un procédé en cascade est une technique se déployant en plusieurs étages d'opérations unitaires identiques, chaque étage opérant sur le produit de l'étage précédent. On trouve des procédés en cascade dans la séparation isotopique, la distillation, la flottation, et divers autres procédés de séparation ou de purification. Les procédés en cascade se rencontrent le plus souvent pour enrichir un mélange hétérogène : il est alors question d'une cascade d'enrichissement. Quand la performance d'un étage ne varie guère suivant la concentration générale, ce qui est le plus souvent le cas, la mise en cascade permet d'augmenter exponentiellement l'efficacité du procédé. Ainsi, pour distiller de l'eau, si une distillation enlève 99 % des impuretés, l'eau bi-distillée sera pure à 99,99 %, et trois distillations ne laisseront que (1-99 %)3 soit 1 ppm des impuretés d'origine.
Un processus spontané est une évolution temporelle d'un système dans laquelle il perd de l'enthalpie libre (souvent sous forme de chaleur) et rejoint un état thermodynamiquement plus stable{,} en parcourant un chemin sur sa surface d'énergie potentielle. La convention de signe des modifications de l'énergie libre suit la convention générale des mesures thermodynamiques, dans lesquelles une libération d'énergie libre depuis le système correspond à une variation négative de l'énergie libre du système, mais une variation positive pour son environnement. Un processus spontané se produit dans une direction (qui peut être contrainte dans le système) sans nécessiter d'apport externe d'énergie. Ce terme est utilisé pour désigner les processus macroscopiques dans lesquels l'entropie croît, comme la diffusion de fumée dans une pièce, la fonte de glace dans de l'eau tiède ou l'oxydation du fer. Les lois de la thermodynamique régissent le processus spontané, faisant qu'un nombre suffisamment important d'interactions individuelles (comme les collisions atomiques) se produisent pour conduire dans le sens d'une entropie croissante (l'entropie étant une grandeur statistique).
La psychrométrie est le domaine scientifique concernant la détermination des caractéristiques physiques et thermodynamiques d'un mélange gaz-vapeur. L'hygrométrie est le cas particulier du mélange air-vapeur d'eau.
Le radiomètre de Crookes consiste en une ampoule sous vide partiel, dans laquelle on a disposé un système rotatif constitué d’un axe de métal sur lequel peut tourner un ensemble de quatre ailettes de mica dont chacune a une des faces noircie au noir de fumée et l’autre argentée. Exposées à un rayonnement électromagnétique, ces ailettes se mettent à tourner d’autant plus vite que le rayonnement est important. Toutefois, la puissance de ce moteur est négligeable, contrairement à ce que "montre" le film Pitch Black, qui lui fait mouvoir un véhicule dans du sable (plusieurs kiloWatts). Il a été inventé en 1873 par William Crookes pour mesurer les radiations magnétiques, mais les causes de la mise en rotation du dispositif ont été le sujet de plusieurs débats scientifiques pendant une dizaine d'années, avant que l'explication actuelle soit publiée en 1879,. On constate que la chaleur de la main est suffisante pour que tourne, même lentement, le dispositif. En effet, la chaleur rayonnée est un rayonnement infrarouge.
La radiosité est une grandeur, principalement utilisée en optique et dans la description de transfert thermique, représentant le flux énergétique total quittant une surface. La radiosité comporte deux composantes : le rayonnement émis par la surface et le rayonnement réfléchi par la surface.
En thermodynamique, on désigne comme étant des réactions endothermiques des processus physiques ou chimiques consommant de la chaleur. La chaleur est une forme d'énergie désordonnée. Dans une réaction chimique endothermique, l'énergie dégagée par la formation des liaisons chimiques dans les produits de réaction est inférieure à l'énergie requise pour briser les liaisons dans les réactifs.
Exothermique provient du grec ancien: exo ἔξω, « hors de » et de thermos θερμός, « chaud ». En thermodynamique, on désigne comme étant des réactions exothermiques des processus physiques ou chimiques produisant de la chaleur. La chaleur est une forme d'énergie désordonnée. Dans une réaction chimique exothermique, l'énergie dégagée par la formation des liaisons chimiques dans les produits de réaction est supérieure à l'énergie requise pour briser les liaisons dans les réactifs. Pendant la réaction, il est possible d'observer un transfert de chaleur entre le milieu réactionnel et son environnement. La chaleur, Q (ou énergie thermique échangée) : c'est de l'énergie en mouvement dont le transfert se fait d'un milieu chaud vers un milieu de plus basse température. Dans une équation chimique, cette énergie est exprimée du côté des produits résultants. La variation d'enthalpie, ΔH, est négative, car l'enthalpie des produits est plus petite que l'enthalpie des réactifs. Dans une équation chimique, il est important de retrouver les mêmes atomes dans les produits et dans les réactifs. La loi de la conservation de la matière s'applique peu importe si la réaction est exothermique ou endothermique. L'appareil utilisé pour mesurer les quantités de chaleur mises en jeu dans les réactions chimiques s'appelle calorimètre. Cette chaleur dégagée brutalement peut contribuer à l'emballement de la réaction qui devient alors explosive. De nombreuses réactions impliquant des acides ou des bases sont exothermiques. La réaction peut être violente. Ainsi un fragment métallique de sodium réagit violemment avec l'eau, ne s'y enfonce pas en raison de l'intense dégagement de gaz hydrogène (inflammable) accompagné de la formation de soude caustique (hydroxyde de sodium) et d'un fort dégagement de chaleur qui peut enflammer l'hydrogène (expérience dangereuse). Plus une réaction est exothermique plus elle produit des substances énergétiquement stables par rapport aux réactifs. L'oxygène et l'hydrogène réagissent spontanément pour former de l'eau (réaction exothermique). Par contre, la réaction inverse, c'est-à-dire transformer l'eau en hydrogène et en oxygène requiert une certaine quantité de chaleur (endothermique). Certaines réactions exothermiques, sans être explosives, peuvent s'entretenir d'elles-mêmes dès qu'elles ont été déclenchées (l'élévation de température favorisant la vitesse de la réaction). C'est le cas de la fermentation sous l'action de levures (voir figure). C'est le cas aussi de la réduction de l'oxyde de fer par l'aluminium métallique (aluminothermie), utilisée par les chemins de fer du monde entier pour effectuer la soudure des rails sur le terrain. La chaleur dégagée est telle que la température dépasse plus de 2 000 °C et que le fer obtenu se trouve à l'état liquide ce qui fait fondre le bout des deux rails, assurant ainsi une soudure parfaite,.
Le fer solide subit une transition de phase, appelée recalescence, lorsqu'il est chauffé au-dessus de 906°C. Sa structure cristalline change, passant de la structure γ (cubique à faces centrées de compacité 74 %) à la structure α (cubique centré de compacité 68 %), ce qui entraîne une variation de sa masse volumique.
Le réfrigérateur à compression de vapeur est fondé sur la condensation de vapeur d'un fluide réfrigérant à la suite d'une compression, et son évaporation à la suite d'une détente. C'est le procédé le plus répandu pour la production du froid. Ce principe est identique à celui employé pour les pompes à chaleur. Ce procédé est à distinguer du turbo réfrigérateur, dans lequel un gaz est comprimé, refroidi à température ambiante, puis détendu dans une turbine. Cet autre procédé ne fait pas intervenir de changement de phase. On peut noter sa ressemblance avec le Cycle de Brayton de la turbine à gaz.
La réfrigération passive par rayonnement est une méthode de réfrigération de très faible coût, n'utilisant pas de source d'énergie apparente, et fondée sur la perte d'énergie radiative (rayonnement) par le système à refroidir. En 1962, les chercheurs français Félix Trombe, Albert et Madeleine Lê Phat Vinh ont proposé d'encastrer ces réfrigérateurs les uns dans les autres, obtenant un réfrigérateur gigogne, où chaque étage du réfrigérateur cumule l'effet de réfrigération des étages inférieurs.
Le dispositif de refroidisseur à buses (en allemand : Düsenkühler) est un échangeur de chaleur dans lequel l'air en se réchauffant produit une certaine poussée. Cet effet est créé par l'introduction de l'air dans le refroidisseur au travers de fentes minces orientées dans le sens du déplacement du véhicule où il se dilate en se réchauffant et sort par une buse dans le sens inverse au déplacement. Le système ne produit aucune poussée lorsque le véhicule est immobilisé. Ce principe de refroidissement a été mis en œuvre sur les avions à moteur refroidi par eau. Le brevet de ce dispositif a été déposé en 1915 par Hugo Junkers. On appelle le carénage spécial d'un moteur en étoile fonctionnant selon le même principe un carénage de type NACA[Quoi ?].  Portail du froid et de la climatisation
La règle des signes en thermodynamique est une règle selon laquelle tout ce qui entre dans le système thermodynamique, travail                         W                 {\displaystyle W}    et chaleur                         Q                 {\displaystyle Q}   , est compté positivement et tout ce qui en sort est compté négativement. C'est en quelque sorte comme pour les comptes bancaires : on se place du point de vue de la banque. Remarque : La plupart des manuels américains de physique inversent le signe du travail                         W                 {\displaystyle W}    (par rapport à la norme européenne), mais non celui de la chaleur                         Q                 {\displaystyle Q}   . En appliquant cette convention, le premier principe est écrit                         Δ         U         =         Q         −         W                 {\displaystyle \Delta U=Q-W}    ; cette expression découle des origines historiques de la thermodynamique comme étude des machines thermiques qui reçoivent de la chaleur et fournissent du travail. Néanmoins, de nombreux manuels américains de chimie-physique suivent la norme européenne et écrivent                         Δ         U         =         W         +         Q                 {\displaystyle \Delta U=W+Q}   .
En thermodynamique, la règle du palier de Maxwell, ou construction de Maxwell, du nom du physicien James Clerk Maxwell qui la publia en 1875, permet, à partir d'une équation d'état représentant à la fois les états liquide et gazeux d'un corps pur, de calculer la pression de vapeur saturante à laquelle s'effectue la transition de phase entre les deux états à une température donnée. Cette règle, établie à l'origine empiriquement à partir de l'équation d'état de van der Waals, peut toutefois se démontrer rigoureusement.
La relation de Gibbs-Duhem est une relation de thermodynamique qui s'applique à tout système thermodynamique à l'équilibre contenant plusieurs espèces chimiques en une seule phase. Lorsque l'on parle de la relation de Gibbs-Duhem, sans autre précision, il s'agit de la relation liée à l'enthalpie libre qui s'écrit sous la forme :  avec :                                    n                        i                                     {\displaystyle n_{i}}    la quantité, ou nombre de moles, de l'espèce                         i                 {\displaystyle i}    au sein du mélange ;                                    μ                        i                                     {\displaystyle \mu _{i}}    le potentiel chimique de l'espèce                         i                 {\displaystyle i}    au sein du mélange ;                         V                 {\displaystyle V}    le volume du mélange ;                         P                 {\displaystyle P}    la pression du mélange ;                         S                 {\displaystyle S}    l'entropie du mélange ;                         T                 {\displaystyle T}    la température du mélange ;                         N                 {\displaystyle N}    le nombre d'espèces chimiques dans le mélange. À pression et température constantes, cette relation devient : À pression et température constantes :                                    ∑                        i             =             1                                   N                                        n                        i                                                 d                             μ                        i                             =         0                 {\displaystyle \sum _{i=1}^{N}n_{i}\,\mathrm {d} \mu _{i}=0}    De ces premières relations basées sur l'enthalpie libre découlent d'autres relations impliquant les fugacités, coefficients de fugacité, activités chimiques, coefficients d'activité d'un mélange. La relation de Gibbs-Duhem peut également être généralisée à toute grandeur extensive autre que l'enthalpie libre.
La relation de Hertz-Knudsen donne le débit massique de matière pour un changement de phase liquide-gaz (évaporation, condensation) ou solide-gaz (sublimation et phénomène inverse). Elle a été ainsi nommée pour les travaux de Heinrich Hertz (1882) et Martin Knudsen (1909). Le nom de Irving Langmuir est parfois associé aux deux premiers.
En physique, et plus particulièrement en thermodynamique, la relation de Mayer, établie au XIXe siècle par Julius Robert von Mayer, est une formule reliant entre elles les capacités thermiques à pression constante                                    C                        P                                     {\displaystyle C_{P}}    et à volume constant                                    C                        V                                     {\displaystyle C_{V}}    d'un gaz parfait selon : Relation de Mayer :                                    C                        P                             −                    C                        V                             =         n         R                 {\displaystyle C_{P}-C_{V}=nR}    avec :                         n                 {\displaystyle n}    la quantité de matière (nombre de moles),                         R                 {\displaystyle R}    la constante universelle des gaz parfaits. Cette relation peut être généralisée aux corps réels selon : Relation de Mayer générale :                                    C                        P                             −                    C                        V                             =         T                                 (                                                                ∂                   P                                                     ∂                   T                                                          )                                   V             ,             n                                                     (                                                                ∂                   V                                                     ∂                   T                                                          )                                   P             ,             n                                     {\displaystyle C_{P}-C_{V}=T\left({\partial P \over \partial T}\right)_{V,n}\left({\partial V \over \partial T}\right)_{P,n}}    avec :                         P                 {\displaystyle P}    la pression,                         T                 {\displaystyle T}    la température,                         V                 {\displaystyle V}    le volume,                         n                 {\displaystyle n}    la quantité de matière (nombre de moles).
En thermodynamique, on appelle relations de Maxwell l'ensemble des équations aux dérivées partielles obtenues grâce aux définitions des potentiels thermodynamiques et au théorème de Schwarz. Pour un système entièrement décrit par les variables pression, température, entropie et volume, on retient généralement un ensemble de quatre relations relatives à l'énergie interne, à l'enthalpie, à l'énergie libre et à l'enthalpie libre. Néanmoins les relations de Maxwell sont généralisables à tous les systèmes thermodynamiques notamment chimiques, électriques et électrochimiques.
En thermodynamique hors équilibre, les relations de réciprocité d'Onsager ou relations de réciprocité d'Onsager-Casimir caractérisent les coefficients phénoménologiques qui apparaissent dans les relations qui relient les flux de variables extensives caractérisant le système considéré aux affinités thermodynamiques correspondantes. Elles ont été établies par Lars Onsager, en 1931 et précisées par Hendrik Casimir en 1945. Comme d'autres relations de ce type, par exemple le principe de Curie, elles sont l'expression des propriétés d'invariance ou de symétrie de ces systèmes. Dans ce cas précis, il s'agit de l'invariance par renversement du temps.
La remontée d'humidité par capillarité, fréquemment appelée  remontées capillaires, désigne la migration d'humidité dans les murs en contact avec un sol humide et du fait de la structure poreuse du matériau qui les constitues (bois, plâtre, torchis, etc.).
En physique, le rendement est défini comme une grandeur sans dimension qui caractérise l'efficacité d'une transformation, physique ou chimique. En physique, la grandeur caractérise généralement la conversion d'une forme d'énergie en une autre.
Le rendement exergétique (appelé aussi rendement selon la seconde loi ou rendement rationnel) est une grandeur permettant de calculer le rendement d’un processus en prenant en compte la deuxième loi de la thermodynamique.
La réversibilité et son complémentaire l’irréversibilité sont des concepts importants en physique et tout particulièrement en thermodynamique. Tout le monde a fait les expériences suivantes : un morceau de verre se brise sur le sol et il ne se reconstitue jamais de lui-même ; en revanche, on peut tirer sur un élastique, le déformer et, dans une certaine limite, quand on le relâche cet élastique retrouve un état semblable à son état initial. La première expérience est typique d'un comportement irréversible, la seconde est ce qui s'approche le plus d'une transformation renversable, mais non exactement réversible. Aussi simples qu'ils puissent paraître, ces deux exemples illustrent respectivement la possibilité ou l'impossibilité pour un système thermodynamique de retrouver spontanément et de manière exacte son état immédiatement antérieur à une modification.
Le séchage est un procédé qui sépare un liquide d'un solide, d'un semi-solide, voire d'un liquide par évaporation. Cette opération est endothermique et nécessite l'apport d'énergie thermique Dans le cas de l'eau, il existe d'autres techniques, pour la séparation, que l'évaporation, telles la pervaporation ou la déshydratation à l'aide d'anhydres. Le traitement thermique n'est pas toujours possible, notamment lorsque le mélange contient des composés plus volatils que l'eau ou en cas de mélange azéotropique, ou souhaitable comme lorsqu'un des composés est sensible à la chaleur.
Le second principe de la thermodynamique exprime le fait que l'état d'équilibre thermodynamique atteint, pour un système matériellement fermé, est le plus probable, c'est-à-dire celui qui a le plus de chances de se produire quand on décompte les différentes possibilités microscopiques de le réaliser, à condition que le système ne soit pas en contact avec une source de chaleur (ce qui viendrait tout perturber). Exprimé analytiquement, cela devient : soient U, V... les variables extensives d'un système de N particules. Soit                         Ω                 {\displaystyle \Omega }    (U,V, ...,N), ce nombre de réalisations possibles d'un état d'équilibre thermodynamique macroscopique, appelé pour faire bref le « Possible », ou nombre de complexions[pas clair] , ou nombre de réalisations microscopiques possibles d'un état macroscopique donné, etc. L'état d'équilibre thermodynamique est celui qui maximise                         Ω                 {\displaystyle \Omega }   , le Possible. Historiquement, ce principe est très long à se dégager sous cette forme intuitive et épurée. On appelle entropie le logarithme du Possible[pas clair]. Ainsi                         S         =         ln         ⁡         Ω                 {\displaystyle S=\ln \Omega }   . Cette valeur dépend de la base de logarithme utilisée. L'usage est de choisir la convention :                         S         =                    k                        B                             .         ln         ⁡         Ω                 {\displaystyle S=k_{B}.\ln \Omega }    où                                    k                        B                                     {\displaystyle k_{B}}    est la constante de Boltzmann. Cette quantité se mesure en J.K−1.
La solidification est l'opération (plus ou moins réversible) au cours de laquelle un liquide passe à l'état solide. Cela peut se faire par refroidissement (cas le plus courant), par augmentation de la pression, par cristallisation, par catalyse ou bien par une combinaison de ces phénomènes. La congélation peut aussi permettre la solidification de certains fluides (l'eau par exemple). La solidification de l'eau se fait à une température de 0 °C.
Une surface isobare est une surface où la pression atmosphérique est égale en tout point. Cette surface n'est pas nécessairement plane, elle varie en altitude selon la pression au sol et le déplacement des systèmes météorologiques.
La surfusion est l'état d’une matière qui demeure en phase liquide alors que sa température est plus basse que son point de solidification. C'est un état dit métastable, c'est-à-dire qu'une petite perturbation peut suffire pour déclencher abruptement le changement vers la phase solide. Un état apparenté, appelé surchauffe, existe pour une matière qui demeure en phase liquide alors que sa température est plus élevée que son point d'ébullition.
En physique, le corps noir est un simple four avec une toute petite ouverture. Cette ouverture laisse passer un rayonnement électromagnétique, caractéristique, décrit par Max Planck. Le corps noir peut aussi être décrit comme un gaz de photons à l'équilibre thermodynamique dans une enceinte fermée de température T. Les deux lois principales sont : l'intensité varie comme T4, appelée loi de Stefan, le maximum de cette intensité a lieu pour la fréquence,                         h                    ν                        m                             =         ℏ                    ω                        m                             =         3         k         T                 {\displaystyle h\nu _{m}=\hbar \omega _{m}=3kT}   , appelée loi de Wien du corps noir. Le système d'unités naturelles construit sur [kT, c,                         ℏ                 {\displaystyle \hbar }   ] permet de retenir ces deux lois.
Un système dissipatif (ou structure dissipative) est un système qui évolue dans un environnement avec lequel il échange de l'énergie ou de la matière. C'est donc un système ouvert, loin d'un équilibre thermodynamique. Un système dissipatif est caractérisé par la balance de ses échanges (échange d'énergie, création d'entropie), et l'apparition spontanée d'une brisure de symétrie spatiale (anisotropie) qui peut quelquefois laisser apparaitre une structure complexe chaotique. Le nouvel état du système est stabilisé grâce à sa « consommation » d'énergie issue de l'environnement.[pas clair] L'expression « structures dissipatives » fut créée par Ilya Prigogine. Un exemple simple est les cellules de Bénard. Des exemples plus complexes incluent les lasers, les réactions de Belooussov-Jabotinski ou même la vie elle-même. L'exemple le plus courant est celui d'un dissipateur de chaleur. Un autre aspect notable des structures dissipatives est la brisure de symétrie temporelle qu'elles présentent.
Un système fermé est un système « isolé de son environnement ». Le terme renvoie souvent à un système idéalisé où la clôture est parfaite. En réalité, aucun système ne peut être complètement fermé ; il y a seulement divers degrés de fermeture. En thermodynamique, un système fermé peut échanger de la chaleur et du travail (énergie AKA), mais pas de la matière, avec ses environnements. En revanche un système isolé ne peut pas échanger de chaleur, de travail ou de la matière avec son environnement, tandis qu'un système ouvert peut échanger de la chaleur, du travail et de la matière.
Un système isolé, par opposition à un système ouvert, est un système physique qui n'interagit pas avec ses environnements. On dit qu'un système isolé d'un système qui n'échange ni matière, ni chaleur, ni travail avec l'extérieur (paroi adiabatique et indéformable) (un système fermé peut échanger de la chaleur ou du travail avec l'extérieur, mais pas de la matière). Ce système obéit à un certain nombre de lois de conservation : le total de son énergie et sa masse (en physique classique) reste constant au cours du temps. S'il ne peut y avoir d'interactions avec l'extérieur, il peut y avoir des réorganisations internes à énergie et masse constante. L'univers dans son ensemble est considéré pour l'instant comme un système isolé, ce qui reste un postulat à démontrer. Des systèmes véritablement isolés n'existent pas dans la réalité physique. Il y a toujours des interactions avec l'environnement (exemple de la gravité opérant entre la masse du système et les masses extérieures). Cependant, un système réel peut se comporter comme un système isolé avec une bonne approximation. Ce concept est une idéalisation acceptable utilisée dans la construction de modèles mathématiques appliqués aux phénomènes physiques ainsi qu'à de nombreux phénomènes naturels (par exemple, le Soleil ainsi que les planètes dans le système solaire qui est souvent traité comme un système isolé). Dans sa tentative de justifier le postulat de l'augmentation de l'entropie de la seconde loi de la thermodynamique, le théorème de Boltzmann utilise des équations qui supposent un système isolé (par exemple, un gaz) dont tous les degrés de liberté mécaniques pourraient être prévus.
En physique, le terme système a une signification technique au sens où il est identifié à une partie de l'univers physique choisi pour l'analyse. Toute chose en dehors du système est appelée environnement et n'est pas pris en compte dans l'analyse excepté pour son influence sur le système. La séparation système/environnement est arbitraire et est généralement faite de façon à simplifier au mieux l'analyse. Un système est dit isolé s'il ne peut échanger ni énergie ni matière avec l'extérieur, fermé s'il peut échanger de l'énergie (et seulement de l'énergie) avec l'extérieur et ouvert s'il peut échanger de la matière et de l'énergie avec l'extérieur. Généralement un système physique est choisi pour correspondre au mieux à ce qu'on appelle usuellement système, soit par exemple une machine particulière en thermodynamique macroscopique, ou un milieu réactionnel en thermochimie. Mais les systèmes physiques peuvent être très variés : un atome, l'eau d'un lac ou encore l'eau dans une moitié dudit lac peuvent être étudiés comme des systèmes physiques. Dans l'étude de la décohérence quantique, le « système » pourra faire référence aux propriétés macroscopiques d'un objet (par exemple la position d'un pendule pesant) tandis que « l'environnement » correspondant pourra représenter ses degrés de liberté internes, décrites classiquement par les vibrations thermiques dudit système (par exemple dans le cas d'un solide les vibrations de phonons).
En thermodynamique classique, on appelle système thermodynamique, une portion de l'univers que l'on isole par la pensée du reste de l'univers que l'on baptise alors milieu extérieur. Le système thermodynamique n'est pas forcément défini par une frontière matérielle, il n'est pas non plus nécessairement connexe. Les gouttes de liquide dans un brouillard, par exemple, définissent un système thermodynamique. Le terme d'univers utilisé ici ne doit pas prêter à confusion, il s'agit en réalité de la portion d'univers en interaction avec le système étudié. Pour être plus précis, le mouvement de la planète Jupiter ou l'heure de la marée n'ont pas besoin d'être considérés pour interpréter le comportement du gaz contenu dans une bulle de savon. La séparation, même fictive, entre le système et le milieu extérieur est appelée paroi ou enceinte. Selon la nature et les propriétés de cette paroi, un système thermodynamique sera qualifié de :
Une table psychrométrique montre la relation entre l'humidité et l'abaissement en température, par évaporation d'eau. S'utilise pour l'hygromètre psychrométrique, et un type de moteur à eau.
On appelle température absolue une mesure de la température qui prend le zéro absolu des températures comme origine. L'échelle de mesure, elle, peut être arbitraire et toutes les échelles sont équivalentes à un facteur multiplicatif près. Par exemple, l'échelle kelvin a pour origine le zéro absolu et utilise l'échelle Celsius ; les degrés Rankine ont également pour origine le zéro absolu mais ils utilisent l'échelle Fahrenheit. Le passage du kelvin au degré Rankine se fait par le simple coefficient multiplicateur 5/9 : K = 5/9 °Ra L'origine des températures absolues vaut par définition 0 K, soit −273,15 ℃ ou −459,67 °F.
La température d'équilibre à la surface d'une planète est une température théorique d'une planète si elle était un corps noir dont la seule source de chaleur est son étoile parente. Dans ce modèle, la présence ou absence d'une atmosphère (et donc de tout effet de serre) n'est pas considérée et l'on traite la température théorique de corps noir comme si elle venait d'une surface idéalisée de la planète. Certains auteurs utilisent d'autres termes, tels température équivalente de corps noir d'une planète ou température de radiation d'émission effective d'une planète. Des concepts semblables incluent la température moyenne globale et la température de l'air de surface globale moyenne qui incluent les effets de réchauffement climatique.
La température normale est une valeur pratique, en partie arbitraire, de la température d'expérimentation et de mesure en laboratoire en physique et en chimie. Le choix d'une température normalisée permet des comparaisons commodes entre résultats expérimentaux. Les conditions les plus usuelles, issues des méthodes expérimentales du XIXe siècle, fixent la température normale à 0 ℃. Toutefois, il existe d'autres définitions de la température normale.
La température thermodynamique est une formalisation de la notion expérimentale de température et constitue l’une des grandeurs principales de la thermodynamique. Elle est intrinsèquement liée à l'entropie. Usuellement notée T, la température thermodynamique se mesure en kelvins (symbole : K). Encore souvent qualifiée de « température absolue », elle constitue une mesure absolue parce qu’elle traduit directement le phénomène physique fondamental qui la sous-tend : l’agitation des particules constituant la matière (translation, vibration, rotation, niveaux d'énergie électronique). Son point origine, ou zéro absolu, correspond par définition à l’état de la matière où ces particules ont une entropie minimale, ce qui correspond généralement à une énergie minimale.
La théorie d'Onsager a été élaborée en 1931 par Lars Onsager. C'est une théorie macroscopique du couplage linéaire de phénomènes irréversibles comme la conduction électrique et la diffusion thermique. Cette théorie émet l'hypothèse qu'il existe une relation linéaire entre les courants volumiques et les forces thermodynamiques. Cette hypothèse est parfois appelée "quatrième principe de la thermodynamique".
Le théorème de Gibbs permet de calculer l'entropie d'un mélange de gaz parfaits. Il s'énonce ainsi :  Le théorème de Gibbs montre qu'un mélange de gaz parfaits est une solution idéale.
Le théorème de Nernst est un théorème de thermodynamique statistique. Il correspond au principe de Nernst en thermodynamique classique. La différence dans l'appellation provient du fait qu'en thermodynamique statistique le théorème de Nernst se déduit de l'équation de Boltzmann. Le théorème de Nernst s'exprime ainsi : Au zéro absolu, l'entropie d'un cristal est nulle.
La théorie cinétique des gaz a pour objet d'expliquer le comportement macroscopique d'un gaz à partir des caractéristiques des mouvements des particules qui le composent. Elle permet notamment de donner une interprétation microscopique aux notions de : température : c'est une mesure de l'agitation des particules, plus précisément de leur énergie cinétique ; pression : la pression exercée par un gaz sur une paroi résulte des chocs des particules sur cette dernière. Elle est liée à leur quantité de mouvement La théorie cinétique des gaz a été développée à partir du XVIIIe siècle. Elle est fondée sur les idées de Daniel Bernoulli, John James Waterston, K.A. Krönig et Rudolf Clausius. James Clerk Maxwell et Ludwig Boltzmann ont formalisé son expression mathématique. Les succès de la théorie cinétique des gaz ont constitué un argument fondamental à l'appui de la théorie atomique de la matière. En effet en prenant en compte les effets des collisions entre les molécules on a accès aux propriétés de transport (viscosité, diffusion de la matière, conductibilité thermique) ;
En physique statistique hors d'équilibre, la théorie de la réponse linéaire permet de définir les susceptibilités et les coefficients de transport d'un système au voisinage de l'équilibre thermique indépendamment des détails du modèle. La théorie de la réponse linéaire a été développée dans les années 1950 par Melville Green, Herbert Callen et Ryōgo Kubo.
La thermalisation est un processus observé en physique. D'une façon générale, la notion de température est liée à celle d'équilibre thermodynamique, sachant que les systèmes que l'on considère dans tous les domaines sont plus ou moins en déséquilibre. Si, pour une raison quelconque (naturelle ou non), un système présente un déséquilibre local par rapport au reste, l'ensemble va évoluer vers un retour vers un autre équilibre, qui peut s'avérer très proche de l'équilibre primitif si la quantité d'énergie introduite lors de l'événement est très faible par rapport à l'énergie thermique du système dans son entier.
Une thermo-frigo-pompe ou thermofrigopompe (TFP) est une pompe à chaleur dont l'énergie utile est à la fois celle rejetée sur la source chaude et celle prélevée à la source froide. Fondamentalement, toute pompe à chaleur est potentiellement une thermofrigopompe. Néanmoins, pour qu'une thermofrigopompe soit utile, il faut généralement un écart de température entre la source froide et la source chaude sensiblement plus important que ceux pour lesquels on installe un système frigorifique ou un système de chauffage.
L'effet thermoacoustique est la conversion de chaleur en énergie acoustique et vice versa,. Une machine thermoacoustique est donc un convertisseur thermomécanique qui peut, soit produire une énergie mécanique de nature acoustique à partir de la consommation d'une certaine quantité de chaleur, soit consommer de l'énergie acoustique afin de pomper de la chaleur d'un milieu froid vers un milieu chaud. Les systèmes en question sont respectivement qualifiés de moteur thermoacoustique et de réfrigérateur thermoacoustique. Les machines thermoacoustiques sont constituées dans leurs versions les plus simples d'un résonateur acoustique à l'intérieur duquel est disposée soit une structure poreuse, soit un stack, munis d'échangeurs de chaleur à leurs extrémités. C'est la différence de température aux extrémités du stack, entretenue par les deux échangeurs de chaleur, qui donne naissance à l'onde acoustique dans le résonateur. On utilise le plus souvent une source/récepteur d'ondes sonores afin d'assurer une conversion électromécanique de la puissance acoustique entretenue par le résonateur. Moteurs et réfrigérateurs sont parfois directement associés, l'onde acoustique créée par le premier servant à faire fonctionner le second. Il existe par ailleurs une troisième utilisation de l'effet thermoacoustique, visant à la séparation de gaz initialement mélangés,.
La thermodynamique des trous noirs est la branche de l'étude des trous noirs qui s'est développée à la suite de la découverte d'une analogie profonde entre certaines propriétés des trous noirs et les lois de la thermodynamique au début des années 1970. Cette analogie est ensuite devenue pertinente grâce à la découverte par Stephen Hawking du phénomène d'évaporation des trous noirs (1975), démontrant qu'un trou noir n'est pas un objet complètement sombre, mais émet un très faible rayonnement thermique.
La thermodynamique hors équilibre est le domaine de recherche étudiant les phénomènes de relaxation et de transport au voisinage de l'équilibre thermodynamique. Il s'agit là de phénomènes dissipatifs donc irréversibles, liés à une augmentation de l'entropie. Les méthodes présentées ici relèvent de la thermodynamique proprement dite qui permet de donner les lois caractérisant un phénomène. Ces phénomènes sont également décrits par la théorie cinétique qui, au-delà de la physique statistique décrivant un milieu à l'équilibre, permet d'accéder aux lois de transport et de relaxation et de plus à l'expression des coefficients présents dans celles-ci par passage du niveau cinétique au niveau continu. Ces méthodes sont les méthodes statistiques de la physique statistique hors d'équilibre, la méthode des moments et celle des développements asymptotiques dans le cas de la méthode de Chapman-Enskog pour un gaz.
La thermophorèse, ou thermodiffusion, ou thermomigration, ou effet Soret, ou encore, effet Ludwig-Soret, est un phénomène observé dans les préparations de particules en mouvement où les différentes catégories de particules présentent des réponses différentes lorsqu'elles sont soumises à un gradient de température. Le terme de « thermophorèse » s'applique le plus souvent à des mélanges aérosols mais peuvent aussi bien faire référence à ce phénomène dans les différentes phases de la matière. Le terme d'« effet Soret » est normalement utilisé pour les préparations liquides qui se comportent selon des mécanismes différents et moins bien connus que les mélanges gazeux.
Le thermoscope de Rumford est un instrument inventé vers 1804 par Benjamin Thompson, comte de Rumford, pour « mesurer, ou plutôt pour découvrir ces très petites variations dans la température des corps, qui sont occasionnés par les rayonnements des corps environnants ». Son principe est identique à celui du thermomètre différentiel inventé à la même époque par le physicien écossais John Leslie.
En thermodynamique, un thermostat est un système fermé de température constante, susceptible d'être utilisé pour réaliser des transferts thermiques avec un corps placé à son contact.
Le tirage thermique, ou effet cheminée, est le mouvement de l'air (dans les bâtiments, cheminées, conduites...) sous l'effet de la poussée d'Archimède.
Le travail d'une force est l'énergie fournie par cette force lorsque son point d'application se déplace (l'objet subissant la force se déplace ou se déforme). Il est responsable de la variation de l'énergie cinétique du système qui subit cette force. Si par exemple on pousse une bicyclette, le travail de la poussée est l'énergie produite par cette poussée. Cette notion avec ce nom fut introduite par Gaspard-Gustave Coriolis. Le travail est exprimé en joules (J), et est souvent noté W, initiale du mot anglais work qui signifie travail.
Le tube de Ranque-Hilsch est un dispositif thermodynamique sans pièce en mouvement permettant de produire de l'air froid, imaginé par le Français Georges Joseph Ranque (en) et amélioré par l'Allemand Rudolph Hilsch. Ce dispositif est connu dans le monde industriel sous le nom de tube vortex.
La méthode UNIversal Functional Activity Coefficient (UNIFAC en anglais, en français Coefficient d'Activité Fonctionnel UNIversel) est un système semi-empirique de prévision d'une activité chimique non-électrolyte dans des mélanges non idéaux. UNIFAC utilise le groupe fonctionnel présent sur les molécules qui constituent le mélange liquide pour calculer les coefficients d'activité. En utilisant les interactions de chacun des groupes fonctionnels présents sur les molécules, ainsi que des coefficients d'interaction binaires, l'activité de chacune des solutions peut être calculée. Ces informations peuvent être utilisées pour obtenir des renseignements sur les équilibres liquides, ce qui est utile dans de nombreux calculs thermodynamiques, tels que la conception d'un réacteur chimique, et les calculs de distillation. Le modèle UNIFAC a d'abord été publié en 1975 par Fredenslund, Jones et Prausnitz, un groupe de chercheurs en génie chimique de l'université de Californie. Par la suite, ils ont publié, tout comme d'autres auteurs, une large gamme d'articles sur UNIFAC, étendant les capacités du modèle. Pour cela, ils ont dû développer les paramètres existants du modèle UNIFAC ou bien en créer de nouveaux. UNIFAC est une tentative de ces chercheurs de fournir un modèle souple des équilibres liquides pour une utilisation plus large en chimie, ainsi que dans des disciplines telles que le génie chimique ou encore le génie des procédés.
L'unité de travail de séparation (UTS, en anglais separative work units SWU, en allemand Urantrennarbeit UTA) représente le travail nécessaire à l'enrichissement isotopique d'un mélange. Cette unité est essentiellement utilisée pour évaluer le coût de la séparation d'un kilogramme d'uranium en deux lots de teneur isotopique différente, dans le cadre d'un processus d'enrichissement de l'uranium.
En thermodynamique, l'état d'équilibre d'un système est caractérisé par plusieurs paramètres, appelés variables d'état, tels que le volume, la température, la pression etc., et ces caractérisations sont elles-mêmes des fonctions d'état du système. Une variable d'état n'a de sens que pour un système à l'équilibre thermodynamique. Une variable d'état est toujours une grandeur physique scalaire. Il s'agit soit d'une grandeur extensive, définie sur l'ensemble du système considéré, soit d'une grandeur intensive, qui doit alors prendre la même valeur en tout point du système.  Portail de la chimie  Portail de la physique
En thermodynamique, l'énergie interne d'un système est exprimé à travers un couple de grandeurs physiques appelé variables conjuguées. Ce couple de deux variables vérifie les propriétés suivantes : l'une est intensive et l'autre extensive ; leur produit est homogène à une énergie (ou parfois une puissance). Le produit de ces deux variables donne ici une énergie, ce qui s'explicite en disant que les deux variables sont « conjuguées par rapport à l'énergie ». D'une manière plus générale, des variables conjuguées peuvent être définies de même par rapport à n'importe quelle fonction d'état thermodynamique. On utilise par exemple, dans l'étude des processus irréversibles, des variables « conjuguées par rapport à l'entropie », pour lesquelles le produit est une entropie, ce qui conduit aux relations de réciprocité d'Onsager. Le présent article ne traite cependant que des variables conjuguées par rapport à l'énergie.
Un vase Dewar — ou vase de Dewar ou simplement dewar — est un récipient conçu pour fournir une très bonne isolation thermique. Par exemple, rempli d'un liquide chaud, le vase Dewar ne laissera la chaleur s'échapper que difficilement, et le liquide restera chaud bien plus longtemps que dans un récipient classique. Ce vase doit son nom au physicien écossais Sir James Dewar. Ce vase se présente sous la forme d'une bouteille en verre ou en métal, en double-couche. Il peut être vu comme deux bouteilles à paroi mince imbriquées l'une dans l'autre. L'espace étroit entre ces deux bouteilles est presque entièrement dépourvu d'air, le quasi-vide empêche conduction et convection de chaleur. La surface intérieure de la bouteille externe et la surface externe de la bouteille intérieure, ont un enduit réfléchissant métallique ou semblable pour empêcher la chaleur d'être transmise par radiation. Dewar a employé de l'argent à cette fin. Le vase Dewar est communément utilisé dans les laboratoires pour stocker de l'azote liquide. L'excellente isolation du vase Dewar se traduit par une très lente « ébullition » de l'azote. Celui-ci reste ainsi plus longtemps à l'état liquide sans besoin d'un coûteux équipement de réfrigération. La bouteille isotherme servant à conserver au chaud (ou au froid) toutes sortes de liquides utilise le même principe que le vase Dewar. Ce vase étant fragile et cher de par sa conception, d'autres concepts à base de métal isolant ou de plastique sont apparus sur le marché.
En histoire des sciences, la vis viva (force vive en latin) est un concept obsolète par lequel est exprimé mathématiquement pour la première fois ce qui sera connu comme la loi de la conservation de l'énergie. Elle peut être considérée comme une sorte d'énergie cinétique ou d'énergie reliée au mouvement des objets. Proposée par Gottfried Wilhelm Leibniz, la théorie entra en concurrence avec le principe de conservation de la quantité de mouvement promu par René Descartes et Isaac Newton. Les deux théories sont maintenant considérées comme complémentaires et la notion de force vive est intégrée dans l'acception moderne de l'énergie. Dans la science moderne, seul le nom de l'équation de la force vive de la mécanique céleste rappelle l'existence de cette controverse longue d'un siècle.
Theodore « Ted » Allen Welton (né le 4 juillet 1918 à Saratoga Springs, New York, décédé le 14 novembre 2010 à Pleasant Hill, Tennessee) est un physicien théoricien américain.
Yeram Sarkis Touloukian (28 décembre 1920 - 12 juin 1981) est un professeur de thermodynamique, fondateur et directeur de centre de Recherches sur les propriétés thermophysiques de l'Université Purdue, aujourd'hui CINDAS LLC (Center for Information and Numerical Data Analysis and Synthesis). Son nom est associé aux synthèses sur les propriétés thermophysiques des matériaux.
Jakob Yngvason (né le 23 novembre 1945 à Reykjavik) est un physicien islandais et autrichien. Il est l'auteur de travaux en thermodynamique, en théorie quantique des champs et sur la condensation Bose-Einstein.
La zéodratation est une technique de déshydratation utilisant des zéolites. Ses domaines d’application sont encore loin d’être tous explorés mais elle révolutionne déjà le domaine du séchage en raison de la grande souplesse du procédé, qui permet d’adapter le traitement, en pression et température, pour répondre aux critères de qualité des produits choisis.
Zéotrope ou zéotropique (du grec zêin bouillir et tropos action de tourner). Sous pression constante, se dit d'un mélange liquide qui bout à température variable en perdant sa composition fixe. Exemples : R407C, le mélange eau - acide acétique
Le zéro absolu, est la température la plus basse qui puisse exister. Il correspond à la limite basse de l'échelle de température thermodynamique, soit l'état dans lequel l'enthalpie et l'entropie d'un gaz parfait atteint sa valeur minimale, notée 0. Cette température théorique est déterminée en extrapolant la loi des gaz parfaits : selon un accord international, la valeur du zéro absolu est fixée à −273,15 °C (Celsius) ou −459,67 °F (Fahrenheit). Par définition, les échelles Kelvin et Rankine prennent le zéro absolu comme valeur 0. En physique quantique, la matière au zéro absolu se trouve dans son état fondamental, point d'énergie interne minimale. Les lois de la thermodynamique impliquent que le zéro absolu ne peut pas être atteint en utilisant uniquement des moyens thermodynamiques : la température de la substance refroidie se rapproche asymptotiquement de l'agent de refroidissement. Un système qui se trouve au zéro absolu possède en mécanique quantique l'énergie du point zéro, soit l'énergie de son état fondamental au zéro absolu. L'énergie cinétique de l'état fondamental ne peut être éliminée. Des scientifiques ont réussi à atteindre des températures proches du zéro absolu, où la matière présentait des effets quantiques tels que la supraconductivité ou la superfluidité.
Dimitri Nikolaevich Zubarev (russe : Дми́трий Никола́евич Зу́барев), né le 27 novembre 1917, décédé le 29 juillet 1992, est un physicien théoricien connu pour ses travaux en mécanique statistique, thermodynamique hors équilibre, physique des plasmas, théorie de la turbulence et ses travaux sur le problème à N corps.
Robert Walter Zwanzig (né à Brooklyn, New York le 9 avril 1928 et décédé à Bethesda, Maryland le 15 mai 2014) est un physico-chimiste américain qui a accompli d'importants travaux dans le domaine de la physique statistique hors d'équilibre, du repliement des protéines et de la théorie des gaz et des liquides.
