Ceci est une liste de chipsets de carte mère fabriqués par le fondeur Intel.
Un client IRC est un logiciel client capable de se connecter au protocole de communication de messagerie instantanée Internet Relay Chat (IRC).
Cet article liste les clones de Macintosh, classés par fabricant. Les clones sont des Macintosh conçus par d'autres sociétés qu'Apple, commercialisés entre 1995 et 1997 (ainsi que 2008, Psystar). Pour la liste des Macintosh fabriqués par Apple, voir Liste des modèles de Macintosh par processeur.
En informatique, le code HTTP (aussi appelé code d'état) permet de déterminer le résultat d'une requête ou d'indiquer une erreur au client. Ce code numérique est destiné aux traitements automatiques par les logiciels de client HTTP. Ces codes d'état ont été définis par la RFC 2616, en même temps que d’autres codes d'état, non normalisés mais très utilisés sur le Web. Ils ont été ensuite étendus par la RFC 7231. Le premier chiffre du code d'état est utilisé pour spécifier une des cinq catégories de réponse (informations, succès, redirection, erreur client et erreur serveur). Les codes les plus courants sont : 200 : succès de la requête ; 301 et 302 : redirection, respectivement permanente et temporaire ; 401 : utilisateur non authentifié ; 403 : accès refusé ; 404 : page non trouvée ; 500 et 503 : erreur serveur ; 504 : le serveur n'a pas répondu. Certains codes ne sont pas encore utilisés, mais sont prévus pour une utilisation future. D'autres codes n'entraînent aucun affichage spécifique pour l’utilisateur, mais sont sous-entendus (par exemple, les codes 200 ou 304, jamais vus par le client car ils concernent la majorité des requêtes réussies).
Cette page présente les principales entreprises produisant du matériel informatique.
Cet article est une liste référençant les correspondances ouvertes aux formats fermés. Les différents formats et leurs extensions courantes entre parenthèses sont divisés en plusieurs catégories.
Cet article recense par ordre alphabétique un certain nombre de distributions Linux.
Liste d'émulateurs d'ordinateur Macintosh vMac (Mac Plus) multiples plates-formes Mac on Linux Pour GNU/Linux PPC (Licence GPL) Basilisk II (Mac 68k) multiples plates-formes (Licence GPL) PearPC (PowerMac OS X) Pour de multiples plates-formes Windows, GNU/Linux, Unix (Licence GPL) SheepShaver (PowerMac OS 7/8/9) pour GNU/Linux, Windows et BeOS Mini vMac (Macintosh Classic) pour Windows et Mac OS x86 Bochs multiples plates-formes QEMU multiples plates-formes MSX RuMSX Pour Windows (Gratuiciel) Oric Euphoric Pour Windows, GNU/Linux, Macintosh (GPL) Thomson Emulation de TO7, TO7/70, TO8, TO8D, TO9, TO9+ Pour Windows, GNU/Linux,(GPL) DCMO5, émulateur de MO5 multi-plateforme (GPL) DCMOTO, émulateur de tous les micro-ordinateurs de la gamme Thomson, pour Windows Multi Ordinateurs MESS existe pour de multiples plates-formes Windows, Macintosh, GNU/Linux... SC 3000 Meka pour Windows Amstrad CPC Arnold pour Windows, Macintosh et Linux CaPriCe32 pour Windows CPC++ pour PowerMac, Solaris et Linux CPCE pour DOS, DPMI et Windows NO$CPC pour DOS et Windows WinAPE pour Windows Xcpc pour UNIX et Linux Reloaded pour GNU/Linux, UNIX, Mac OS X, Windows... Atari ST Site de Steem, Steem pour Windows, GNU/Linux (Graticiel) Site de Saint, Saint pour Windows. Amiga WinFellow pour Windows UAE pour GNU/Linux (FAQ) WinUAE pour Windows ([1]) Commodore 64 Vice pour GNU/Linux (GPL) Frodo pour BeOS/Unix/MacOS/AmigaOS/RiscOS/Windows/EPOC/... CCS64 pour GNU/Linux et Windows Route 64 pour eclipse DOS DOSEMU pour GNU/Linux (GPL) DOSBox pour GNU/Linux et Windows (GPL) Palm OS POSE Diverses machines de jeux d'arcade MAME pour Windows XMAME pour Unix MacMAME et MAME OS X pour Macintosh Ordinateurs anciens SIMH peut émuler : Data General Nova, Eclipse Digital Equipment Corporation PDP-1, PDP-4, PDP-7, PDP-8, PDP-9, PDP-10, PDP-11, PDP-15, VAX GRI Corporation GRI-909 IBM 1401, 1620, 1130, System 3 Interdata (Perkin-Elmer) 16b and 32b systems Hewlett-Packard 2116, 2100, 21MX Honeywell H316/H516 MITS Altair 8800 (cpu 8080 ou Z80), système CPM Royal-Mcbee LGP-30, LGP-21 Scientific Data Systems SDS 940  Hercules émule les mainframes IBM série 370, 390 et z. Desktop CYBER Emulator émule des machines CDC, dont le 6600 et des CYBERs. UNIVAC émule les UNIVAC I et II. PDP-10 émule le DEC PDP-10, processeur KL10.
Un émulateur de console de jeux vidéo permet de créer une console virtuelle dans votre ordinateur. L'émulateur de console fonctionne de la même façon que lorsque vous utilisez un PC virtuel, son avantage est la dématérialisation et donc le gain d'espace dans une pièce, son principal inconvénient réside dans le fait que les PC grand publics ne sont parfois pas assez performants pour bénéficier de la qualité optimale du jeu vidéo chargé dans l'émulateur.
Les frameworks (web) suivants ont été écrits en Python : CherryPy - open source Django — open source Flask — licence BSD Bottle — licence MIT Grok (en) — open source Karrigell — open source Pylons (en) — open source Pyramid — licence BSD TurboGears — open source Tornado (framework) (en) — Licence Apache Twisted — licence MIT Web2py — open source Zope — open source   Portail de la programmation informatique  Portail des logiciels libres  Portail d’Internet
WebGL est un langage bas niveau rarement utilisé directement, mais plutôt par l'intermédiaire de frameworks. WebGL étant une interface de programmation web, ces frameworks sont eux-mêmes essentiellement des bibliothèques javascript ou des environnements de développement (IDE) en ligne. La liste suivante mélange les bibliothèques, les IDE ainsi que les Plate-forme en tant que service qui permettent de produire des applications WebGL.
Voici une liste de frameworks PHP.
On appelle paquet GNU un paquet logiciel maintenu par le projet GNU et soutenu par la Free Software Foundation. Ces programmes sont destinés à être utilisés dans le cadre du système d'exploitation GNU et de ses variantes, notamment GNU/Linux, bien qu'ils puissent très souvent être utilisés sur d'autres plateformes. Ces logiciels sont tous des logiciels libres, le système GNU ayant été précisément créé dans le but de fournir un équivalent libre au système Unix. Les paquets GNU sont au nombre de 395. Ils sont fédérés au sein de la forge GNU Savannah. Le projet GNU a été fondé par Richard Stallman, auteur du premier paquet GNU Emacs.
Le jargon informatique est un pseudo-langage (jargon) propre aux représentants des professions gravitant autour de l'informatique. Discutant essentiellement par messages courts, ces personnes utilisent des abréviations, souvent d'origine anglophone. Il existe d'autres jargons plus ou moins « colorés » informatique, comme le langage SMS ou l'argot Internet (voir ces pages pour les « lol », « amha » et autres termes). L'application pratique du décret du 3 juillet 1996 relatif à l'enrichissement de la langue française peut s'avérer difficile par rapport à l'usage de termes en français. On pourra pour cela consulter : le site FranceTerme, qui indique les termes officiellement en usage et publiés au Journal officiel de la République française, le site du Grand dictionnaire terminologique, de l'Office québécois de la langue française. Ces deux grands dictionnaires terminologiques sont généralistes, et proposent des termes dans toutes les disciplines. L'informatique en fait bien sûr partie. Les quelques sites web indiqués en liens externes de cet article peuvent aussi aider les développeurs. On peut également consulter une liste des abréviations employées en informatique.
Le but de cette liste de langages de programmation est d'inclure tous les langages de programmation existants, qu'ils soient actuellement utilisés ou historiques, par ordre alphabétique. Ne sont pas listés ici les langages informatiques de représentation de données tels que XML, HTML ou YAML. Par ailleurs, cette liste répertorie les langages de programmation, et non leurs implémentations -ou "Framework"- (par exemple : JRuby et IronRuby sont deux implémentations différentes du même langage Ruby).
Le terme libriste désigne une personne attachée aux valeurs éthiques véhiculées par le logiciel libre et la culture libre en général. En matière d'œuvres ou de logiciels, le libriste défend les quatre libertés fondamentales des utilisateurs finaux telles que définies par Richard Stallman pour le logiciel.
Cet article est une liste de licences libres.
Cet article fournit une liste non exhaustive de machines virtuelles java Java SE (JVM). Elle n'inclut pas un grand nombre de machines virtuelles Java ME. Un important travail de développement prend place sur Windows, Solaris et Linux, principalement avec la JVM de Sun. La première référence à une JVM a été HotSpot, produit par Oracle Corporation.
Les systèmes d'exploitation GNU/Linux, Unix et « Unix-like » sont en général considérés comme protégés des virus informatiques. En effet, jusqu'à présent, aucun virus opérant sous Linux n'a été répertorié comme étant très répandu, comme c'est parfois le cas avec Microsoft Windows. Ceci est souvent attribué au fait que les virus Linux ne peuvent accéder aux privilèges « root » et à la rapidité des corrections pour la plupart des vulnérabilités de GNU/Linux. Un autre facteur est le fait qu'il y a moins d'incitations pour un programmeur à écrire des malwares opérant sous Linux : il est très répandu sur les serveurs (il est utilisé par exemple par Google et Wikipédia) et les superordinateurs, mais ceux-ci sont la plupart du temps gérés par des professionnels formés aux enjeux de la sécurité informatique ; il est assez peu répandu sur le marché des ordinateurs personnels, meilleurs vecteurs de propagation des malwares en raison de la fréquente inexpérience de leurs utilisateurs. Cependant, le nombre de programmes malveillants (incluant les virus, trojans et autres types) sous Linux a augmenté ces dernières années, et plus particulièrement doublé en 2005, passant de 422 à 863.
Cet article présente la liste des ministres chargé de l'économie numérique, et des technologies de l'information et de la communication.
Cet article liste les Macintosh fabriqués par Apple, classés par processeur. Pour la liste des Macintosh non commercialisés par Apple, voir Liste des clones Macintosh. Voir aussi Liste des modèles de Macintosh par année
Liste des moteurs de workflow classée par ordre alphabétique.
Cet article contient une liste de moteurs de recherche web ou d'entreprise.
Ceci est une liste des ordinateurs à transistors (transistor en tant que composant électronique discret, non partie intégrante d'un circuit intégré). Les transistors sont utilisés à partir du milieu des années 1950 pour remplacer les tubes électroniques. Ils sont eux-mêmes remplacés à partir des années 1970 par les circuits intégrés. Les ordinateurs annoncés mais non réalisés ne sont pas inclus dans cette liste. On notera aussi que certains des premiers ordinateurs à transistors utilisent encore des tubes électroniques pour l'alimentation électrique ou certaines fonctions auxiliaires.
Cette liste d'ordinateurs de fiction constitue une liste non exhaustive des ordinateurs (et des superordinateurs) en tant que personnages présents dans la fiction. L'ordinateur est un personnage récurrent de la culture populaire, notamment — mais pas exclusivement — dans le domaine de la science-fiction.
Cette page dresse une liste non exhaustive de différentes implémentations connues de pare-feu.
Cet article présente une liste de polices d'écriture.
Liste de ports TCP et UDP. Afin de permettre aux clients de connecter des serveurs, des numéros de ports logiciels sont bien connus. Les informations qui suivent sont en général reprises par chaque système dans un fichier ; par exemple : Les ports affichés ci-dessous peuvent être changés selon la configuration du programme sur Linux et OS X : /etc/services sur Windows : %SystemRoot%\system32\drivers\etc\services.
Cet article concerne les ports matériels.
Cette liste indique les principaux projets et réalisations marquants dans le domaine de l’intelligence artificielle. La quasi-totalité de ces travaux ont été accomplis aux États-Unis, et il est à noter que nombre d’entre eux ont été financés par l’armée américaine. L’un des principaux financeurs de ces travaux fut la DARPA (Defense Advanced Research Projects Agency), célèbre pour avoir initié le réseau Arpanet, qui a donné naissance à Internet. La liste est organisée par ordre chronologique.
Cet article est une liste des produits, logiciels ou hardware, crées par la société Apple puis abandonnés ou remplacés. Le premier produit abandonné est le Apple I, remplacé par le Apple II en 1977. Actuellement, Apple classifie ses produits abandonnés en trois catégories : ancien, obsolète ou legacy suivant le niveau de support technique fourni.
Framasoft propose de nombreux services libres (une grande majorité disponible en ligne, accessibles tous sous la forme nom.org) dont voici une liste non exhaustive :
Voici une liste non-exhaustive de codes pour effectuer le programme Hello world.
Aperçu des programmes pour le traitement des nuages de points 3D des relevés photogrammétriques ou des relevés à l'aide des scanners laser.
Voici une liste de publications importantes en informatique, organisés par domaine. Quelques raisons pour lesquelles une publication peut être considérée comme importante : sujet créateur – Une publication qui a créé un nouveau sujet ; découverte – Une publication qui a changé de manière significative les connaissances scientifiques ; influence – Une publication qui a considérablement influencé le monde, ou qui a eu un impact massif sur l'enseignement de l'informatique.
Un quine (ou programme autoreproducteur, self-reproducing en anglais) est un programme informatique (non vide) qui imprime son propre code source. L'opération qui consiste à ouvrir le fichier source et à l'afficher est considérée est exclu par le règlement tacite. Plus généralement, un programme qui utilise une quelconque entrée de données ne peut être considéré comme un quine valide. Dans beaucoup de langages de programmation, un quine est une variante de la commande suivante : Recopier puis recopier entre guillemets la phrase « Recopier puis recopier entre guillemets la phrase » À titre de défi ou d'amusement, certains programmeurs essaient d'écrire le plus court quine dans un langage de programmation donné. Donald Knuth (Prix Turing 1974) et Ken Thompson (prix Turing 1983) expliquent dans leurs conférences Turing le rôle que ces programmes autoreproducteurs minimaux ont joué dans leurs formations et le pourquoi de ce rôle,.
Cet article présente une liste de systèmes d’exploitation libres. Un logiciel libre est, selon la définition de la Free Software Foundation, un programme qui accorde 4 libertés essentielles à l'utilisateur : liberté d'exécuter le programme pour n'importe quel usage, liberté de le modifier, liberté de redistribuer des copies du programme et liberté de distribuer des versions modifiées. Un système d'exploitation libre est donc un système d'exploitation composé de logiciels libres. La Free Software Foundation tient une liste de systèmes libres, la présence dans cette liste nécessitant de respecter des conditions strictes : le système ne doit contenir aucun programme propriétaire, et ne doit pas inciter à en installer.
Cet article présente une liste de systèmes de gestion de contenu (en anglais CMS, pour Content management System).
Vous trouverez ci-dessous une liste de visionneuses d'images.
Un dépannage informatique est une opération technique, généralement faite par un professionnel, visant à rétablir le fonctionnement normal de tout équipement informatique, à la suite d'une panne matérielle ou à un usage dommageable d'un des logiciels utilisés. En cas de panne matérielle et sous certaines conditions, notamment de validité de garantie, un ordinateur équivalent peut être prêté au client le temps de procéder à la remise en état. À moins de s'engager dans des frais très considérables, le dépannage se limite à des remplacements de pièce ou à des opérations de maintenance simples : la récupération massive de données sur un disque dur défectueux est notamment hors de son champ d'action.
L'indice Syntec sert à mesurer l'évolution du coût de la main d'œuvre, essentiellement de nature intellectuelle, pour des prestations fournies. Il est généralement utilisé pour la révision ou l'actualisation d'une clause financière d'un contrat ou d'un marché. Cet indice a été créé en 1961. Sa page sur le site de la fédération Syntec (17 septembre 2013) le présente comme "reconnu par le Ministère de l'Économie et des FInances depuis le 11 mars 1974". L'indice Syntec est utilisé dans les contrats des branches professionnelles représentées par la Fédération Syntec : Syntec Informatique, Syntec Ingénierie et Groupement Syntec des Syndicats d'Etudes et de Conseil. Il est utilisé pour traduire l'évolution des coûts salariaux, dans le cas de projets au forfait par exemple, dans le cas des contrats dont la facturation peut être fondée sur des unités d'œuvre, de régie de longue durée, de maintenance de progiciels, d'applications ou encore de systèmes informatiques ou industriels, d'infogérance, …
Le terme de pupitreur désigne deux métiers différents selon le contexte professionnel :
Une saisie de données est une opération consistant à intégrer manuellement (généralement avec des opérateurs ou opératrices de saisie spécialisés) dans la mémoire d'un appareil électronique (généralement une base de données) des données d'une autre origine. Il peut s'agir d'une étape relativement longue lors, par exemple, d'une enquête par questionnaire si celle-ci n'est pas automatisée. Quelques exemples de saisie de données : saisie de questionnaires, saisie de formulaires, saisie d'enquêtes, saisie de coupons réponses, etc. La saisie de données se fait de plus en plus en offshore par des sociétés spécialisées dans l'externalisation.
Une application, un applicatif ou encore une appli, une app est, dans le domaine informatique, un programme (ou un ensemble logiciel) directement utilisé pour réaliser une tâche, ou un ensemble de tâches élémentaires d'un même domaine ou formant un tout. Typiquement, un éditeur de texte, un navigateur web, un lecteur multimédia, un jeu vidéo, sont des applications. Les applications s'exécutent en utilisant les services du système d'exploitation pour utiliser les ressources matérielles.
De 1985 à 1995, le consortium AMICE était une organisation européenne réunissant de grandes entreprises, incluant les utilisateurs, fournisseurs, sociétés de conseil, et le milieu universitaire, concernés par le Computer Integrated Manufacturing (CIM). Il a été lancé dans le cadre du projet Esprit (European Strategic Program on Research in Information Technology) et a joué un rôle dans le développement de CIMOSA (Computer Integrated Manufacturing Open Systems Architecture (en)) .
L'auralisation est un procédé visant à recréer un environnement acoustique à partir de données mesurées ou simulées. Ce procédé permet par exemple de se rendre compte du rendu sonore d'un édifice (théâtre, opéra, etc.) en projet ou bien de simuler une salle existante. Cette technique peut être utilisée pour rendre un environnement sonore cohérent au sein d'un système d'immersion virtuelle.
L'autopublication est le « processus de mise en forme d’un contenu sélectionné, collecté, agrégé, synthétisé, en vue de sa diffusion sans intermédiaire ». Ce terme s'applique surtout au domaine de l'Internet : on s'autopublie sur un blog, sur un site de partage de documents, etc. C'est un terme à mettre en parallèle avec l'auto-édition même si ces deux termes sont proches. Ce qu'il faut comprendre à travers le terme d'autopublication c'est la gratuité de l'opération. C'est-à-dire que la personne qui s'autopublie évite les frais d'envoi du manuscrit à une maison d'édition en choisissant de se publier sur une plateforme en ligne gratuitement.
Un bureau à distance (en anglais Remote Desktop) est une application informatique de contrôle à distance permettant d'accéder à l'interface graphique d'un ordinateur physiquement éloigné via un autre ordinateur local.
On nomme calcul automatique le fait de laisser des calculs se faire sans intervention humaine. Les moyens utilisés ont été ou sont les suivants : calculatrice mécanique règle à calcul simulateur physique, table vibrante, table rhéologique calculateur analogique calculette ordinateur calculateur stochastique calculateur ADN calculateur quantique Ces moyens peuvent bien entendu être combinés entre eux, et on trouvera parfois avantage à le faire.  Portail de l’informatique
Le calcul réversible est, en informatique, un modèle où le processus de calcul est physiquement réversible. Cela implique que ce calcul n'implique pas de phénomène dissipatif conduisant à une augmentation de l'entropie ; bien qu'il soit physiquement impossible d'atteindre cet objectif du fait du second principe de la thermodynamique, s'en rapprocher permet l'augmentation de l'efficacité des processeurs. En effet, la puissance des processeurs peut être améliorée en augmentant leur fréquence d'horloge ; une limite à cette méthode est une tendance à la surchauffe. Un processeur capable de calculs peu dissipatifs, et donc générant peu de chaleur, permettra d'adopter une fréquence d'horloge plus élevée.
Un catalogue interactif est un mode de présentation d'information sur Internet qui simule un catalogue traditionnel que l'on peut feuilleter grâce à la souris de l'ordinateur. Basé principalement sur la technologie Flash, le catalogue interactif autorise un rendu hyperréaliste avec des feuilles qui tournent, se plient, voire se déchirent. Les ombres portées en temps réel ajoutent au réalisme. Le catalogue interactif peut intégrer des images, des textes, des animations flash, du son et des vidéos.
Une chaîne éditoriale ou chaîne d'édition est la suite d'opérations (ou procédé industriel) par lequel un document rédigé par un auteur est transformé en document publiable et publié. Elle consiste à formater le document écrit, à élaborer des modèles de documents, et à effectuer les conversions de fichiers nécessaires. Elle s'occupe également du stockage et de la diffusion des documents. Le procédé est aujourd'hui largement informatisé.
Le computer-integrated manufacturing (CIM), ou production intégrée par ordinateur, est un concept décrivant l'automatisation complète des procédés de fabrication, Créé en 1996 par C.Fénérol. C’est-à-dire que tous les équipements de l'usine fonctionnent sous le contrôle permanent des ordinateurs, automates programmables et autres systèmes numériques,. L'idée du "digital manufacturing" émergea au début des années 1970, avec les travaux et le livre du Dr. Joseph Harrington, Computer Integrated Manufacturing,. Ce paradigme fut notamment promu par les fabricants de machines et d'équipement industriels dans les années 1980 et largement soutenu par la Society for Manufacturing Engineers (CASA/SME). Le concept aura trop souvent et abusivement été confondu avec la notion hyperfuturiste de l'époque d'usine sans ouvriers (« lights out » factory). Le CIM consiste et intègre les équipements de Conception assistée par ordinateur (CAO ou CAD/CAM, computer-aided design/computer-aided manufacturing), d'ateliers flexibles, de centres d'usinage à commande numérique (CNC),, de progiciel de gestion intégré (PGI/ERP, ou plutôt de MRP2 pour material requirement planning ou encore GPAO pour « gestion de production assistée par ordinateur » comme on avait coutume de nommer cette fonction dans les années 1970 et 1980), les applications ERP de stockage et de manutention automatisés ainsi que de méthodologies conceptuelles d'intégration de ces composants à l'intérieur d'un système global d'information de l'entreprise, par exemple avec la méthode GRAI.
La consolidation est en informatique le regroupement cohérent de données. Elle concerne généralement des données organisées logiquement ou liées entre elles. Plus spécifiquement pour les tableurs, il s’agit du regroupement de plusieurs tableaux issus de feuilles différentes (les feuilles sont des composantes des tableurs) voire de classeurs différents.
Le Cybercar est un véhicule routier entièrement automatisé, au moins sur certaines infrastructures, sous contrôle d'un système de gestion, et pouvant transporter des personnes ou des marchandises. Ce nouveau concept de transport (aussi appelé CTS pour Cybernetic Transport System) a été proposé en 1991 par l'INRIA (Institut National de Recherche en Informatique et en Automatique) et a été mis en place pour la première fois à l'aéroport Schiphol d'Amsterdam en décembre 1997 avec les véhicules ParkShuttles de la société Frog, puis en 1998 dans une banlieue de Rotterdam. Plusieurs sociétés fabriquent des cybercars et mettent en place les CTS : Navya, Robosoft et Lohr en France, ATS Ltd en Grande-Bretagne, APGM en Finlande, Toyota au Japon. Chaque véhicule coute entre 50,000 et 200,000€. La Commission Européenne a financé le développement des cybercars dans les projets CyberCars et CyberMove entre 2001 et 2004.
L'enseignement assisté par ordinateur (EAO) est une spécialité informatique qui regroupe les logiciels permettant l'aide à l'apprentissage dans des domaines divers, ainsi que les outils utilisés pour créer ces programmes. Dans la littérature, il est assez difficile d'établir ce qu'est réellement l'EAO. On peut toutefois se référer à des chercheurs tels que Skinner, Crowder, et Jean Piaget. Parmi les théoriciens contemporains il faut aussi noter l'apport de Seymour Papert qui, à travers la création du langage Logo, a permis de donner une dimension plus concrète à l'EAO. On s'est alors éloigné d'une approche directive, où en quelque sorte le logiciel programmait l'enfant, pour adopter une approche constructive et plus motivante pour celui-ci, qui devenait dès lors plus concentré parce qu'adoptant un rôle plus actif. En 2011, c'est cette approche qui fait l'objet des principaux développements, comme Sankoré 3.1 ou Moodle 2.0, qui sont par ailleurs des logiciels libres utilisant des contenus libres de droits. Les fondements de l'EAO remontent au milieu des années 1980, avec l'avènement et le développement des premiers ordinateurs personnels, des premiers logiciels éducatifs et de périphériques d'interaction homme-machine spécialisés comme la souris ou le crayon optique. Les sciences cognitives réunissent des domaines qui permettent de faire évoluer un tel enseignement. Les termes anglais CALL (computer assisted language learning) ou CALLT(computer assisted language learning and teaching) renvoient à un EAO appliqué à l'enseignement des langues et non à l'EAO en général, qui se traduit plutôt par les termes de computer-aided learning, ou, plus communément de nos jours, formation en ligne (e-learning). Les logiciels didactiques d'EAO sont appelés didacticiels. Il en existe différents types (ex: Micro-monde, Tuteur Intelligent). La baisse très rapide des coûts informatiques associée à la gratuité tant des logiciels Open Source que des contenus placés sous des licences libres telles que Creative Commons, associée à la pénurie d'enseignants dans les pays du Sud, en fait une solution que plusieurs pays d'Afrique comme le Mali explorent actuellement, par exemple avec le programme Sankoré (2011) dans le cadre de la solidarité francophone.
L'expérimentation assistée par ordinateur (ExAO) est une méthode de réalisation et d'exploitation de mesures utilisant l'ordinateur. Elle est un domaine important d'utilisation des TICE en sciences expérimentales et en particulier pour les établissements secondaires et universitaires français. La terminologie "officielle" ATIDEX est devenue marginale.
Un fab lab (contraction de l'anglais fabrication laboratory, « laboratoire de fabrication ») est un tiers-lieu de type makerspace cadré par le Massachusetts Institute of Technology (MIT) et la FabFoundation en proposant un inventaire minimal permettant la création des principaux projets fab labs, un ensemble de logiciels et solutions libres et open-sources, les Fab Modules, et une charte de gouvernance, la Fab Charter. Pour être identifié en tant que fab lab par la FabFoundation, il faut passer par plusieurs étapes et il est possible de suivre une formation à la Fab Academy. Les fab labs sont réunis en un réseau mondial très actif, d'après son initiateur Neil Gershenfeld.
Le but de la fabrication assistée par ordinateur ou FAO est d'écrire le fichier contenant le programme de pilotage d'une machine-outil à commande numérique. Ce fichier va décrire précisément les mouvements que doit exécuter la machine-outil pour réaliser la pièce demandée. On appelle également ce type de fichiers : programme ISO ou blocs ISO.
Les FRP (en anglais finance resource planning) sont une catégorie de logiciels qui sont aux flux financiers ce que les progiciels de gestion intégrés (ERP) sont aux flux physiques. Ils incluent l'ensemble des progiciels experts nécessaires aux directions financières sur un référentiel commun : comptabilité rapprochement comptable du journal de banque avec le relevé de compte bancaire gestion des immobilisations déclarations fiscales trésorerie consolidation échanges bancaires …  Portail de l’informatique
Un logiciel de GPAO, gestion de la production assistée par ordinateur, est un programme de gestion de production permettant de gérer l'ensemble des activités, liées à la production, d'une entreprise industrielle : Gestion des stocks et des achats Gestion de commandes Gestion des produits engendrés par ces commandes Gestion des articles entrant dans la fabrication de ces produits et de leurs nomenclatures-gammes Gestion des ressources par familles (couple homme/spécialité) permettant la création des gammes (nomenclature de fabrication) Création et gestion du planning de fabrication Expédition des produits Facturation L'industrie du logiciel a développé depuis les années 1970 un certain nombre d'outils informatiques permettant de mieux gérer la production sous ses divers aspects : Ordres de Fabrication (OF) - suivi des stocks - suivi des temps - gestion des coûts - ordonnancement-planning. La GPAO est notamment caractérisée par un système de réapprovisionnement en produits et composants appelé calcul des besoins nets (CBN), aujourd'hui plus connu sous son vocable anglais "Material Requirement Planning" ou MRP. II est le fruit des travaux de Joseph Orlicky en 1964 en réponse au Toyota Manufacturing Program. En 1984, Oliver Wight a fait évoluer le concept MRP vers le MRP2 (Manufacturing Resources Planning). Le MRP, cœur de la GPAO a été largement promu par l'APICS (American Production and Inventory Control Society). Les fonctions de la GPAO sont communément incorporées, depuis les années 1990, aux Progiciels de gestion intégrés (ERP ou PGI) qui s'appliquent à toutes les fonctions de l'entreprise.
La gestion de maintenance assistée par ordinateur (souvent abrégée en GMAO) est une méthode de gestion assistée d'un logiciel destiné aux services de maintenance d'une entreprise afin de l'aider dans ses activités.
La gestion des langues est une discipline qui consiste à satisfaire des besoins d'usage de plusieurs langues, dans un même État, dans des entreprises, dans des institutions internationales où l'on doit pratiquer plusieurs langues, ou dans des organismes culturels.
Les GDSS (Group Decision Support System) sont des logiciels d'aide à la prise de décision collective. Ils permettent notamment de préparer en ligne des réunions, et d'éviter ainsi le gaspillage de temps qui caractérisent souvent les réunions d'équipe. Ce sont des systèmes qui tentent de structurer la prise de décision en groupe.
Hawk-Eye (qu'on pourrait traduire par Œil de Faucon ou Œil d'Aigle, mais qui fait aussi allusion à son inventeur, Paul Hawkins), est un système informatique propriétaire destiné à l'arbitrage et réalisé par la société Hawk-Eye Innovations Ltd. Il est utilisé sur les circuits professionnels de tennis WTA et ATP, ainsi que pour certains matchs de cricket, ou de billard. C'est d'ailleurs au cours d'un match de cricket que Paul Hawkins, qui dans les années 1990 travaillait sur une thèse en intelligence artificielle, eut l'idée de créer un "système vidéo" capable d'être plus performant qu'un arbitre humain. En effet, il avait eu la désagréable sensation d'avoir été victime d'une erreur d'arbitrage. Il a transformé sa frustration en innovation technologique. Ce système est également utilisé en volley-ball lors de Jeux Européens à Bakou en juin 2015. L'usage du Hawk-Eye pour le football, à l'étude depuis 2008 pour régler les litiges liés à la ligne de but, est effectif dès la saison 2013-2014 en Premier League anglaise. Ce système est utilisé depuis juin 2013 dans le championnat de football gaélique irlandais, exclusivement pour déterminer si le ballon est bel et bien passé entre les poteaux. En avril 2016, le système Hawk-Eye a été choisi par l'UEFA pour être utilisé lors des matches de l'Euro 2016.
Le projet Hyperbraille consiste à produire et tester un système (encore expérimental en 2010) de plage braille permettant la reproduction (via des structures saillantes) de graphiques, dessins, à l'usage d'aveugles ou personnes très malvoyantes . Le modèle tel qu'existant en 2010 permet également la conversion automatiquement de textes numériques en caractères braille.
Un hyperpaysage est à l'image ce qu'un hypertexte est au texte. C'est un média dans lequel il est possible de naviguer grâce à des zones sensibles (hotspots) qui se comportent comme des liens web internes ou externes.
On désigne sous le terme informatique embarquée les aspects logiciels se trouvant à l'intérieur des équipements n'ayant pas une vocation purement informatique. L'ensemble logiciel, matériel intégré dans un équipement constitue un système embarqué.
L'informatique industrielle est une branche de l'informatique appliquée qui couvre l'ensemble des techniques de conception, d'analyse et de programmation de systèmes informatiques à vocation industrielle.
Une infrastructure énergétique est une infrastructure matérielle (ou éventuellement pour partie et localement immatérielle) qui permet le transport (ex : Oléoducs, gazoducs, réseau électrique...) et éventuellement le stockage de différentes formes d'énergies. Entre le producteur et le consommateur, un gestionnaire de réseau de transport ou « GRT » gère l'infrastructure. Ces réseaux sont généralement structurés en maillages ou en arborescence, et plus ou moins maillés et interconnectés. Dans certains cas ou à certains moments l'énergie peut circuler dans les deux sens (un producteur peut devenir consommateur et inversement, situation qui est amenée à se développer avec la production décentralisée d'énergies renouvelables). Avec les progrès des TICs, une partie de cette gestion tend à être automatisée, et contrôlée par des systèmes dits "intelligents".
L'ingénierie numérique ou ingénierie assistée par ordinateur (IAO) regroupe l'ensemble des moyens numériques et logiciels habituellement utilisés par les ingénieurs et techniciens des bureaux d'études pour concevoir, simuler et valider de nouveaux produits et processus industriels. L'aboutissement de la phase d'ingénierie numérique d'un produit est sa maquette numérique. Font classiquement partie de la catégorie des outils d'ingénierie numérique : les logiciels de CAO (conception assistée par ordinateur), les logiciels de simulation des comportements (statiques, cinématiques, dynamiques, rhéologiques…), les logiciels de FAO (fabrication assistée par ordinateur), les SGDT (système de gestion des données techniques), les systèmes de PLM (product lifecycle management), les SGBC (système de gestion de bases de connaissance), les systèmes de KLM (knowledge lifecycle management), les environnements immersifs.
L'instruction assistée par ordinateur, abrégé sous le sigle IAO, est une application de la gestion électronique des documents (GED) au service de l'instruction ou le cas échéant du Ministère public. Ce concept permet de faciliter le travail d'analyse du juge d'instruction pour la mise en évidence d'éléments fondamentaux issus d'un dossier d'instruction ou pour retrouver des éléments ou des informations, en particulier dans des dossiers contenant plusieurs milliers de pages. Il a été inventé et partiellement développé par un magistrat, Emmanuel Barbe, alors à la sous-direction des affaires économiques et financières du ministère de la justice.
La conception et la fabrication assistée par ordinateur utilisent de nombreuses abréviations, dont vous trouverez ci-dessous une liste qui ne prétend pas être exhaustive. CAE/CAD/CAM Computer-Aided Engineering, Design and Manufacturing (ingénierie, conception et fabrication assistées par ordinateur)
La mécatronique est la combinaison synergique et systémique de la mécanique, de l'électronique, de l'automatique et de l'informatique en temps réel. L'intérêt de ce domaine d'ingénierie interdisciplinaire est de concevoir des systèmes automatiques puissants et de permettre le contrôle de systèmes complexes. Le terme mechatronics a été introduit par un ingénieur de la compagnie japonaise « Yaskawa Electric Corporation (en) » en 1969. Le terme mécatronique est apparu officiellement en France dans le Larousse 2005. La norme NF E 01-010 (2008) définit la mécatronique comme une « démarche visant l’intégration en synergie de la mécanique, l’électronique, l’automatique et l’informatique dans la conception et la fabrication d’un produit en vue d’augmenter et/ou d’optimiser sa fonctionnalité »
La méthode Maxer est une méthode servant à analyser les pannes et les défaillances, en particulier en milieu industriel. Elle a été conçue par le groupe Lausanne pour le compte de la société Michelin dans les années 1970.
Une application de mobile device management (MDM) ou « gestion de terminaux mobiles », est une application permettant la gestion d'une flotte d'appareils mobiles, qu'il s'agisse de tablettes, de smartphones, voire d'ordinateurs hybrides au format tablette ou d'ordinateurs portables. Cette gestion est effectuée au niveau du service informatique de l'organisation (entreprise, association ou collectivité). L'objectif du MDM est d'harmoniser et de sécuriser la flotte de la société en s'assurant que tous les collaborateurs aient des programmes à jour et que leurs appareils soient correctement sécurisés. Le programme facilite également la propagation de patchs de sécurités ou de nouveaux logiciels pour l'ensemble des collaborateurs. La MDM gère des tailles et des types de flottes variées allant d'une dizaine de terminaux identiques, jusqu'à des milliers de terminaux tous différents et utilisant différents systèmes d'exploitation. C'est au début des années 2000 que l'on a vu les solutions de MDM apparaître dans un premier temps pour les PDA, les assistants personnels digitaux précurseurs des tablettes numériques.
Navya est une entreprise française spécialisée dans la conception et la construction de véhicules autonomes, électriques et robotisés. En 2015, elle a lancé la navette autonome Arma, premier véhicule autonome de série sans chauffeur à être commercialisé.
La physique numérique (ou parfois physique informatique) est l'étude et l'implémentation d'algorithmes numériques dans le but de résoudre des problèmes physiques pour lesquels une théorie existe déjà. Elle est souvent considérée comme une sous-discipline de la physique théorique mais certains la considèrent comme une branche intermédiaire entre la physique théorique et la physique expérimentale. En général, les physiciens définissent un système et son évolution grâce à des formules mathématiques précises. Il arrive souvent que la solution des équations basées sur les principes de la physique fondamentale ne soit pas adaptée à la description du système. Ceci est particulièrement vrai dans le cas de la mécanique quantique, où seulement une poignée de modèles simples possèdent des solutions analytiques complètes. Dans les cas où les systèmes ont seulement des solutions numériques, des calculs numériques sont employés.
Une Plate-forme éditoriale est un outil de travail collaboratif utilisable en ligne au moyen d'un navigateur web permettant la gestion et/ou la production de publications destinées à l'impression (ou publications print). Les solutions les plus ergonomiques permettent aux collaborateurs de "rentrer" littéralement dans la mise en page d'un document pour intégrer leurs rédactionnels ou leurs photos. Ce type de dispositif éditorial est généralement distribué en mode hébergé (ASP) ou sous forme d'application web (SaaS).
Plei@d est une plateforme LMS qui existe depuis 1998, développée à l'origine pour le CNAM Pays de la Loire. Pleiad = Pays de la Loire, Enseignement Interactif A Distance. En 2002, date à laquelle le CEANTE (centre d'étude et d'application des nouvelles technologies éducatives) fut créé, les serveurs ont migré vers du LAPP (Linux, Apache, PostgreSQL, PHP). Elle appartient au CNAM et est mise en œuvre par les centres CNAM et certaines universités.
Le post-processeur est un outil informatique utilisé en fabrication assistée par ordinateur. Un post-processeur permet de traduire le langage d'une FAO (programme, logiciel de fabrication assistée par ordinateur), le processeur, vers une MOCN (machine-outil à commande numérique). En effet, un logiciel de FAO crée un fichier qui n'est pas directement assimilable par la MOCN. Il faut traduire ce fichier pour créer un nouveau fichier exploitable par la MOCN. Ce fichier peut être par exemple au format ISO, dit aussi G-code, qui est supporté par la plupart des machines. Chaque constructeur de MOCN a ses spécificités qui nécessitent des adaptations de post-processeurs. Chaque logiciel de FAO a lui aussi ses propriétés qui font que le post-processeur d'un logiciel A ne sera pas le même que le post-processeur d'un logiciel B pour la même machine outil. On peut dire aussi qu'un post-processeur est un programme conçu pour traiter des données lues à partir d’un fichier source (APT par exemple) comprenant des informations, des trajectoires d’outil et les paramètres d’usinage pour les adapter à une machine spécifique sous forme d’un programme conforme aux standards ISO (par exemple) qui sera sauvegardé dans un fichier NCC (Numerical Control Code).
La prévision numérique du temps (PNT) est une application de la météorologie et de l'informatique. Elle repose sur le choix d'équations mathématiques offrant une proche approximation du comportement de l'atmosphère réelle. Ces équations sont ensuite résolues, à l'aide d'un ordinateur, pour obtenir une simulation accélérée des états futurs de l'atmosphère. Le logiciel mettant en œuvre cette simulation est appelé un modèle de prévision numérique du temps.
La programmation de commande numérique (CN) permet de définir des séquences d'instructions permettant de piloter des machines-outil à commande numérique. Cette programmation est actuellement fortement automatisée à partir de plans réalisés en CAO. Dans le cas d'une commande numérique physique, c'est le directeur de commande numérique (DCN) qui interprète les instructions contenues dans les séquences, reçoit les informations des capteurs et agit sur les actionneurs. On trouve aussi des pilotes de commande numériques qui sont des programmes (logiciels) informatiques s'exécutant sur un PC, avec éventuellement une délégation partielle des calculs vers une carte spécialisée.
Le project Cybersyn ou en espagnol, proyecto Synco a été un projet chilien visant à créer une économie planifiée contrôlée par ordinateur en temps réel durant les années 1970–1973 (sous le gouvernement du président Salvador Allende). Il s'agissait essentiellement d'un réseau de télex qui reliait les entreprises à un ordinateur central situé à Santiago qui était contrôlé suivant les principes de la cybernétique. Le principal architecte de ce système était le scientifique britannique Stafford Beer.
Les « réseaux intelligents » sont des réseaux matériels de distributions de fluides (électricité, eau, gaz, pétrole...), et/ou d'information (télécommunications) qui ont été « augmentés » (rendus intelligents) par des systèmes informatiques, capteurs, interfaces informatiques et électromécaniques leur donnant des capacités d'échange bidirectionnel et parfois une certaine capacité d'autonomie en matières de calcul et gestion de flux et traitement d'information.
Le réseautage social terme francophone (Québec) se rapporte à l'ensemble des moyens virtuels (internet) mis en œuvre pour relier des personnes physiques ou personnes morales entre elles. Avec l'apparition d'Internet, il recouvre les applications Web connues sous le nom de « service de réseautage social en ligne ». Ces applications ont de multiples objectifs et vocations. Elles servent à constituer un réseau social afin d'échanger des informations et de s'y mettre en valeur. Elles offrent à des groupes d'amis, des associés ou plus généralement des personnes partageant des centres d'intérêt communs divers outils pouvant faciliter, par exemple, la gestion des carrières professionnelles, la distribution et la visibilité artistique ou les rencontres privées. Ces applications sont pointées du doigt du fait de l'exploitation commerciale de données personnelles, de technologies favorisant l'addiction et de leur impact sur l'équilibre psychologique des jeunes,.
SD-WAN est l'acronyme de Software-Defined Wide Area Network, soit réseau étendu à définition logicielle, et est présenté comme la nouvelle évolution majeure des télécommunications. Un SD-WAN facilite la gestion en séparant la partie contrôle de la partie réseau.
Un serveur vocal interactif, ou SVI (en anglais Interactive Voice Response, ou IVR) est un système informatique capable de dialoguer avec un utilisateur par téléphone. Il est capable de recevoir et d'émettre des appels téléphoniques, de réagir aux actions de l'utilisateur (appui sur des touches du téléphone, reconnaissance vocale ou reconnaissance de son numéro téléphonique d'appel) selon une logique préprogrammée, diffuser des messages préenregistrés ou en synthèse vocale, et d'accéder à des bases de données d'autre part. Un serveur vocal interactif est généralement capable de traiter de nombreux appels simultanés indépendants,.
Le show control est le terme utilisé pour désigner la technique qui consiste à commander et superviser tous les systèmes audiovisuels de lieux tel que les salles de spectacles, les musées, les parcs d'attraction, les théâtres, les salles de conférences, les show-rooms... Le système de show control est donc un système centralisateur qui permet de synchroniser les diverses technologies utilisées dans ces lieux : la lumière, le son, la vidéo, la machinerie, la pyrotechnie et les effets spéciaux. Le terme de show control est apparu en 1990 lors de la création du protocole MIDI Show Control (MSC) : protocole de communication standard qui utilise le MIDI pour communiquer entre les différents appareils. Les appareils ont évolué depuis cette époque, et les logiciels de show control utilisent maintenant de nombreux protocoles autres que le MIDI pour gérer la communication entre les appareils.
La simulation des réseaux est une technique par laquelle un logiciel (simulateur) modélise le comportement d'un réseau, soit par le calcul de l'interaction entre les entités du réseau en utilisant des formules mathématiques, ou en capturant et reproduisant des observations à partir d'un réseau réel.  Portail de l’informatique
La sociologie informatique est une branche de la sociologie développée récemment. Son approche consiste à utiliser le calcul pour analyser des phénomènes sociaux. Il s'agit d'utiliser la technique de la simulation sur ordinateur pour la construction de modèles sociaux. Cela implique un certain arrangement des agents sociaux, et des interactions entre eux. Enfin, on examine l'effet de ces interactions sur un agrégat social. Bien que les thèmes et les méthodologies de cette science sociale diffèrent de ceux de la science normale ou informatique, plusieurs des approches utilisées dans les simulations contemporaines du social provient des champs comme la physique et l’intelligence artificielle. Dans la littérature scientifique, la sociologie informatique est souvent liée à l'étude de complexité sociale. Concepts sociaux de complexité comme systèmes complexes, interconnexions non linéaires dans de macro et micro processus, et apparitions sont le lot de la sociologie informatique. Cette branche semble toutefois devoir être distinguée de la « socio-informatique » qui utilise des outils informatiques pour décrire et analyser des dossiers complexes. Un exemple pratique et bien connu est la construction d'un modèle informatique sous forme de société artificielle, par lesquels les chercheurs peuvent analyser la structure d'un système social. Une autre approche apparue sous le concept de paradigme socio-informatique avec Thérèse Étienne* cherche à "remplacer" la vérification statistique des hypothèses en sciences sociales par un prototypage informatique. Ce prototypage met l'accent : sur l'importance des relations au détriment des objets, l'articulation de la logique propositionnelle et des logiques déontiques et l'articulation des ontologies. Le prototype est un tangent organisationnel et informationnel d'une situation ou d'une réalité sociale. La théorie mathématique des catégories permet de réfléchir utilement sur les isomorphismes "actions réelles et fonctionnalités prototypales". La notion de socio-informatique est également utilisée dans la sociologie des controverses pour désigner les dispositifs d'analyse et de cartographie des problèmes publics et des débats qu'ils engendrent
Un SoftPLC est un logiciel permettant d'utiliser un ordinateur (le plus souvent un PC) en tant qu'automate programmable industriel (en anglais, Programmable Logic Controller ou PLC). Ce terme générique a été ultérieurement repris directement dans le nom d'un produit.
L'arrivée de l'informatique et du stockage d'information sous forme numérique a entraîné une véritable révolution dans le domaine musical. Cette révolution a commencé avec le CD audio, puis avec la compression des fichiers audios, puis les lecteurs dits MP3 et continue de nos jours avec l'intégration de la composante numérique dans le monde de la Hi-Fi et dans les lecteurs multimédias. Il y a pour le grand public plusieurs sources possibles pour obtenir de la musique sous forme numérique. Sources analogiques Disque microsillon Cassette audio Radio  Sources numériques CD audio et ses dérivés Magasin de musique en ligne Webradio et Diffusion audionumérique Streaming
Straight-through processing (en abrégé STP) est un principe de traitement des opérations au fil de l'eau, c'est-à-dire sans rupture ou sans délai (littéralement : "traitement tout droit à travers"). Cette expression est employée dans les milieux financiers ; on peut faire l'analogie avec le principe de flux tendu employé dans l'industrie.
Un système de messagerie vocale (voice messaging system ou VMS en anglais) est un système informatique qui gère des boîtes vocales (répondeurs).
Les systèmes de transport intelligents (STI) (en anglais intelligent transportation systems (ITS)) sont les applications des nouvelles technologies de l'information et de la communication au domaine des transports et de sa logistique. On les dit "Intelligents" parce que leur développement repose sur des fonctions généralement associées à l'intelligence : capacités sensorielles et de choix, mémoire, communication, traitement de l'information et comportement adaptatif. On trouve les STI dans plusieurs champs d'activité : dans l'optimisation de l'utilisation des infrastructures de transport, dans l'amélioration de la sécurité (notamment de la sécurité routière) et de la sûreté ainsi que dans le développement des services. Le recours aux STI s'intègre aussi dans un contexte de développement durable : ces nouveaux systèmes concourent à la maîtrise de la mobilité en favorisant entre autres le report de la voiture vers des modes plus respectueux de l'environnement. Ils font l'objet d'une compétition économique serrée au niveau mondial.
Les tableaux de bord informatique ou tableaux de bord de la DSI sont des outils de gestion qui vont permettre à la DSI de contrôler et d’anticiper les activités de l’entreprise. Ils sont définis selon plusieurs indicateurs qui sont souvent à déterminer en fonction des objectifs et de l’activité de l’entreprise. Ce sont des outils de pilotage essentiels pour la gouvernance des systèmes d’information.
Les technologies de l'information et de la communication pour l'enseignement (TICE) recouvrent les outils et produits numériques pouvant être utilisés dans le cadre de l'éducation et de l'enseignement (TICE = TIC + Enseignement). Les TICE regroupent un ensemble d’outils conçus et utilisés pour produire, traiter, entreposer, échanger, classer, retrouver et lire des documents numériques à des fins d'enseignement et d'apprentissage. L'étude des méthodes d'enseignement intégrant les TICE est quant à elle l'objet de la technopédagogie.
Un logiciel de gestion des services d'assistance est un logiciel applicatif qui permet le suivi de l'activité des services d'assistance dans des organisations vouées à ce type d'activité (centre d'assistance, cellules d'assistance réparties, centre d'appel, ...).
En informatique, le terme traitement de données renvoie à une série de processus qui permettent d'extraire de l'information ou de produire du savoir à partir de données brutes. Ces processus, une fois programmés, sont le plus souvent automatisés à l'aide d'ordinateurs. Si les résultats finaux produits par ces processus sont destinés à des humains, leur présentation est souvent essentielle pour en apprécier la valeur. Cette appréciation est cependant variable selon les personnes. Si la finalité n'est pas de présenter des résultats à un utilisateur humain, l'objectif du traitement de données est généralement d'offrir une information de plus haut niveau ou une information de meilleure qualité à un autre outil de traitement ou d'analyse. Ce traitement de l'information peut alors relever de la fusion de données, de l'extraction d'information ou de la transformation de la représentation. Par exemple, la fusion peut consister à combiner plusieurs sources de données afin de les compiler en une information plus sûre et l'extraction peut être un traitement destiné à sémantiser ou synthétiser les données. L'ensemble des processus de traitement des données d'un système compose le système d'information.
Le traitement de la parole est une discipline technologique dont l'objectif est la captation, la transmission, l'identification et la synthèse de la parole. Dans ce domaine, on peut définir la parole comme un texte oral. On s'intéresse à l'intelligibilité, c'est-à-dire à la possibilité, pour la personne qui écoute, de comprendre sans erreur le texte émis ; à l'amélioration de l'intelligibilité quand le signal est dégradé ; à l'identification de la personne qui parle ; à l'établissement automatique d'un texte écrit à partir de la parole ; à la synthèse de la parole à partir d'un texte écrit. Les caractères expressifs (au sens de l'art dramatique) et musicaux de la voix humaine font partie du domaine plus vaste du traitement du signal. Les principales disciplines qui contribuent aux technologies du traitement de la parole sont la psychoacoustique la phonétique la phonologie les méthodes générales du traitement du signal
Le travail coopératif assisté par ordinateur (TCAO) (en anglais : Computer Supported Cooperative Work, ou CSCW), est une méthode de travail. Bien que souvent assimilé à tort au groupware, le TCAO s’en distingue du fait qu’il représente l’étude des outils et techniques des groupwares ainsi que leurs effets sociaux, psychologiques et d'organisation. Cependant, il se distingue du système expert par le fait que chaque utilisateur est expert en son domaine, rendant son utilisation la plus simple possible afin qu’elle puisse être comprise de tous.
Visone est une application informatique permettant d'analyser des réseaux sociaux. L'analyse met l'accent sur l'aspect graphique : de nombreux algorithmes de tracé sont disponibles et permettent à l'utilisateur de comprendre des mécanismes du réseau selon la façon dont il est alors tracé. L'interface graphique simple permet l'utilisation du programme par les chercheurs expérimentés en réseaux sociaux ou les débutants, et ses algorithmes de tracé en font un des outils possibles pour l'enseignement.
Visualisation scientifique, ou graphiques scientifiques, est l'étude et/ou la réalisation de la représentation sous forme graphique de résultats scientifiques. Cette visualisation est donc intimement liée aux sciences et à l'informatique.
XAO est un sigle désignant l'ensemble des tâches assistées par ordinateur (d'où le sigle AO), en particulier dans les processus de conception et de fabrication industriels. En anglais on parle de « computer aided ».
En informatique, architecture désigne la structure générale inhérente à un système informatique, l'organisation des différents éléments du système (logiciels et/ou matériels et/ou humains et/ou informations) et des relations entre les éléments. Cette structure fait suite à un ensemble de décisions stratégiques prises durant la conception de tout ou partie du système informatique, par l'exercice d'une discipline technique et industrielle du secteur de l'informatique dénommée elle aussi architecture, et dont le responsable est l'architecte informatique. La structure d'un système informatique n'est à ce jour assujettie à aucune norme. En matière de logiciels, elle est représentée sous forme de graphiques tels que des organigrammes, des diagrammes de workflow (flux de travaux en français) ou des diagrammes entité-relation. Le diagramme peut concerner un logiciel, une pièce de matériel, un réseau informatique, un groupe de machines, un sous-système, voire l'ensemble des dispositifs informatiques d'une entreprise ou d'une institution. Un diagramme d'architecture est une perspective qui dépend du point de vue adopté par son auteur, en fonction des éléments qu'il cherche à mettre en évidence. Le diagramme omet volontairement certains détails pour rendre la perspective plus visible. Il peut y avoir plusieurs diagrammes d'architecture pour un même système, tels que : architecture des informations, architecture métier, architecture applicative et architecture technique.
L'architecture informatique ajax (acronyme d'asynchronous JavaScript and XML) permet de construire des applications Web et des sites web dynamiques interactifs sur le poste client en se servant de différentes technologies ajoutées aux navigateurs web entre 1995 et 2005. Ajax combine JavaScript, les requêtes de type XMLHttpRequest, les manipulations du DOM, ainsi qu'un format de données (XML ou JSON), afin d'améliorer maniabilité et confort d'utilisation des applications internet riches : DOM et JavaScript permettent de modifier l'information présentée dans le navigateur en respectant sa structure ; l'objet XMLHttpRequest sert au dialogue asynchrone avec le serveur Web ; XML, cité dans l'acronyme, était historiquement le moyen privilégié pour structurer les informations transmises entre serveur Web et navigateur, de nos jours le JSON tend à le remplacer pour cet usage. L'usage d'ajax fonctionnent sur tous les navigateurs Web courants : Google Chrome, Safari, Mozilla Firefox, Internet Explorer, Microsoft Edge, Opera, etc.
L’algorithme de Tomasulo est un algorithme facilitant le parallélisme au sein des processeurs mis au point en 1967 par Robert Tomasulo. Cet algorithme, est l'une des implémentations possibles pour l'exécution dans le désordre : il trie les instructions de manière à traiter plus tard les instructions qui seraient normalement bloquées en raison de dépendances de données. Mise en place pour la première fois pour l'unité de calcul en virgule flottante de l'IBM 360 modèle 90. Robert Tomasulo a reçu le Prix Eckert-Mauchly en 1997 pour cet algorithme.  Portail de l'informatique théorique  Portail de l’informatique
En génie informatique, on appelle architecte informatique la personne chargée de l'analyse technique nécessaire à la conception du diagramme d'architecture, c'est-à-dire le plan de construction d'un logiciel, d'un réseau, d'une base de données, etc. Un architecte informatique peut travailler avec les développeurs du système actuel ou d'autres architectes informatique et produire par exemple les diagrammes suivants : diagramme d'architecture : un diagramme qui décrit dans les grandes lignes les acteurs d'un système informatique - humains ou machines, leur rôle, les flux d'informations entre les différents acteurs et les protocoles. diagramme d'architecture applicatif : diagramme qui décrit les flux d'informations entre un logiciel donné, et les autres acteurs du système d'information dans lequel il va être implanté. (usagers, autres logiciels: SGBD, annuaire,…) diagramme d'architecture technique : diagramme qui décrit les flux d'informations entre les différentes pièces d'un logiciel. diagramme d'architecture physique : diagramme qui décrit les flux d'informations entre les différents appareils d'un système d'informations. (serveurs, ordinateurs personnels, routeurs, …)  ...
Dans l'architecture des ordinateurs, les entiers 8 bits (adresses de mémoire, ou autres unités de données) sont ceux qui ont au maximum 8 bits, c'est-à-dire 1 octet de large. Aussi, les processeurs 8 bits architecture ALU sont ceux qui sont fondés sur des registres, des bus d'adresse, ou des bus de données de cette taille. « 8 bits » est aussi un terme donné à une génération de calculateurs dans lesquels les processeurs 8 bits étaient la norme. Les processeurs 8 bits utilisent normalement un bus de données 8 bits et un bus d'adresse 16 bits qui signifie que leur espace d'adressage est limité à 64 kiB. Ce n'est pas une « loi naturelle », cependant, il y a des exceptions. Le premier microprocesseur 8 bits largement adopté a été l'Intel 8080, qui a été utilisé dans les ordinateurs de nombreux amateurs de la fin des années 1970 et début 1980, souvent utilisé sous système d'exploitation CP/M. Le Zilog Z80 (compatible avec le 8080) et le Motorola 6800 ont également été utilisés dans des ordinateurs similaires. Les processeurs 8 bits Z80 et le MOS Technology 6502 ont été largement utilisés dans les ordinateurs personnels et des consoles de jeux des années 1970 et 1980. Beaucoup de processeurs 8 bits ou microcontrôleurs sont à la base des systèmes embarqués actuels. Il y a 28 (256) valeurs possibles de 8 bits. La valeur maximale est 255 (28-1). Environ 55 % de tous les processeurs vendus dans le monde sont des microcontrôleurs ou microprocesseurs 8 bits.
En informatique, l'architecture 16 bits est un type de structure d'ordinateur exploitant des mots (processeur, mémoire, bus, , etc.) d'une taille de 16 bits (soit deux octets). Un mot de 16 bits peut stocker 216 valeurs différentes, soit 65 536 valeurs. Un processeur 16 bits peut donc adresser directement 64 kio de mémoire.
En informatique, on parle d’une architecture 32 bits lorsque les mots manipulés par le processeur ont une largeur de 32 bits, ce qui leur permet de varier entre les valeurs 0 et 4 294 967 295 pour un mot non signé (c'est-à-dire un mot non doté d'un signe mathématique positif ou négatif), et entre −2 147 483 648 et 2 147 483 647 pour un mot signé. Lorsque le mot de 32 bits sert à coder l’adresse d’un élément, il peut adresser une mémoire de taille 4 Gio (232). Donc un système d'exploitation Microsoft Windows de 32 bits ne pourra pas utiliser plus de 4 Gio de RAM[réf. incomplète]. Cependant, des processus bénéficiant de la fonctionnalité PAE, ainsi que d'un système d'exploitation compatible, peuvent voir la taille de leur mémoire atteindre jusqu'à 64 gibioctets (236). Par exemple, sur n'importe quel système GNU/Linux (tel que Debian, Linux Mint ou Ubuntu pour ne citer qu'eux), il est possible d'installer le noyau linux-pae qui permet au système de prendre en charge jusqu'à 64Go de RAM sur un système 32 bits.  On peut citer comme exemple d’architecture 32 bits l’architecture Intel i386.
Un ordinateur dataflow (flux de données) décrit une architecture où les données sont des entités actives qui traversent le programme de manière asynchrone, contrairement à l'architecture classique von Neumann où elles attendent passivement en mémoire pendant que le programme est exécuté séquentiellement suivant le contenu du pointeur de programme (PC). On parle aussi d'ordinateur cadencé par les données.
L’architecture dite architecture de von Neumann est un modèle pour un ordinateur qui utilise une structure de stockage unique pour conserver à la fois les instructions et les données demandées ou produites par le calcul. De telles machines sont aussi connues sous le nom d’ordinateur à programme enregistré. La séparation entre le stockage et le processeur est implicite dans ce modèle.
L'architecture dirigée par les données (en anglais Data Driven Architecture, DDA) est un modèle d'architecture informatique qui insiste sur la structuration des données.
L’architecture de type Harvard est une conception des processeurs qui sépare physiquement la mémoire de données et la mémoire programme. L’accès à chacune des deux mémoires s’effectue via deux bus distincts.
L’architecture matérielle décrit l’agencement interne de composants électroniques ainsi que leurs interactions. Le terme interne employé ici permet de bien faire la différence avec l’architecture (externe) de processeur (ou architecture de jeu d'instruction), qui s'intéresse à la spécification fonctionnelle d'un processeur, du point de vue du programmeur en langage machine. Plusieurs architectures internes peuvent implémenter une même architecture externe.
Dans le domaine informatique, l'architecture physique (également nommée architecture technique) décrit l'ensemble des composants matériels supportant l'application. Ces composants peuvent être des calculateurs (ou serveurs matériels) des postes de travail des équipements de stockage (baie de stockage, SAN, filers…) des équipements de sauvegarde des équipements réseaux (routeurs, firewalls, switches, load-balancers, accélérateurs SSL). L'architecture technique décrit des composants logiciels déployés sur les composants matériels. Ces composants logiciels sont : des systèmes d'exploitation des middlewares des SGBD des composants liés à l'exploitation (ordonnanceur, supervision système et réseau, logiciel de sauvegarde…) des serveurs web des serveurs d'applications autres serveurs (DNS, SMTP, NTP…) les composants logiciels spécifiques à l'application (application web java, composant EJB, schéma de base de données) L'architecte technique décrit comment les composants logiciels sont déployés sur les composants matériels. L'architecture technique décrit les interfaces et flux entre les divers composants. Cette architecture peut être décrite sous forme de modèles statiques, indiquant les divers composants et les relations entre ceux-ci. Elle peut être décrite également sous forme de modèles dynamiques, indiquant comment les composants interagissent pour remplir les fonctionnalités attendues. Une architecture technique répond à des exigences d'architecture technique. La personne responsable de la conception de cette architecture est l'architecte matériel.  Portail de l’informatique
Le Cortex-A55 est un modèle de microprocesseur implémentant le jeux d'instructions ARMv8.2-A 64 bits designé par ARM Holding plc, et annoncé en juin 2017. Il s'agit d'un microprocesseur avec pipeline superscalaire à l’exécution in-order, destiné à remplacer le Cortex-A53.
Une Branch History Table (BHT) est une méthode de prédiction dynamique utilisée lors de l'apparition d'aléas de contrôle sur un pipeline (informatique).  Portail de l’informatique
En informatique, une bulle est un temps d'attente lors de l'exécution d'une instruction sur un processeur à pipeline. Cette bulle est le plus souvent représentée par l'exécution d'une instruction NOP, qui n'a pas d'autre utilisé que d’introduire un délai dans le pipeline.  Portail de l’informatique
Un bus informatique est un dispositif de transmission de données partagé entre plusieurs composants d'un système numérique. Le terme dérive du latin omnibus (à tous) ; c'est le sens, d'un usage plus ancien, du terme bus en électronique. Le bus informatique est la réunion des parties matérielles et immatérielles qui permet la transmission de données entre les composants participants. On distingue généralement un bus d'une part d'une liaison point à point, qui ne concerne que deux composants qui en ont l'usage exclusif, et d'autre part, d'un réseau, qui implique des participants indépendants entre eux, c'est-à-dire pouvant fonctionner de manière autonome, et qui comprend plusieurs canaux permettant des communications simultanées.
Un calculateur quantique (anglais quantum computer parfois traduit ordinateur quantique, ou système informatique quantique), utilise les propriétés quantiques de la matière, telle que la superposition et l'intrication afin d'effectuer des opérations sur des données. À la différence d'un ordinateur classique basé sur des transistors qui travaille sur des données binaires (codées sur des bits, valant 0 ou 1), le calculateur quantique travaille sur des qubits dont l'état quantique peut posséder plusieurs valeurs. De petits calculateurs quantiques ont été construits à partir des années 1990. Jusqu'en 2008, la difficulté majeure concerne la réalisation physique de l'élément de base : le qubit. Le phénomène de décohérence (perte des effets quantiques en passant à l'échelle macroscopique) freine le développement des calculateurs quantiques. Le premier processeur quantique est créé en 2009 à l'université Yale : il comporte deux qubits composés chacun d'un milliard d'atomes d'aluminium posés sur un support supraconducteur. Ce domaine est soutenu financièrement par plusieurs organisations, entreprises ou gouvernements en raison de l'importance de l'enjeu : au moins un algorithme conçu pour utiliser un circuit quantique, l'algorithme de Shor, rendrait possible de nombreux calculs combinatoires hors de portée d'un ordinateur classique en l'état actuel des connaissances. La possibilité de casser les méthodes cryptographiques classiques est souvent mise en avant.
En informatique, la chorégraphie est une généralisation de l’approche par orchestration qui consiste à concevoir une coordination décentralisée des applications, dans laquelle il n’y a pas de machine privilégiée (serveur informatique) mais un réseau de machines interconnectées qui échangent des messages et effectuent des calculs. Cette approche soulève de nombreux problèmes, en particulier pour le traitement des erreurs et l’évolution dynamique. Mais elle sera certainement une technologie requise pour la généralisation et le passage à l’échelle des applications à base de services web. Il s’agit ici d’identifier les concepts, de proposer un formalisme, et de réaliser les moteurs, permettant l’exécution efficace et décentralisée d’applications utilisant des services web.
La compression de mémoire virtuelle (en anglais, virtual memory compression, RAM compression ou memory compression) est une technique de gestion de mémoire virtuelle qui utilise la compression de données pour réduire la taille ou le nombre de requêtes de pagination vers et depuis le stockage auxiliaire. Dans un système de compression de mémoire virtuelle, les pages sont compressées et stockées dans la mémoire physique, généralement une mémoire RAM, ou envoyées compressées vers un stockage auxiliaire, tel qu'un disque dur ou un disque SSD. Dans les deux cas, la plage de mémoire virtuelle dont le contenu a été compressé est inaccessible, de sorte que les tentatives d'accès aux pages compressées déclenchent des erreurs de page et inversent le processus de compression (récupération des informations et décompression). L'empreinte des données paginées est réduite par le processus de compression et la mémoire RAM libérée est renvoyée dans le pool de mémoire physique disponible. Dans le cas où les pages compressées sont conservées dans la mémoire RAM, les pages compressées utilisent évidemment moins d'espace que les pages originales. Dans le cas où les pages compressées sont conservées dans un stockage auxiliaire, la mémoire RAM est complètement libérée et les opérations d'écriture et de lecture sur la mémoire auxiliaire sont plus rapides que si les pages n'avaient pas été compressées,. Dans certaines implémentations, incluant zswap (en), zRam et Helix de Hurricane Software Company (en), l'ensemble du processus est implémenté au moyen de logiciel. Dans d'autres systèmes, tels que MXT d'IBM, le processus se fait dans un processeur dédié qui gère les transferts entre un cache local et la mémoire RAM.
DRBD (Distributed Replicated Block Device en anglais, ou périphérique en mode bloc répliqué et distribué en français) est une architecture de stockage distribuée pour GNU/Linux, permettant la réplication de périphériques de bloc (disques, partitions, volumes logiques etc.) entre des serveurs. DRBD est un logiciel libre, mais un support existe. DRBD est composé d'un module noyau, d'outils d'administration en espace utilisateur ainsi que de scripts shell. La réplication des données se fait : En temps réel. En permanence, pendant que les applications modifient les données présentes sur le périphérique. De façon transparente. Les applications qui stockent leurs données sur le périphérique répliqué n'ont pas conscience que ces données sont en fait stockées sur plusieurs ordinateurs. De façon synchrone, ou asynchrone. En fonctionnement synchrone, une application qui déclenche une écriture de donnée est notifiée de la fin de l'opération seulement après que l'écriture a été effectuée sur tous les serveurs, alors qu'en fonctionnement asynchrone, la notification se fait après que la donnée a été écrite localement, mais avant la propagation de la donnée. À ce jour, DRBD ne permet que la réplication entre deux nœuds, mais la feuille de route nous indique qu'il est prévu pour la version 9 de corriger cela.
L'échafaudage ou scaffolding en anglais est une manière de concevoir des logiciels liés à une base de données. Cette technique est souvent fournie avec le patron de conception Modèle-Vue-Contrôleur, dans lequel le développeur écrit une spécification décrivant comment la base de données sera utilisée. Le compilateur génère le code source de création, lecture, mise à jour et suppression (CRUD) des données en base pour l'application. Cet échafaudage est le point de départ d'une application plus puissante. L'échafaudage fut popularisé par le framework Ruby on Rails en 2005. Il a été adopté par d'autres tel que Monorail (.Net), Symfony (PHP), CodeIgniter, CakePHP, Yii Framework, Model-Glue, Grails, Gaia Flash Framework et AngularJS.
En informatique, certaines données telles que les nombres entiers peuvent être représentées sur plusieurs octets. L'ordre dans lequel ces octets sont organisés en mémoire ou dans une communication est appelé endianness (mot anglais traduit par « boutisme » ou par « endianisme »). De la même manière que certains langages humains s'écrivent de gauche à droite, et d'autres s'écrivent de droite à gauche, il existe une alternative majeure à l'organisation des octets représentant une donnée : l'orientation big-endian et l'orientation little-endian. Ces expressions sont parfois traduites par gros-boutiste et petit-boutiste ou par grand-boutien et petit-boutien. Les expressions byte order, ordre des octets, ou byte sex, sont également utilisées (bien qu’ordre des octets fasse référence à l'unité d’une base numérale précise sur 8 bits, que les autres termes plus généraux ne traduisent pas). L'endianness qualifie aussi bien un fichier ou un protocole (dans lesquels ce sont les octets qui sont ordonnés différemment) qu'un processeur (dans lequel la gestion des bits a aussi un ordre).
Enterprise Information Integration (EII) est une approche d'architecture (voire d'urbanisme) permettant d'obtenir une vue unifiée des données informatiques de l'entreprise. En effet, par essence, ces données sont hétérogènes et à échelle de temps différentes. En fonction des choix retenus, l'utilisateur aura la possibilité de : modifier les données (et non pas seulement un accès en lecture seule) ; agir en temps réel sur les données (et non pas en différé) ; accéder à des données structurées ; accéder à des données cohérentes ; accéder à des services ; remonter des informations jusque dans le modèle métier (objet) ; accéder au patrimoine Mainframe.
L'espace d'échange, en anglais swap, est une partie de la mémoire de masse d'un ordinateur utilisée par le système d'exploitation pour stocker des données qui, du point de vue des applications, se trouvent en mémoire vive. L'espace d'échange peut prendre la forme d'une partition dédiée (la partition d'échange, courante sous les systèmes Unix) ou d'un simple fichier (le fichier d'échange, C:\pagefile.sys sous Windows par exemple), ou de plusieurs partitions et/ou fichiers. La mémoire vive et l'espace d'échange constituent ensemble la mémoire virtuelle du système. Sur certains systèmes tels que GNU/Linux, la partition d'échange est aussi utilisée pour la mise en hibernation, ou veille sur disque, du système.
En informatique, l'utilisation d'un noyau, permet de distinguer deux types d'accès à la mémoire informatique : L'espace noyau ; L'espace utilisateur. L'espace noyau se définit par son opposition à l'espace utilisateur, et réciproquement. Sur des noyaux modernes, tout processus utilisateur (par opposition au noyau) croit manipuler une mémoire qui a les propriétés suivantes : la mémoire allouée commence systématiquement à l'adresse 0, la mémoire peut être indéfiniment étendue, la mémoire est privée (protégée), un processus ne peut pas accéder à la mémoire d'un autre processus (sauf allocations et autorisations spécifiques). Pour que les processus utilisateurs aient l'illusion que la mémoire qu'ils manipulent a effectivement ces propriétés, le noyau dispose d'un gestionnaire de mémoire. Ce gestionnaire de mémoire a : accès à l'ensemble de la mémoire disponible accès à la mémoire tant en lecture qu'en écriture (même si cette mémoire est utilisée) accès à l'ensemble des propriétés de cette mémoire (vitesse, mode d'accès) différencie les zones de mémoire (pages, RAM, mémoire virtuelle...). Ainsi, du point de vue du gestionnaire de mémoire, la mémoire n'a aucune des propriétés de l'espace utilisateur. La mémoire, utilisée sans ces propriétés, s'appelle l'espace noyau. L'ensemble des fonctions et processus s'exécutant dans le noyau travaillent sur de la mémoire de l'espace noyau. Par extension, ces fonctions et processus sont dits de l'espace noyau. Les processeurs modernes tels que les i386 disposent de fonctions spécialisées dans la gestion de la mémoire (MMU). Pour pouvoir activer/désactiver ces fonctions, les gestionnaires de mémoires s'appuient sur l'activation/désactivation, au niveau processeur du mode noyau.   Portail de l’informatique
Quand le processeur d'un système informatique possède au moins deux modes de fonctionnement, dont un mode dit superviseur ou mode noyau qui n'impose pas de restrictions sur les instructions exécutées, et un mode dit utilisateur qui limite ce que peuvent faire les instructions, et quand le système d'exploitation met en œuvre cette distinction en faisant fonctionner les autres programmes en mode utilisateur et en se réservant le mode superviseur, on dit que les programmes ainsi restreints font partie de l'espace utilisateur (en anglais, userspace). Cette partition entre espace utilisateur et espace noyau est l'élément de base du contrôle d'accès : les applications de l'espace utilisateur ne peuvent, par accident ou intentionnellement, accéder à une zone mémoire ne leur appartenant pas car une telle action déclenche immédiatement une trappe du noyau, qui doit envoyer un signal particulier au programme et, généralement, y mettre fin. Pour que ce mécanisme fonctionne, il faut que les processeurs disposent d'une unité de gestion mémoire (MMU) exploitable par le noyau. La trappe est en effet déclenchée par une interruption matérielle. Le mécanisme de protection mémoire ne peut être implémenté efficacement de façon logicielle. Le noyau lui-même opère sans restrictions, c'est pourquoi il doit être construit avec le plus grand soin.   Portail de l’informatique
Une architecture physique ou architecture technique est conçue de manière à répondre à des exigences. Ces exigences recouvrent de nombreuses notions : Exigences fonctionnelles Disponibilité / Fiabilité / plage d’ouverture Reprise de service en cas d'incident Sécurité (Disponibilité, Intégrité, Confidentialité, Traçabilité) Niveaux de performance Scalabilité (montée en charge) Conservation des données Modifiabilité (évolutivité) Utilisabilité (interaction avec les utilisateurs)
En architecture informatique, un système en style à filtres et tubes (anglais pipes and filters) est composé d'un réseau d'unités autonomes (les filtres) qui effectuent des transformations sur des informations transmises par un réseau de connexions (les tubes). Dans cette construction, qui imite celle d'une usine chimique, chaque filtre comporte des entrées et des sorties auxquelles sont branchés les tubes. Le filtre lit les informations en entrée, effectue une transformation, puis envoie le résultat en sortie. Le traitement est effectué petit à petit, et le filtre produit des résultats en sortie avant d'avoir consommé toutes les informations en entrée. Les filtres communiquent entre eux uniquement à travers les tubes, et aucun filtre ne connait l'identité du filtre qui est branché à l'autre bout du tube. La mise en œuvre la plus connue est celle du système d'exploitation Unix: L'interpréteur de commandes (Unix shell) comporte une notation qui permet de relier plusieurs commandes sous forme d'un réseau de filtres et de tubes. La construction à filtre et tubes est également utilisée pour les compilateurs, le outils de traitement numérique du signal, et de calcul parallèle.
Un fond de panier (en anglais backplane) est une carte ou un bâti sur laquelle plusieurs connecteurs sont disponibles pour la connexion d'autres cartes à un appareil. Ces connecteurs peuvent être reliés à des bus d'alimentation, de contrôle ou de communication. En informatique, les emplacements de cartes avec leurs connecteurs permettant d'intégrer un ensemble d'éléments périphériques s'appelle couramment slots.
Un goulot d'étranglement est un point d'un système limitant les performances globales, et pouvant avoir un effet sur les temps de traitement et de réponse. Les goulots d'étranglement peuvent être matériels et/ou logiciels. Dans le premier cas, c'est le sous dimensionnement d'un élément de l'architecture physique qui a un effet sur le fonctionnement des autres éléments. L'élément limitant d'une architecture est toujours l'élément le plus faible. Par exemple, un goulot d'étranglement peut être créé par un routeur fonctionnant en 100 Mb/s dans un réseau Gigabit. Dans le deuxième cas, c'est le manque d'efficacité d'un module logiciel qui a un effet sur les performances globales du logiciel. Un goulot d'étranglement peut être créé par l'enregistrement non bufferisé (non-utilisation de l'écriture bufferisée) de résultats à l'intérieur d'une boucle de calcul. L'enregistrement étant une opération très lente, le processeur passe alors son temps à attendre que les résultats soient écrits au lieu d'effectuer les calculs. L'effet de goulot d'étranglement d'un réseau informatique est formellement défini par la constante de Cheeger du réseau, en théorie spectrale des graphes.
Les hypercubes, ou n-cubes, forment une famille de graphes. Dans un hypercube                                    Q                        n                                     {\displaystyle Q_{n}}   , chaque sommet porte une étiquette de longueur                         n                 {\displaystyle n}    sur un alphabet                         A         =         {         0         ,         1         }                 {\displaystyle A=\{0,1\}}   , et deux sommets sont adjacents si leurs étiquettes ne diffèrent que d'un symbole. C'est le graphe squelette de l'hypercube, un polytope n-dimensionnel, généralisant la notion de carré (n = 2) et de cube (n = 3). Dans les années 1980, des ordinateurs furent réalisés avec plusieurs processeurs connectés selon un hypercube : chaque processeur traite une partie des données et ainsi les données sont traitées par plusieurs processeurs à la fois, ce qui constitue un calcul parallèle. L'hypercube est couramment introduit pour illustrer des algorithmes parallèles, et de nombreuses variantes ont été proposées, soit pour des cas pratiques liés à la construction de machines parallèles, soit comme objets théoriques.
En architecture d'ordinateur, instructions par cycle d'horloge (instruction par cycle ou IPC) est un terme utilisé pour décrire un aspect de la performance d'un microprocesseur: le nombre moyen d'instructions exécutées pour chaque cycle du signal d'horloge. À ne pas confondre avec le nombre de cycles par instruction.
INT 10H ou INT 16 est une interruption logicielle fournie par le BIOS, le 17e vecteur d'interruption d'un ordinateur à base d'un processeur x86, utilisée pour la gestion de l'affichage et des modes vidéo. Pour appeler une fonction de cette interruption, il faut mettre le numéro de la fonction désirée dans le registre AH, éventuellement charger les arguments dans les autres registres et invoquer l'interruption. Bien que cette routine soit utile, voire indispensable, pour certains réglages peu fréquents (par exemple, changer de mode vidéo), elle est relativement lente, et la plupart des programmes préfèrent accéder à la mémoire vidéo directement plutôt que d'appeler l'interruption pour chaque pixel à afficher.
Un intergiciel pour étiquettes électroniques est un logiciel tiers destiné à simplifier l'accès et l'exploitation des informations stockées dans des étiquettes RFID issues de l'industrie électronique. Les étiquettes électroniques sont le plus souvent utilisées pour des besoins de marquage et de traçabilité d'objets (suivi de palettes, de livres, de chaînes de montage de voitures, sous la forme d'étiquettes électroniques de gondole dans les grandes surfaces...) aussi bien que d'êtres vivants (élevage, suivi d'animaux sauvages, ou encore suivi de la chaine du froid). La conception et le développement de nouveaux logiciels pour ces applications et pour bien d'autres (monétique, authentification...) s'avère néanmoins souvent délicat. L'intégration, au sein des systèmes d'information, des informations stockée dans la mémoire des étiquettes électroniques est entravé par des difficultés techniques liées à l'organisation de leurs collecte mais aussi de leur filtrage. C'est à cette fin que sont conçus les intergiciels pour étiquettes électroniques. Ils ont pour ambitions de : gérer un parc hétérogène de lecteurs ; traiter les données issues des lecteurs (formatage, agrégation, filtrage) ; collecter des données lues à distance et de Notifier les événements significatifs survenus dans l'infrastructure (apparition de nouvelles étiquettes, retraits de lecteurs...). Il existe aujourd'hui différents intergiciels assumant tout ou partie de ces fonctions, dont certains sont des logiciels libres.
Le maître-esclave est un modèle utilisé en technologie, notamment en informatique.
La mémoire d'un système informatique multiprocesseur est dite distribuée lorsque la mémoire est répartie en plusieurs nœuds, chaque portion n'étant accessible qu'à certains processeurs. Un réseau de communication relie les différents nœuds, et l'échange de données doit se faire explicitement par « passage de messages ». La mémoire est organisée de cette manière par exemple lorsque l'on utilise des machines indépendantes pour former une grille. Dans de nombreux cas, la mémoire est physiquement distribuée mais cela est caché au programmeur : on dit que l'espace mémoire est partagé. Toute la mémoire est accessible depuis n'importe lequel des processeurs, même si les temps d'accès varient selon le nœud où se trouve la donnée. On parle alors d'architecture NUMA.
Dans un contexte de la programmation concurrente, le partage de mémoire est un moyen de partager des données entre différents processus : une même zone de la mémoire vive est accédée par plusieurs processus. C'est le comportement de la mémoire de threads issus d'un même processus. Pour cela, dans un système utilisant la pagination, la table de page de chaque processus contient les pages mémoires communes, mais chaque processus ne les voit pas nécessairement à la même adresse.
En informatique, le mécanisme de mémoire virtuelle a été mis au point dans les années 1960. Il repose sur l'utilisation de traduction à la volée des adresses (virtuelles) vues du logiciel, en adresses physiques de mémoire vive. La mémoire virtuelle permet : d'utiliser de la mémoire de masse comme extension de la mémoire vive ; d'augmenter le taux de multiprogrammation ; de mettre en place des mécanismes de protection de la mémoire ; de partager la mémoire entre processus.
MMIX, prononcé /ɛm.mɪks/ et usuellement typographié dans une police à chasse fixe (MMIX), est à la fois un jeu d'instructions 64-bit RISC et une architecture informatique conçus par Donald Knuth, avec une aide importante de John LeRoy Hennessy (en), un des concepteurs de l'architecture MIPS, et de Richard L. Sites, un des concepteurs de l’architecture Alpha. Knuth lui-même présente ce projet en ces mots :  « MMIX est un ordinateur destiné à illustrer les aspects de la programmation au niveau machine. Dans mes livres The Art of Computer Programming, il remplace MIX, une machine répliquant le style des années 1960, laquelle jouait un tel rôle... J'ai tenté de concevoir MMIX de façon que son langage machine soit simple, élégant et facile à apprendre. En parallèle, j'ai fait attention d'inclure toutes les complexités nécessaires pour obtenir une haute performance dans la pratique, de sorte que MMIX pourrait en principe être construit et peut-être faire concurrence à certains des langages généraux les plus rapides disponibles commercialement, »
Multimodal Architecture and Interfaces (« architecture multimodale et interfaces » en anglais) ou MMI-Arch est un standard ouvert en développement par le W3C depuis 2005. Depuis 2010 est une recommandation d'architecture à composants conteneurs du W3C. Le document est le rapport technique de spécification d'une architecture informatique générique et de ses interfaces pour faciliter l’intégration et la gestion des interactions multimodales dans un environnement informatique. Il a été produit par le groupe de travail en Interaction multimodale du W3C.
On parle de multitraitement asymétrique (en anglais, asymmetric multiprocessing ou AMP ou ASMP) dans le cas d'une architecture multiprocesseur où tous les processeurs ne sont pas traités de la même façon par le système d'exploitation. Par exemple, certains périphériques ou certains processus du système d'exploitation peuvent être attachés exclusivement à un processeur particulier.
En informatique, un système NUMA (pour non uniform memory access ou non uniform memory architecture, signifiant respectivement accès mémoire non uniforme et architecture mémoire non uniforme) est un système multiprocesseur dans lequel les zones mémoire sont séparées et placées en différents endroits (et sur différents bus). Vis-à-vis de chaque processeur, les temps d'accès diffèrent donc suivant la zone mémoire accédée. Le système NUMA est conçu pour pallier les limites de l'architecture SMP (symmetric multiprocessing) dans laquelle tout l'espace mémoire est certes accessible par un unique bus, mais rend de ce fait inefficaces, par encombrement, les accès concurrents par les différents processeurs. Une architecture plus adaptée devient donc nécessaire pour les systèmes ayant de nombreux processeurs. NUMA représente une position médiane entre le SMP et le clustering (diverses machines). ccNUMA est acronyme de « cache coherent NUMA ». Par abus de langage et d'usage, les deux sont devenus synonymes.
L'orchestration décrit le processus automatique d'organisation, de coordination, et de gestion de systèmes informatiques complexes, de middleware et de services. On en parle souvent comme ayant une intelligence inhérente ou même un contrôle implicitement autonome, mais il s'agit plutôt d'aspirations ou d'analogies plutôt que de descriptions techniques. En réalité, l'orchestration est largement le résultat de l'automatisation de systèmes qui déploient des éléments de régulation. On parle souvent de cet usage de l'orchestration dans le contexte de la virtualisation, de la fourniture de services et des centres de données dynamiques. C'est un mot à la mode. Un usage quelque peu différent désigne le processus de coordination d'un échange d'information à travers l'interaction de services web.
L'ordinateur à ADN est une des voies non électroniques actuellement explorées pour résoudre des problèmes combinatoires. Il ne prétend pas à la généralité et à la flexibilité d'un ordinateur général. Il s'agit plutôt d'un dispositif spécialisé comme peut l'être un processeur graphique, une carte son ou un convolveur. Son principe, énoncé par Leonard Adleman en 1994, « consiste à coder une instance du problème avec des brins d'ADN et à les manipuler par les outils classiques de la biologie moléculaire pour simuler les opérations qui isoleront la solution du problème, si celle-ci existe. »
Pour l'ordinateur du futur, l'architecture actuelle[Quand ?] des ordinateurs n'est pas la seule envisageable. D'autres voies ont déjà été explorées, avec des succès divers. Il se peut que certains des ordinateurs suivants remplacent ou complètent un jour les ordinateurs actuels : ordinateur quantique ; ordinateur à ADN ; ordinateur optique ; processeur autosynchrone ; processeur asynchrone ; ordinateur neuronal.  Portail de l’informatique
Un ordinateur neuronal est un ordinateur dans lequel le processeur est constitué d'un ensemble de neurones biologiques ou imitant son fonctionnement. D'ores et déjà, des circuits basés sur des neurones de sangsue ont réalisé des additions et une impulsion nerveuse circulant entre deux neurones d'escargot a transité par une puce électronique. À ne pas confondre avec un ordinateur à réseau de neurones, qui est un ordinateur doté d'un processeur électronique qui imite le fonctionnement du réseau des neurones du cerveau.
Un ordinateur optique (ou ordinateur photonique) est un ordinateur numérique qui utilise des photons pour le traitement des informations, alors que les ordinateurs conventionnels utilisent des électrons. Les photons ont la particularité de ne pas créer d’interférence magnétique, de ne pas générer de chaleur et de se propager très rapidement. Les transistors optiques sont beaucoup plus rapides que les transistors électroniques. Des ordinateurs optiques pourraient être plus puissants que les ordinateurs conventionnels actuels.
L'architecture de Kane pour la réalisation de l'ordinateur quantique est une proposition théorique suggérant l'utilisation des spins nucléaires d'atomes donneurs uniques de 31P dans un cristal de silicium isotopiquement pur (28Si) comme composants de base (qubits).
Le modèle de passage de messages (message passing en anglais) est un modèle de communication entre ordinateurs ou entre processus à l'intérieur d'un même ordinateur. Il réalise l’envoi de messages simples. Il constitue la couche de base des Middleware Orientés Messages.
En informatique, un patron d'architecture est un patron de conception, c'est-à-dire un modèle de référence qui sert de source d'inspiration lors de la conception de l'architecture d'un système ou d'un logiciel informatique en sous-éléments plus simples.
En microarchitecture, un pipeline (ou chaîne de traitement), est l'élément d'un processeur dans lequel l'exécution des instructions est découpée en plusieurs étapes. Le premier ordinateur à utiliser cette technique est l'IBM Stretch, conçu en 1961. Avec un pipeline, le processeur peut commencer à exécuter une nouvelle instruction sans attendre que la précédente soit terminée. Chacune des étapes d’un pipeline est appelé étage. Le nombre d'étages d'un pipeline est appelé sa profondeur.
Un processeur neuronal est un processeur équipé d'un système de réseau interne à base de neurones, comme dans un cerveau. À ne pas confondre avec : Un processeur à réseau de neurones, qui est un processeur électronique qui imite le fonctionnement du réseau des neurones du cerveau.
Un protocole informatique (ou parfois tout simplement un protocole quand le contexte de l'informatique est clair) est un ensemble de règles qui régissent les échanges de données ou le comportement collectif de processus ou d'ordinateurs en réseaux ou d'objets connectés. Un protocole a pour but de réaliser une ou plusieurs tâches concourant à un fonctionnement harmonieux d'une entité générale.
La redondance appliquée aux centres de données signifie que les services de base et les systèmes auront des doublons (équipement, liaisons, alimentation et chemins, données, logiciels…) afin de garantir les fonctionnalités dans l’éventualité où l’un de ces composants s’avérerait défaillant. Aujourd’hui, la « haute disponibilité » est pour l’essentiel obtenue par la redondance au niveau de tout l’écosysteme illustrée dans la figure. L’organisation « Uptime Institute » classe les centres de données en quatre niveaux : TIER I, II, III et IV. Ces niveaux correspondent à un certain nombre de garanties sur le type de matériel déployé dans le centre de données en vue d’assurer sa redondance.
En informatique, la réplication est un processus de partage d'informations pour assurer la cohérence de données entre plusieurs sources de données redondantes, pour améliorer la fiabilité, la tolérance aux pannes, ou la disponibilité. On parle de réplication de données si les mêmes données sont dupliquées sur plusieurs périphériques. La réplication n'est pas à confondre avec une sauvegarde : les données sauvegardées ne changent pas dans le temps, reflétant un état fixe des données, tandis que les données répliquées évoluent sans cesse à mesure que les données sources changent.
La réplication multi-maîtres est une architecture pour la réplication des bases de données permettant aux données d'être stockées sur un groupe d'ordinateurs et mises à jour par n'importe quel membre du groupe. Tous les membres peuvent répondre aux requêtes des clients. Le système de réplication multi-maître est responsable de propager les modifications de données faite par chaque membre et résoudre les conflits provoqués par des modifications concurrentes faites sur des membres différents. La réplication multi-maîtres peut être comparée avec la réplication maître-esclave où un unique membre du groupe est désigné comme le maître pour certaines données et est le seul nœud autorisé à modifier ces données. Les autres membres désirant modifier les données doivent d'abord contacter le nœud maître. Autoriser un unique maître permet d'obtenir plus facilement la cohérence des données entre les différents membres du groupe, mais est moins flexible que la réplication multi-maîtres. La réplication multi-maîtres peut aussi être comparée avec le basculement dans les grappe de serveurs où les serveurs esclaves répliquent les données maître pour préparer la bascule dans le cas où le maître ne fonctionnerait plus. Le maître est le seul serveur actif pour les interactions avec le client. Le but premier de la réplication multi-maîtres est une disponibilité améliorée et des temps de réponse améliorés.
Network-on-Chip ou Network-on-a-Chip (NoC or NOC) ou en français réseau sur une puce est une technique de conception du système de communication entre les cœurs sur les System on Chip (SoC). Les NoCs peuvent passer dans les domaines d'horloge synchrone ou asynchrone ou bien utiliser une logique de circuit asynchrone sans horloge. Le NoC applique les théories et méthodes de réseau aux communications à l'intérieur d'une puce et permet ainsi l'amélioration des performances par rapport aux interconnexions de bus et commutateur matriciel conventionnelles. Le NoC améliore la scalabilité des SoCs et l'efficacité énergétique des SoCs complexes relativement aux autres conceptions. Des recherches ont été menées sur des guides d'ondes lumineuse intégrées et des périphériques comprenant les Optical Network-on-Chip (ONoC, reseau optique sur une puce),.
La suprématie quantique désigne le nombre de qbits au-delà duquel aucun superordinateur classique n'est capable de gérer la croissance exponentielle de la mémoire et la bande passante de communication nécessaire pour simuler son équivalent quantique. Les superordinateurs de 2017 peuvent reproduire les résultats d'un ordinateur quantique de 5 à 20 qubits, mais à partir de 50 qubits cela devient physiquement impossible. Le seuil d'environ 50 qubits correspond à la limite de la suprématie quantique. D’après Harmut Neven, responsable des recherches en calcul quantique chez Google, son équipe est sur le point de construire un système de 49 qubits d’ici la fin de l’année 2017. En novembre 2017, IBM réussit à faire fonctionner un calculateur quantique de 50 qbits pendant 90 microsecondes atteignant donc le seuil théorique de la suprématie quantique. Cependant, une équipe de recherche d'IBM a montré en octobre 2017 que ce seuil n'était pas aussi fixe que pensé dans un premier temps et qu'il est possible de modéliser le comportement d'un ordinateur quantique avec des ordinateurs conventionnels au-delà de 49 qbits grâce à des techniques mathématiques,. Pour certains chercheurs notamment chez IBM, le nombre de qubits seul ne permet pas de capturer la complexité inhérente aux calculateurs quantiques et ils suggèrent que la puissance d'un calcul quantique sur un appareil donné soit exprimée par un nombre appelé « volume quantique », qui regrouperait tous les facteurs pertinents, à savoir, le nombre et la connectivité des qubits, la profondeur de l'algorithme utilisé ainsi que d'autres mesures telles que la qualité des portes logiques, notamment le bruit du signal. L'utilisation du mot « suprématie » est également mise en cause en raison de ces connotations raciales et politiques.
Un multiprocesseur symétrique (à mémoire partagée), ou symmetric shared memory multiprocessor (SMP), est une architecture parallèle qui consiste à multiplier les processeurs identiques au sein d'un ordinateur, de manière à augmenter la puissance de calcul, tout en conservant une unique mémoire. Disposer de plusieurs processeurs permet d'exécuter simultanément plusieurs processus du système, utilisateur ou noyau en leur allouant l'un ou l'autre des processeurs disponibles, ce qui augmente la fluidité lors de l'exécution de plusieurs programmes, et permet à un processus d'utiliser plus de ressources de calcul en créant plusieurs threads. Ce principe crée néanmoins un goulot d'étranglement au niveau de la mémoire principale, qui est partagée par tous les processeurs. C'est pour cette raison que des systèmes à mémoire répartie partagée et à mémoire distribuée sont apparus.
Systancia est un éditeur français de logiciels de virtualisation des postes de travail et des applications, et de sécurité des accès externes. La société a été créée en 1998, et dispose de trois centres de recherche et d'innovation basés à Sausheim en Alsace, où se trouve le siège social, à Rennes et à Paris. Elle dispose de la première solution européenne sur le marché de la virtualisation et du cloud computing. Elle développe et commercialise, en France et à l'international via un réseau de partenaires, des logiciels permettant de virtualiser les applications et les postes de travail. En 2012, Systancia a annoncé une levée de fonds de 4 M€, réalisée par la Caisse des dépôts et consignations (aujourd’hui BPI), gestionnaire du fonds FSN PME et par A Plus Finance. À la fin de 2013, Systancia acquiert la société française IPdiva, éditeur dans la sécurisation, le contrôle et la traçabilité d’accès en mode applicatif VPN SSL. Fin 2016, Systancia acquiert la société française AVENCIS.
Le System Management Bus (en abrégé SMBus ou SMB) est un bus de communication développé en 1995 par Intel sur la base du bus I²C avec lequel il est partiellement compatible. Il est principalement utilisé pour la communication à faible bande passante entre périphériques de cartes mères d’ordinateurs, particulièrement les périphériques liés à la gestion de l’énergie, comme les alimentations et les systèmes de recharge de batteries, mais également avec les ventilateurs, les capteurs de température, les capteurs de tension électrique, les contacts de détection de fermeture de l’écran des ordinateurs portables et les horloges intégrées. Un périphérique SMB peut fournir des informations sur son fabricant, son modèle, son numéro de série, enregistrer son état avant sa mise en veille, annoncer différents types d’erreurs et diagnostics et peut accepter des paramètres de contrôle. Le bus SMB n’est pas prévu pour être contrôlé directement par l’utilisateur final. Les périphériques ne peuvent pas identifier leurs fonctionnalités, mais une nouvelle coalition PMBus a étendu la spécification de base pour permettre cela.
Un système déterministe est un système qui réagit toujours de la même façon à un événement, c'est-à-dire que, quoi qu'il se soit passé auparavant, à partir du moment où le système arrive dans un état donné, son évolution sera toujours identique. En automatique, les systèmes déterministes forment une classe de systèmes qui, à une suite d'événements entrants, produisent une suite d'événements sortants, toujours la même et selon un ordre déterminé par l’ordre des événements entrants. En philosophie, le déterminisme est une vision du monde dans laquelle l'univers est conçu comme un système déterministe.
En informatique, un système multi-agents (SMA) est un système composé d'un ensemble d'agents (un processus, un robot, un être humain, etc.), situés dans un certain environnement et interagissant selon certaines relations. Un agent est une entité caractérisée par le fait qu'elle est, au moins partiellement, autonome. Objet de longue date[Quand ?] de recherches en intelligence artificielle distribuée, les systèmes multi-agents forment un type intéressant de modélisation de sociétés, et ont à ce titre des champs d'application larges, allant jusqu'aux sciences humaines.
En informatique, on parle d'un système temps réel lorsque ce système est capable de contrôler (ou piloter) un procédé physique à une vitesse adaptée à l'évolution du procédé contrôlé. Les systèmes informatiques temps réel se différencient des autres systèmes informatiques par la prise en compte de contraintes temporelles dont le respect est aussi important que l'exactitude du résultat, autrement dit le système ne doit pas simplement délivrer des résultats exacts, il doit les délivrer dans des délais imposés.
La taxonomie de Flynn est une classification des architectures d'ordinateur, proposée par Michael Flynn (en) en 1966,. Les quatre catégories définies par Flynn sont classées selon le type d'organisation du flux de données et du flux d'instructions.
En informatique, le thrashing (littéralement « emballement ») est l'état d'un ordinateur avec mémoire virtuelle caractérisé par une fréquence des échanges de pages si élevée entre la mémoire RAM et le disque dur que ses performances s'en trouvent considérablement affectées et que très peu de ressources sont disponibles pour exécuter les applications des utilisateurs de l'ordinateur. Cette situation peut causer une dégradation très importante de la performance de l'ordinateur. Elle peut ne pas se résoudre d'elle-même et peut nécessiter un ajustement de l'ordinateur ou des programmes impliqués.
Le UML Profile for DoDAF/MODAF (UPDM) est un standard industriel UML de représentation des produits d'architecture DoDAF et des vues MODAF à travers le processus de standardisation de l'Object Management Group (OMG)  basé sur une approche d'architecture dirigée par les modèles (Model-driven architecture, MDA) pour les spécifications. Se référer au site UPDM pour plus d'information.
Viiv (prononcer "vaïve") est un terme marketing de la société Intel, recouvrant une architecture matérielle pour PC, soumise à certification. En d'autres termes, Intel a émis une spécification et les constructeurs souhaitant s'y conformer doivent faire certifier leurs ordinateurs. Viiv désigne donc une architecture analogue à l'architecture pour ordinateur portable Centrino, mais à l'usage des ordinateurs de bureau.
XPCOM (Cross-Platform Component Object Model) est un modèle libre de composants développé par la Fondation Mozilla. Il s'agit d'une bibliothèque logicielle qui, schématiquement, permet de concevoir une application comme un ensemble de plugins. Ces composants peuvent être développés dans des langages distincts, auquel cas les communications entre ces composants sont assurées par la technologie XPCONNECT. Cette bibliothèque a servi, notamment, à développer Mozilla (Mozilla Firefox, Mozilla Thunderbird, Mozilla Composer), Nvu, Komodo… XPCOM est inspiré des technologies CORBA ou Microsoft COM, qui elles-mêmes apportent à des langages statiques une partie des bénéfices (et inconvénients) de langages dynamiques comme Objective-C ou Python. Comme CORBA et COM, XPCOM permet de développer des architectures de composants. L'architecture est alors : dynamique : les composants sont chargés lors de l'exécution, de nouveaux composants peuvent être ajoutés lors de l'exécution, sans avoir à recompiler ou recharger les composants existants.  typée semi-dynamiquement : certaines informations sur un composant sont connues statiquement, un composant peut être interrogé afin d'apprendre s'il dispose de certaines capacités, qui pourront être utilisées si le composant répond par l'affirmative.  « polyglotte » : le langage de base de XPCOM est C++, les langages JavaScript, Python, Ruby, Perl peuvent utiliser et définir des composants XPCom, le projet JavaXPCOM vise à permettre de faire de même en Java (BlackConnect avait le même objectif), Monoconnect, une plate-forme d'interaction entre XPCom et .Net est actuellement en cours de développement.  multiplate-forme : un composant C++/XPCOM rédigé selon les consignes de Mozilla pourra être recompilé et réutilisé tel quel sur toute plate-forme Mozilla, les composants JavaScript peuvent être réutilisés immédiatement sur toute plate-forme Mozilla, tout comme les composants non compilés Python, Ruby, Perl.  De plus, les composants XPCOM ont accès aux bibliothèques de Mozilla, ce qui permet notamment d'accéder au moteur de rendu Gecko, le cœur de l'affichage des pages web par Mozilla et au moteur de réseau Necko, ainsi que de développer des interfaces graphiques en XUL. Une initiative de Mozilla, XULRunner, vise à séparer les bibliothèques XPCOM de Mozilla des logiciels construits à l'aide de ces bibliothèques. L'objectif est de faire de Mozilla une plate-forme de développement multiplate-forme comparable à la Java ou à l'environnement .Net.
L'Agence de l'Informatique (ADI) était un producteur et éditeur, de 1979 à 1988, situé à La Défense. Elle produisait des documentaires, éditait des revues, sur l'état de l'informatisation en France, et organisait des colloques.
En France, rattachée au ministère de l'Économie et des Finances, l'Agence pour l'informatique financière de l'État (AIFE) a été créée pour définir et mettre en œuvre la stratégie informatique financière de l'État (décret du 11 février 2005 amendé par le décret du 7 mai 2014). L'AIFE est un service à compétence nationale (SCN) dont la gouvernance est interministérielle. L'AIFE gère le Système d’Information Chorus et propose des solutions de dématérialisation au profit de l’ensemble des personnes publiques et des entreprises. Les missions de l'AIFE sont : promouvoir et contribuer à la transformation de la fonction financière Maintenir en condition opérationnelle le système d'information Chorus de gestion financière de l’État et de dépôt des factures électroniques de la sphère publique Maintenir en condition opérationnelle PLACE, la plateforme des achats de l’État Piloter de nouveaux projets interministériels ou ministériels et leur intégration dans le système d'information Chorus Accompagner le changement auprès des utilisateurs Exercer l’ensemble de ses missions pour le compte de l'Etat et plus généralement pour l’ensemble de la sphère publique
Ars Mathematica est une association loi de 1901 qui a été fondée en 1992 par Christian Lavigne et Alexandre Vitkine, pour favoriser la rencontre de l'art, de la science et de la technique en général, et pour promouvoir en particulier la recherche en matière d'objets numériques dans le contexte des arts électroniques, pour développer les domaines de la 3D et de la sculpture par ordinateur. L'association s'est donnée deux objectifs principaux : créer un centre de recherche pluridisciplinaire à vocation européenne et dédié aux objets numériques: le CREATRON (Centre de Ressources Européen de l'Art de la Technologie et de la Recherche des Objets Numériques) qui intéresse à la fois les artistes, les architectes, les designers, les chercheurs, les ingénieurs, les industriels de l'informatique, du prototypage rapide, de la chimie… ; organiser des rencontres, des colloques, des expositions, au premier rang desquels la biennale mondiale de sculpture numérique, lancée en 1993 par C. Lavigne et A. Vitkine, et qui a pris le nom d'INTERSCULPT, en 1995, cet événement étant alors devenu interactif et simultané entre la France et les États-Unis, avec démonstrations, visioconférences, et transmissions de fichiers par Internet. À cette occasion fut d'ailleurs réalisée la première télésculpture mondiale. INTERSCULPT a été relayé par The Computer and Sculpture Forum (USA), Fast-UK (GB), The PRISM Lab at ASU (USA)… En France, la manifestation (exposition et colloque) a été présentée à Paris (Galerie Graphes, mairie du 6e arrondissement de Paris, Sénat), et depuis 2005 en Lorraine (Nancy, Metz), toujours en liaison avec d’autres lieux (GB, USA, HK ou SG, NZ ou AU).
Créée en 1978, l'association ADELI est une association loi de 1901 de droit français à but non lucratif dont l'objectif est de faciliter la mise en œuvre des systèmes d'information, des organisations et des outils numériques.
La fédération AtlanSTIC est une fédération de recherche en informatique et cybernétique. Ses quatre axes de recherche majeurs sont : Décision ; Logiciel ; Interaction ; Robotique. Elle a été créée en 2006 par le CNRS, l'Université de Nantes, École centrale de Nantes, École des Mines de Nantes, l'Université d'Angers. Les laboratoires fondateurs de la fédération sont : le LINA (UMR6241) et l'IRCCyN (UMR6597). Riche de cette expérience fructueuse en Loire Atlantique, AtlanSTIC acquiert en 2012 un taille régionale par l'intégration de 2 laboratoires angevins : le LISA et le LERIA. À la création des instituts du CNRS, INS2I (Institut des Sciences de l'information et de leurs Interactions) et INSIS, (Institut des Sciences de l'ingénierie et des Systèmes), la fédération a choisi de demander le rattachement principal d’AtlanSTIC à l’INS2I et son rattachement secondaire à l’INSIS.
La fondation Bantuhub est une organisation à but non lucratif œuvrant pour la promotion des TIC et de l’entrepreneuriat juvénile en République du Congo.
Créé le 1er juillet 2013 à partir du Centre de Pilotage des Systèmes d'Information de l'Armée de Terre (CPSIAT), qui effectuait des missions similaires au profit de l’armée de Terre française jusqu'à cette date, le Centre d'Appui aux Systèmes d'Information de la Défense (CASID) est né du constat que la gouvernance stratégique du SI du ministère, confiée à la Direction générale des Systèmes d'information et de communication (DGSIC), devait disposer d'une expertise technique propre afin d'objectiver ses orientations pour chacun des quatre niveaux d'architecture (métier, fonctionnelle, applicative, technique). De ce fait, les missions du centre sont autant orientées vers la gouvernance que vers les projets, les Centres de Développement des Applications de la Défense (CDAD) et les centres de développement propres à chaque armée, direction ou service. Pour la gouvernance, il fournit des avis techniques. Pour les projets et les centres, il apporte un appui et définit le cadre technique indispensable à leurs travaux. Son périmètre est limité aux Système d’Information de Fonctionnement (SIF). Rattaché organiquement à la DIRISI-IdF comme les autres centres nationaux informatiques d’Ile-de-France, le CASID, organisme interarmées, dépend fonctionnellement de la DGSIC.
Au sein du ministère de la défense en France, l’armée de terre a organisé sa chaîne informatique d’administration et de gestion en trois niveaux : la gouvernance, sous la responsabilité du Bureau Programmes et Systèmes d’Information et de Communication de l’État-Major de l’Armée de Terre (EMAT/BPSIC) ; la direction technique, assurée par le Centre de Pilotage des Systèmes d'Information de l'Armée de Terre (CPSIAT); les centres de réalisation, chargés de la conduite opérationnelle des projets informatiques, du développement et de la maintenance des applications. Le Centre de Pilotage des Systèmes d'Information de l'Armée de Terre (CPSIAT), fondé [Quand ?], a été dissous par l'armée de Terre le 30 juin 2013 . Son personnel a rejoint à titre individuel le Centre d'Appui aux Systèmes d'Information de la Défense (CASID) au 1er juillet 2013.
Le Centre de recherche en informatique de Montréal (CRIM) est un centre de recherche créé en 1985 par un groupe d'entreprises et d'universités. En plus de faire de la recherche appliquée, sa mission principale est le transfert technologique des connaissances en vue de la commercialisation.
La création en 1990 du Centre de recherche informatique de Corée s'inscrit dans le cadre de la nécessaire modernisation de l'économie nord-coréenne. Il comporte huit unités de développement et onze bureaux régionaux, tant aux étapes de production que de formation. La Corée du Nord entend aussi favoriser le développement des logiciels libres, en privilégiant le développement d'applications basées sur Linux.  Portail de l’informatique  Portail de la Corée du Nord
Le Centre Informatique Provence Alpes Méditerranée (CIPAM) est un centre informatique des URSSAF qui dépend de l'ACOSS. L'ensemble du CIPAM est certifié ISO 9001 version 2000.
Le Cigref, Association loi 1901 créée en 1970, est un réseau de grandes entreprises et administrations publiques françaises qui se donnent pour mission de réussir le numérique. Il n’exerce aucune activité lucrative. Le Cigref regroupe à ce jour près de 150 grandes entreprises et organismes français de tous les secteurs d'activité (banque, assurance, énergie, distribution, industrie, services…). Il représente près de 85% du CAC 40, et 1/4 de ses membres sont des administrations publiques (Ministère de l'Intérieur, Ministère des Armées, Caisse des Dépôts, etc.) Les membres du Cigref partagent la même ambition, définie lors de son dernier plan stratégique adopté en octobre 2015 de développer leur capacité à intégrer et maîtriser le numérique. Cette mission, ainsi que le projet stratégique Cigref 2020, vise à accroître et renforcer durablement la performance des grandes entreprises et administrations, leur potentiel d'innovation, et plus largement contribuer au développement économique, social et humain de la société. Le Cigref est présidé depuis le 19 octobre 2016 par Bernard Duverneuil, directeur du digital et des systèmes d'information d'Elior Group. Henri d'Agrain en est le délégué général depuis janvier 2017. Sa gouvernance est assurée par 15 Administrateurs, élus en Assemblée générale. Son activité est animée par une équipe de 10 permanents.
La Commission spécialisée de terminologie et de néologie de l'informatique et des composants électroniques (CSTIC) est, en France, une Commission spécialisée de terminologie et de néologie qui intervient dans le domaine de la terminologie informatique. Elle fait partie du dispositif gouvernemental coordonné par la Commission générale de terminologie et de néologie, mis en place par le décret du 3 juillet 1996 relatif à l'enrichissement de la langue française. Elle est rattachée au Ministère de l'Économie, de l'Industrie et de l'Emploi. Elle a été créée en 1997.
Le Computer Pioneer Award, terme que l'on peut traduire par « Prix des pionniers en informatique », est un prix créé en 1981 par le conseil des gouverneurs de l'association IEEE Computer Society. Il a pour objectif de reconnaître et d'honorer la vision des personnes dont les efforts ont permis la création et assuré la vitalité de l'industrie informatique. Le prix est décerné à des personnes aux mérites exceptionnels, à condition que la contribution principale aux concepts et au développement du domaine de l'informatique ait été faite au moins quinze ans plus tôt. La reconnaissance est exprimée par une médaille d'argent frappée spécialement pour la Société. Tous les membres de la profession sont habilités à désigner un collègue qui leur semble le à même d'être considéré comme un candidat pour ce prix. La date limite pour la nomination est le 15 octobre de chaque année.
Le Consortium X du MIT était un organisme à but non lucratif créé en janvier 1988 par l'institut de technologie du Massachusetts pour piloter le développement de l'environnement graphique X, une technologie initiée en 1984 dans le cadre du projet Athena. La structure devait préserver l'équilibre entre intérêts commerciaux et pédagogiques. En 1992, le MIT et les membres du groupe jugèrent opportun de remplacer le Consortium par une organisation indépendante du MIT. Un nouvel organisme à but non lucratif fut donc créé en 1993 sous l'appellation X Consortium, Inc, et succéda donc au Consortium X du MIT. Tous les droits sur X lui furent concédés le premier janvier 1994. L'organisme X Consortium, Inc annonça en milieu d'année 1996 un transfert vers l'Open Group prévu pour 1997 de ses droits et responsabilités sur X, puis fut dissous fin 1996,   Portail de l’informatique  Portail du Massachusetts
En France, la Direction générale des systèmes d'information et de communication a pour mission d'élaborer une politique d'ensemble des Systèmes d'information et de communication (SIC) du Ministère des Armées, en mettant en place des structures de pilotage et de concertation, dont elle devra contrôler la mise en œuvre effective. Elle est placée sous l'autorité d'un directeur général relevant directement du ministre des Armées. Le directeur général des SIC, actuellement l'amiral Pénillard, est également membre du comité directeur de la DIRISI et de l'Agence nationale de la sécurité des systèmes d'information.
La direction interarmées des réseaux d'infrastructure et des systèmes d'informations de la défense (DIRISI) est un service interarmées dépendant du chef état-major des armées. La DIRISI a été créée le 1er janvier 2004 par décret du 31 décembre 2003. L'état-major de la DIRISI est installé au Fort de Bicêtre sur la commune du Kremlin-Bicêtre (Val-de-Marne). La DIRISI est composée de plusieurs entités dont celle des centres de contrôle opérationnels des systèmes de communication et de chiffrement ainsi que son Bureau Opérations qui se situent à Maisons-Laffitte. Ce dernier a rejoint la portion centrale au Fort de Bicêtre à la mi-septembre 2008. Ayant déjà fusionné avec la DCTEI (direction centrale des télécommunications et de l'informatique) de l'armée de terre en 2006, puis avec le CASSIC (commandement air des systèmes de surveillance, d'information et de communications) de l'armée de l'air en 2007, elle a intégré en 2008 le SERSIM (service des systèmes d'information de la Marine) de la Marine nationale situé à Houilles. Finalement, cet organisme représente l'opérateur unique de télécommunications de la défense permettant un contrôle global de l'ensemble des communications entre les états-majors et les forces déployées en opérations (quelle que soit leur armée d'appartenance) notamment grâce au contrôle des liaisons satellites (SYRACUSE et Telcomarsat) et du réseau fédérateur des réseaux d'infrastructure métropolitains (SOCRATE). Les entités de la DIRISI sont : Direction centrale (DC-DIRISI), stationnée au fort de Bicêtre (Sud-Est parisien), avec 5 sous-directions (Clients, Stratégie, Sécurité des Systèmes d’Information, Achat Finances, Ressources Management) ; 7 DIRISI locales (Brest, Bordeaux, Lyon, Metz, Rennes, Toulon ainsi que "DIRISI Ile de France / 8e Régiment de transmissions") ; 10 DIRISI outre-mer (Cayenne, Fort-de-France, Saint-Denis de la Réunion, Nouméa, Papeete) et à l’étranger (Abu Dhabi, Djibouti, Libreville, Dakar, Port-Bouët) ; 24 centres nationaux de mise en œuvre (CNMO) ; 1 Centre national de soutien opérationnel (CNSO) 39 CIRISI (relais locaux des centres nationaux) et de plus d’une centaine de détachements répartis sur tout le territoire ; SCOE ; SICO. Le 8e Régiment de transmissions et le 43e Bataillon de transmissions, qui relevaient de la DIRISI, ont été dissous et intégrés à la chaîne (respectivement au sein de la "DIRISI Ile de France / 8e Régiment de transmissions" et de la DIRISI-Rennes). La DIRISI en quelques chiffres : près de 7 300 personnels civils et militaires ; 450 000 mouvements SSI par an ; 1 500 systèmes d’information ; 1 500 000 adresses IP. Récompenses obtenues par la DIRISI : 2015 : témoignage de satisfaction collectif du Chef d'état-major des armées dans le cadre de l'opération « Projet Balard ». Traditions : Structure interarmées récente, la DIRISI ne dispose que de traditions très fraiches. Le Saint patron des transmissions de l'armée de terre est Saint Gabriel (mentionné dans la Bible et le Coran en tant que messager divin) et c'est tout naturellement que la DIRISI l'a également adopté. DIRISI Ile de France/8e Régiment des transmissions, issue en filiation directe du 8e Régiment, en conserve naturellement son drapeau. Le Centre national de soutien opérationnel (CNSO) d'Orléans conserve le drapeau du 43e Régiment de transmissions. Sous l'impulsion du colonel Didier Bronoël, officier de traditions de la DIRISI, DIRISI Metz s'est vue attribuer la garde du drapeau de la 38e Escadre de bombardement et DIRISI Rennes, celui du 38e Régiment de transmissions. Chaque DIRISI locale (comprendre, direction régionale) dispose de son propre insigne métallique ; il n'existe pas pour l'instant d'insigne de manche spécifique.
L’Association européenne pour les études d'opinion et de marketing (en anglais European Society for Opinion and Market Research, ESOMAR) est une association internationale des professionnels des études de marché. Fondée en 1948, l'ESOMAR débute comme une association d'envergure européenne. En 2013, l'association compte plus de 4900 membres dans 130 pays, et regroupe des professionnels du monde des études, du marketing, de la publicité, des affaires et des relations publiques. En septembre 2012, ESOMAR lance une formule d'adhésion pour les entreprises. La mission d'ESOMAR est d'encourager et améliorer les pratiques dans le domaine des études. Son but est de promouvoir les meilleurs standards dans le domaine des études de marché afin d'améliorer les prises de décision, dans les secteurs publics et privés, afin de : Protéger les intérêts de l'industrie des études au niveau mondial. Améliorer et promouvoir les meilleurs pratiques au quotidien. Promouvoir les valeurs de cette industrie auprès des entreprises et de la société. L'ESOMAR a établi un code international des pratiques éthiques, le Code international ICC/ESORMAR pour les études sociales et de marché, en collaboration avec la Chambre de commerce internationale. Tous les membres de l'association doivent accepter de respecter cette norme ainsi que les codes éthiques ESOMAR, dans leurs pratiques quotidiennes des études. Ce Code a en outre été adopté par plus de 65 corps syndicaux majeurs à travers le monde. Chaque année, l'ESOMAR organise un congrès mondial dans un pays différent, et révèle à cette occasion le trophée du Best Paper annuel. En septembre 2013, le congrès a eu lieu à Istanbul, Turquie, et avait pour thème principal le Big Data. L'Excellence for Best Paper Award a été décerné à Annie Pettit, Melanie Courtright, Kartik Pashupati et Roddy Knowles pour leur papier "Multimode, Global scale usage". En 2015, le congrès annuel de l'ESOMAR a eu lieu à Dublin, avec pour thème Révélations. L'Excellence for Best Paper Award a été décerné à Eric Singler, Richard Bordenave, Etienne Bressoud (tous, BVA) et Françoise Waintrop (SGMAP) pour leur papier "French Governement : nudge me tender".
L'Institut d'électronique et d'informatique Gaspard-Monge, dont le sigle est IGM, est l'organisme de recherche et d'enseignement de l'Université Paris-Est Marne-la-Vallée spécialisé dans les domaines de l'informatique, de l'électronique, des télécommunications et des réseaux. L'Institut Gaspard-Monge comporte deux unités d'enseignement, l'une en électronique, l'autre en informatique et accueille deux laboratoires de recherche l'ESYCOM (Laboratoire d'Électronique, SYstèmes de Communication et Microsystèmes) et le LIGM (Laboratoire d'informatique Gaspard-Monge). Il dispense des formations universitaires (licence, master, doctorat) qui s'appuient sur ses laboratoires de recherche.
La Gesellschaft für Informatik (en abrégé "GI" et qui l'on peut traduire par Société pour l'Informatique) est une association  allemande d'environ 20 000 informaticiens, regroupant professionnels, enseignants et étudiants.
IBM Research est une division d'IBM créée en 1945 et constituée de treize laboratoires à travers le monde. Elle est dédiée à la science pure et à l'exploration des innovations technologiques.  Portail de l’informatique  Portail des technologies
L'IEEE Computer Society (CS) est une Association professionnelle fondée en 1971 pour faire avancer la théorie, la pratique et les applications de l'informatique et des technologies de l'information. L'association aide au développement professionnel de ses membres au niveau mondial en organisant des ateliers et des conférences, en publiant des revues à comité de lecture, en organisant des comités de spécialistes. C'est une composante de l’association IEEE. L'association CS participe aux activités éducatives comprenant de l'enseignement à distance, l'accréditation de programmes d'enseignement supérieur en informatique et des certifications professionnelles en génie logiciel.
L'Institut Africain d'Informatique (IAI) est une école inter-États basée à Libreville, Gabon. Il a été créé en janvier 1971 à Fort-Lamy, actuel Ndjamena, la capitale du Tchad. Ses états membres sont : le Bénin, le Burkina Faso, le Cameroun, la Centrafrique, la Côte d’Ivoire, la république du Congo, le Gabon, le Niger, le Sénégal, le Tchad, le Togo. Cette école forme des ingénieurs informaticiens, des maitres ingénieurs (MIAGE), des analystes-programmeurs. Les ambitions de l'Institut Comme toute organisation, L'institut Africain d'Informatique a plusieurs ambitions, nous pouvons citer entre autres : la mise à la disposition des États membres d'un centre de formation d’excellence des cadres dans les TIC et conforme aux normes internationales; Faire de l’IAI un centre d’expertise et de prestations en soutien aux administrations et aux entreprises avec pour mission d’assurer: -la veille technologique: « une activité qui met en œuvre des techniques d'acquisition, de stockage et d'analyse d'informations, concernant un produit ou un procédé, sur l'état de l'art et l'évolution de son environnement scientifique, technique, industriel (...), afin de collecter, organiser, puis analyser et diffuser les informations pertinentes qui vont permettre d’anticiper les évolutions et qui vont faciliter l'innovation. »1 -la vulgarisation des solutions découlant des innovations technologiques: diffusion accessible de connaissances scientifiques ou techniques par le grand public; Les atouts • L’IAI dispose d’un label de qualité reconnu non seulement dans l’ensemble des États membres dont une forte proportion des cadres informaticiens sont des produits IAI, mais aussi dans les États non membres et au-delà des frontières africaines (France, USA, Canada, etc). • une expertise et une expérience avérées de bientôt 40 ans, consolidées par un important réseau de partenaires universitaires (Université de Nantes, Université de Laval, Université de Versailles, Université de Poitiers, Institut National Polytechnique de Toulouse, Institut Supérieur de Technologie – Libreville, etc.) -le pilotage des processus de découverte, d’acquisition et d’intégration des technologies auprès des entreprises. la réduction de la fracture numérique des pays africains et en particulier des États membres
L'Institut national de recherche en informatique et en automatique (Inria) est un institut de recherche français en mathématiques et informatique. Créé le 3 janvier 1967 dans le cadre du plan Calcul, il a le statut d'établissement public à caractère scientifique et technologique. Son objectif est de mettre en réseau les compétences et talents de l'ensemble du dispositif de recherche français et international, dans ses domaines de compétence.
L'International Federation for Information Processing (IFIP) est une organisation non-gouvernementale reconnue par les Nations unies, liée à plus de 50 sociétés et académies de sciences nationales et internationales, qui regroupe plus d'un million professionnels de l'informatique et des réseaux.
L'International Technology Roadmap for Semiconductors (ITRS) est un ensemble de documents produits par un groupe d'experts de l'industrie, représentatifs d'organisations de l'industrie des semiconducteurs américains, européens, japonais, de Corée du Sud et de Taiwan. Le groupe produit régulièrement des rapports concernant ce domaine, et en particulier les feuilles de route correspondant à l'évolution des technologies de gravure des processeurs. Avant 1993, l'ITRS portait le nom de « National Technology Roadmap for Semiconductors »
L'ISACA est une association professionnelle internationale dont l'objectif est d'améliorer la gouvernance des systèmes d'information, notamment par l'amélioration des méthodes d'audit informatique. Elle est aussi l'organisme promoteur des référentiels de meilleures pratiques COBIT concernant les volets audit et gouvernance ainsi que Val IT en ce qui concerne les aspects de l'alignement stratégique et de la création de valeur. Auparavant connue sous le nom de l'Information Systems Audit and Control Association, seulement l'acronyme est désormais utilisé afin de refléter le large éventail de professionnels de la gouvernance que celle-ci représente,. Plusieurs sections dans le monde représentent l'ISACA et offrent des informations en français, dont l'Association française de l’audit et du conseil informatiques (AFAI) en France ainsi que l'ISACA Montréal et l'ISACA Québec.
Le laboratoire franco-chinois de recherche en informatique, automatique et mathématiques appliquées (LIAMA), en chinois : 中法信息自动化与应用数学联合实验室 ; pinyin : Zhōngfǎ xìnxī zìdònghuà yǔ yìngyòngshùxué liánhéshíyànshì) est un laboratoire de recherche sino-français créé en 1997 par l'Académie chinoise des sciences et l'Institut national de recherche en informatique et en automatique. Le LIAMA est hébergé par le CASIA, l'Institut d'automatique de l'Académie chinoise de sciences, à Pékin, près du technopôle Zhongguancun. Les projets sont réalisés en coopération entre l'Europe et la Chine et peuvent être hébergés par toute institution partenaire. Le LIAMA a été désigné par le ministère chinois de la Recherche comme « centre national pour la recherche internationale » en juin 2008 afin de renforcer les coopérations potentielles en informatique. Le LIAMA est géré par un consortium de partenaires qui a été fondé en octobre 2008. Le LIAMA est composé d'une équipe de 120 chercheurs permanents, associés et d'étudiants chinois, français mais aussi d'autres pays européens.
Le Leibniz-Zentrum für Informatik (LZI), en français le Centre Leibniz d’informatique, situé dans le château de Dagstuhl à Wadern dans le Land de Sarre, a été fondé en 1989. Jusqu'en avril 2008, son nom était Internationales Begegnungs- und Forschungszentrum für Informatik (IBFI), soit le « centre international de rencontre et de recherche en informatique ». Le LZI est membre de la Leibniz-Gemeinschaft et obtient, en tant que tel, son financement de base par l'État fédéral et les Länder.
Le MUNCI ,Mouvement pour une Union Nationale et Collégiale des Informaticiens (anciennement le Mouvement pour une Union Nationale des Consultants en Informatique), est la première association professionnelle du numérique en France (en nombre d'adhérents)[réf. nécessaire]. L'association fédère les membres salariés (principalement dans les services informatiques/SSII), indépendants et demandeurs d’emploi des professions Informatique, Web et Télécoms (appelés plus communément "métiers IT"), plus généralement des professions relatives au numérique. L’association se mobilise pour l’emploi dans le numérique en publiant régulièrement sur son site des dossiers, prises de position et propositions en faveur des professions concernées (sujets socioprofessionnels, économiques, règlementaires...). Elle permet à ses membres de bénéficier d'informations et de conseils sur les métiers et les entreprises du numérique, ainsi que certains services utiles à leur vie professionnelle. Elle intervient fréquemment dans la presse écrite, auprès des institutions et des pouvoirs publics pour défendre les intérêts des professions concernées.
L'Open Group est un consortium de normalisation neutre vis-à-vis des fournisseurs et des technologies (c'est-à-dire qu'il ne dépend pas d'une marque ou d'une technologie en particulier) composé de plus de trois cents organisations membres. L'Open Group définit des normes dans le domaine de l’ingénierie informatique, en particulier les interfaces de programmation.
L'OpenGL Architecture Review Board (ARB) est un consortium qui, entre 1992 et 2006, encadre les spécifications d'OpenGL. Il a été formé en 1992 et défini les tests de conformité pour valider les spécifications d'OpenGL et continue de mettre à jour les standards. Le 31 juillet 2006, le contrôle d'OpenGL a été transféré au groupe Khronos. Les membres collaborant à ce projet sont nombreux dont 3Dlabs (en), Apple, ATI, Dell, IBM, Intel, NVIDIA, SGI et Sun Microsystems. Microsoft était intégré au projet mais a décidé de le quitter en mars 2003.
Le Syndicat de l’industrie informatique (SII-RP) est un syndicat français faisant partie intégrante de la Confédération Nationale des Travailleurs - Solidarité Ouvrière (CNT-Solidarité Ouvrière ou CNT-SO). Ce syndicat regroupe principalement les salariés de la région parisienne du secteur de l'informatique et des centres d'appels. Il a pour buts l'amélioration immédiate des conditions de travail et une transformation sociale. Il a été créé en 1998 au sein de la CNT-F avant de rejoindre la CNT-Solidarité Ouvrière en avril 2013. Il anime l'émission de radio Les débogueurs du système sur Radio libertaire tous les premiers lundi du mois de 19h30 à 21h.
La Fédération Syntec regroupe des syndicats professionnels spécialisés dans les professions de l'ingénierie, du numérique, des études et du conseil, de la formation professionnelle et de l’événementiel dont les sociétés sont rattachées à la Convention collective nationale des bureaux d'études techniques, des cabinets d'ingénieurs-conseils et des sociétés de conseils plus communément appelée convention Syntec. La fédération Syntec est membre du Medef et adhère au GPS (groupement des professions de services).
Le Transaction Processing Performance Council (TPC) est un organisme a but non lucratif dont l'activité consiste à élaborer des tests de performance de systèmes informatiques transactionnels. Les éditeurs publient régulièrement les records atteints lors de mesures (benchmark), en particulier pour le plus utilisé, le TPC-C.
Uptime Institute est un consortium d'entreprises créé en 1993 dont l'objectif est de maximiser l'efficacité des centres de traitement de données. Uptime Institute est connu en particulier pour avoir défini la notion de « Tier » pour les datacenters, largement adopté dans le monde,. Uptime Institute a été racheté par The 451 Group (un des concurrents du Gartner) en 2009. La classification comporte les niveaux Tier I, Tier II, Tier III et Tier IV. Un datacenter doit être certifié par l’Uptime Institute pour revendiquer un niveau de Tier. La classification d’un site est fixée par son sous-système de plus bas niveau. Les notions intermédiaires de type Tier III+ n’existe pas bien qu'elles soient parfois mentionnées pour désigner une conformité incomplète au niveau supérieur. En 2015, Uptime Institute est le seul organisme à délivrer des certifications de datacenter. Les autres organismes proposant des classifications de datacenter (BICSI 002, TIA 942, Syska Hennessy) ne proposent pas de certification.
XMPP Standards Foundation (XSF, fondation pour les standards XMPP) (anciennement Jabber Software Foundation, JSF) est la fondation chargée du développement, de la standardisation et de la supervision des extensions du protocole XMPP/Jabber, le standard ouvert de l'IETF dédié entre autres à la messagerie instantanée et la présence. La XSF supervise le processus de création, de développement et de validation des XEP (XMPP Extension Protocol), mais aussi les tests d'interopérabilité entre les solutions des différentes acteurs du monde XMPP. La XSF est également chargée des propositions de protocoles formulées à l'IETF.
L'automation consiste à utiliser les services d'un logiciel dans une application informatique. L'automation peut donc être considérée comme une procédure d'automatisation. En informatique musicale, dans un séquenceur, l'automation consiste à programmer des changements de réglages pendant la lecture d'un morceau, comme la variation de volume d'une piste audio. Cette mise en place peut se faire par mimétisme : le logiciel enregistre en temps réel des mouvements venant de l'utilisateur pour les reproduire lors des prochaines exécutions du morceau. Elle peut également se faire au moyen du tracé d'une courbe dans le séquenceur, représentant l'évolution du paramètre indiqué en fonction du temps (dans cet exemple, le volume en fonction du temps).
Appium, est un outil pour automatiser des tests sur android, iOS, windows 10 et Firefox OS. Appium a gagné en 2014 le prix Bossie awards d'InfoWord (en) de meilleur logiciel open source pour mobile et ordinateur. Appium a aussi été sélectionné comme Open Source Rookie de l'année par Black Duck Software,.
Expect est un outil d'automation et de tests de non-régression écrit par Don Libes comme extension au langage de script Tcl pour tester des applications interactives comme telnet, ftp, passwd, fsck, rlogin, ssh, ou bien d'autres.
Dans un système informatique ou un réseau, un runbook, retranscrit littéralement dossier d'exploitation, est une compilation systématique des procédures et des opérations que l'administrateur ou l'opérateur du système effectue. Les runbooks sont souvent utilisés dans les technologies de l'information dans les sociétés commerciales et dans les centres d'opération de réseau comme une référence pour les administrateurs systèmes. Les runbooks peuvent être soit électroniques, soit en forme de document papier. Typiquement, un runbook contient les procédures pour commencer, arrêter et surveiller le système. Il peut également contenir des descriptions pour le traitement de demandes spéciales et des éventualités. Un runbook efficace permet aux opérateurs ayant une expertise préalable, de gérer et procéder à un troubleshooting du système. Grâce à l'automatisation du runbook, ces processus peuvent être effectués en utilisant des outils de logiciel d'une manière prédéterminée. Les runbooks sont généralement créés par les meilleurs fournisseurs des services de management. Ils comprennent des procédures pour tous les scénarios prévus et, en général, étape par étape, les arbres de décision déterminent la ligne de conduite efficace donnant un scénario particulier.
Une base de données (database en anglais), permet de stocker et de retrouver l'intégralité de données brutes ou d'informations en rapport avec un thème ou une activité ; celles-ci peuvent être de natures différentes et plus ou moins reliées entre elles,. Dans la très grande majorité des cas, ces informations sont très structurées, et la base est localisée dans un même lieu et sur un même support. Ce dernier est généralement informatisé. La base de données est au centre des dispositifs informatiques de collecte, mise en forme, stockage et utilisation d'informations. Le dispositif comporte un système de gestion de base de données (abréviation : SGBD) : un logiciel moteur qui manipule la base de données et dirige l'accès à son contenu. De tels dispositifs — souvent appelés base de données — comportent également des logiciels applicatifs, et un ensemble de règles relatives à l'accès et l'utilisation des informations. La manipulation de données est une des utilisations les plus courantes des ordinateurs. Les bases de données sont par exemple utilisées dans les secteurs de la finance, des assurances, des écoles, de l'épidémiologie, de l'administration publique (notamment les statistiques) et des médias. Lorsque plusieurs choses appelées bases de données sont constituées sous forme de collection, on parle alors d'une banque de données.
Le plan de préservation numérique est un procédé concernant la conservation et la préservation à long terme de données et de métadonnées numériques. L’établissement d’un tel plan sert principalement à établir la pérennité des données en plus d'évaluer la fiabilité des techniques et des outils de conservation d'une collection virtuelle. Le maintien de l’accès aux données dans leur forme originale est certainement le point fondamental d’un plan de préservation numérique en raison de l’évolution constante des technologies. Le matériel informatique ainsi que les logiciels devenant rapidement obsolètes, il est important de prendre en compte, non seulement la préservation des données, mais également la préservation de leurs supports.
Les 12 règles de Codd sont un ensemble de règles édictées par Edgar F. Codd, conçues pour définir ce qui est exigé d'un système de gestion de base de données (SGBD) afin qu'il puisse être considéré comme relationnel (SGBDR),.
L'administrateur de base de données (DBA : DataBase Administrator en anglais) est une personne responsable du bon fonctionnement de serveurs de bases de données, essentiellement relationnelles (OLTP) ou décisionnelles (OLAP), tant au niveau de la conception des bases, des tests de validation, de la coordination des intervenants, de l'exploitation, de la protection et du contrôle d'utilisation. Les compétences requises pour cette fonction sont multipolaires : système, développement, sécurité et fonctionnement des serveurs de bases de données. L'expérience y est prépondérante. Le DBA travaille souvent en relation étroite avec les administrateurs systèmes et les développeurs au sein d'une direction des systèmes d'information.
L'algèbre relationnelle est une théorie mathématique proche de la théorie des ensembles qui définit des opérations qui peuvent être effectuées sur des relations — des matrices contenant un ensemble de n-uplets.
An Etymological Dictionary of Astronomy and Astrophysics, littéralement en français Un dictionnaire étymologique d'astronomie et d'astrophysique, est une base de données interactive développée à l'Observatoire de Paris qui contient la définition en anglais de plus de 13 000 termes, ainsi que leur traduction en français et en persan.  Portail de l’astronomie  Portail d’Internet  Portail de l’édition numérique
L'architecture Ansi/Sparc est une architecture fondamentale sur laquelle reposent les SGBD modernes, elle fut proposée en 1975 par Charles Bachman qui reçut le prix Turing pour ses travaux. L’architecture Ansi-Sparc est divisée en trois niveaux, celui du schéma interne (SI), celui du schéma conceptuel (SC) et celui des schémas externes (SE). La première innovation de l’architecture Ansi-Sparc est la distinction claire entre la représentation interne des données au niveau physique (structure de données) et la représentation logique de celles-ci. Une base de données est définie et manipulée via le niveau conceptuel (SC) sans avoir à se soucier des détails de l’implémentation physique (SI). Par exemple, il est possible de définir un index sur un ensemble de données, mais comment celui-ci est réalisé au niveau physique n’a pas besoin d’être spécifié. Sur le même principe, lors d’une requête, l’usager n’a pas besoin d’indiquer comment utiliser l’index pour maximiser l’efficacité de la recherche. La deuxième grande innovation de ce modèle est la possibilité de créer des schémas externes qui sont en fait des portions de la base de données (sous-bases virtuelles) destinées à différents usagers. Un usager particulier ne peut manipuler que les données appartenant à son propre schéma externe. De nos jours, le terme schéma externe est remplacé par celui de « vue ».  Portail de l’informatique
Une architecture de données en Informatique est composée de modèles, de règles ou de standards qui désignent quelles données sont collectées et comment elles sont stockées, triées, intégrées et utilisées dans des systèmes de données. En d’autres termes, elle décrit la structure de données utilisée par une organisation et / ou des applications et inclut les descriptions des données stockées. Elle fournit les critères pour les opérations de traitement des différents types de données et contrôle donc celles qui circulent dans le système. L’architecte de données est chargée de définir la situation future, l'alignement au cours du développement et le suivi nécessaire pour s'assurer que la conception est faite selon les spécifications architecturales d'origine. Il considère ces systèmes de données comme une ressource stratégique de l'organisation, en les représentant indépendamment des processus des différentes unités qui les utilisent. Son principal but est de concevoir des structures de données d'une manière organisée, fournissant ainsi une base pour la construction de systèmes d'information flexibles et intégrés.  
L'architecture fédérée est une forme d'architecture centrée sur les données. Le terme de système de base de données fédérée a été introduit par Heimbigner et McLeod. Ils définissent l’essentiel de ce qu’est une base de données fédérée faiblement couplée : « collection of components to unite loosely coupled federation in order to share and exchange information » et une base de données fédérée fortement couplée : « an organization model based on equal, autonomous databases, with sharing controlled by explicit interfaces. » (un modèle d'organisation basé sur des bases de données égales, autonomes, avec partage contrôlées par des interfaces explicites) L’objectif de la création d’une base fédérée est de donner aux utilisateurs une vue unique des données présentes sur plusieurs systèmes a priori hétérogènes. Il s’agit d’une problématique typique lors de la concentration ou de la fusion d’entreprises : faire cohabiter les différents systèmes tout en leur permettant d’interopérer d’une manière harmonieuse. Il ne s’agit pas ici de reconstruire une base répartie à partir des anciennes (ré-ingénierie), mais de conserver les anciennes bases et leurs autonomies locales tout en permettant l’intégration de l’ensemble. Sheth et Larson divisent les systèmes fédérés en deux catégories : les systèmes faiblement couplés réalisant l’intégration des différentes bases de données à l’aide des mécanismes des bases de données multibases et réparties (et donc la tâche de maintenir la fédération est laissée aux administrateurs locaux) et les systèmes fortement couplés réalisant l’intégration à l’aide d’un SGBD fédéré (le maintien de la fédération est sous le contrôle de l’administrateur de la fédération).
En informatique, une base de données distribuée (en anglais : distributed database) est une base de données dont la gestion est traitée par un réseau d'ordinateurs interconnectés qui stockent des données de manière distribuée. Ce stockage peut être soit partitionné entre différents nœuds du réseau, soit répliqué entièrement sur chacun d'eux, ou soit organisé de façon hybride.
Une base de données hiérarchique est une base de données dont le système de gestion lie les enregistrements dans une structure arborescente où chaque enregistrement n'a qu'un seul possesseur.
Une base de données orientée colonnes est une base de données qui stocke les données par colonne et non par ligne. L'orientation colonne permet d'ajouter des colonnes plus facilement aux tables (les lignes n'ont pas besoin d'être redimensionnées). Elle permet de plus une compression par colonne, efficace lorsque les données de la colonne se ressemblent.
En informatique, une base de données orientée composant (component-oriented database ou CODB) est un ensemble d'informations stockées sous un modèle particulier de gestion de données et de programmation de systèmes de gestion de base de données. La particularité principale d'une CODB est l'usage du paradigme de la programmation orientée composant,.
Une base de données orientée documents est une base de données destinée aux applications qui gèrent des documents. Ce type de bases de données peut être une sur-couche d'une base de données relationnelle ou non. Deux langages sont maintenant principalement utilisés pour représenter les documents structurés : XML et JSON. L'avantage des bases de données orientées documents est l'unité d'information et l'adaptation à la distribution. En effet, d'une part, comme tout est compris dans la structure cela évite de faire des jointures pour reconstituer l'information car elle n'est plus dispersée dans plusieurs tables. Il n'y a plus besoin de transaction car l'écriture est suffisante pour créer des données sur un document pour modifier un objet. Une seule lecture est suffisante pour reconstituer un document. D'autre part, les documents étant autonomes, on peut les déplacer facilement, ils sont indépendants les uns des autres. Il existe plusieurs inconvénients : la hiérarchisation d'accès, l'absence de perspective dans la base de données et la perte d'autonomie des entités. L'objectif d'une base de données orientée documents est la représentation des informations plus ou moins complexes en satisfaisant les besoins suivants : Flexibilité Richesse de la structure Autonomie Sérialisation
Une base de données orientée graphe est une base de données orientée objet utilisant la théorie des graphes, donc avec des nœuds et des arcs, permettant de représenter et stocker les données.
En informatique, une base de données à objets (anglais object database) est un stock d'informations groupées sous formes de collections d'objets persistants. Une base de données est un ensemble d'informations connexes stockées dans un dispositif informatique. Dans une base de données à objets les informations sont regroupées sous forme d'objets : un conteneur logique qui englobe des informations et des traitements relatifs à une chose du monde réel. Les bases de données à objets sont mises en œuvre par un système de gestion de base de données objet — logiciel qui manipule le contenu de la base de données — et un programme écrit dans un langage de programmation orientée objet. Les premiers systèmes de gestion de bases de données à objets sont apparus dans les années 1990, en même temps que se sont répandus les langages de programmation orientés objet.
Une base de données orientée texte (ou base de données dans un fichier plat, de l'anglais flat file database) est un modèle de base de données (en) (généralement une table) sous la forme d'un simple fichier (formats .txt ou .ini). Un fichier plat est un fichier texte ou du texte combiné avec un fichier binaire contenant généralement un seul enregistrement par ligne.
En informatique, une base de données relationnelle est une base de données où l'information est organisée dans des tableaux à deux dimensions appelés des relations ou tables, selon le modèle introduit par Edgar F. Codd en 1970. Selon ce modèle relationnel, une base de données consiste en une ou plusieurs relations. Les lignes de ces relations sont appelées des nuplets ou enregistrements. Les colonnes sont appelées des attributs. Les logiciels qui permettent de créer, utiliser et maintenir des bases de données relationnelles sont des systèmes de gestion de base de données relationnels. Pratiquement tous les systèmes relationnels utilisent le langage SQL pour interroger les bases de données. Ce langage permet de demander des opérations d'algèbre relationnelle telles que l'intersection, la sélection et la jointure.
Le modèle réseau est une manière de représenter les données dans le cadre d'une base de données. Ce modèle est en mesure de lever de nombreuses difficultés du modèle hiérarchique grâce à la possibilité d'établir des liaisons de type n-m en définissant des associations entre tous les types d'enregistrements. Ce modèle est une extension du modèle précédent (hiérarchique), les liens entre objets peuvent exister sans restriction. Pour retrouver une donnée dans une telle modélisation, il faut connaître le chemin d'accès (les liens), ceci rend encore les programmes dépendants de la structure de données.
Une base de données temporelle est une base de données avec des aspects de temps intégrés, c'est-à-dire un modèle de données temporel et une version temporelle du langage structuré de requêtes (Structured Query Language - SQL). Plus spécifiquement, les aspects temporels contiennent habituellement le temps-valide et le temps-transaction. Ces attributs marchent ensemble pour former une donnée bitemporelle. Le temps-valide dénote la période durant laquelle un fait est vrai par rapport à la réalité. Le temps-transaction est la période pendant laquelle un fait est stocké dans la base de données. La donnée bitemporelle combine à la fois le temps-valide et le temps-transaction. À noter que ces deux périodes n'ont pas à être égales pour un fait unique. Imaginez que nous ayons une base de données temporelle stockant des données relatives au XVIIIe siècle. Le temps-valide de ces faits se situe quelque part entre 1701 et 1800, tandis que le temps-transaction débute quand on insère le fait dans la base de données, par exemple le 21 janvier 1998. Il est possible d'avoir dans la base de données des échelles de temps autres que le temps-valide et le temps-transaction, comme le temps-décision. Dans ce cas, la base de données est nommée base de données multitemporelle par opposition à base de données bitemporelle. Cependant, cette approche introduit des complexités additionnelles comme la gestion de la validité des clés (étrangères).
Le BLOB, pour binary large object, est un type de donnée permettant le stockage de données binaires (le plus souvent des fichiers de type image, son ou video) dans le champ d'une table d'une base de données. Dans le monde des logiciels libres, le BLOB est un terme péjoratif pour désigner l'inclusion d'un pilote sous forme de fichier objet dans le noyau libre d'un système d'exploitation afin de ne pas divulguer le code source du programme. On retrouve ces blobs dans les systèmes NetBSD, FreeBSD, DragonFly BSD et la plupart des distributions GNU/Linux. En revanche le projet OpenBSD les refuse pour des raisons de maintenance et de sécurité mais aussi pour leur incompatibilité avec le concept des logiciels libres. La Free Software Foundation s'oppose fermement à l'utilisation de ces blobs et promeut une version du noyau Linux qui en est expurgée : Linux-libre.
En informatique, la Call Level Interface (traduction: interface de niveau appels) est une interface de programmation normalisée qui permet à des applications informatiques de manipuler des bases de données mises à disposition par des systèmes de gestion de base de données. La norme a été créée en 1992. En 2010 la quasi-totalité des systèmes de gestion de bases de données du marché offrent une telle interface de programmation. Ce qui permet de les manipuler depuis un logiciel applicatif par l'intermédiaire de bibliothèques logicielles courantes du marché telles que JDBC, ADO ou ODBC.
Un champ, est l'information élémentaire d'une base de données, d'un fichier informatique, et plus généralement d'une ressource informatique où existent des termes indexés.  Dans la programmation orientée objet, un champ (ou attribut) est une propriété d'un objet. Cette propriété a un nom, un type de données et une valeur.  Dans les bases de données, un champ peut être considéré comme étant l'intersection d'une colonne et d'une ligne d'une table, c'est-à-dire une valeur particulière d'une colonne d'une table de base de données, telle que l'adresse ou le numéro de téléphone de M. Dupont. Il est en effet impropre d'appeler "champ" une colonne de table de base de données. Selon cette définition, on peut faire une analogie entre la cellule d'une feuille d'un tableur et le champ dans une table de base de données. Un champ est aussi une zone de saisie ou d'affichage d'une information dans un formulaire papier ou informatisé. Dans un tableur, lors de la création d’un tableau croisé dynamique, les cellules de la première ligne du tableau de données brutes deviennent les différents champs que l’on peut faire glisser dans l’assistant.  Portail de l’informatique  Portail des bases de données
Le type CLOB ou character large object est un type de donnée permettant le stockage de données en volume important à base de caractères (pas comme le BLOB) dans le champ d'une table d'une base de données.  Portail des bases de données
Dans le domaine des bases de données, une clé artificielle (en opposition à une clé naturelle), aussi parfois appelée clé de remplacement (de l'anglais surrogate key) désigne un ensemble de données adjointes aux données d'une table pour les indexer. La génération de la clé artificielle est effectuée par le concepteur de la table. Toute clé indexant chaque ligne de manière unique est valable. Parmi les méthodes de génération courantes de clé artificielle nous pouvons citer les clés incrémentales (les lignes sont numérotées au fur et à mesure de leur introduction dans la table). Les clés artificielles sont les seules clés possibles quand : il n'existe pas de clé candidate dans la table initiale (il existe des lignes doublons) ; D'autres raisons courantes pour adjoindre une clé artificielle à une table sont : il existe un risque significatif que des doublons de lignes existantes soient introduits ultérieurement dans la table ; les clés candidates sont peu utilisables (par exemple elles sont trop longues, ce qui nuit aux performances d'interrogation) ; il existe une instabilité trop importante sur les données faisant partie des clés candidates entraînant des modifications trop fréquentes (correction d'erreurs de saisie, changement de référentiel…). Pour être considérée comme une clé artificielle, le mode de génération de la clé doit être indépendant ou partiellement indépendant des données indexées. Une clé générée par une fonction déterministe à partir de données de l'enregistrement (en général pour en réduire la taille) est appelée une clé hachée (en référence aux fonctions de hachage).
Dans le domaine des bases de données, une clé candidate est un ensemble des données permettant d'indexer chaque ligne d'une table donnée de manière différenciée. Une même base de données peut posséder plusieurs clés candidates distinctes. La clé primaire appartient nécessairement à l'ensemble des clés candidates. Une table sans lignes en doublon possède nécessairement une ou plusieurs clés candidates. Une table avec des lignes en doublon ne possède aucune clé candidate. Lorsqu'une table ne possède aucune clé candidate, ou que les clés candidates sont peu adaptées (par exemple très longues), le gestionnaire de la base peut être amené à ajouter des données d'indexation arbitraires à la table, créant ainsi une clé artificielle. Une clé choisie parmi les clés candidates pour indexer une base est appelée une clé naturelle.
Une clé étrangère, dans une base de données relationnelle, est une contrainte qui garantit l'intégrité référentielle entre deux tables. Une clé étrangère identifie une colonne ou un ensemble de colonnes d'une table comme référençant une colonne ou un ensemble de colonnes d'une autre table (la table référencée). Les colonnes de la table référencée doivent faire partie d'une contrainte de clé primaire ou d'une contrainte d'unicité. La contrainte de clé étrangère garantit que les valeurs de chaque ligne de la table référençant existent dans la table référencée : ainsi une ligne de la table référençant ne peut pas contenir un ensemble de valeurs qui n'existe pas dans la table référencée. Une contrainte de clé étrangère permet ainsi d'établir des liens entre plusieurs tables : il s'agit d'un des principes fondamentaux des bases de données relationnelles.
Dans le domaine des bases de données, une clé naturelle est une clé (en général clé primaire) choisie parmi les clés candidates pour indexer une base.
Dans une base de données relationnelle, une clé primaire est la donnée qui permet d'identifier de manière unique un enregistrement dans une table. Une clé primaire peut être composée d'une ou de plusieurs colonnes de la table. Deux lignes distinctes de la table ne peuvent pas avoir les mêmes valeurs dans les colonnes définies comme clé primaire. Il est possible de définir pour une même table plusieurs contraintes d'unicité, mais au plus une seule clé primaire. Une clé primaire est choisie parmi les clés candidates. Suivant les cas il peut être nécessaire ou préférable d'utiliser une clé artificielle ajoutée aux données comme clé primaire. La clef primaire d'une table doit se placer sur des colonnes qui permettent d'identifier chaque ligne de la table. Il peut donc sembler intéressant de placer une clé primaire sur un numéro de sécurité sociale par exemple, mais on préfèrera utiliser une clé primaire complètement indépendante des données métier, afin de s'assurer que le champ est toujours rempli (un étranger ou un enfant peuvent ne pas avoir de numéro de sécurité sociale) et n'évolue pas dans le temps. Toutes les bases de données proposent des mécanismes prenant en charge une numérotation utilisable pour les clés primaires. Il n'est pas nécessaire de définir un index sur les colonnes définissant la clé primaire, car un index implicite est associé à la clé primaire. L'ensemble constitué d'une clé primaire et d'une clé étrangère sert à établir des relations entre tables.
ClickHouse est un logiciel libre de Base de données (DBMS) orientée colonnes pour le Traitement analytique en ligne (OLAP). ClickHouse a été développé par la société russe Yandex pour le service d'analyse web Yandex.Metrica,,,. ClickHouse permet l'analyse de données mises à jour en temps réel. Ce système est développé pour la haute performance. Le projet a été publié sous forme de logiciel libre sous les termes de la Licence Apache en juin 2016. ClickHouse est utilisé par la suite de test Yandex.Tank. Yandex.Market utilise ClickHouse pour surveiller l’accessibilité de sites et KPIs. ClickHouse a aussi été déployé au CERN LHCb pour stocker et traiter les métadonnées de 10 milliards d’événements contenant 1000 attributs chacun, et Tinkoff Bank utilise ClickHouse comme stockage de données pour l'un de ces projets. ClickHouse est également utilisé par Cloudflare pour stocker et traiter les logs provenant de ses serveurs DNS.
Le CODASYL (sigle de Conference on Data Systems Languages, en français « Conférence sur les langages de systèmes de traitement de données ») est l'organisme américain de codification des systèmes de bases de données. Il a publié en 1959 les spécifications du langage COBOL. Par la suite, ses travaux entre 1974 et 1981 ont donné naissance au modèle navigationnel de SGBD (opposé au modèle hiérarchique largement promu par IBM). En 1965, le CODASYL a constitué un groupe de travail chargé de développer les extensions du langage COBOL qui permettront de manipuler des données en préservant l'indépendance entre les applications et les périphériques de stockage. La contribution de Charles Bachman revient à la logique de chaines de données et de pointeurs mise notamment en œuvre dans le système IDS. En 1967, ce groupe de travail s'est rebaptisé Data Base Task Group (DBTG) qui publiait notamment COBOL extensions to handle data bases (extensions de COBOL pour manipuler des données). En octobre 1969, le DBTG publie alors les premieres spécifications du modèle de bases de données réseau qui prendra très vite le nom de modèle de données CODASYL. Le CODASYL a défini en 1970 les normes de standardisation des Systèmes de Gestion des Bases de Données (SGBD), uniquement mis en œuvre sur les grands systèmes à cette époque. Le langage de manipulation des données était alors exclusivement le COBOL et le databasic. C'est la première norme de base de données décidée sans l'avis d'IBM.
En informatique, la cohérence est la capacité pour un système à refléter sur la copie d'une donnée les modifications intervenues sur d'autre copies de cette donnée. Cette notion est principalement utilisée dans trois domaines informatiques : les systèmes de fichiers, les bases de données, et les mémoires partagées. Un modèle de cohérence contraignant (cohérence « forte ») permet un comportement intuitif et simplifie la compréhension du comportement des programmes, mais des modèles de cohérence « faible » ou « relâchée » permettent souvent d'améliorer les performances, à charge pour les programmes d'assurer la cohérence des données lorsqu'elle est nécessaire.
En informatique, notamment dans les systèmes de bases de données et de révision de fichier, les termes d’archivage, de soumission, de validation,, ou encore l’anglais commit désignent l’enregistrement effectif d’une transaction. Cet enregistrement entérine l’exécution de la tâche préalablement confiée, marquant à la fois la fin de la demande de transaction et le début de l’exécution de la tâche confiée, qui devra être exécutée atomiquement. Le terme anglais fait référence à la commande éponyme Commit présente dans la plupart des systèmes de gestion de base de données et des logiciels de gestion de versions, qui ne proposent généralement pas d’interface de programmation régionalisée. Il provient du latin committere, de « co(m)- », signifiant « ensemble » et « mittere », signifiant « envoyer », ce terme latin a également donné en français le terme « commettre ». Il conserve en anglais plusieurs sens, celui de confier (comme on confie une mission) et d'effectuer une action, comme le commettre du français moderne.
La conservation des données (en anglais Retention Management) définit les règles et procédures de conservation des données personnelles ainsi que des registres d'appels téléphoniques (statistiques d'appel) que doivent respecter les opérateurs de télécommunications, les fournisseurs d'accès, et les hébergeurs de sites web et de courriels. Elle vise principalement à faire de l'analyse de trafic et à la surveillance.
Dans une base de données, une contrainte d'intégrité permet de garantir la cohérence des données lors des mises à jour de la base. En effet, les données d'une base ne sont pas indépendantes, mais obéissant à des règles sémantiques, après chaque mise à jour, le SGBD contrôle qu'aucune contrainte d'intégrité n'est violée. Sous Oracle, les clés primaires sont systématiquement accompagnées d'un index défini de la même manière. Il existe pour ce SGBD différents types d'index qui autorisent la conservation de l'intégrité de la table.
L'acronyme informatique anglais CRUD (pour create, read, update, delete) (parfois appelé SCRUD avec un "S" pour search) désigne les quatre opérations de base pour la persistance des données, en particulier le stockage d'informations en base de données. Soit : create : créer read : lire update : mettre à jour delete : supprimer Plus généralement, il désigne les opérations permettant la gestion d'une collection d'éléments. Ce terme est aussi un jeu de mot en anglais sur l'adjectif crude (en français brut ou rudimentaire). Voir aussi les noms REST, RESTful et HATEOAS.
En informatique, un nom de source de données (DSN, parfois connu sous le nom de la source de base de données si les sources de données ne sont pas limitées à des bases de données) est une structure de donnée utilisée pour décrire une connexion à une source de donnée. Le plus souvent utilisé en référence à ODBC, DSN peut également être défini pour JDBC et d'autres mécanismes d'accès aux données.
Le DataBasic, développé par Charles Bachman, est un produit qui permet au langage Basic d'avoir accès à la base de données et qui est utilisé par le système Pick et les SGBDR Multivalués apparentés. Le DataBasic est présent dans toutes les implémentations de Pick et seules des différences mineures séparent les différents DataBasic : IN-Basic (IN-Pick), Universe-basic (IBM), Pick-basic (Raining Data), Qmbasic (openQM) et Rbasic (advance-revelation).
Dans un système de gestion de base de données (SGBD), tel qu'Oracle Database ou PostgreSQL, un DBLink, ou database link est un objet d'une base de données permettant d'exécuter des requêtes sur une autre base de données, qu'elle se trouve physiquement sur la même machine ou qu'elle soit distante.
En programmation procédurale, un déclencheur (trigger en anglais) est un dispositif logiciel qui provoque un traitement particulier en fonction d'événements prédéfinis. Par extension, c'est l'événement lui-même qui est qualifié de déclencheur. En programmation objet, tout message à un objet est lui-même un déclencheur. Dans les interfaces graphiques, ces déclencheurs sont nommés en général callbacks. En programmation système, la modification d'un fichier peut constituer un déclencheur, soit pour maintenir à jour les informations affichées (contenu d'un répertoire, par exemple), soit pour lancer des opérations de sécurité. En Linux, c'est Gamin (anciennement : FAM, File Access Monitoring) qui est utilisé à cette fin.
DELETE​ est une commande SQL qui supprime un ou plusieurs tuples dans une table d'une base de données relationnelle.
Dialog est le langage d'interrogation d'un serveur de bases de données documentaires. Il permet, selon un moteur de recherche et des opérateurs booléens, d’interroger, via Internet ou d'autres réseaux informatiques, plus de 15 000 Go de données, sur des domaines très variés. Dialog est aussi le nom de la société, créée en 1966 sous la direction de Roger Summit, qui gère et commercialise les bases de données. C’est actuellement Thomson Corporation qui en est propriétaire. Selon son propre slogan, elle est actuellement « le plus important système en ligne permettant d’interroger de volumineuses bases de données ». L’ensemble des 900 bases accessibles par Dialog sont interrogées plus de 700 000 fois par mois, et renvoient plus de 17 millions de pages de documents vues par mois. Dialog n’est pas visible des robots des moteurs de recherche grand public (Google, par exemple). Pour s'adapter aux concurrents et surtout aux nouvelles habitudes de recherche d'information prises par l'utilisation des moteurs de recherche, Dialog propose parallèlement à son ancien langage de commandes des interfaces plus conviviales. De même, Dialog propose parallèlement au paiement à la durée des abonnements annuels. La plus grande menace qui pèse sur Dialog est la prolifération d’information sur Internet et sa stratégie marketing discutable.
Un dictionnaire des données est une collection de métadonnées ou de données de référence nécessaire à la conception d'une base de données relationnelle. Il revêt une importance stratégique particulière, car il est le vocabulaire commun de l'organisation. Il décrit des données aussi importantes que les clients, les nomenclatures de produits et de services, les annuaires, etc. C'est donc le référentiel principal de l'entreprise, sur lequel s'appuient les décisions de celle-ci. Il est souvent représenté par un tableau à quatre colonnes contenant le nom, le code et le type de donnée ainsi que des commentaires. Un dictionnaire des données doit respecter les contraintes suivantes. Tous les noms doivent être monovalués et non décomposables. Il ne doit pas y avoir d'homonymes, ni de synonymes. Les données y sont regroupées par entité. Les identifiants sont complètement précisés, Les commentaires doivent être pertinents.
Discipulus est un langage de programmation informatique servant à l’exploitation de bases de données relationnelles. Il a été introduit par le groupe Μῆτις de l’Université de Sherbrooke et a été présenté dans le mémoire de maîtrise de Zouhir Abouaddaoui en juillet 2012. Il a été conçu de sorte à : Permettre la représentation d’éléments complexes de la vie réelle par l’intégration de la programmation orientée objet au modèle relationnel Assurer l’évolutivité avec les mécanismes d’héritage et d’héritage multiple Garantir la fiabilité par l’incorporation des principes de programmation par contrat et en corrigeant certaines lacunes qu’on retrouve dans les systèmes de gestion de base de données (abr. SGBD) relationnels existants. Il possède un système de typage statique et fort inspiré du langage de programmation Eiffel. Il inclut également un langage algorithmique et un langage relationnel qui sont tous deux basés sur ce système de typage.
En informatique, une donnée est la représentation d'une information dans un programme : soit dans le texte du programme (code source), soit en mémoire durant l'exécution. Les données, souvent codées, décrivent les éléments du logiciel tels qu'une entité (chose), une interaction, une transaction, un évènement, un sous-système, etc. Les données peuvent être conservées et classées sous différentes formes : textuelles (chaîne), numériques, images, sons, etc. Les données variables qui font la souplesse d'un programme sont généralement lues depuis un appareil d'entrée utilisateur (clavier, souris…), un fichier, ou en réseau. Le processus d'enregistrement des données dans une mémoire s'appelle la mémorisation.
Dans le contexte des bases de données, la durabilité est la propriété qui garantit qu'une transaction informatique qui a été confirmée survit de façon permanente, quels que soient les problèmes rencontrés par la base de données ou le système informatique où cette transaction a été traitée. Par exemple, dans un système de réservation de sièges d'avion, la durabilité assure qu'une réservation confirmée restera enregistrée quels que soient les problèmes rencontrés par l'ordinateur qui gère le système de réservation (panne d'électricité, écrasement de la tête sur le disque dur, etc.). La durabilité est l'une des quatre propriétés ACID qui garantissent qu'une transaction informatique est exécutée de façon fiable. Plusieurs système de gestion de base de données implémentent la durabilité en écrivant les transactions sur un journal des transactions qui peut être utilisé pour recréer la base de données dans l'état où elle était immédiatement avant une panne. Une transaction est confirmée seulement après son enregistrement dans le journal des transactions. Dans le cas de transactions distribuées, tous les serveurs impliqués doivent se coordonner pour émettre une confirmation uniquement lorsque la transaction est enregistrée de façon permanente sur tous les serveurs. Cela est habituellement fait au moyen d'un two-phase commit protocol (en).
L'échafaudage ou scaffolding en anglais est une manière de concevoir des logiciels liés à une base de données. Cette technique est souvent fournie avec le patron de conception Modèle-Vue-Contrôleur, dans lequel le développeur écrit une spécification décrivant comment la base de données sera utilisée. Le compilateur génère le code source de création, lecture, mise à jour et suppression (CRUD) des données en base pour l'application. Cet échafaudage est le point de départ d'une application plus puissante. L'échafaudage fut popularisé par le framework Ruby on Rails en 2005. Il a été adopté par d'autres tel que Monorail (.Net), Symfony (PHP), CodeIgniter, CakePHP, Yii Framework, Model-Glue, Grails, Gaia Flash Framework et AngularJS.
EIDER est une base de données régionales et départementales sur l'environnement, l'énergie, le transport, le logement et la construction, mise à disposition de tous, sur Internet, par le Ministère de l'Environnement, de l'Énergie et de la Mer en France. Elle est gérée par le Service de l'observation et des statistiques (SOeS ) au sein du Commissariat général au développement durable.
L'Encyclopédie des planètes extrasolaires;,,,, (en anglais The Extrasolar Planets Encyclopaedia ; abrégé en EPE dans entre autres ces deux langues) est un site web dédié à l'astronomie, fondé en 1995 par Jean Schneider à l'observatoire de Paris. Il maintient une base de données sur les exoplanètes confirmées et hypothétiques.
Un enregistrement (de l'anglais record) est un élément d’un tableau à deux dimensions ou d’une base de données. L’enregistrement contient habituellement plusieurs informations (entrées) qui se rapportent au même objet. Par exemple, un enregistrement d’un fichier contenant la description des clients d’une entreprise contiendra plusieurs informations (les rubriques) sur un client : son numéro de client, son nom, son adresse postale, son numéro de téléphone, etc.
L'enregistrement est l’action de fixer une information sur un support matériel comme un disque dur, un CD-ROM, une clé USB ou une bande magnétique. L’enregistrement et la lecture sur le support matériel peuvent se faire de façon séquentielle ou aléatoire. Dans le mode séquentiel, les enregistrements sont traités (écrits ou lus) un à un dans l’ordre de leur apparition sur le support en commençant par le premier. Dans le mode aléatoire, un index contenant l’identification de l’enregistrement et son adresse permet à l’ordinateur d’accéder directement à l’enregistrement désiré pour le lire ou pour le modifier.  Portail de l’informatique
Electronic Quality Shipping Information System (EQUASIS) est une base de données, consultable sur internet, regroupant des informations concernant la sécurité des navires de 100 tonneaux ou plus.
Le terme espace de noms (namespace) désigne en informatique un lieu abstrait conçu pour accueillir des ensembles de termes appartenant à un même répertoire, comme dans l'exemple suivant où les espaces de noms sont nommés « Jean-Paul » et « Jean-Pierre » :
Ronald Fagin (né en 1945) est un informaticien américain, Fellow IBM au IBM Almaden Research Center. Il est réputé pour ses travaux pionniers en complexité descriptive, logique mathématique, théorie des bases de données, théorie des modèles finis, et le raisonnement sur la connaissance,. Il est un des lauréats du Prix Gödel 2014.
Le fichier des titres électroniques sécurisés ou (fichier TES) est un fichier du ministère de l'Intérieur français qui consiste en une base de données rassemblant l'identité, le sexe, la couleur des yeux, la taille, l'adresse du domicile, les données relatives à la filiation, l'image numérique du visage et de la signature, l'adresse e-mail et les empreintes digitales de tous les détenteurs d'une carte nationale d'identité ou d'un passeport français.
Dans une base de données relationnelle, une forme normale désigne un type de relation particulier entre les entités. Le but essentiel de la normalisation est d'éviter les anomalies transactionnelles pouvant découler d'une mauvaise modélisation des données et ainsi éviter un certain nombre de problèmes potentiels tels que les anomalies de lecture, les anomalies d'écriture, la redondance des données et la contre-performance. La normalisation des modèles de données permet de vérifier la robustesse de leur conception pour améliorer la modélisation (et donc obtenir une meilleure représentation) et faciliter la mémorisation des données en évitant la redondance et les problèmes sous-jacents de mise à jour ou de cohérence. La normalisation s’applique à toutes les entités et aux relations porteuses de propriétés. Les formes normales s'emboitent les unes dans les autres, tant et si bien que le respect d'une forme normale de niveau supérieur implique le respect des formes normales des niveaux inférieurs. Dans le modèle relationnel de type OLTP, il existe huit formes normales, les trois premières étant les plus connues et utilisées : la première forme normale notée 1FN (1NF en anglais) ; la deuxième forme normale notée 2FN (2NF en anglais) ; la troisième forme normale notée 3FN (3NF en anglais) ; la forme normale de Boyce Codd notée FNBC (BCNF en anglais) ; la quatrième forme normale notée 4FN (4NF en anglais) ; la cinquième forme normale notée 5FN (5NF en anglais) ; la forme normale domaine clef notée FNDC (DKNF en anglais) ; la sixième forme normale notée 6FN (6NF en anglais) rarement présentée. La forme normale vient après la simple validité d'un modèle relationnel, c'est-à-dire que les valeurs des différents attributs soient bien en dépendance fonctionnelle avec la clé primaire (complètement déterminés par la clé primaire).
Global Administrative Areas ou GADM est une base de données des contours vectoriels des frontières administratives. Le projet est développé par Robert Hijmans de l'université de Californie à Berkeley. Les données sont géoréférencées au format WGS 84.
En informatique, l'imbrication d'ensembles, nested sets en anglais, est une technique pour représenter des données hiérarchisées dans une base de données relationnelle. En substance, elle consiste à attribuer à chaque nœud deux bornes, dite gauche et droite, qui permettent de statuer sur les liens de parentés entre les différents nœuds. L'implémentation la plus simple utilise des entiers naturels pour définir les bornes. Cette méthode présente entre autres inconvénients la nécessité de modifier une grande partie de l'arbre à chaque ajout d'un enregistrement. Une méthode beaucoup plus efficace utilise des nombres rationnels. Cette deuxième méthode est cependant peu connue, beaucoup plus difficile à appréhender, et comporte des développements assez poussés sur le plan mathématique. Elle comporte notamment des liens avec les fractions continues et un usage possible du calcul matriciel, qui sont autant de fonctionnalités souvent difficiles à implémenter directement en SQL.
En informatique, dans les bases de données, un index est une structure de données utilisée et entretenue par le système de gestion de base de données (SGBD) pour lui permettre de retrouver rapidement les données. L'utilisation d'un index simplifie et accélère les opérations de recherche, de tri, de jointure ou d'agrégation effectuées par le SGBD. L’index placé sur une table va permettre au SGBD d'accéder très rapidement aux enregistrements, selon la valeur d'un ou plusieurs champs.
La faille SQLi, abréviation de SQL Injection, soit injection SQL en français, est un groupe de méthodes d'exploitation de faille de sécurité d'une application interagissant avec une base de données. Elle permet d'injecter dans la requête SQL en cours un morceau de requête non prévu par le système et pouvant en compromettre la sécurité.
INSERT​ est une commande SQL qui ajoute un ou plusieurs tuples dans une table d'une base de données relationnelle.
L'une des missions d'une base de données est d'assurer à tout instant l'intégrité, c'est-à-dire la cohérence, la fiabilité, et la pertinence des données qu'elle contient.
En informatique, et plus particulièrement dans les bases de données relationnelles, l´intégrité référentielle est une situation dans laquelle pour chaque information d'une table A qui fait référence à une information d'une table B, l'information référencée existe dans la table B. L'intégrité référentielle est un gage de cohérence du contenu de la base de données. Les systèmes de gestion de base de données (SGBD) permettent de déclarer des règles de maintien de l'intégrité référentielle (contrainte). Une fois la contrainte déclarée, le SGBD refusera toute modification du contenu de la base de données qui violerait la règle en question et casserait l'intégrité référentielle. Par exemple : on définira qu'un livre a un ou plusieurs auteurs. Une contrainte d'intégrité référentielle interdira l'effacement d'un auteur, tant que dans la base de données il existera au moins un livre se référant à cet auteur. Cette contrainte interdira également d'ajouter un livre si l'auteur n'est pas préalablement inscrit dans la base de données.
Dans les systèmes de gestion de base de données (SGBD), l'isolation est la capacité d'un système d'isoler les modifications dans une transaction en cours de celles faites dans les autres transactions conduites simultanément, jusqu'à ce qu'elle soit complétée. C'est l'une des quatre propriétés ACID d'une base de données.
Java Persistence Query Language (JPQL) est un langage de requête orienté objet indépendant de la plateforme, défini dans la spécification Java Persistence API. JPQL sert à exécuter des requêtes sur des entités persistées en base de données mais en travaillant sur les entités Java correspondant aux tables plutôt que sur les tables elles-mêmes.  Portail de la programmation informatique
En informatique et plus particulièrement dans les bases de données relationnelles, la jointure est l'opération permettant d’associer plusieurs tables ou vues de la base par le biais d’un lien logique de données entre les différentes tables ou vues, le lien étant vérifié par le biais d'un prédicat. Le résultat de l'opération est une nouvelle table. En SQL, une jointure est définie dans la clause FROM, en indiquant le mot clef JOIN pour chaque nouvelle table à joindre à l'une des précédentes et en spécifiant comment, dans un prédicat de jointure introduit par le mot clef ON. Une ancienne syntaxe, remplacée par le JOIN en 1992, consistait à énumérer les tables dans la clause FROM effectuant ainsi un produit cartésien que l'on filtrait par restriction dans la clause WHERE. Cette ancienne syntaxe est aujourd'hui abandonnée du fait de son incapacité à réaliser pleinement les jointures externes. La syntaxe de la jointure est la suivante :  Une extension de l'opération de jointure a consisté à rajouter le concept de jointure dite "naturelle" opérant automatiquement sur des colonnes de même nom dans chaque table en jeu. Syntaxe :  Cette syntaxe est à éviter car elle peut entraîner de multiples erreurs ou problèmes de performances (jointures circulaires involontaires, jointures non voulues en cas d'évolution du modèle, etc.). Les jointures peuvent être classées en équijointures (jointures basées exclusivement sur des égalités entre les colonnes des différentes tables) et théta jointure, c'est-à-dire par inégalité, différences... La plupart du temps les équijointures sont le résultat de la décomposition des relations inhérentes à la modélisation relationnelle des données (sauf forme normale domaine clef). En algèbre relationnelle, une jointure est une composition de relations, tout comme on peut composer des fonctions (par exemple g o f). En effet, les tables définissent des relations entre les différents champs qui les composent.
En informatique, le journal des transactions (en anglais, transaction log, transaction journal, database log, binary log ou audit trail) est la liste des transactions informatiques exécutées sur une base de données. Cette liste de transactions est utilisée pour rétablir l'intégrité de la base de données dans les cas de problèmes logiciels ou matériels du système qui gère la base de données. Physiquement, le journal des transactions est un fichier contenant une copie des modifications apportées à la base de données. Ce fichier est conservé sur un support non volatil pour être accessible même dans un cas de mauvais fonctionnement de l'ordinateur qui le gère. Lors du démarrage d'une base de données, le système de gestion de la base de données s'assure que la base de données a été fermée correctement et que la base de données est dans un état cohérent. Si ce n'est pas le cas, le système de gestion de la banque de données utilise le journal des transactions pour faire un retour en arrière (rollback) sur les transactions qui ne sont pas validées (committed). De plus, toujours en utilisant le journal des transactions, il réapplique les transactions validées dont les changements n'apparaissent pas dans la base de données. Ces corrections sont faites pour assurer l'atomicité et la durabilité des transactions.
Un langage de contrôle de données (LCD ; en anglais data control language, DCL) est un langage de programmation et un sous-ensemble de SQL pour contrôler l'accès aux données d'une base de données.
Le langage de contrôle des transactions (LCT) est un langage de programmation assujetti au SQL. Il est utilisé pour le contrôle transactionnel dans une base de données, c’est-à-dire les caractéristiques des transactions, la validation et l’annulation des modifications. Exemples d'instructions du LCT : COMMIT, SAVEPOINT, ROLLBACK, SET TRANSACTION.
Un langage de définition de données (LDD ; en anglais data definition language, DDL) est un langage de programmation et un sous-ensemble de SQL pour manipuler les structures de données d'une base de données, et non les données elles-mêmes. Il permet de définir le domaine des données, c'est-à-dire l'ensemble des valeurs que peut prendre une donnée : nombre, chaîne de caractères, date, booléen. Il permet aussi de regrouper les données ayant un lien conceptuel au sein d'une même entité. Il permet également de définir les liens entre plusieurs entités de nature différente. Il permet enfin d'ajouter des contraintes de valeur sur les données.
Un langage de manipulation de données (LMD ; en anglais data manipulation language, DML) est un langage de programmation et un sous-ensemble de SQL pour manipuler les données d'une base de données. Ces commandes de manipulation de données doivent être validées à l'issue d'une transaction pour être prises en compte.
Un langage de requête est un langage informatique utilisé pour accéder aux données d'une base de données ou d'autres systèmes d'information. Il permet d'obtenir les données vérifiant certaines conditions (on parle de critères de sélection), comme toutes les personnes qui habitent une ville donnée. Les données peuvent être triées, elles peuvent également être regroupées suivant les valeurs d'une donnée particulière (par exemple on va regrouper toutes les personnes qui habitent la même rue). La grammaire d'un langage de requête est adaptée à la structure des données interrogées. Le langage de requête le plus connu et le plus utilisé est SQL.
Main Core est le nom de code d'une base de données du gouvernement fédéral des États-Unis qui existerait depuis les années 1980 et qui comprendrait des millions de données d'ordre personnel et financier de citoyens américains. Elle ne comporterait des entrées que sur des citoyens pouvant porter atteinte à la sécurité nationale. Les données, qui proviendraient de la NSA, du FBI, de la CIA et d'autres agences moins connues, seraient recueillies et stockées sans supervision judiciaire. La suggestion de créer Main Core proviendrait de la Federal Emergency Management Agency (FEMA) en 1982, à la suite d'un projet du président américain Ronald Reagan, la National security directive (NSD) 69 / National Security Decision Directive (NSDD) 55, qui vise à maintenir le leadership des États-Unis sur le plan international, mis en vigueur le 14 septembre 1982,. En 2008, les données de Main Core porterait sur huit millions d'Américains, plusieurs soupçonnés pour des raisons mineures, citoyens que le gouvernement pourrait décider de surveiller, interroger ou encore emprisonner lors d'une crise. Cette base de données est révélée en mai 2008, puis son existence confirmée en juillet 2008 par Tim Shorrock (en), spécialiste de la politique étrangère américaine.
Un mapping objet-relationnel (en anglais object-relational mapping ou ORM) est un type de programme informatique qui se place en interface entre un programme applicatif et une base de données relationnelle pour simuler une base de données orientée objet. Ce programme définit des correspondances entre les schémas de la base de données et les classes du programme applicatif. On pourrait le désigner par là, « comme une couche d'abstraction entre le monde objet et monde relationnel». Du fait de sa fonction, on retrouve ce type de programme dans un grand nombre de frameworks sous la forme de composant ORM qui a été soit développé, soit intégré depuis une solution externe.
La commande Merge en SQL est une instruction qui permet de mettre à jour ou insérer un ou plusieurs tuples dans une table (ou vue) d'une base de données relationnelle à partir de données sources, selon des conditions. Cette commande a été rajoutée dans le standard SQL:2003, et étendue dans le standard SQL:2008, et permet d'éviter des combinaisons d’instructions Insert, Update et Delete sur la table cible.
La mise à jour perdue (en anglais lost update) est un type d'erreur qui peut apparaître en informatique lorsque plusieurs accès en écriture à une information partagée ont lieu en parallèle. Lorsque deux transactions modifient la même information, les modifications de la première peuvent être recouvertes immédiatement par celles de la seconde si aucune précaution n'est prise contre ce problème; on dit que les mises à jour effectuées par la première transaction ont été perdues. Ce problème peut se poser indépendamment de la forme que revêtent les informations partagées, qu'elles soient dans un fichier, dans une table de base de données ou en mémoire partagée entre plusieurs threads. Ce problème est également distinct de celui de la corruption des données qui peut intervenir en cas de mauvaise synchronisation des écritures, bien que les solutions aux deux problèmes utilisent souvent les mêmes mécanismes de base.
Le modèle entité-association (le terme « entité-relation » est une traduction erronée largement répandue), ou diagramme entité-association ou (en anglais « entity-relationship diagram », abrégé en ERD), est un modèle de données ou diagramme pour des descriptions de haut niveau de modèles conceptuels de données. Il fournit une description graphique pour représenter de tels modèles de données sous la forme de diagrammes contenant des entités et des associations. De tels modèles sont utilisés dans les phases amont de conception des systèmes informatiques. Ils sont utilisés, par exemple, pour décrire les besoins en information et/ou le type d'information qui doit être enregistré dans les bases de données pendant la phase d'élaboration du cahier des charges. La technique de modélisation des données peut être utilisée pour décrire toute ontologie (i.e. une vue globale et des classifications des termes utilisés et de leurs relations) dans un domaine d'intérêt. Dans le cas de la conception par la méthode Merise d'un système d'information construit sur une base de données, le modèle conceptuel de données est, à un stade ultérieur, transformé en modèle logique de données, tel que le modèle relationnel ; puis ce modèle est transformé en modèle physique pendant la phase de conception physique. Quelquefois, ces deux dernières phases sont appelées "conception physique". Cette méthode est employée depuis les années 1970 pour concevoir les bases de données informatiques.
Le modèle objet-relationnel est, en informatique, une manière de modéliser les informations contenues dans une base de données qui reprend le modèle relationnel en ajoutant quelques notions empruntées au modèle objet, venant combler les plus grosses lacunes du modèle relationnel. La technologie objet-relationnelle est née en 1992, elle est donc assez nouvelle sur le marché des SGBD, dominé depuis environ 1970 par les bases de données relationnelles.   Portail de l’informatique
Dans la méthode Merise, le modèle physique des données (MPD) consiste à implanter une base de données dans un SGBDR. Le langage utilisé pour ce type d'opération est le SQL. On peut également faire usage d'un AGL (PowerAMC, WinDesign, etc.) qui permet de générer automatiquement la base de données.
Le modèle relationnel est une manière de modéliser les relations existantes entre plusieurs informations, et de les ordonner entre elles. Cette modélisation qui repose sur des principes mathématiques mis en avant par E.F. Codd est souvent retranscrite physiquement (« implémentée ») dans une base de données.
En informatique, un moteur de base de données (anglais database engine ou storage engine) est un composant logiciel qui contrôle, lit, enregistre et trie des informations dans une ou plusieurs bases de données. Le moteur de base de données est le composant central d'un système de gestion de base de données. C'est un composant essentiel des systèmes d'informations, ainsi que de nombreux logiciels qui manipulent des grandes quantités de données (voir Informatique de gestion),. La majorité des moteurs de base de données sont prévus pour manipuler les bases de données relationnelles. Le système de fichier est une forme primitive de base de données hiérarchique. Un composant moteur inclus dans le système d'exploitation, manipule le système de fichiers.
Une liaison multibases, permet à un composant logiciel de se coupler à une ou plusieurs bases de données relationnelles. Un serveur multibases est intégré dans le SGBD. Le standard ISO RDA, SQL*NET d'Oracle, NETLIB de MS SQL Server, ESQL/DRDA pour DB2 d'IBM sont des exemples de tels serveurs. Le composant logiciel peut accéder aux bases de données via un client multibases. La norme ISO RDA, ou un client CLI du SAG comme ODBC de Microsoft ou JDBC de SUN sont des exemples de tels clients.
Multiversion concurrency control (abrégé en MCC ou MVCC) est une méthode informatique de contrôle des accès concurrents fréquemment utilisée dans les systèmes de gestion de base de données et les langages de programmation concernant la gestion de la mémoire. À cet effet, une base de données ne mettra pas en œuvre des mises à jour par écrasement des anciennes données par les nouvelles, mais plutôt en indiquant que les anciennes données sont obsolètes et en ajoutant une nouvelle "version". Ainsi, plusieurs versions sont stockées, dont l'une seule d'entre elle est la plus récente. Cela évite en outre à la base de données d'avoir à gérer le remplissage des "trous" en mémoire ou sur le disque mais nécessite (généralement) une purge régulière des données obsolètes. Dans le cas des bases de données orientées document comme CouchDB, cela a aussi pour incidence de réécrire une version complète du document à chaque mise à jour, plutôt que de gérer des mises à jour incrémentales constituées de petits morceaux de document liés entre eux et rangés de manière non contigüe. MVCC autorise aussi la création de prise de vue "à un instant donné". En réalité, les transactions avec MVCC utilisent un marqueur temporel (timestamp en anglais) ou un identifiant de transaction pour déterminer l'état de la base à lire. Ce mécanisme permet d'éviter l'usage de verrous dans les transactions car les écritures peuvent être virtuellement isolées des opérations de lecture sur les anciennes versions de la base qui ont été maintenues. Ainsi, considérant une requête en lecture ayant un identifiant de transaction donné, toutes ses valeurs sont consistantes car les opérations d'écriture disposent d'un identifiant de transaction plus élevé. En d'autres termes, MVCC permet à chaque utilisateur connecté de voir une capture de la base. Les modifications apportées ne seront pas visibles par les autres utilisateurs avant que la transaction ne soit validée (commit).
En informatique, NewSQL désigne une catégorie de systèmes de gestion de base de données (SGBD) relationnelles modernes qui cherchent à fournir la même puissance évolutive que les systèmes NoSQL pour le traitement transactionnel en ligne (lecture-écriture), tout en maintenant les propriétés ACID d'un système de base de données traditionnel.
Object Query Language est une extension du langage SQL pour base de données orientée objet, langage de requête utilisé pour interroger des SGBDO, normalisé par l'ODMG. Ce langage est typé (les requêtes retournent des objets), et utilise les règles du polymorphisme.
Un objet d'accès aux données (en anglais data access object ou DAO) est un patron de conception (c'est-à-dire un modèle pour concevoir une solution) utilisé dans les architectures logicielles objet.
Le ODMG (sigle de Object Data Management Group) était un groupe responsable de la normalisation de OQL, disparu en 2001. Ses travaux sur les liens de OQL avec Java ont été supplantés par le JDO de Sun Microsystems.  Portail de l’informatique
L'Open Database License (ODbL) est un contrat licence de base de données favorisant la libre circulation des données. Elle est issue du projet opendatacommons.org de l'Open Knowledge Foundation. Sa traduction en français est le fruit d'une collaboration entre l'association VeniVidiLibri et la Mairie de Paris dans le cadre du projet ParisData.
Une clause ORDER BY en SQL indique qu'une instruction SQL SELECT retourne un jeu de résultats ayant des lignes triées suivant les valeurs d'une ou plusieurs colonnes. Le critère de tri n'a pas besoin d'être inclus dans le jeu de résultats. Les critères de tri peuvent être des expressions incluant – mais non limitées à – des noms de colonnes, des fonctions définies par l'utilisateur, des opérations arithmétiques, or des expressions CASE. Les expressions sont évaluées et leurs résultats sont utilisés pour le tri, c'est-à-dire que les valeurs stockées dans la colonne ou les résultats de l'appel de la fonction sont utilisés. ORDER BY est la seule façon de trier les lignes dans le jeu de résultats. Sans cette clause, le système de base de données relationnel peut retourner les lignes dans un ordre quelconque. Si un ordonnancement est requis, la clause ORDER BY doit être fournie dans l'instruction SELECT émise par l'application. Bien que certains systèmes de base de données permettent la spécification d'une clause ORDER BY dans des sous-requêtes ou des définitions de vues, leur présence y sera sans effet. Une vue est une table relationnelle logique, et le modèle relationnel requiert  qu'une table soit un ensemble de lignes, ce qui implique qu'il n'y a pas d'ordre de tri quel qu'il soit. Les seules exceptions sont les constructions telles que ORDER BY ORDER OF ... (non standardisé en SQL:2003), qui permettent la propagation du critère de tri à travers les sous-requêtes imbriquées. La fonctionnalité de base du standard SQL ne définit pas explicitement un ordre de  tri par défaut pour les Nulls. Avec l'extension T611 du SQL:2003, "Opérations OLAP élémentaires", les nuls peuvent être triés avant ou après toutes les valeurs des données en utilisant les clauses NULLS FIRST ou NULLS LAST de la liste ORDER BY respectivement. Les vendeurs de SGBD n'implémentent pas tous cette fonctionnalité cependant. Les vendeurs qui ne l'implémentent pas peuvent spécifier différents traitements pour le tri des Nulls dans le SGBD. La syntaxe ORDER BY ... DESC triera suivant  l'ordre descendant, sinon l'ordre ascendant sera utilisé (ce dernier peut être spécifié explicitement en utilisant ASC).
L'organisation séquentielle indexée, aussi appelée ISAM, est une manière d'organiser le contenu des fichiers de données qui permet un accès séquentiel et un accès direct aux enregistrements. Ces fichiers comportent un index qui permet l'accès direct aux enregistrements, lors d'opérations de recherche. Cette technique a été popularisée par le service ISAM des ordinateurs IBM en 1966. Les fichiers manipulés par ce service doivent d'abord être remplis avec des données triées. Une zone de débordement sert aux ajouts ultérieurs; ils comportent plusieurs index. Des fichiers séquentiel-indexés ayant une organisation différente sont aussi parfois dénommés ISAM.
Dans une base de données, une partition est une division logique d'une table stockée en plusieurs parties indépendantes. Le partitionnement de tables est généralement effectué pour améliorer la gestion, la performance ou la disponibilité. Chaque partition se retrouve sur des serveurs ou des disques différents. Cela permet également d'obtenir une capacité de base de données supérieure à la taille maximum des disques durs ou d'effectuer des requêtes en parallèle sur plusieurs partitions.
Le partitionnement de la base de données Oracle est un module de division des données d'Oracle, payant et fréquemment utilisé sur des tables de volumétrie importante pour en augmenter les performances.
PL/pgSQL (Procedural Language/PostgreSQL Structured Query Language) est un langage procédural géré par PostgreSQL. Ce langage est très similaire au PL/SQL d'Oracle, ce qui permet de porter des scripts de ou vers Oracle au prix de quelques adaptations.
En informatique, le Problème d'Halloween fait référence à un phénomène dans les bases de données, lorsqu'une opération de mise à jour provoque un changement dans l'emplacement physique d'une ligne, permettant potentiellement à cette ligne d'être parcourue plus d'une fois durant l'opération. Ceci peut même provoquer une boucle infinie dans certains cas où les mises à jour placent continuellement l'enregistrement modifié après le curseur réalisant l'opération de mise à jour. La possibilité de cette erreur dans une base de données a été découverte pour la première fois par Don Chamberlin, Pat Selinger et Morton Astrahan en 1976, le jour d'Halloween, en travaillant sur une requête qui était supposée donner une augmentation de 10 % à chaque employé qui gagnait moins de 25 000 $. Cette requête s'exécuta apparemment avec succès, sans déclencher d'erreur, mais une fois achevée tous les employés dans la base de données gagnaient au moins 25 000 $, parce que la requête continua de leur donner des augmentations jusqu'à ce qu'ils aient atteint ce seuil. Le comportement attendu était que la requête devait parcourir chaque employé avec un salaire inférieur à 25 000 $ exactement une seule fois. En fait, parce que même les enregistrements mis à jour étaient visibles par le moteur d'exécution de la requête et continuaient donc à correspondre au critère de recherche, les enregistrements de salaires étaient traités plusieurs fois, et recevaient des augmentations de 10 % jusqu'à ce qu'ils aient tous atteint 25 000 $. Le nom de ce problème n'est pas représentatif de sa nature, mais lui a été donné en raison du jour de sa découverte. Selon les propos de Don Chamberlin :  « Pat et Morton ont découvert ce problème le jour d'Halloween... Je me souviens qu'ils vinrent à mon bureau et dirent "Chamberlin, regarde ça. Nous devons nous assurer que lorsque l'optimiseur prépare l'exécution d'une mise à jour, il n'utilise pas un index qui soit basé sur le champ en train d'être mis à jour. Comment allons-nous faire cela ?" Il s'avéra que nous étions un vendredi, et nous nous accordâmes "Écoutez, nous ne serons pas capables de résoudre ce problème cet après-midi. Donnons-lui simplement un nom. Nous allons l'appeler le Problème d'Halloween et nous travaillerons dessus la semaine prochaine." Et il s'avéra que depuis, il a toujours été appelé ainsi. »  — Citation de Donald D. Chamberlin, Charles Babbage Institute, OH 329
En informatique, dans la technologie des bases de données, une procédure stockée (ou stored procedure en anglais) est un ensemble d'instructions SQL précompilées, stockées dans une base de données et exécutées sur demande par le SGBD qui manipule la base de données. Les procédures stockées peuvent être lancées par un utilisateur, un administrateur DBA ou encore de façon automatique par un événement déclencheur (de l'anglais "trigger"). Il existe des procédures stockées pour ce qui est de la manipulation de données comme pour le 'tuning de base'.
En informatique, les propriétés ACID (atomicité, cohérence, isolation et durabilité) sont un ensemble de propriétés qui garantissent qu'une transaction informatique est exécutée de façon fiable. Dans le domaine des bases de données, une opération sur les données est appelée une transaction ou transaction informatique. Par exemple, un transfert de fonds d'un compte de banque à un autre, même s'il implique plusieurs actions comme le débit d'un compte et le crédit d'un autre, est une seule transaction. Jim Gray a défini les propriétés qui garantissent des transactions fiables à la fin des années 1970 et a développé des technologies pour les mettre en œuvre automatiquement. En 1983, Andreas Reuter et Theo Härder ont créé l'acronyme ACID pour désigner ces propriétés.
La protection juridique des bases de données est une forme de réglementation relative aux bases de données. Les bases de données ne sont pas uniquement saisies de manière négative par le droit, leur contenu devant respecter la réglementation sur les données personnelles et l’ordre public, mais aussi de manière positive à cause d'enjeux économiques importants et d'investissements souvent lourds. Cette protection a connu un développement important à la fin des années 1990 sous l’impulsion du droit européen. Ainsi la directive communautaire du 11 mars 1996 sur la protection des bases de données, transposée par la loi du 1er juillet 1998, a mis en place une double protection pour les bases de données : une protection par le droit d'auteur et une protection par un droit sui generis, c’est-à-dire un droit spécifique au producteur de base de données. Des licences ouvertes ont été spécifiquement créées pour les bases de données, dont l'Open Database License (ODBL).
Query by Example (abrégé QBE, en français interrogation par l'exemple), est un type d'interface utilisateur servant à effectuer des recherches dans des bases de données relationnelles. Le principe d'une interface QBE est que l'utilisateur présente un exemple du résultat de recherche attendu - sous forme d'une matrice, puis le soumet au SGBD. Celui-ci recherchera alors toutes les données qui correspondent à cet exemple. Les tables de la base de données sont présentées à l'écran, et l'utilisateur peut les manipuler en vue de créer l'exemple,. QBE a été inventé par Moshe Zloof pour le compte de IBM, en 1977.
Un référentiel, c'est un ensemble de bases de données contenant les « références » d'un système d'information. Un référentiel clair, logique et précis est un des gages de bonne interopérabilité d'un système d'information.
En gestion de base de données, une relation de plusieurs à un détermine que plusieurs enregistrements de la table principale sont en relation avec une seule valeur de la table secondaire.
En gestion de base de données, une relation de un à un détermine que pour chaque enregistrement d'une table, il ne peut y avoir que zéro ou un enregistrement d'une autre table qui lui soit lié. Il est intéressant d'utiliser ce type de relation pour cacher certaines données, pour utiliser des paramètres différents sur ces deux tables (par exemple un moteur de stockage différent dans MySQL), ... Ce type de relation peut toutefois être évitée. En effet, une relation de un à un peut souvent être éliminée et remplacée par la fusion des deux tables en relation.
En gestion de base de données, une relation multivaleur détermine que pour chaque enregistrement d'une table, il peut y avoir aucun, un ou plusieurs enregistrements d'une autre table qui lui soit liés. On dit en Anglais une relation many-to-many, plusieurs vers plusieurs, de par la possibilité de lier plusieurs éléments à une entrée. Par exemple, un livre peut être écrit par plusieurs auteurs, et un auteur peut avoir écrit plusieurs livres.
En informatique, la réplication est un processus de partage d'informations pour assurer la cohérence de données entre plusieurs sources de données redondantes, pour améliorer la fiabilité, la tolérance aux pannes, ou la disponibilité. On parle de réplication de données si les mêmes données sont dupliquées sur plusieurs périphériques. La réplication n'est pas à confondre avec une sauvegarde : les données sauvegardées ne changent pas dans le temps, reflétant un état fixe des données, tandis que les données répliquées évoluent sans cesse à mesure que les données sources changent.
La réplication multi-maîtres est une architecture pour la réplication des bases de données permettant aux données d'être stockées sur un groupe d'ordinateurs et mises à jour par n'importe quel membre du groupe. Tous les membres peuvent répondre aux requêtes des clients. Le système de réplication multi-maître est responsable de propager les modifications de données faite par chaque membre et résoudre les conflits provoqués par des modifications concurrentes faites sur des membres différents. La réplication multi-maîtres peut être comparée avec la réplication maître-esclave où un unique membre du groupe est désigné comme le maître pour certaines données et est le seul nœud autorisé à modifier ces données. Les autres membres désirant modifier les données doivent d'abord contacter le nœud maître. Autoriser un unique maître permet d'obtenir plus facilement la cohérence des données entre les différents membres du groupe, mais est moins flexible que la réplication multi-maîtres. La réplication multi-maîtres peut aussi être comparée avec le basculement dans les grappe de serveurs où les serveurs esclaves répliquent les données maître pour préparer la bascule dans le cas où le maître ne fonctionnerait plus. Le maître est le seul serveur actif pour les interactions avec le client. Le but premier de la réplication multi-maîtres est une disponibilité améliorée et des temps de réponse améliorés.
Dans le contexte des bases de données transactionnelles, rollback est un terme anglais qui désigne une méthode permettant dans un contexte défini, d'annuler l'ensemble des requêtes que l'on vient pourtant de réaliser (le fait inverse du commit). On parle alors de transaction : un ensemble de requêtes réalisées en une seule opération atomique. Les traitements réalisés durant cette transaction ne seront pas pris en considération.   Portail des bases de données
SELECT​ est une commande SQL qui permet d'extraire des données des tables d'une base de données relationnelle. Une commande SELECT​ peut obtenir zéro ou plusieurs tuples provenant de tables et de vues. Dû à la nature déclarative du langage SQL, une commande SELECT​ décrit un jeu de résultat voulus, et non la manière de les obtenir. La base de données transforme donc la requête en un plan d’exécution de requête, qui peut varier dans le temps, en fonction de la version du serveur, ou du serveur utilisé.
SQL/MM est une norme pour contrôler des données spatiales dans des systèmes de bases de données relationnelles.  Portail de l’informatique
SQL (sigle de Structured Query Language, en français langage de requête structurée) est un langage informatique normalisé servant à exploiter des bases de données relationnelles. La partie langage de manipulation des données de SQL permet de rechercher, d'ajouter, de modifier ou de supprimer des données dans les bases de données relationnelles. Outre le langage de manipulation des données, la partie langage de définition des données permet de créer et de modifier l'organisation des données dans la base de données, la partie langage de contrôle de transaction permet de commencer et de terminer des transactions, et la partie langage de contrôle des données permet d'autoriser ou d'interdire l'accès à certaines données à certaines personnes. Créé en 1974, normalisé depuis 1986, le langage est reconnu par la grande majorité des systèmes de gestion de bases de données relationnelles (abrégé SGBDR) du marché. SQL fait partie de la même famille que les langages SEQUEL (dont il est le descendant), QUEL (intégré à Ingres) ou QBE (Zloof).
Dans les bases de données relationnelles, une table est un ensemble de données organisées sous forme d'un tableau où les colonnes correspondent à des catégories d'information (une colonne peut stocker des numéros de téléphone, une autre des noms...) et les lignes à des enregistrements, également appelés entrées. Chaque table est l'implémentation physique d'une relation entre les différentes colonnes. Chaque correspondance est définie par une ligne de la table. La notion de table est apparue dans les années 1970 chez IBM avec l'algèbre relationnelle qui est une théorie mathématique en relation avec la théorie des ensembles. Cette théorie a pour but d'éclaircir et de faciliter l'utilisation d'une base de données.
La table DUAL est une table spéciale d'une seule colonne et d'une seule ligne présente par défaut dans toutes les installations du SGBD Oracle. Elle est utilisée généralement pour sélectionner les pseudo-colonnes telles que SYSDATE ou USER. La table contient une seule colonne de type VARCHAR2(1) appelée DUMMY qui a pour unique valeur 'X'.
Le théorème de Codd, démontré par Edgar Frank Codd en 1971, est l’un des théorèmes fondamentaux de la théorie des bases de données. Dans son énoncé le plus simple, il affirme qu’une requête sur une base de données relationnelle est exprimable en calcul relationnel si, et seulement si elle l’est en algèbre relationnelle. Dans le modèle de la base de données relationnelle, une table, ou relation, est vue comme un ensemble (ou multi-ensemble dans la plupart des implémentations) de tuples donc chaque élément correspond à un attribut, ou champ. Deux méthodes existent afin de modéliser mathématiquement une requête : Le calcul relationnel, qui consiste en des requêtes purement déclaratives, basées sur des ensembles, quantificateurs et variables et donc proches de la logique formelle. L’algèbre relationnelle, permettant de construire des requêtes de façon impérative, c’est-à-dire que le client à l’origine de la requête donne au système gérant la base de données une séquence d’instructions permettant de construire le résultat. Par exemple, si l’on imagine une base de données constituée d’une table « films » contenant une liste quelconque de films et de leurs informations, et une table « séances » contenant une liste de séances définies par leur heure, le film projeté et le cinéma où le film est visible, obtenir la liste des films à l’affiche se fait de deux manières radicalement différentes : Le calcul relationnel est la méthode naturelle : on recherche l’ensemble des tuples de la relation « films » tels qu’il existe une entrée de la table « séances » dont l’attribut « film » pointe vers ledit tuple, c’est-à-dire                         {         f         ∈         F         i         l         m         s                    |                  ∃         s         ∈         S         e         a         n         c         e         s                    |                  s         .         i         d         _         f         i         l         m         =         f         .         i         d         }                 {\displaystyle \{f\in Films|\exists s\in Seances|s.id\_film=f.id\}}   . L’algèbre relationnelle, en revanche, impose de chercher à construire le résultat à l’aide notamment de jointures : ici, une procédure possible consiste à joindre les deux tables selon la contrainte « Film.id = Seance.id_film », et projeter le résultat pour ne récupérer que les informations souhaitées. Le théorème de Codd assure que toute requête pouvant s’exprimer en calcul relationnel pourra être traduite en algèbre relationnelle, et donc en un langage de requêtes intelligible par un ordinateur (en particulier le SQL). C’est notamment cette équivalence entre la méthode de requêtes « naturelle », et celle pouvant être utilisée en pratique sur des systèmes automatisés, qui explique le succès rencontré par les systèmes de bases de données relationnelles.
En informatique et plus particulièrement dans le domaine des bases de données, le traitement transactionnel en ligne (en anglais online transaction processing, abrégé en OLTP) est un type d'application informatique qui sert à effectuer des modifications d'informations en temps réel. Ce type d'application est utilisé dans des activités opérationnelles, typiquement des transactions commerciales (opérations bancaires, achats de biens, billets, réservations). Ce type d'application se connecte à des bases de données en lecture et écriture. Ce type d'application est typiquement opposé au traitement OLAP (pour online analytical processing ou traitement analytique en ligne) qui fonctionne en principe en lecture.   Portail des bases de données
Le Transact-SQL (T-SQL) est une extension propriétaire de Sybase et Microsoft au langage SQL. Transact-SQL a été développé à l'origine par la société Sybase, dès les premières versions de son moteur de base de données du même nom. De manière similaire au PL/SQL d'Oracle, Transact-SQL fournissait le moyen d'étendre les fonctionnalités de base du SGBD, via des programmes appelés "procédures stockées". Le code source comme compilé, est en effet stocké dans la base de données, par opposition aux programmes écrits en langage de programmation classique, dont le code source d'une part, le code compilé d'autre part, sont stockés dans des fichiers du système de fichiers. Lorsque Microsoft a souhaité étendre son offre logicielle pour inclure un SGBD relationnel, il a passé des accords avec l'un des acteurs de l'époque, le challenger derrière principalement Oracle, RTI Ingres, Informix : Sybase. Le SGBD Sybase a été porté sur Windows. Microsoft a peu à peu acquis le savoir-faire en matière de SGBD relationnel, et développé son propre moteur de base de données, Microsoft SQL Server, à partir de la souche Sybase. Sybase de son côté a fait évoluer son SGBD, devenu depuis Adaptive Server Enterprise. Transact-SQL est ainsi aujourd'hui le langage de programmation associé à la fois aux SGBD Microsoft SQL Server et Sybase Adaptive Server Enterprise. Par rapport au SQL, le T-SQL ajoute les fonctionnalités suivantes : Éléments de programmation procédurale (boucle, conditions...) ; La possibilité de créer et d'utiliser des variables locales ; Des fonctions de manipulations de chaîne de caractères, de dates et de calculs mathématiques.
Un triplestore est une base de données spécialement conçue pour le stockage et la récupération de données RDF (Resource Description Framework). Tout comme une base de données relationnelle, un triplestore stocke des données et il les récupère via un langage de requête. Mais contrairement à une base de données relationnelle, un triplestore ne stocke qu'un seul type de données : le triplet. Elle n'a donc pas besoin de phase d'initialisation pour enregistrer de nouvelles données. C'est-à-dire qu'elle n'a pas besoin de créer des tables comme dans une base de données relationnelle. De plus, un triplestore est optimisé pour le stockage d'un grand nombre de triplets et pour la récupération de ces triplets à l'aide du langage de requête SPARQL. Certains triplestores peuvent stocker des milliards de triplets RDF et demain des téras. La performance d'un triplestore peut être mesurée avec le Benchmark Lehigh University (LUBM), ou avec des données réelles provenant d'UniProt.
Tutorial D est un langage d'interaction avec les bases de données relationnelles, conçu par Hugh Darwen (en) et Christopher J. Date (en) dans leur document The Third Manifesto (en) en 1994, pour être plus simple et plus cohérent que le traditionnel SQL.
UPDATE​ est une commande SQL qui modifie un ou plusieurs tuples dans une table d'une base de données relationnelle.
Le varchar (issu de l'anglais variable character field) est un type de données utilisé par les systèmes de gestion de base de données (SGBD) pour allouer de l'espace mémoire dynamiquement aux chaînes de caractères stockées, c'est-à-dire sans allouer une quantité immuable de mémoire. Ceci permet d'optimiser l'espace utilisé lorsque la longueur de la chaîne est variable mais qu'il est possible d'estimer une longueur maximale qui ne sera jamais dépassée.  Portail de la programmation informatique  Portail de l’informatique
Very Large DataBase (VLDB) est une base de données contenant plus de 1 téraoctet de données.
Une vue dans une base de données est une synthèse d'une requête d'interrogation de la base. On peut la voir comme une table virtuelle, définie par une requête. Les avantages des vues sont : d'éviter de taper une requête très longue : la vue sert à donner un nom à la requête pour l'utiliser souvent, de masquer certaines données à certains utilisateurs. En SQL, les protections d'une vue ne sont pas forcément les mêmes que celles des tables sous-jacentes.
En informatique, dans les systèmes de gestion de base de données de type relationnel, une vue est une table virtuelle représentant le résultat d’une requête sur la base. Comme son nom l'indique et à la différence d'une vue standard, dans une vue matérialisée les données sont dupliquées. On l’utilise essentiellement à des fins d'optimisation et de performance dans le cas où la requête associée est particulièrement complexe ou lourde, ou pour faire des réplications de table. La fraîcheur des données de la vue matérialisée dépend des options choisies lors de sa création. Le décalage entre les données de la table maître et la vue matérialisée peut être nul (rafraîchissement synchrone) ou d'une durée planifiée : heure, jour, etc. Suivant le contexte il existe différents types de vue matérialisée possibles : sur clé primaire, rowid (identifiant unique des tuples), et plus ou moins complexes : avec fonctions d'agrégation, sous-requêtes, jointures, etc.
Yeram Sarkis Touloukian (28 décembre 1920 - 12 juin 1981) est un professeur de thermodynamique, fondateur et directeur de centre de Recherches sur les propriétés thermophysiques de l'Université Purdue, aujourd'hui CINDAS LLC (Center for Information and Numerical Data Analysis and Synthesis). Son nom est associé aux synthèses sur les propriétés thermophysiques des matériaux.
Facebook Artificial Intelligence Research (FAIR) est un organisme regroupant plusieurs laboratoires de recherche en intelligence artificielle rattaché à l'entreprise Facebook.
Le Thomas J. Watson Research Center est le siège central de la recherche d'IBM avec au total huit laboratoires en six pays. Il se compose de trois localisations différentes avec quatre ensembles d'imeubles. Le siège est à Yorktown Heights (Comté de Westchester, New-York ), deux autres bâtiments à Hawthorne (New York) (en) et un autre à Cambridge (Massachusetts). Le centre porte les noms de Thomas John Watson sénior et Thomas John Watson junior.
En informatique, la comparaison de fichiers consiste à comparer leur contenu, en isolant leurs différences de leur contenu commun. Le résultat de la comparaison peut être affiché en environnement graphique GUI, en mode texte, ou comme partie de tâches plus larges en réseau, dans un système de fichiers ou un système de gestion de versions. Les programmes de comparaison de fichiers les plus largements utilisés sont diff, cmp (Unix) (en) ou WinMerge. De nombreux éditeurs de texte et logiciels de traitement de texte utilisent la comparaison de fichiers pour mettre en évidence les modifications apportées à un document.
Cet article compare le support et l'implémentation des normes du protocole IPv6 selon les systèmes d'exploitation.
Le tableau ci-dessous compare les caractéristiques générales et techniques des systèmes d'exploitation les plus connus. Pour plus d'informations sur un système, regardez l'article correspondant. Les distributions GNU/Linux ne peuvent être représentées ici à cause de leur nombre ; elles sont regroupées en une entrée (du nom de GNU/Linux) dans le tableau. Voyez la page Comparaison des distributions Linux pour une comparaison détaillée des distributions Linux.
L’ACM International Collegiate Programming Contest (abrégé en ACM-ICPC) est un concours annuel de programmation ouvert aux étudiants. À l’origine américain, il rassemble aujourd’hui des universités du monde entier.
Le concours Castor, ou Castor informatique, ou Bebras (qui signifie castor en lituanien), est une compétition d'informatique annuelle, créée en Lituanie en 2004 et organisée dans 30 pays, avec 734 427 participants en 2013. Son objectif est d'éveiller l'intérêt pour les sciences du numérique auprès des jeunes. Il montre comment l'informatique peut être amusante et variée, en couvrant plusieurs thèmes comme la représentation de l'information, la pensée algorithmique, des jeux de logique, etc. Il aide à comprendre les fondements de l'informatique, pas uniquement ses usages. Le principe général du concours Castor s'inspire de celui du concours Kangourou en mathématiques.
HackerRank est une entreprise spécialisée dans les concours de programmation pour développeurs et entreprises. Elle a une communauté en ligne d'1 million de programmeurs. Les concours de programmation d'HackerRank utilisent une multitude de langages (Java, C++, PHP, SQL) et couvrent de nombreux domaines de l'informatique. Côté utilisateur, lorsqu'un programmeur soumet une solution à un challenge, sa proposition est notée sur la base du temps mis pour à trouver cette solution mais aussi sur la justesse de la proposition. Les programmeurs intègrent ainsi le classement mondial de HackerRank et gagnent des badges liés à leurs travaux, ce qui permet de nourrir la compétition entre eux. En plus des programmations individuelles, HackerRank accueille aussi des concours (généralement appeés « CodeSprints » par le site) au cours desquels les utilisateurs s'affrontent  sur les mêmes challenges et dans un temps imparti. Ils sont classés à l'issue du concours. HackerRank apparaît comme un leader dans son domaine à l'heure de la montée de la gamification dans un univers de la programmation toujours plus compétitif. Le côté utilisateur de leur site web est gratuit pour les codeurs.
L'Olympiade suisse d'informatique (Swiss Olympiad in Informatics) est un concours suisse d'informatique pour les jeunes de moins de vingt ans. L'olympiade suisse d'informatique est aussi le nom porté par le groupe qui organise les Olympiades. Le concours existe depuis 1996, les lauréats peuvent participer aux Olympiades internationales d'informatique (International Olympiad in Informatics).
Les Olympiades internationales d'informatique ou IOI (International Olympiad in Informatics) sont une compétition annuelle en sciences informatiques (algorithmique) rassemblant des lycéens et collégiens du monde entier. Chaque pays peut envoyer jusqu'à quatre candidats. Depuis les cinq dernières années, leur nombre est compris entre 299 et 308. Ces derniers participent à deux épreuves de cinq heures qui regroupent chacune trois problèmes de nature algorithmique. Les programmes des candidats sont ensuite soumis à une batterie de tests permettant de déterminer le score de chacun. Enfin, les meilleurs sont récompensés : la moitié des candidats reçoit une médaille (le rapport Or:Argent:Bronze est de 1:2:3).
Prologin est une association qui cherche à promouvoir l'informatique et l'algorithmique auprès des étudiants. Elle organise chaque année le concours du même nom, aussi appelé le Concours national d'informatique. Ce concours est ouvert aux jeunes francophones de l'Union européenne âgés de 20 ans ou moins. L'association a reçu le Google RISE award 2014.
Le débat « Linux ou GNU/Linux » est une controverse divisant les partisants du logiciel libre sur le nom à donner aux systèmes d'exploitation fondés sur le système GNU et un noyau Linux. Les plus nombreux (avec le grand public) l'appellent simplement « Linux », les autres (peut-être plus proches du projet GNU) l'appellent « GNU/Linux ».
La convergence numérique a pour objectif de regrouper le maximum de fonctionnalités en utilisant un minimum de canaux et de transcripteurs. Le concept de convergence numérique repose sur la numérisation des informations relatives à différentes catégories de service (en particulier : téléphonie, informatique, audiovisuel), ce qui permet ensuite de traiter ces informations relatives avec des systèmes communs et de les transmettre sur des réseaux communs. Historiquement chaque technologie avait ses propres canaux et ses propres appareils : le téléphone par les câbles composées de fils de cuivre, interconnectés par des centraux téléphoniques d'abord manuels puis électromécaniques avant de devenir électroniques ; la télévision par les antennes « râteau » dont les émissions étaient diffusées par de puissants émetteurs nationaux ; La musique par le « disque microsillon », initialement gravés par des systèmes mécaniques ; Les images par la photographie argentique sur papier.
On appelle « révolution numérique » (ou plus rarement « révolution technologique » ou « révolution Internet ») le bouleversement profond des sociétés survenu globalement dans les nations industrialisées (notamment Europe occidentale, Amérique du Nord, Japon) et provoqué par l'essor des techniques numériques, principalement l'informatique et Internet. Cette mutation se traduit par une mise en réseau planétaire des individus, de nouvelles formes de communication (courriels, réseaux sociaux) et une décentralisation dans la circulation des idées.
Le couplage téléphonie informatique ou CTI (Computer telephony integration) regroupe l'ensemble des concepts, des standards, des techniques et des fonctionnalités permettant ou facilitant le dialogue entre un appareil informatique et un appareil téléphonique. Ce terme décrit principalement les interactions menées à partir d'un poste de travail dans le but d'améliorer la productivité de l'utilisateur, bien qu'il puisse également décrire des fonctionnalités sur serveur comme le routage automatique des appels. Il s'inscrit dans la logique de convergence numérique. Ce dispositif reliant un centre d'appels interne ou externe au système informatique d'une entreprise, apporte aux sociétés la possibilité d'utiliser les ressources du système d'informations et d'Internet, afin d'améliorer le service rendu aux clients et la productivité d'un centre d'appels. Ceci est rendu possible par l'automatisation de certaines tâches et la possibilité d'instaurer une personnalisation poussée de la relation avec le client grâce aux informations contenues dans les applications métiers CRM & ERP.
Le MultiService Access Node (MSAN) est une nouvelle technologie de télécommunications qui permet de rapprocher les équipements des clients, ce qui autorise des débits plus élevés et intégrant l'ADSL et la voix ainsi que certains services comme la visiophonie, conférence à trois, etc.
1024 est le bulletin de la Société informatique de France (SIF).
The Angry Video Game Nerd (Le Nerd furieux du jeu vidéo, aussi connu sous les initiales AVGN) est une web-série de critiques humoristiques d'anciens jeux vidéo mettant en vedette et crée par James Duncan Rolfe. L'émission tourne autour des commentaires du personnage du Nerd à propos de vieux jeux qu'il considère de médiocre qualité, ayant parfois une difficulté exagérée ou ayant été mal conçus en général. Le personnage interprété par James, The nerd, est un fanatique de jeux vidéo colérique et grossier. Lors des critiques, le personnage consomme de l'alcool, souvent de la bière, et fait usage d'une panoplie de blasphèmes. La série débute en 2004, et est publiée à l'origine sur le site Cinemassacre. Plus tard en 2006, Mike Matei convainc James de publier les vidéos sur YouTube. Il devient peu après partenaire avec ScrewAttack, puis avec GameTrailers pour finalement retravailler exclusivement avec Cinemassacre. À l'origine intitulée The Angry Nintendo Nerd, l'émission est renommée The Angry Video Game Nerd pour éviter d'avoir des problèmes de droits avec Nintendo, ainsi que pour diversifier davantage les jeux critiqués. Un jeu vidéo basé sur la série, nommé The Angry Video Game Nerd Adventures, sort sur PC en 2013 puis est porté sur Wii U et sur Nintendo 3DS en 2015. Un long-métrage basé sur la série, nommé Angry Video Game Nerd: The Movie, sort à l'été 2014.
Animateur de communauté ou CM, l'abrégé de community manager, est un métier qui consiste à animer et à fédérer des communautés sur Internet pour le compte d'une société, d'une marque, d’une célébrité ou d’une institution. Profondément lié au web 2.0 et au développement des réseaux sociaux, le métier est aujourd'hui encore en évolution. Le cœur de la profession réside dans l'interaction et l'échange avec les internautes (animation, modération) ; mais le gestionnaire de communauté peut occuper des activités diverses selon les contextes. L'appellation community manager est intégrée au Larousse 2016, confirmant l'utilisation de cet anglicisme dans la langue française.
Le terme architecture de participation décrit la nature de systèmes conçus pour faire contribuer ses utilisateurs, tels que l'open source ou les Wikis.
En informatique, l'avatar désigne la représentation informatique d'un internaute, que ce soit sous forme 2D, (sur les forums et dans les logiciels de messagerie) ou sous forme 3D (dans les jeux vidéo, par exemple). Le terme avatar peut également référer à la personnalité en rapport avec le nom d'écran d'un internaute.
Un buzzword est un terme ou une expression de jargon qui est utilisée, pendant une certaine période, comme slogan pour désigner une nouveauté (technologie, produit, concept, etc.) et ainsi attirer l'attention sur cette nouveauté. Son utilisation donne l'impression qu'il s'agit de quelque chose d'important et à la mode ainsi qu'une impression de compétence auprès du public (exemple en français : « synergie » ; exemple en anglais : « business model »). Le mot buzzword lui-même est un buzzword. Il est apparu en 1946 en anglais (où il s'écrit également buzz word). Dans cette langue, on emploie également buzz phrase lorsqu'il s'agit non pas d'un seul mot mais d'une expression. Selon divers dictionnaires papier ou en ligne, buzzword peut se rendre en français par « mot à la mode » (Reverso, Larousse, Google), « expression à la mode » (Reverso), « mot en vogue » (Wordreference), « mot branché » (Word Watch Civilisation 2000)  ou « mot d'ordre » (Ultralingua). Le site Linguee recense « terme en vogue », « vocable à la mode », « beau mot » et « formule ronflante », entre autres. L'équivalence buzzword/mot à la mode se rencontre sous la plume de quelques auteurs : « la mondialisation […] est le mot à la mode, le buzzword » (Thierry Dutour, 2004), « le développement durable doit devenir plus qu’un "buzzword" (mot à la mode) » (Roche Ingénieurs-conseils, novembre 2007). La traduction recommandée par l'Office québécois de la langue française est également « mot à la mode ».  
Camel case (de l'anglais, littéralement « casse de chameau ») est une pratique qui consiste à écrire un ensemble de mots en les liant sans espace ni ponctuation, et en mettant en capitale la première lettre de chaque mot. Utilisée en programmation informatique, où elle facilite la lisibilité lorsque l'espace est un caractère interdit, cette façon d'écrire est aussi devenue une mode, le marketing ayant généré de nombreuses marques ou noms de produits de cette forme, comme PlayStation, QuickTime et MasterCard, eBay, iPhone, etc. Ce terme fait référence aux creux et bosses d'un dos de chameau que représente l'alternance de capitales et de bas-de-casse dans les mots écrits de cette manière. En informatique, le camel case fait par défaut référence au lower camel case, où la première lettre est en bas-de-casse. Le Pascal case fait lui référence à la version commençant par une capitale.
Code et autres lois du cyberespace est un livre de Lawrence Lessig paru en 1999. Il est connu en anglais sous le titre de Code: Version 2.0 (en) (version remaniée de Code and Other Laws of Cyberspace). Il traite de la différence entre les lois dans le monde physique et dans le monde virtuel, sujet de nombreux débats depuis l'apparition de l'Internet. Code: Version 2.0 (en) (en) existe en deux versions.
L'organisation CryptoParty est un mouvement populaire mondial visant à enseigner au grand public les bases de la cryptographie pratique telle que l'utilisation de réseaux anonymes comme Tor, de réseaux privés virtuels, d'outils de chiffrement comme TrueCrypt ou de signature de correspondances ou de données. Le projet consiste principalement en l'organisation d'ateliers publics. Ces rencontres peuvent aussi être l'occasion de signature de clés.
Cyber- est un préfixe à la mode à partir de la deuxième moitié du XXe siècle. Son usage est consécutif au développement exponentiel de l'informatique et de la robotique, plus généralement à l'avènement de la "révolution numérique", qui en est la synthèse. Il est tiré du mot grec Kubernêtikê signifiant « gouvernail ». Ce préfixe est présent notamment dans cybernétique, cyberespace, cybertexte...
Apparu au début des années 1990, le terme cyberculture désigne à la fois un certain nombre de productions culturelles et un nouveau rapport à la culture en général, notamment par les internautes. La cyberculture succède à un certain nombre d'autres cybertermes dont elle est censée faire l'addition, tels que cyberpunk, cyberespace, etc. et qui ont pour origine lointaine la cybernétique.
Les cypherpunks (mot-valise composé à partir des mots anglais cipher (chiffrement) et punk sur le modèle de cyberpunk) forment un groupe informel de personnes intéressées par la cryptographie. Leur objectif est d'assurer le respect de la vie privée par l'utilisation proactive de la cryptographie. Le terme cypherpunk a été inventé par Jude Milhon, se voulant un jeu de mot pour décrire des cyberpunks qui avaient recours à la cryptographie. Cypherpunk, cypherpunks ou cpunks sont aussi le couple login/mot de passe de comptes créés sur des sites web requérant un enregistrement. Ils sont alors utilisés par des utilisateurs qui ne souhaitent pas divulguer de données personnelles.
Antoine Daniel est un vidéaste Web, monteur, ingénieur du son et comédien français né le 23 avril 1989 à Enghien-les-Bains. Il est notamment connu pour avoir créé l'émission humoristique What The Cut !? qu'il présente depuis le 1er mars 2012 sur YouTube,. Début janvier 2018, sa chaîne YouTube comptabilise plus de 2,7 millions d'abonnés et plus de 283 millions de vues.
Le démarrage d'un ordinateur (boot en anglais) est la procédure de démarrage d’un ordinateur et comporte notamment le chargement du programme initial (l’amorçage ou boostrap en anglais). On distingue : le « démarrage à froid » (cold boot), obtenu en allumant la machine, ou en l’éteignant puis en la rallumant ; du « démarrage à chaud » (warm boot), ou « re-boot », obtenu en rechargeant le programme initial ; il ne s’agit pas d’un redémarrage au sens strict (coupure puis remise de l’alimentation électrique) ; l’option est présente au niveau du système d’exploitation.
L'enfouissement de jeux vidéo par Atari est un événement qui s'est déroulé en septembre 1983 dans une décharge située à Alamogordo au Nouveau-Mexique. En 1983, l'entreprise de jeu vidéo Atari enfouit de nombreuses cartouches de jeu vidéo et des consoles de jeu invendues dans la décharge d'Alamogordo, à la suite de mauvais résultats commerciaux. Les causes de cette action sont essentiellement économiques : il s'agit de réduire les stocks afin de bénéficier d'allègements fiscaux. Selon la presse de l'époque, les jeux vidéo enfouis sont notamment E.T. the Extra-Terrestrial, l'un des plus grands échecs commerciaux de l'histoire du jeu vidéo, et la version de Pac-Man sortie sur Atari 2600, succès commercial mais décrié par la critique. Sitôt l'opération d’enfouissement rapportée par la presse, des doutes sont émis sur la véracité et sur l'étendue des faits ; certains considèrent pendant longtemps qu'il s'agit d'une légende urbaine. Cet événement, cependant devenu une sorte d'icône culturelle, symbolisant le krach du jeu vidéo de 1983, est le point d'orgue d'une année fiscale désastreuse pour Atari, finalement revendue en 1984 par sa société mère Warner Communications. Le 26 avril 2014, dans le cadre d'un documentaire, des centaines de copies du jeu vidéo E.T. the Extra-Terrestrial, ainsi que d'autres jeux Atari, sont déterrés à Alamogordo. Cette découverte confirme ainsi un fait considéré jusqu'alors comme une légende. En novembre 2014, une vente sur eBay permet d'écouler des cartouches de jeux divers pour un total de 37 000 dollars. Une cartouche E.T. est alors vendue au prix de 1 537 dollars.
Un évangéliste technologique (terme traduit de l'anglais « technology evangelist » par certaines entreprises) est une personne qui essaye de rassembler une masse critique de personnes adhérant à une technologie, dans le but de la consacrer comme standard, au sein d'un marché dépendant de l'effet de réseau.
Le terme fake (de l'anglais : faux) désigne globalement quelque chose de faux, de truqué. Il a plusieurs sens dans le domaine de l'informatique et des réseaux.
Une foire aux questions, par rétroacronymie à partir de l’acronyme anglais FAQ pour frequently asked questions (littéralement « questions fréquemment posées »), est une liste faisant la synthèse des questions posées de manière récurrente sur un sujet donné, accompagnées des réponses correspondantes, que l’on rédige afin d’éviter que les mêmes questions soient toujours reposées, et d’avoir à y répondre constamment. Cette pratique est essentiellement présente sur Internet, initialement sur Usenet, où elle tient de la tradition. Dans son sens premier, ce terme se réfère exclusivement aux listes de questions et de réponses couramment rencontrées à propos d'un sujet précis. Toutefois, par abus de langage, il est parfois utilisé pour : un seul couple question / réponse ; des listes de questions / réponses qui ne sont pas toutes fréquemment posées ; tout document destiné à expliciter un certain sujet (mode d’emploi d’un appareil, analyse d’un texte, etc.). Il y a pléthore de FAQ disponibles sur les sujets les plus variés. Certains sites les cataloguent et permettent de faire des recherches sur celles disponibles (Internet FAQ Consortium (en)).
La définition du geek peut varier d'un individu à l'autre, mais la définition qui en ressort le plus souvent est celle-ci : un geek (/gi:k/) ou guic, -cque est une personne passionnée par un ou plusieurs domaines précis, plus souvent utilisé pour les domaines liés aux « cultures de l'imaginaire » (certains genres du cinéma, la bande dessinée, les jeux vidéo, les jeux de rôles, etc.), ou encore aux sciences, à la technologie et l'informatique. Du fait de ses connaissances pointues, le geek est parfois perçu comme trop cérébral. Le mot a été peu à peu utilisé au niveau international sur Internet de manière revendicative par les personnes s'identifiant comme tel. Le terme a alors acquis une connotation méliorative et communautaire. Avec le succès commercial des gadgets de techniques avancées, une personne qui aime de tels objets voudra s'autoproclamer « geek », bien que cela ne corresponde ni au sens premier (péjoratif) ou second (passionné) du terme. Il y a souvent confusion entre les geeks, les nolifes, les Gamers, les Otaku et les nerds. Son comportement d'avant-garde peut en faire un lead user typique. Cependant, bien que le geek ne soit pas à confondre avec le nolife, ou freak, il existe différents niveaux d'intensité du geek, qui rapproche parfois la personnalité de certains geeks à des nolifes, ou des freaks.
Le Geek Code est une suite de lettres et de symboles que les geeks utilisent pour renseigner les autres geeks sur leur personnalité, leur apparence, leurs domaines d'intérêt et leurs opinions. L'idée est qu'un geek peut écrire (encoder) tout ce qui le rend unique, différent des autres geeks, dans un format très compact : le Geek Code. Après quoi, d'autres geeks peuvent lire le Geek Code et le déchiffrer, pour en déduire à quoi ressemble l'auteur, ce qu'il aime, etc. Ce code est considéré très geek : difficile à lire, il évoque bien l'archétype du message informatique incompréhensible. Le Geek Code a été inventé en 1993 par Robert Hayden. Il est défini sur www.geekcode.com. La traduction du Geek Code est assez peu accessible, celle-ci étant contraire au principe même du Geek Code selon certains, cependant, certains programmes permettent de retrouver les propositions (en anglais) sélectionnées par l'auteur du Geek Code choisi (par exemple geekcode, disponible dans le gestionnaire de paquets Debian : sudo apt-get install geekcode). À une époque, parmi certains utilisateurs de l'internet, il était courant d'ajouter son Geek Code dans sa signature, mais cette époque est aujourd'hui largement révolue. Pour plusieurs raisons, une bonne partie de la définition du Geek Code semble bien vieillote au goût du jour. Par exemple, le World Wide Web y est décrit comme quelque chose de « relativement nouveau et peu compris. » Sans doute toujours peu compris de nos jours, mais certainement plus nouveau. Diverses tentatives ont été faites de relancer des versions modifiées du Geek Code ; la dernière en date est Geek Code 3.20 qui vise à donner un permabloc[Quoi ?] à tous les geeks. Une fois qu'il a créé son Geek Code, le geek peut le replacer où bon lui semble. Quand cela était la mode, on pouvait en trouver dans les courriels, des sites web, des créations artistiques, des commentaires au milieu des codes sources de programmes et même sur des lettres ou des T-shirts. Aujourd'hui, on en trouve surtout sur des pages de présentation d'auteurs de sites web.
Le glider ou « symbole des hackers » a été proposé pour la première fois en octobre 2003 par Eric S. Raymond, qui a mis en évidence le manque de symbole reconnaissable pour la culture hacker. Cela ne fait pas référence aux crackers, mais bien à la culture hacker autour de BSD, MIT, GNU, Linux, Perl… Ainsi que la communauté autour du logiciel libre et de l'open source. Raymond suggéra que « par l'utilisation de ce symbole, vous exprimez votre accord avec les objectifs des hackers, leurs valeur, ainsi que leur mode de vie ». L'image représente la formation du planeur dans le jeu de la vie.
Gravatar (abréviation de globally recognized avatar) est un service de centralisation d'avatar créé par Tom Preston-Werner.
Il existe une tradition datant du début des années 1970 (et donc dès les débuts du travail courant sur écran d'ordinateur) chez les programmeurs, qui consiste à défendre son logiciel éditeur de texte favori avec une passion qui n'est pas sans rappeler à certains celle du fanatisme religieux. De nombreuses flamewars se sont déroulées entre des groupes qui s'insultaient les uns les autres en soutenant que l'éditeur de texte de leur choix était l'outil parfait pour l'édition.
Le Guide du voyageur galactique (titre original : The Hitchhiker's Guide to the Galaxy, abrégé notamment en H2G2) est une œuvre de science-fiction humoristique multidisciplinaire imaginée par l’écrivain britannique Douglas Adams. Son nom provient de l’objet symbolique de la série : un livre électronique intitulé Le Guide du voyageur galactique. Née en 1978 sous forme de feuilleton radiophonique, l’œuvre a depuis été déclinée dans de nombreux médias au cours de différentes adaptations : romans, pièces de théâtre, série télévisée, jeux vidéo, bande dessinée, long métrage pour le cinéma.
Le Guide du voyageur galactique (The Hitchhiker's Guide to the Galaxy) est un livre fictif imaginé par l'écrivain britannique Douglas Adams dans sa saga du même nom. Élément central de la série, dans laquelle il sert de fil rouge, c'est un objet électronique, connecté au réseau Sub-Ether et utilisé aussi bien en tant qu'encyclopédie qu'en tant que guide de voyage. Très apprécié des astrostoppeurs, en complément d'un poisson Babel et d'une serviette, il est introduit de manière quasiment identique dans les différentes adaptations de l'œuvre, dont le prologue du premier tome :  « C'était sans doute l'ouvrage le plus remarquable jamais publié par les éditeurs de la Petite Ourse. [...] Auprès de bon nombre de civilisations parmi les plus peinardes des confins orientaux de l'anneau galactique, Le Guide du voyageur galactique a même supplanté la grande Encyclopædia galactica comme dépositaire classique de la sagesse et de la connaissance. [...] Primo, il est légèrement moins cher et, secundo, sur sa couverture on peut lire en larges lettres amicales la mention : PAS DE PANIQUE »  — Douglas Adams, The Hitchhiker's Guide to the Galaxy Dans les adaptations audio et vidéo du Guide du voyageur galactique, le Guide est généralement interprété en voix-off. Peter Jones est le premier acteur a lui avoir prêté sa voix, dans le premier feuilleton radio en 1978. Il reprend son rôle dans le vinyle, la seconde phase du feuilleton radio et la série télévisée. En 2004, pour les phases trois à cinq du feuilleton, le rôle est assuré par William Franklyn. Pour l'adaptation cinématographique de 2005, la voix du Guide est confiée à Stephen Fry. Dans les versions visuelles de l'œuvre, la voix off est souvent accompagnée d'animations graphiques, réalisées par Rod Lord pour la série télévisée et par le collectif Shynola (en) pour le film. En avril 1999, Douglas Adams lance avec sa compagnie The Digital Village le site web h2g2.com, une encyclopédie collaborative en ligne inspirée du Guide (« The guide to life, the universe and everything, written by you »). Le site est racheté par la BBC en 2001 et redevient indépendant en 2011.
Le Guide du voyageur galactique ou Le Guide galactique au Québec (titre original : The Hitchhiker's Guide to the Galaxy, souvent abrégé en H2G2) est le premier volume de la « trilogie en cinq tomes » H2G2 imaginée par Douglas Adams. Publié en 1979 et adapté des quatre premiers épisodes du feuilleton radio The Hitchhiker's Guide to the Galaxy écrit par Douglas Adams un an plus tôt, le roman fut réécrit en français par Jean Bonnefoy en 1982 qui ajouta de nombreux jeux de mots et modifia considérablement le niveau de langage de l'original.  
Le hexspeak, du français hexadécimal et de l'anglais « speak », est une écriture utilisant uniquement les caractères du système hexadécimal : 0123456789ABCDEF. En programmation, le hexspeak permet de créer des nombres magiques faciles à mémoriser. Les chiffres peuvent ne pas être utilisés du tout, comme dans 0xDEADBEEF (dead beef) et 0xCAFEBABE (café babe). 0xDEADBEEF est parfois utilisé comme remplissage pour de la mémoire non-initialisée (à l'allocation) ou effacée (après désallocation). 0xCAFEBABE sont les 4 premiers octets d'un fichier .class (du langage JAVA). Les chiffres peuvent être utilisés pour leur ressemblance graphique avec des lettres : 0 pour O, 1 pour I ou pour L, 5 pour S, 6 pour G, 7 pour T, comme dans 0xFACEB00C ou 0xC01055E (pour colosse). Certains chiffres peuvent être utilisés pour leur consonance en anglais, 8 pour ate comme dans 0xDEFEC8ED (pour "defecated"). À la différence du Leet speak, le hexspeak n’a pas pour objectif de compliquer la lecture au néophyte, ce n'est qu'un moyen de choisir un nombre pour symboliser quelque chose par une astuce mnémotechnique qui rend ce nombre facilement reconnaissable et vérifiable.
Un howto (de l'anglais how to = comment faire) est un document, souvent court, décrivant comment réaliser certaines tâches. Ils sont généralement créés dans le but d'aider les moins expérimentés. Ainsi, dans une volonté de simplification, ils mettent de côté certains détails réservés aux experts. On trouve un nombre important de howtos ou de « mini-howtos » pour la réalisation de certaines tâches sous Linux. Les traductions des HOWTOs (notamment le HowTo d'installation par René Cougnenc) ont eu un rôle déterminant dans la diffusion initiale de Linux en France. Elles sont actuellement réalisées par les volontaires du groupe traduc.org.
L'identité numérique (IDN) peut être définie comme un lien technologique entre une entité réelle (personne, organisme ou entreprise) et des entités virtuelles (sa ou ses représentation(s) numériques). Le développement et l'évolution des moyens de communication, au travers notamment de la multiplication des blogs et des réseaux sociaux, changent le rapport de l'individu à autrui. Ainsi, l'identité numérique permet l'identification de l'individu en ligne et la mise en relation de celui-ci avec cet ensemble de communautés virtuelles qu'est Internet. Dès lors, l'identité numérique peut être divisée en trois catégories (Modèle de l'Identité Numérique, Georges, 2009). Tout d'abord l'identité déclarative, qui se réfère aux données saisies par l'utilisateur comme son nom, sa date de naissance, ou autres informations personnelles directement renseignées par l'individu. Puis, l'identité agissante, qui est indirectement renseignée par les activités de l'utilisateur sur la toile. Dernièrement, l'identité calculée, qui résulte d'une analyse de l'identité agissante par le système, comme le nombre de like, le nombre de communautés virtuelles dans lesquelles l'individu évolue ou bien son nombre d'amis sur les réseaux sociaux. Le décalage ou du moins les divergences qui peuvent subsister entre l'identité déclarative et l'identité agissante soulèvent une question majeure. Qui est vraiment l'individu présent sur la toile ? Enfin, Internet étant accessible par tous et offrant de plus en plus de services, se pose le problème de la sécurité de l'information et plus particulièrement des données personnelles. La gestion de l'identité numérique devient alors un enjeu majeur.
L'idéologie californienne (The Californian Ideology) est une critique du néolibéralisme dotcom faite par Richard Barbrook (en) et Andy Cameron, théoriciens des médias de l'université de Westminster. La critique affirme que la montée des technologies de réseau informatique dans la Silicon Valley au cours des années 1990 est liée au néolibéralisme américain et est le résultat d'une hybridation paradoxale entre la gauche et la droite politique en une forme de déterminisme technologique optimiste, ayant créé une forme de technolibertarianisme (en). L'essai fondateur de la critique a été publié en 1995 dans le magazine Mute (en). Il a été diffusé plus tard par la liste de diffusion nettime. Une version peaufinée a été publiée dans Science as Culture en 1996. Depuis, la critique a été révisée à plusieurs reprises et a été formulée dans plusieurs langues, dont le français.
Une install-partie ou install party,, en anglais installfest, terme anglais qu'on peut traduire littéralement par « fête d'installation », est une réunion qui permet à des novices de rencontrer des utilisateurs expérimentés de systèmes ou de logiciels libres (tels que Linux, FreeBSD, Haiku, FreeDOS ou autre) qui les aideront afin d'installer un système libre sur leur propre machine ou à le configurer s'il est déjà installé. Le but de cette rencontre est que les novices repartent à la fin de la journée avec leur propre ordinateur fonctionnant sous un nouveau système d'exploitation, correctement installé, configuré et agrémenté de nombreux logiciels. Au cours de ces réunions, les utilisateurs confirmés tentent de transmettre une partie de leur savoir aux novices, qui deviendront passeurs à leur tour. Les install parties sont souvent organisées par des LUG (Linux user groups) locaux (université, ville, etc.).
interstices est une revue de culture scientifique en ligne, créée par des chercheurs de l'INRIA pour rendre accessible à un large public la recherche en informatique,. Ce site « a pour but de faire connaître la variété des recherches dans le domaine des sciences et technologies de l'information et de la communication ». Lancé en 2004 à l'initiative de l'INRIA, Institut national de recherche en informatique et en automatique, il se développe en partenariat avec le CNRS, les universités et l'association des sciences et technologies de l'information. Le magazine Sciences et Avenir d'octobre 2010 le classe parmi les cent meilleurs sites scientifiques.
Le Jargon File est un glossaire spécialisé dans l'argot des programmeurs. Le projet, distribué sous licence libre, est lancé en 1975 puis entretenu durant près de trois décennies. Le projet est maintenu à partir de 1990 par Eric Raymond avec l'aide de Guy Steele. Une version imprimée est connue sous le titre « Le nouveau dictionnaire du bidouilleur » (de l'anglais « The New Hacker's Dictionary »). La dernière version est réalisée par Eric Raymond le 1er octobre 2004.
Ray William Johnson, né le 14 août 1981, est un blogger, producteur et acteur américain principalement connu pour sa série vidéo sur YouTube appelée Equals Three (abrégée =3). Cette série présente des vidéos dites vidéos virales de la semaine et donne des commentaires humoristiques à leurs propos. Les vidéos présentées à son émission Equals Three gagnent souvent une hausse de popularité. En septembre 2013, sa chaîne comptait plus de 2 milliards de vues. Elle compte plus de 10 millions d'abonnés ce qui en fait la 7e chaîne ayant le plus d'abonnés.
Joueur du Grenier, communément abrégé en « JDG », est une émission humoristique de tests de jeux vidéo rétro, diffusée sur YouTube depuis septembre 2009 et sur Dailymotion depuis octobre 2009. Joueur du Grenier est également le nom du personnage principal de l'émission, interprété par le vidéaste Frédéric Molas, et celui de la chaîne YouTube qu'il gère conjointement avec un autre vidéaste, Sébastien Rassiat, co-créateur de l'émission. L'émission Joueur du Grenier a donné lieu à plusieurs séries et projets annexes, tels que Papy Grenier ou la chaîne Bazar du Grenier. Elle implique plusieurs collaborateurs intervenant à fréquences variables, dont Karim Debbache (de Crossed et Chroma), et Nico (de NESblog).
Le Leet speak (en leet speak : 1337 5|*34|<), de l'anglais « elite speak » (littéralement, « langage de l'élite »), est un système d'écriture utilisant les caractères alphanumériques ASCII d'une manière peu compréhensible pour le néophyte (appelé noob et déclinaisons) pour s'en démarquer. Le principe est d'utiliser des caractères graphiquement voisins des caractères usuels, par exemple « 5 » au lieu de « S », « 7 » au lieu de « T » et, pour les extrémistes, « |_| » au lieu de « U » ou « |< » au lieu de « K », sans respect de l'orthographe ou des majuscules. Ce système d'écriture se retrouve chez certains geeks technophiles, utilisateurs de jeux en réseau et demosceners.
Le terme libriste désigne une personne attachée aux valeurs éthiques véhiculées par le logiciel libre et la culture libre en général. En matière d'œuvres ou de logiciels, le libriste défend les quatre libertés fondamentales des utilisateurs finaux telles que définies par Richard Stallman pour le logiciel.
La loi de Koomey décrit une tendance à long terme dans l'histoire des ordinateurs. Selon cette loi, le nombre de calculs par joule d'énergie dépensé double tous les 18 mois environ. Il s'avère que cette tendance a été remarquablement stable depuis les années 1950, le nombre de calculs par joule d'énergie dépensé ayant doublé environ tous les 1,57 ans. Cette loi énoncée par Jonathan Koomey aurait été énoncée comme suit: la quantité d'énergie dont une machine a besoin pour effectuer un nombre donné de calculs va diminuer d'un facteur deux chaque année et demi. « The idea is that at a fixed computing load, the amount of battery you need will fall by a factor of two every year and a half » Elle établit un parallèle avec la loi de Moore.
En informatique, la loi de Linus (en anglais : Linus's Law), nommée en l'honneur de Linus Torvalds, formulée par Eric S. Raymond dans son essai et livre La Cathédrale et le Bazar (1999), concerne le développement de logiciel. La loi indique « avec suffisamment d'yeux, tous les bugs sont superficiels » ; ou plus formellement: « avec un groupe de beta-testeurs et de co-développeurs suffisamment large, presque tous les problèmes seront rapidement analysés et le correctif sera évident pour l'un d'entre eux ». En présentant le code à une multitude de développeurs avec l'objectif d'avoir un consensus sur son acceptation est une forme simple de la revue de logiciel. Des chercheurs et praticiens ont régulièrement montré l'effectivité de différentes méthodes de revue logicielle dans la détection de bugs et problèmes de sécurité, et aussi que la revue logicielle pourrait-être plus efficace que le test[citation nécessaire]. La loi de Linus fait généralement partie de la philosophie de base des adeptes du mouvement open source et du logiciel libre.
La loi de Wirth est une loi empirique formulée par Niklaus Wirth en 1995, selon laquelle « les programmes ralentissent plus vite que le matériel accélère ». Ou bien : le nombre d'instructions nécessaires à l'interprétation de programmes plus sophistiqués demande des ordinateurs plus rapides. Niklaus Wirth attribuait quant à lui cette loi à Martin Reiser.
Le média-planneur élabore un plan média en vue de prévoir et de coordonner les différents passages d'une campagne publicitaire dans les grands médias traditionnels et le web. En fonction du public-cible, il détermine les stratégies optimales et les moins coûteuses afin d'atteindre au mieux les cibles de la campagne ou d'améliorer l'image médiatique d'un produit ou de l'entreprise. En relation permanente avec d'autres secteurs publicitaires (annonceurs, agences de vente d'espaces, etc.), il est véritablement le lien entre les annonceurs et la publicité. Cependant, le média-planneur se voit de plus en plus supplanté par l'informatique avec le développement du Big Data.
Message Of The Day, abrégé MOTD (en français : « message du jour ») est un message envoyé à un logiciel client lors de son identification sur des serveurs (tels que les serveurs IRC, SSH ou encore FTP). Généralement, ce message est utilisé pour afficher les règles, les contacts administratifs, ou encore un dessin ASCII Art.
Microserfs (titre original : Microserfs), publié par Harper Collins en 1995, est un roman de Douglas Coupland. Ce fut d'abord une nouvelle dans l'édition de janvier 1994 du magazine Wired, puis elle fut complétée en tant que roman.
Une miniature, ou vignette, ou un carré, ou encore désignée par l'anglicisme thumbnail ou son apocope thumb (littéralement « ongle de pouce », en référence à sa taille), est, en informatique, une version d'une image dont la taille est réduite par rapport à l'original. En principe, cliquer sur une miniature dirige vers l'image dans sa version normale, par exemple au moyen d'un hyperlien.
Un nerd est, dans le domaine des stéréotypes de la culture populaire, une personne solitaire, passionnée voire obnubilée par des sujets intellectuels abscons, peu attractifs, inapplicables ou fantasmatiques, et liés aux sciences (en général symboliques, comme les mathématiques, la physique ou la logique) et aux techniques - ou autres sujets inconnus aux yeux de tous. Apparu à la fin des années 1950 aux États-Unis, le terme est devenu plutôt péjoratif, à la différence de geek. En effet, comparé à un geek qui est axé sur des centres d'intérêts liés à l'informatique et aux nouvelles technologies, un nerd est asocial, obsessionnel, et excessivement porté sur les études et l'intellect. Excluant tout sujet plus commun ou partagé par ses pairs académiques, il favorise le développement personnel d'un monde fermé et obscur. On le décrit timide, étrange et repoussant. Toute activité sportive est pour celui-ci difficile. Au même titre que le stéréotype véhiculé par le mot geek, il est de plus en plus envisagé comme un gage de fierté et d'appartenance identitaire.
Neuromancien (titre original : Neuromancer) est le premier roman de science-fiction de William Gibson. Publié en 1984, il est généralement considéré comme le roman fondateur du mouvement Cyberpunk, ayant inspiré bon nombre d'œuvres telles que les mangas Ghost in the Shell ou Akira et le film Matrix. Il a notamment remporté le Prix Nebula du meilleur roman en 1984. Il est suivi de Comte Zéro (1986) et de Mona Lisa s'éclate (1988).
Newbie (prononcé [ˈnjuːbɪ] en anglais britannique, ou [ˈnuːbɪ] en anglais américain) est une personne novice, qui débute (notamment dans le jeu vidéo). L’étymologie du mot newbie est incertaine, il pourrait soit s’agir d’une variante de new boy issu du langage familier de l'école publique anglaise et de l'argot militaire anglais et américain et qui désigne un néophyte, soit d’une variante de newie issu de l'argot américain et australien, qui désigne aussi un néophyte. Dans le domaine des jeux vidéo en ligne, il a été déformé par le Leet speak en « Newb », « Nub », « N00blet », « Noob », « N00b »... On fait une distinction entre un newbie et un noob. On s'accorde généralement à dire qu'un newbie est une personne qui, parce qu'elle est nouvelle, est inexpérimentée et ignorante des mécanismes du jeu et de l'étiquette, tandis qu'un noob est un joueur qui est généralement considéré comme étant expérimenté, mais qui néanmoins fait souvent des erreurs, des solécismes, ou qui adopte un comportement qui s'apparenterait plus à celui d'un débutant. On trouve diverses contractions du mot Newbie : Newb (prononcé [ˈnuːb]), difficile à différencier phonétiquement de noob, naab qui est plutôt une insulte envers les personnes qui ne réfléchissent pas au bon fonctionnement d'une partie, qui ne savent pas jouer correctement, qui n'en font qu'à leur tête ou tout simplement empêchent le bon fonctionnement de la partie et des autres joueurs. nob,"nobsis" et même nub ainsi que Noobie. En français, on parle parfois de « débutants » ou « d'amateurs » dans un langage courant.
Un no life, de l'anglais « no life », littéralement « pas de vie » ou « sans vie », est une personne qui consacre une très grande partie (si ce n'est la totalité) de son temps à pratiquer sa passion, voire son travail, au détriment d'autres activités. L'article met l'accent sur les jeux vidéo mais la passion n'est pas forcément un synonyme de futilité, par exemple un chercheur ou un inventeur qui se passionne pour son domaine à ne vivre que pour cela et qui permet à l'Humanité d'évoluer. Il y a aussi ceux qui se passionnent pour leur travail et ce qu'il apporte à la société. Cette addiction affecte ses relations sociales et sentimentales. Le terme est régulièrement utilisé pour désigner une personne atteinte de cyberdépendance.
L'orthographe pour calculatrice (parfois nommée beghilos, l'alphabet des lettres disponibles) est une technique d'écriture de mots en lisant à l'envers les calculatrices équipées de certains afficheurs 7 segments.
L'overclocking, ou parfois sur-cadencement, ou surcadençage est une manipulation ayant pour but d'augmenter la fréquence du signal d'horloge d'un processeur au-delà de la fréquence nominale afin d'augmenter les performances de l'ordinateur. Le processeur surcadencé exécutera davantage d'instructions par seconde, d'où réduction du temps d'exécution des programmes. La production de chaleur étant liée au carré de la fréquence, il chauffera aussi davantage, ce qui peut être source d'erreurs ou d'autobridage du processeur. Si elle est trop faible, sa tension d'alimentation le rendra instable. Si elle est trop forte, le composant peut casser prématurément.
Le passage informatique à l'an 2000 (couramment appelé bug de l'an 2000 ou bogue de l'an 2000) a suscité de sérieuses inquiétudes à cause de problèmes de conception et donc de programmation portant sur le format de la date dans les mémoires des ordinateurs et, par conséquent dans les matériels informatiques, ainsi que dans les logiciels. Dans de nombreux programmes et beaucoup de bases de données, il manquait les deux chiffres 19 correspondant au siècle, de sorte qu'au passage de 99 à 100, en réalité 00, de nombreux dysfonctionnements devaient se produire dans ces traitements informatiques, 00 correspondant à l'année 1900 au lieu de 2000. Contrairement à ce que laisse entendre l'appellation commune de « bug de l'an 2000 », le problème de l'an 2000 n'était pas à proprement parler un bogue, comme l'ont bien souligné des experts américains, mais une erreur de conception systémique. Cette erreur a nécessité dans bien des cas de revoir en profondeur l'architecture des systèmes d'information selon une approche systémique, en particulier lorsque certains composants critiques du système d'information ne pouvaient pas être réparés parce qu'ils étaient trop anciens et n'étaient plus maintenus. Il a donc souvent fallu remplacer complètement des systèmes d'information, généralement spécifiques, par des progiciels du marché compatibles an 2000. Les systèmes plus récents ont pu être réparés par conversion. Finalement, aucun problème critique ne s'est produit. Cependant, des sommes considérables, se chiffrant en centaines de milliards de dollars dans le monde, ont dû être dépensées pour prévenir tout incident lors du passage.
Un PC bang (coréen : PC방, soit « salle PC ») est un type de cybercafé spécialisé dans les jeux vidéo et les jeux en ligne multijoueur. Spécifique à la Corée du Sud, le tarif pour une heure de jeu coûte entre 500 et 1 500 wons (environ 1 dollar américain). Les PC bangs se sont notamment développés avec la sortie du jeu vidéo StarCraft en 1998.
L’anglais dans le domaine de l’informatique est parfois décrit comme la langue véhiculaire. Là où les autres sciences utilisent le latin et le grec comme source principale de vocabulaire, l’informatique préfère emprunter des termes à l’anglais. En raison des limites techniques des premiers ordinateurs et de l’absence de standards internationaux pour Internet, les premiers utilisateurs étaient cantonnés à l’utilisation de l’anglais et l’alphabet latin. Aujourd’hui ce problème est moins fréquent, la plupart des logiciels étant internationalisés dans plusieurs langues et tous les problèmes d’alphabets non latins sont résolus grâce à l’encodage des caractères en Unicode. Certaines restrictions n’ont été modifiées que récemment, tels que le nom de domaine qui ne pouvait supporter que les caractères (ASCII).
+ ou − geek (Plus ou Moins Geek) est une émission de télévision aussi émise sur le web sur la culture geek diffusée sur Vosges Télévision puis sur Dailymotion et YouTube depuis septembre 2011. En 2012, la saison 2 est diffusée sur Planète + No Limit et rediffusée sur la chaîne Énorme TV en juillet 2013,,. La saison 3 est diffusée sur la même chaîne depuis janvier 2014.
Le postcyberpunk est un sous-genre de la science-fiction qui a émergé du mouvement cyberpunk. Comme celui-ci, le postcyberpunk se concentre sur les développements technologiques dans des sociétés du futur proche, en examinant par exemple les impacts sociaux de la télécommunication à outrance, de la génétique et de la nanotechnologie. Néanmoins, à la différence du cyberpunk « classique », les œuvres postcyberpunk mettent en scène des personnages qui essaient d’améliorer leurs conditions sociales (ou en tout cas d’empêcher les choses d’empirer). Par ailleurs, dès les années 1990, certains artistes du monde de l'art performance tels que Stelarc, Eduardo Kac, Orlan, Zhu Yu et André Éric Létourneau ont fait sortir ces concepts des seuls univers littéraires et cinématographiques.
Une seconde intercalaire, également appelée saut de seconde ou seconde additionnelle, est un ajustement occasionnel d'une seconde du temps universel coordonné (UTC) lié au temps atomique international (TAI) pour que le temps universel coordonné demeure proche du temps solaire moyen donné par le temps universel (UT1) lié à la rotation de la Terre et donc lentement variable. En effet, si UTC est extrêmement stable, établi par un ensemble d'horloges atomiques, UT1 l'est beaucoup moins. De nombreux facteurs plus ou moins périodiques influencent cette instabilité. Le facteur dominant à long terme est le ralentissement de la rotation de la Terre dû à la dissipation d'énergie dans les phénomènes des marées. D'une façon générale, la date de la prochaine seconde intercalaire n'est pas prévisible avec exactitude. Depuis l'introduction de ce système correctif en 1972, 27 secondes intercalaires ont été ajoutées, la plus récente le 31 décembre 2016 à 23:59:60 UTC.
Le septembre éternel ou septembre sans fin est une expression employée sur Usenet pour désigner la période qui a débuté en septembre 1993. Cette expression signifie que le flux sans fin d'arrivée de nouveaux utilisateurs depuis cette date a dès lors constamment fait baisser les standards de discours et de comportement sur Usenet, et plus largement Internet.
Mathieu Sommet, né le 23 septembre 1988 à Saint-Étienne, est un comédien et vidéaste français. Il est connu pour avoir créé l'émission humoristique Salut les geeks (SLG) qu'il présente de mars 2011 à décembre 2016 sur YouTube, où il interprète plusieurs personnages caricaturaux tout en analysant des vidéos virales. Dès septembre 2014, il apparaît aussi dans trois autres émissions moins régulières : L'Envers du décor dans laquelle il répond aux questions de ses abonnés par rapport à ses dernières vidéos postées, et en profite pour montrer les bêtisiers de son émission, L'IA et Mathieu dans laquelle Mathieu discute avec Jeanne, une intelligence artificielle, sur un paradoxe humain, et les SLG shot, un SLG plus court dans lequel il ne critique qu'une seule vidéo. En décembre 2016, Mathieu Sommet annonce la fin de Salut les geeks et son retrait temporaire des réseaux sociaux. Il revient cependant début 2018 avec une web-série inédite, YouTube Hero.
Marcello Vitali-Rosati est un philosophe du virtuel, de l’identité numérique et des questions qui s'y rattachent (éthique appliquée, éditorialisation, etc.). Détenteur d’une maîtrise en Lettres et philosophie de l’Université de Pise (1998-2002), il détient également un doctorat en Philosophie et Littérature et civilisation française de l’Université Paris IV Sorbonne (2003-2006). Professeur au Département des Littératures de langue française de l'Université de Montréal depuis 2012, il devient titulaire en 2016 de la Chaire de recherche du Canada sur les Écritures numériques.
Le terme « vrai programmeur » est issu du folklore de la programmation informatique pour décrire le programmeur « pur et dur » typique. Un vrai programmeur méprise les outils modernes ou graphiques comme les environnements intégrés de développement ou les langages autres que le langage assembleur ou le code machine au profit de solutions plus directes et plus efficaces – langage de bas niveau plus proche du matériel. Les caractéristiques déterminantes présumées d'un « vrai programmeur » sont extrêmement subjectives, différant avec le temps et le lieu, à la manière de l'erreur « No true Scotsman ».
WYSIWYM, acronyme utilisé en informatique qui vient de l’anglais « what you see is what you mean », signifiant « Ce que vous voyez est ce que vous voulez dire », par opposition au WYSIWYG : what you see is what you get (« Ce que vous voyez est ce que vous obtenez »). Cette catégorie de logiciels représente les informations en fonction de leur sens, de l’information à véhiculer (par opposition à « représenter les informations sous leur forme finale, pour impression, avec mise en forme à l’identique… »). Un des objectifs des interfaces WYSIWYM est de permettre une meilleure séparation du fond et de la forme lors de la création de documents. LyX ou Scenari sont des exemples de logiciels WYSIWYM.
Yet another signifie en anglais encore un autre. En informatique, "Yet Another" est le début de nombreux acronymes tels que : Yabasic (Yet Another Beginner's All-purpose Symbolic Instruction Code) qui est un langage de programmation, YaBB (Yet Another Bulletin Board) qui est un script open source de forums en PHP, Yabox (Yet Another Box) qui est moniteur de bases de données basé sur Nagios, Yacas (Yet Another Computer Algebra System) qui est un logiciel de calcul formel (CAS, comme Computer Algebra System), Yacc (Yet Another Compiler Compiler) qui est un compilateur d'analyseur syntaxique, YaCS (Yet Another Content Management), un CMS SEO pour développement Web, YAFC (Yet Another FTP Client) qui est un client FTP, YafRay (Yet Another Free Ray Tracer), qui est un moteur de rendu pour Blender basé sur la technologie du RayTracing. Yahoo! (Yet Another Hierarchical Officious Oracle) un moteur de recherche qui était initialement un annuaire Web, YaKuake (Yet another Kuake) qui est un terminal graphique pour KDE, YAM (Yet Another Mailer) qui est un logiciel de messagerie, Yamb (Yet Another MP4 Box) qui est un utilitaire pour MP4, YamiPod (Yet Another iPod manager) qui est un logiciel pour gérer un iPod, Yaourt (Yet AnOther User Repositery Tool) qui est un gestionnaire de packets pour Arch Linux YAP (Yet Another Previewer) qui est un logiciel de prévisualisation de Postscript, YAPBAM (Yet Another Personal Bank Account Manager) qui est un logiciel de gestion de compte bancaire, YaPSNapp (Yet Another PlayStation Network Application) qui est une application Android, YARN (Yet Another Resource Negotiator), yaRTI (Yet Another Run-Time Interface) qui est une implémentation libre de l'interface HLA, YaST (Yet Another Setup Tool) qui est le programme de configuration de la distribution SuSE, YATA (Yet Another Totem Addon) qui est un addon pour le jeu World of Warcraft, Yayacc (Yet Another Yet Another Compiler Compiler) qui est un compilateur d'analyseur syntaxique (à vérifier), ABC (Yet Another BitTorrent Client), Yaws (Yet Another Web Server) qui est un serveur web programmé en Erlang, YAWMM (Yet Another Wad Manager Mod) qui est un homebrew Wii permettant la gestion des fichiers .wad.  Portail de l’informatique
You asked for it, you got it, abrégé YAFIYGI est une expression informatique anglaise signifiant Vous l’avez demandé, vous l’obtenez. Cette expression, utilisée le plus souvent ironiquement, fait allusion aux éditeurs de texte et autres outils d'assez bas niveau utilisés directement en ligne de commande et sans aucune interface graphique autre que le terminal. On obtient alors exactement ce que l'on a demandé à la commande, mais il est souvent difficile d'anticiper ce que l'on a effectivement demandé avant qu'il ne soit trop tard. L'expression s’inspire du WYSIWYG : ce que vous voyez est ce que vous obtenez, la difficulté étant d’afficher ce qu’on veut. Dans le YAFIYGI, il est facile d’obtenir ce qu’on demande, mais il peut être très complexe de formuler une commande qui véhicule exactement cette demande.
Le prix Friedrich L. Bauer (Friedrich L. Bauer-Preis en allemand) est un prix informatique de l'université technique de Munich, nommé ainsi en l’honneur de Friedrich L. Bauer. Le prix est doté d'un montant de 25 000 €. Il est décerné irrégulièrement depuis 1992, date du 25e anniversaire de l'introduction d'un cursus d'informatique en Allemagne. Il a été attribué sept fois entre sa création et 2009.
Le Computer Pioneer Award, terme que l'on peut traduire par « Prix des pionniers en informatique », est un prix créé en 1981 par le conseil des gouverneurs de l'association IEEE Computer Society. Il a pour objectif de reconnaître et d'honorer la vision des personnes dont les efforts ont permis la création et assuré la vitalité de l'industrie informatique. Le prix est décerné à des personnes aux mérites exceptionnels, à condition que la contribution principale aux concepts et au développement du domaine de l'informatique ait été faite au moins quinze ans plus tôt. La reconnaissance est exprimée par une médaille d'argent frappée spécialement pour la Société. Tous les membres de la profession sont habilités à désigner un collègue qui leur semble le à même d'être considéré comme un candidat pour ce prix. La date limite pour la nomination est le 15 octobre de chaque année.
Le Prix Seymour Cray (en anglais Seymour Cray Computer Engineering Award ou Seymour Cray Award) est une distinction honorifique attribuée annuellement par la IEEE Computer Society, en reconnaissance de contribution significatives et innovantes dans les domaine du calcul haute performance. Le prix veut honorer des scientifiques qui ont « montré une créativité similaire à celle de Seymour Cray », le fondateur de l'entreprise Cray Research, Inc. et un des pionniers du calcul haute performance. Le lauréat reçoit un insigne commémoratif en cristal, une attestation, et un honoraire de 10 000 dollars. Le prix fait partie de la série des quatorze « Technical Awards » décernés par la IEEE parmi lesquels il y a aussi le prix Eckert-Mauchly. Il existe également un prix Seymour Cray décerné par le CNRS.
Le prix Edsger W. Dijkstra en algorithmique répartie, anciennement prix PoDC de l'article influent, est décerné chaque année, depuis 2000, aux auteurs d'un article dont l'impact est particulièrement important pour la théorie ou la pratique des systèmes distribués depuis au moins dix ans. Remis à l'origine lors de la conférence ACM Principles of Distributed Computing (PoDC), il est, depuis 2007, remis alternativement lors de PoDC les années paires et lors de la conférence EATCS Distributed Computing (DISC) les années impaires, chacune des deux conférences fournissant la moitié de la somme de 2 000 $. Il change de nom en 2003 pour prendre celui de Dijkstra, qui vient de mourir peu après avoir reçu le prix.
L'Electronic Frontier Foundation Pioneer Award (qui pourrait se traduire par prix pionnier de l'Electronic Frontier Foundation) est une distinction annuelle pour les gens qui ont contribué de façon significative à l'émancipation des individus grâce à l'informatique. De 1992, sa première édition a 1998, le prix pionner de l'EEF était présenté durant une cérémonie a Washington, D.C aux États-Unis. Depuis, la cérémonie a lieu durant la conférence Computers, Freedom, and Privacy.
Le prix Gödel est une distinction créée en 1992 par l'European Association for Theoretical Computer Science (EATCS) et le Special Interest Group on Algorithms and Computation Theory (SIGACT) de l'Association for Computing Machinery (ACM) pour honorer des travaux remarquables d'informatique théorique. Il est nommé en l'honneur du logicien Kurt Gödel.
Le Prix Herbrand est une distinction scientifique de la Conference on Automated Deduction (CADE). Il est, depuis 1992, remis chaque année et il est doté de 1 000 dollars. Il récompense un scientifique pour contribution exceptionnelle dans le domaine de la Preuve assistée par ordinateur. Le prix est nommé d'après le logicien et mathématicien français Jacques Herbrand et c'est le plus prestigieux des prix de la recherche internationale de cette discipline.
Le prix Grace Murray Hopper est un prix de l'Association for Computing Machinery (ACM) créé en 1971. Il récompense un informaticien ou une informaticienne pour une contribution exceptionnelle à l'informatique faite avant l'âge de 35 ans. Il porte le nom de Grace Hopper, pionnière dans les domaines de la programmation et de la compilation.
Les IEEE Internet Awards (récompenses internet de l'IEEE) ont été créés en juin 1999 par le conseil d'administration de l'Institute of Electrical and Electronics Engineers. Cette distinction vise à récompenser des avancées dans la technologie Internet, particulièrement en rapport avec l'architecture réseaux, la mobilité ou encore les applications finales.
Les Prix Inria - Académie des sciences sont trois prix, de niveau différents, attribués depuis 2013 en partenariat par l'Académie des sciences et Inria : le Grand prix, le prix Jeune Chercheur et le prix de l'Innovation,.
Le prix Jon Postel (Jonathan B. Postel Service Award) est une distinction attribuée en l'honneur de Jon Postel. Elle est décernée annuellement depuis 1999 par l'Internet Society pour récompenser une personne pour ses contributions exceptionnelles à la communauté des télécommunications. Il a été créé par Vint Cerf alors président de l'Internet Society et annoncé dans « I remember IANA » (RFC 2468).
Le prix de thèse Gilles-Kahn, patronné par l'Académie des Sciences et décerné par la Société informatique de France (SIF), récompense chaque année une excellente thèse en informatique soutenue dans une école ou une université française. Le prix est nommé en l'honneur de Gilles Kahn (1946-2006), chercheur en informatique, membre de l'Académie des sciences, directeur scientifique et président de l'Institut national de recherche en informatique et en automatique (Inria).
Le prix Paris-Kanellakis est décerné par l'ACM depuis 1996, pour honorer les avancées théoriques ayant un impact important et démontrable sur l'informatique pratique. Il est nommé en l'honneur de Paris Kanellakis.
Le prix Knuth récompense les scientifiques ayant apporté une contribution exceptionnelle en informatique théorique. Il porte le nom de Donald E. Knuth, l'un des plus grands contributeurs à l'informatique théorique. Le prix Knuth est attribué tous les dix-huit mois, depuis 1996 ; il inclut une récompense de 5 000 USD. Le prix est attribué par le groupe d'intérêt SIGACT de l'ACM et le comité technique d'informatique théorique de l'IEEE. Les prix sont attribués en alternance au colloque ACM STOCS (Symposium on Theory of Computing) et à la conférence IEEE FOCS (Symposium on Foundations of Computer Science) qui sont parmi les conférences les plus prestigieuses d'informatique théorique. Contrairement au prix Gödel qui récompense les articles exceptionnels, le prix Knuth est attribué aux individus.
La médaille du mérite Konrad-Zuse en informatique, en bref médaille Konrad-Zuse est une distinction créée en 1967 par la Gesellschaft für Informatik. Elle est décernée tous les deux ans. Les lauréats sont choisis parmi les personnalités dont les réalisations exceptionnelles en technique et science ont permis des avancées significatives en informatique. La médaille Konrad-Zuse est la plus prestigieuse distinction en informatique en Allemagne.
La médaille Lovelace, instituée par la British Computer Society (en) en 1998, est attribuée aux personnes qui ont fait avancer les systèmes d'information ou ont contribué de manière significative à leur compréhension. Le prix est nommé d'après Ada Lovelace, qui entretenait une correspondance avec Charles Babbage, pionnier des ordinateurs, et qui est souvent décrit comme la première programmeur informatique. La médaille est destinée à être remise aux personnes qui ont apporté une contribution d'importance majeure dans le développement des systèmes d'Information ou qui contribué considérablement à la compréhension des systèmes d'Information dans les domaines de l'industrie, le milieu universitaire, la secteurs technique ou de gestion. Il y a généralement un médaillé par année, mais le règlement ne fait pas obstacle à ce qu'il y en ait plusieurs ou aucun.
Le Prix Microsoft de la Royal Society et de l’Académie des sciences, créé en 2006, récompense des scientifiques, travaillant en Europe, ayant contribué de façon majeure aux avancées de la science par l'utilisation de méthodes informatiques. Il est attribué pour des travaux remarquables à l'intersection de l'informatique et des sciences biologiques, des sciences physiques, des mathématiques, ou de l'ingénierie. Le jury rassemble en nombre égal des Membres de la Royal Society et de l'Académie des sciences. Le prix est remplacé après 2009 par le Royal Society Milner Award.
Le prix Monpetit ou prix Michel-Monpetit est un prix scientifique français créé en 1977 par l'Académie des sciences pour récompenser un chercheur ou un ingénieur pour ses travaux dans le domaine des mathématiques appliquées ou de l'informatique. Son nom vient de Michel Monpetit, directeur adjoint de l'IRIA (devenu l'INRIA entretemps), décédé en 1976 dans un accident de voiture. Ce prix est référencé par l'Académie des sciences sous la dénomination « Prix Michel Monpetit – INRIA », dans la catégorie « Prix Thématiques/Sciences mécaniques et informatiques ».
La médaille John von Neumann est une récompense attribuée tous les ans depuis 1992 par l'IEEE à une ou deux personnes en reconnaissance de leurs « accomplissements extraordinaires en sciences et technologies informatiques ». Elle est nommée en l'honneur de John von Neumann, pionnier de l'informatique. Les faits reconnus peuvent être d'ordre théorique, pratique ou entrepreneuriaux.
Le prix ACM en informatique est un prix annuel décerné par l'Association for Computing Machinery (ACM) depuis 2007, pour honorer « une contribution novatrice fondamentale à la mi-carrière d'un chercheur en informatique qui, par sa profondeur, son impact et ses implications générales, illustre les plus grands succès de la discipline. »,. Il portait jusqu'en 2016 le nom de prix ACM-Fondation Infosys en informatique (« ACM-Infosys Foundation in the Computing Sciences »). Le prix est doté de 250 000 $. Le soutien financier est fourni par une donation de la société Infosys. La fondation Infosys continue à attribuer des prix Infosys sous ce nom dans d'autres disciplines, comme les mathématiques, les sciences de la vie, la physique ou les sciences sociales. En 2016, il a été annoncé que les récipiendaires du prix ACM en informatique étaient invités à participer au Heidelberg Laureate Forum, avec les lauréats des prix Turing, prix Abel, médaille Fields et prix Nevanlinna .
Le prix ACM Software System ou ACM Software System Award, est attribué tous les ans depuis 1983 à une ou plusieurs personnes sélectionnée pour le développement d'un logiciel ayant une influence sur le long terme, conceptuellement, commercialement, ou les deux. La récompense est décernée par l’Association for Computing Machinery (ACM).
Le Prix de l'innovation en informatique distribuée, également appelé SIROCCO award, est une distinction qui récompense chaque année une ou plusieurs personnes pour leurs contributions dans le domaine de l'informatique distribuée.
Les prix du logiciel libre (de l'anglais « Free Software Awards ») sont deux récompenses décernées chaque année par la Free Software Foundation. La première est attribuée depuis 1998 à une personnalité pour le développement du logiciel libre et la seconde depuis 2005 récompense un projet d'intérêt social et son impact sur la société.
Le prix EATCS est un prix, remis par l'European Association for Theoretical Computer Science (EATCS) à un chercheur pour honorer sa brillante carrière en informatique théorique.
Le prix IPEC Nerode récompense à un ou plusieurs chercheurs pour un article commun de qualité exceptionnelle dans le domaine de l'algorithmique multivariée (aussi appelé complexité paramétrée). Il est remis chaque année par l'EATCS.
Le prix Milner ou prix Milner de la Royal Society est un prix décerné, pour des réalisations exceptionnelles, à un chercheur européen en informatique, avec le soutien de Microsoft Research,. Il prend la suite du prix Microsoft de la Royal Society et de l'Académie des Sciences, et est appelé ainsi en hommage à Robin Milner, un pionnier en informatique. Le lauréat doit être un chercheur en activité en informatique, à l'exception des chercheurs employés par Microsoft; de plus, il doit avoir résidé en Europe pendant au moins 12 mois avant sa nomination. Le récipiendaire obtient une médaille et un prix de 5 000 £. Le lauréat est invité à donner une conférence publique sur ses travaux de recherche devant la Royal Society. Le récipiendaire est choisi par le conseil de la Royal Society sur recommandation du comité de sélection du prix Milner. Ce comité est formé de fellows de la Royal Society, de membres de l'Académie des sciences et de membres de la Leopoldina. Le comité examine les candidatures à nomination; une candidature est examinée au plus trois fois, et entre deux candidatures, il y a un délai d'attente d'un an.
Le prix O'Reilly open source (en anglais O'Reilly Open Source Award) est décerné aux particuliers pour leur dévouement, leur innovation, leur leadership et leur contribution exceptionnelle à l'open source. De 2005 à 2009, le prix a été connu sous le nom Google-O'Reilly Open Source Award, mais depuis 2010, le prix ne portent que le nom O'Reilly,.
Le prix Presburger est une distinction qui est décernée chaque année, depuis 2010 par l'European Association for Theoretical Computer Science (EATCS). Il est attribué un jeune scientifique qui a apporté des contributions exceptionnelles au domaine de l'informatique théorique. Le prix porte le nom du mathématicien polonais Mojżesz Presburger. Il est remis au lauréat lors du colloque ICALP.
Le Prix David P. Robbins pour des articles rapportant de nouvelles recherches en algèbre, combinatoire, ou en mathématiques discrètes, est décerné à la fois par l'American Mathematical Society (AMS) et par la Mathematical Association of America (MAA). Le prix de l'AMS distingue des articles ayant une forte composante expérimentale sur un sujet qui est largement accessible, qui fourniront un énoncé simple du problème et une exposition claire des travaux. Les articles admissibles pour le prix de la MAA sont jugés sur la qualité de la recherche, la clarté de l'exposé, et l'accessibilité à des étudiants de premier cycle. Les deux prix se composent de 5 000 $ chacun et sont décernés une fois tous les trois ans. Ils sont nommés en l'honneur de David P. Robbins (en), mathématicien à l'Institute for Defense Analyses (en) et ont été fondés en 2005 par les membres de sa famille.
Les Bourses Sloan de recherche sont accordées chaque année par la Fondation Alfred P. Sloan, depuis 1955 afin « de fournir le soutien et la reconnaissance à des scientifiques et des savants en début de carrière ». Les lauréats sont appelés Sloan Fellows ou boursiers Sloan. Les boursiers Sloan de recherche sont distinctes des Sloan Fellows (en) dans le domaine des affaires. Les bourses ont été initialement attribuées en physique, en chimie et en mathématiques. Des récompenses ont été ajoutées plus tard dans les neurosciences (1972), les sciences économiques (1980), l'informatique (1993) et la biologie moléculaire computationnelle et évolutionnaire (2002). Ces bourses de deux ans sont attribuées à 126 chercheurs chaque année. Depuis le début du programme en 1955, 43 boursiers ont gagné un Prix Nobel et 16 ont obtenu la médaille Fields en mathématiques.
Le prix Turing ou ACM Turing Award, en hommage à Alan Turing (1912 – 1954), est attribué tous les ans depuis 1966 à une personne sélectionnée pour sa contribution de nature technique faite à la communauté informatique. Les contributions doivent être d’une importance technique majeure et durable dans le domaine informatique.
Le prix Wilkinson pour les logiciels de calcul numérique (J. H. Wilkinson Prize for Numerical Software) est une distinction mathématique attribuée tous les quatre ans par la Society for Industrial and Applied Mathematics (SIAM), afin d'honorer les contributions exceptionnelles dans le domaine des logiciels de calcul numérique. Le prix est nommé pour commémorer la contribution exceptionnelle de James H. Wilkinson dans ce même domaine. Le prix, qui consiste en 3 000 $ et un trophée, est présenté conjointement tous les quatre ans par le Laboratoire national d'Argonne, le National Physical Laboratory et le Numerical Algorithms Group[réf. nécessaire]
L'Agence du Numérique est un service à compétence nationale français, d'une quarantaine de personnes (à parité hommes-femmes, et d’un âge moyen de 31 ans en 2016) créé par décret le 3 février 2015. Placée sous la responsabilité du Ministère de l’Économie et des Finances et rattachée à la Direction générale des entreprises, l'Agence du Numérique pilote trois politiques publiques : le Plan France Très Haut Débit, le Programme Société Numérique (qui remplace la Délégation aux Usages de l'Internet) et l'Initiative French Tech,.
L'archivage électronique désigne le stockage à long terme de documents et données numériques. Les problématiques liées sont le coût et la durée de vie des supports, mais aussi l'accès au contenu malgré les avancées technologiques rendant les anciens supports obsolètes.
Atari Games Corp. v. Nintendo of America Inc., 975 F.2d 832 (Fed. Cir. 1992), est un procès dans lequel la cour d'appel des États-Unis pour le circuit fédéral a statué qu'Atari a enfreint le copyright détenu par Nintendo en copiant illégalement le 10NES. Ce programme est destiné à protéger la console NES de lire des jeux non autorisés par la firme nippone. Atari, après avoir tenté de faire de la rétro-ingénierie sur le système, a obtenu une copie non autorisée auprès du United States Copyright Office (en) et l'a utilisé afin de répliquer le système, nommé « the Rabbit ».
Le Berkman Center for Internet & Society (littéralement « Centre Berkman pour l'Internet et la société ») est un centre de recherche à l'université Harvard qui traite essentiellement sur l'étude du cyberespace. Créé au sein de la faculté de droit de Harvard, le centre était traditionnellement focalisé sur des questions légales en rapport avec Internet. En mai 2008, le centre est promu à une initiative interfacultaire de l'Université de Harvard dans son ensemble. Il est baptisé en l'honneur de la famille Berkman, à qui appartenait l'entreprise The Associated Group (vendu plus tard à Liberty Media).
Le brevet logiciel désigne à l'échelle d'un pays le fait de posséder des réglementations et une jurisprudence claires permettant l'octroi de brevets sur les logiciels, c'est-à-dire un droit d'interdiction de l'exploitation par un tiers de l'invention brevetée, à partir d'une certaine date et pour une durée limitée (20 ans en général). La jurisprudence aux États-Unis est traditionnellement favorable à la protection des logiciels par le brevet. Les décisions récentes paraissent cependant relativiser cette position. En Europe, l'article 52(2) de la Convention sur le brevet européen exclut la brevetabilité des programmes d'ordinateur. Mais, dans la pratique des brevets y sont accordés pour des « inventions mises en œuvre par logiciel » c'est-à-dire liant un logiciel ayant un « effet technique » (par exemple certains systèmes ABS), l'on évoque à ce propos la brevetabilité des inventions mises en œuvre par ordinateur. L'on définit en général la notion d'effet technique comme une transformation de la Nature par l'Homme, autrement comme la production d'un effet matériel. Il existe donc différentes positions à l’échelle internationale, et la pertinence de la possibilité de dépôt de brevets dans le domaine des logiciels fait l'objet d'un débat politique et technique opposant diverses parties dans lequel les lobbies industriels jouent un rôle de premier plan. Des débats au parlement européen ont ainsi eu lieu dans les années 2000, conclus par le maintien de la non-brevetabilité des logiciels « en tant que tels » en 2005.
Créé en 1979 par Yves Poullet, le centre de recherches informatique et droit (CRID) des facultés universitaires Notre-Dame de la Paix à Namur (Belgique) a pour objectif de susciter une réflexion universitaire et pluridisciplinaire sur le droit et l'économie des Technologies de l'information et de la communication. Sa dénomination a été modifiée en Centre de recherches information, droit et société, pour rendre compte de l'extension de son champ de recherche.
Le clean room design (également connu sous le nom de « technique de la muraille de chine ») est une méthode consistant en la copie d'un modèle par rétroingénierie, puis par sa recréation sans enfreindre les copyrights et secrets de fabrication liés au modèle original. Le Clean Room Design est utile comme moyen de défense contre les droits d'auteur et les secrets commerciaux car il repose sur une invention indépendante. Toutefois, parce que l'invention indépendante n'est pas un moyen de défense contre les brevets, cette méthode ne peut généralement pas être utilisée pour contourner les restrictions de brevets. Le terme implique que l'équipe de conception travaille dans un environnement qui est « propre », à l'image d'une salle blanche, manifestement sans contact ni connaissance des techniques possédées et employées par le concurrent. Généralement, le clean room design est effectué par quelqu'un qui examine le système devant être réimplémenté, avant d'en rédiger une description détaillée. Cette notice est ensuite examinée par un avocat pour s'assurer qu'aucun matériel sous copyright n'est inclus. Enfin, la notice est mis en œuvre par une équipe sans aucun lien avec les examinateurs initiaux.
Le comité d'enquête sur la surveillance électronique de masse de citoyens de l'Union européenne a été créé le 4 juillet 2013 par le Parlement européen à la suite des nombreuses révélations d'Edward Snowden sur les programmes de surveillance de la National Security Agency (NSA).
La Commission de contrôle des informations nominatives (ou CCIN) est une institution monégasque chargée de veiller à la protection des données personnelles. Instituée par l'article 2 de la loi n° 1.165 du 23 décembre 1993 réglementant les traitements d'informations nominatives, la CCIN est une autorité administrative indépendante. À ce titre, elle agit « au nom de l'État et dispose d'un réel pouvoir, sans pour autant relever de l'autorité du Gouvernement ».
Le Conseil consultatif de l'internet (CCI) est un conseil placé auprès du gouvernement français pour le conseiller sur les questions relatives à internet. L'idée d'un tel conseil date du 10 juillet 2003 à l'issue d'un Comité interministériel à la société de l'information (CISI). Lors de cette réunion, le gouvernement a décidé de la création d'un « conseil de sages » associant des utilisateurs de l'internet, représentants d’acteurs économiques et associatifs. Il a été créé par un décret du 8 décembre 2003.
Le Conseil général des technologies de l'information (CGTI) était une institution française mise en place par le décret n° 96-1092 du 13 décembre 1996. Sa création est l'un des éléments de la réforme de 1996 dans le domaine des télécommunications, réforme dont les principaux volets étaient la Loi de Réglementation des Télécommunications, le changement de statut de France Télécom et le rattachement à l'État des établissements publics d'enseignement supérieur des télécommunications. Il a été regroupé avec le Conseil général des mines par le décret n° 2009-64 du 16 janvier 2009 pour créer le Conseil général de l'industrie, de l'énergie et des technologies (CGIET).
La conservation des données (en anglais Retention Management) définit les règles et procédures de conservation des données personnelles ainsi que des registres d'appels téléphoniques (statistiques d'appel) que doivent respecter les opérateurs de télécommunications, les fournisseurs d'accès, et les hébergeurs de sites web et de courriels. Elle vise principalement à faire de l'analyse de trafic et à la surveillance.
Les contrats intelligents (en anglais Smart Contracts) sont des protocoles informatiques qui facilitent, vérifient et exécutent la négociation ou l'exécution d'un contrat, ou qui rendent une clause contractuelle inutile (car rattachée au contrat intelligent). Les contrats intelligents ont généralement une interface utilisateur et émulent la logique des clauses contractuelles. Les partisans des contrats intelligents affirment que de nombreux types de clauses contractuelles peuvent ainsi être partiellement ou totalement auto-exécutées ou exécutées à la validation ou les deux. Les contrats intelligents visent à assurer une sécurité supérieure à la mise en application de la loi sur les contrats et de réduire les coûts de transaction associés à la passation des contrats.
La Convention pour la protection des personnes à l’égard du traitement automatisé des données à caractère personnel du Conseil de l'Europe de 1981 étend la protection du citoyen et de ses libertés fondamentales plus particulièrement au droit du respect de sa vie privée, en prenant en compte l'augmentation des flux, nationaux et internationaux, de données personnelles au travers de traitement automatisés. Cette convention a largement inspiré la directive de la Commission européenne de 1995 sur ce sujet.
Le Cyber Intelligence Sharing and Protection Act (CISPA), également connu sous le nom de H.R. 3523, est un projet de loi déposé le 30 novembre 2011 à la Chambre des représentants des États-Unis par le représentant républicain Mike Rogers (Michigan), et 29 de ses collègues. Il n'a pas encore été débattu. Ce projet de loi faciliterait l'accès de différentes agences gouvernementales aux données personnelles des fournisseurs d'accès à internet, lorsqu'un internaute serait soupçonné d'agir de manière délictuelle ou criminelle sur le réseau. Comme les précédents projets SOPA et PIPA, CISPA est critiqué par plusieurs organisations au nom de la protection de la vie privée et de la neutralité du réseau. L'Electronic Frontier Foundation estime que le projet de loi « donnerait à des entreprises et au gouvernement de nouveaux pouvoirs pour surveiller et censurer les communications qui porteraient atteinte au droit d'auteur ». L'ONG Reporters sans frontières a exprimé sa « profonde inquiétude » au sujet de ce projet de loi, qui montre selon elle que « la liberté d’expression et la protection de la vie privée en ligne sont de plus en plus menacées, dans des pays réputés démocratiques, par une série de projets ou de propositions de loi qui les sacrifient sur l’autel de la protection de la sécurité nationale ou de la protection du droit d’auteur. » Une pétition contre le projet de loi, lancée par l'organisation civique internationale Avaaz.org, a recueilli plus de 670 000 signatures au 17 avril 2012. Le 25 avril 2012, par un communiqué, le président Barack Obama a menacé d'opposer son veto à l'adoption de la loi, en raison des risques que celle-ci ferait planer sur la protection de la vie privée. Cette loi a été refusée le 18 avril 2013 par le sénat. Cependant, et contrairement aux projets SOPA et PIPA, le projet est soutenu par de nombreuses entreprises, dont AT&T, Boeing, Facebook, IBM, Intel, Microsoft, Oracle, Symantec, Google et Verizon.
Une cyberattaque est un acte malveillant envers un dispositif informatique via un réseau cybernétique. Une cyberattaque peut émaner de personnes isolées, Kevin Mitnick étant une des plus célèbres, d'un groupe de pirates ou plus récemment de vastes organisations ayant des objectifs géopolitiques.
La cybercriminalité au Canada (ou cybercrime) est l'ensemble des activités criminelles menées le en ligne sur Internet ou d'autres réseaux au Canada. Le Canada est un signataire de la Convention sur la cybercriminalité. En 2016, le nombre de cybercrimes rapportés à la police au Canada étaient de 23 996, une hausse de 34% comparativement à 2015.
Data East USA, Inc. v. Epyx, Inc. 862 F.2d 204, 9 U.S.P.Q.2d (BNA) 1322 (9th Cir. 1988) est un procès opposant les éditeurs de jeux vidéo Data East et Epyx. Ce dernier a en effet publié un jeu vidéo de karaté, baptisé International Karate, dont le gameplay est très similaire à celui d’un jeu de karaté publié précédemment par Data East, Karate Champ, qui accuse donc Epyx de violation de copyright. Après un jugement de la cour fédérale du district nord de Californie donnant raison à Data East, Epyx fait appel devant la cour d'appel des États-Unis pour le neuvième circuit qui annule cette décision le 30 novembre 1988 car elle considère que les ressemblances entre les deux jeux sont inhérentes à tous les jeux vidéo de karaté,.
La délégation aux Usages de l'Internet (DUI) est une ancienne délégation dont la création au sein du ministère de l'Éducation nationale a été décidée par le gouvernement français lors du Comité interministériel à la société de l'information (CISI) du 10 juillet 2003, pour remplacer la Mission d'accès public à l'informatique (MAPI). Un décret du 8 décembre 2003 a procédé à sa création. Elle est ensuite rattachée au Ministère de l'Économie, du Redressement productif et du Numérique, et mise à disposition de la Secrétaire d'État chargée du Numérique et du Ministre chargé de l'Économie, du Redressement productif et du Numérique. Elle disparait au sein de l'Agence du numérique en février 2015.
Le Délégué à la protection des données (DPD) (en anglais : Data Protection Officer, DPO) est l’expression du règlement européen relatif à la protection des personnes physiques à l'égard du traitement des données à caractère personnel et à la libre circulation de ces données (Règlement général sur la protection des données), voté par le Parlement européen le 14 avril 2016 sur la base d’une proposition publiée par la Commission européenne le 25 janvier 2012.
Le Digital Millennium Copyright Act (DMCA) est une loi américaine adoptée en 1998. Le but de ce texte est de fournir un moyen de lutte contre les violations du droit d'auteur. Il vise à établir une législation de la propriété intellectuelle adaptée à l'ère numérique. Parmi les dispositions contenues dans le texte, il y a la possibilité d'interdire explicitement le contournement des technologies utilisées pour protéger les documents assujettis au droit d'auteur. Ainsi, la loi interdit le détournement d'une protection contre la copie, mais aussi la distribution ou la mise à disposition de procédés qui permettent ce détournement. On assiste notamment à la censure de résultats sur Google (par exemple en ce qui concerne les requêtes portant sur le logiciel de peer to peer Kazaa) et à la fermeture du site Megaupload le 19 janvier 2012. Aujourd'hui, de grands hébergeurs américains de sites Internet, de blogs et de forums tels que Google, MSNGroup, Blogger, Tumblr, Wordpress et Yahoo! permettent de remplir un DMCA[Quoi ?] en ligne dans le but de simplifier et accélérer le dépôt de plainte. Si la plainte de l'auteur lésé est confirmée, les pages concernées sont soit retirées ou le blog concerné ayant violé les droits d'auteur est fermé. Son équivalence européenne est l'EUCD et la transcription en France est la loi DADVSI, qui a été adoptée le 1er août 2006.
La directive 95/46/CE est une directive de l'Union européenne qui constituait le texte de référence en matière de protection des données à caractère personnel. Publiée au Journal officiel de l'Union européenne du 23 novembre 1995, elle est officiellement intitulée « directive 95/46/CE du Parlement européen et du Conseil, du 24 octobre 1995, relative à la protection des personnes physiques à l'égard du traitement des données à caractère personnel et à la libre circulation de ces données ». Elle ne couvre pas les données personnelles traitées dans le cadre du troisième pilier de l'UE, à savoir la coopération policière et judiciaire en matière pénale, ce qui inclut l'ensemble des fichiers de police, de justice et de renseignement. Par ailleurs, elle ne concerne que la réglementation des États-membres ; les données personnelles collectées par des institutions communautaires sont régies par le règlement 45/2001, lequel institua le Contrôleur européen de la protection des données (CEPD). La directive 95/46/CE institua, elle, le G29 afin de coordonner l'activité des différentes autorités de protection des données personnelles. Cette directive a été rendue caduque par le Règlement général sur la protection des données adopté en 2016.
La directive 2006/24/CE sur la conservation des données de l'Union européenne, du 15 mars 2006 (nom complet : Directive 2006/24/CE du Parlement européen et du Conseil du 15 mars 2006 sur la conservation de données générées ou traitées dans le cadre de la fourniture de services de communications électroniques accessibles au public ou de réseaux publics de communications, et modifiant la directive 2002/58/CE), exige la conservation des données pendant une période allant de six mois à deux ans, en particulier en vue de : pouvoir tracer et identifier la source d'une communication; pouvoir tracer et identifier la destination d'une communication; pouvoir identifier la date, l'heure et la durée d'une communication; pouvoir identifier le type de communication; pouvoir identifier la machine utilisée pour communiquer; pouvoir identifier la localisation des équipements de communication mobile.
La directive du 12 juillet 2002 sur la protection de la vie privée dans le secteur des communications électroniques (2002/58) (en anglais, Directive on Privacy and Electronic Communications) est une directive européenne qui vise à protéger de façon spécifique la vie privée sur Internet. Elle couvre les aspects laissés de côté par la directive de 1995 sur la protection des données personnelles (1995/46, dite Data Protection Directive, laquelle reprend la plupart des dispositions de la loi Informatique et libertés de 1978). Cette nouvelle directive ne couvre toutefois pas tout ce qui a trait à la sécurité nationale et au droit pénal.
Un document numérique est une forme de représentation de l'information consultable à l'écran d'un appareil électronique. L’affichage de ce type de document peut être apparenté soit au « document » même, ou soit à l’interface logicielle. Suivant l'intervention d'applications informatiques dans une partie de son contenu (bases de données, POO), les changements dans l'organisation logique de ses données peuvent être apportés. À l'inverse du document sur papier, qu'il soit manuscrit ou imprimé, le document numérique permet de séparer la présentation (les techniques de mise en page) de l'information (composition de texte, données). Des multimédias (image fixe ou animée, vidéo, son) peuvent être insérés à l’intérieur du document numérique. Sa technique de production et de communication se résume en quatre grandes familles de logiciels : les outils de traitement de texte, les tableurs, les logiciels de courriel, les logiciels de gestion documentaire.
En droit français, des données sensibles sont des données à caractère personnel qui font apparaître, directement ou indirectement, les origines raciales ou ethniques, les opinions politiques, philosophiques ou religieuses ou l'appartenance syndicale des personnes, ou qui sont relatives à leur santé ou à leur vie sexuelle. De même, les données relatives aux infractions, condamnations et mesures de sûreté bénéficient d'un régime spécifique. Il est a priori interdit de collecter et d'enregistrer ces données, seuls les fichiers d'État (police, Renseignements Généraux, etc.) pouvant déroger à cette règle, sous certaines conditions.  Portail du droit  Portail de l’informatique
Une double licence est une clause de licence qui permet à un utilisateur d'une œuvre protégée par un droit d'auteur d'utiliser l'œuvre en se conformant à l'un de deux ensembles de conditions au choix de l'utilisateur.
En informatique, le droit d'accès est, d'une façon générale, le droit nécessaire à un utilisateur pour l'accès à des ressources : ordinateur, données, imprimante, etc. Selon le contexte, le droit d'accès peut s'entendre au sens de l'organisation des accès au système d'information, au sens de la sécurité des systèmes d'information, ou bien au sens juridique.
On peut regrouper sous l'expression droit de l'informatique l'ensemble des dispositions normatives ou jurisprudentielles relatives aux nouvelles technologies de l'information et de la communication (NTIC). On ne peut toutefois pas le décrire comme une unité organique telle que le droit civil ou le droit commercial. À cause de la diffusion de l'informatique dans un grand nombre d'activités aussi bien professionnelles que privées, ce droit consiste plutôt en modifications, parfois substantielles (droit de la communication portant sur les usages de l'information), parfois mineures, d'un grand nombre de domaines existants du droit.
Le droit numérique est la partie du droit spécifique aux nouvelles technologies. Il régit les problèmes créés par l'émergence de la société de l'information, et vise principalement : la protection de la vie privée mise à mal par la collecte informatique des données, la protection de la propriété intellectuelle, les œuvres étant facilement copiables illicitement sous leur forme numérique. l'accessibilité numérique contre fracture numérique.
Electronic IDentification Authentication and trust Services (eIDAS) est un règlement de l'UE sur l'identification électronique et les services de confiance pour les transactions électroniques au sein de l'Union Européenne. C'est un ensemble de normes pour l'identification électronique et les services de confiance pour les transactions électroniques dans l'Union Européenne. Il a été établi dans le règlement de l'UE n ° 910/2014 du 23 juillet 2014 sur l'identification électronique et abroge la directive 1999/93/CE.,
L'éthique de l'informatique est une branche de l'éthique appliquée qui traite de la façon dont les usagers et les professionnels de l'informatique font un usage de l'information et prennent des décisions au regard de critères éthiques. L'éthique de l'informatique s'intéresse tant à la gouvernance (décision du management) qu'au comportement individuel des utilisateurs et des professionnels de l'informatique. En 2009, l'utilisation massive de courriers électroniques nécessite par exemple la définition de règles éthiques pour l'usage de l'information.
Un fabricant d'équipement d'origine (FEO), en anglais Original Equipment Manufacturer (OEM), ou équipementier est une entreprise fabriquant des pièces détachées, principalement pour le compte d'une autre entreprise, l'intégrateur ou l'assembleur. Ce type d'entreprise se retrouve usuellement dans les industries automobile, aéronautique, informatique ou électronique. On parle également de sous-traitance pour définir le lien entre l'équipementier et l'assembleur ou bien même de co-traitance parfois lorsque l'équipementier est partenaire dans l'innovation. C'est majoritairement le cas dans l'automobile.
En informatique industrielle, on parle de fonction de sécurité lorsqu'une tâche où un ensemble réalisé par un système informatique peut causer des dommages humains ou financiers.
La fondation Gentoo est une organisation américaine à but non lucratif fondée par Daniel Robbins pour la protection légale du projet Gentoo. Elle représente également le projet politique de la communauté Gentoo. Cette gouvernance s'inspire des mécanismes juridiques instaurés par la fondation pour le logiciel libre et du contrat social du projet Debian.
Le forum des droits sur l'internet (connu également sous le sigle FDI) a été un organisme de corégulation de l'internet créé sous la forme d'une association loi de 1901. Fondé avec le soutien des pouvoirs publics, le FDI avait pour mission de réfléchir aux questions de droit et de société liées à l'internet. Il avait pour mission d'informer le public et d'organiser la concertation entre les pouvoirs publics, les entreprises et les utilisateurs sur ces questions. Il proposait également un service de médiation à destination du grand public. Le Forum comptait près de 70 membres, organismes publics, associations et entreprises privées. Le 7 décembre 2010, l'Assemblée générale extraordinaire de l'association annonce sa dissolution anticipée, l'État n'ayant pas renouvelé pour 2011 la subvention du FDI qui la faisait vivre à 80 %.
La Free Software Foundation (FSF) (littéralement « Fondation pour le logiciel libre »), est une organisation américaine à but non lucratif fondée par Richard Stallman le 4 octobre 1985, dont la mission est la promotion du logiciel libre et la défense des utilisateurs. La FSF aide également au financement du projet GNU depuis l'origine. Son nom est associé au mouvement du logiciel libre.
L’Institut belge des services postaux et des télécommunications (IBPT) est un organisme public belge dont la mission consiste à réglementer et contrôler les services postaux et de télécommunications, notamment en prenant les dispositions nécessaires pour ce qui concerne la gestion du spectre, la numérotation et le respect de la concurrence. L’IBPT a été fondé en 1991 pour reprendre le rôle de régulateur autrefois attribué à la Régie des Télégraphes et des Téléphones à la suite de la privatisation de cette dernière. Il fait partie de l’IRGIS.
La journée européenne de la protection des données à caractère personnel est une initiative du Conseil de l'Europe relayée par la Commission européenne qui a proclamé solennellement le 28 janvier de chaque année comme journée de protection des données. Cette journée a eu lieu pour la première fois le 28 janvier 2007.
Liberty Alliance, aussi connu sous le nom de Project Liberty, est un projet qui réunit des acteurs des mondes industriel, informatique, bancaire et gouvernemental sous la forme d'un consortium. L'objectif est de définir des ensembles de spécifications de protocoles de fédération d'identité et de communication entre services web. Ces protocoles sont conçus pour être mis en œuvre aussi bien dans le cadre de déploiements intra-entreprise qu'inter-entreprise.
Il existe dans le monde diverses autorités chargées de la protection des données (Data Protection Authority pour les anglophones), dont beaucoup sont impliquées dans un processus de conférences internationales pour la protection des données et de la vie privée, ayant abouti à la Résolution de Madrid . Ce projet est inscrit au programme de travail de la Commission du droit international des Nations unies .
La loi d'orientation et de programmation pour la sécurité intérieure ou LOPSI est une loi française, promulguée le 29 août 2002, relative à la sécurité intérieure. Elle permet de recourir d'une manière générale à des procédures d'un partenariat public-privé (au sens du droit européen) allégées (la mise en concurrence n'est pas obligatoire) sans nécessité d'une qualité d'urgence ou de complexité, à l'opposé d'un contrat de partenariat.
La loi n°78-17 du 6 janvier 1978 relative à l'informatique, aux fichiers et aux libertés, plus connue sous le nom de loi informatique et libertés, est une loi française qui réglemente la liberté de traitement des données personnelles, c'est-à-dire la liberté de ficher les personnes humaines. Cette liberté étant indissociable de l'activité informatique, cette loi réglemente donc les conséquences potentiellement antisociales de l'activité informatique.
Un magasin d'applications, boutique d'applications, marché d'applications ou app store est une plateforme en ligne d'applications destinées à des systèmes d'exploitation informatiques grand-public. Une telle boutique en ligne est accessible par Internet depuis un système compatible. Le téléchargement d’une application peut être gratuit ou payant, ou même faire partie du processus normal de mise à jour du système d’exploitation.
La norme de sécurité de l’industrie des cartes de paiement (Payment Card Industry Data Security Standard ou PCI DSS) est un standard de sécurité des données pour les principaux groupes de cartes de paiement tels que Visa, MasterCard, American Express, Discover Card (en) et JCB. Les autres groupes ne font pas partie de cette norme. La norme PCI DSS est établie par les fournisseurs de cartes de paiement et est gérée par le Conseil des normes de sécurité PCI (forum international ouvert pour l'amélioration, la diffusion et la mise en œuvre de normes de sécurité pour la protection des données de comptes). Ce standard a été créé afin d’augmenter le contrôle des informations du titulaire de la carte dans le but de réduire l'utilisation frauduleuse des instruments de paiement.
La notarisation électronique est la certification des différentes étapes de l'évolution d'un document électronique en vue : de permettre lors d’un échange entre deux parties de garantir le contenu, l'origine, la date et la destination d'un message électronique d'archiver de façon sécurisée des documents numériques La notarisation électronique permet la vérification et l'archivage des preuves d'échanges et d'archivage électroniques par un tiers de confiance agréé (à la manière d'un notaire). Cette technique améliore la sécurité des échanges et de l'archivage électronique dans la mesure où elle assure aux parties différents mécanismes de suivi et d’archivage des transactions émises et reçues (l’intégrité, l’origine, la date et la destination des données). Depuis juillet 2003 en France, l’admission d’un écrit sous forme électronique au même titre qu’un écrit classique est conditionnée par deux faits : son auteur doit pouvoir être identifié et l’intégralité de son contenu doit être garanti lors de sa rédaction et pérennisé dans sa conservation. Ces garanties sont l'un des besoins essentiels des entreprises travaillant avec des documents électroniques pouvant avoir une valeur juridique.
L'obligation générale d'information est une obligation juridique selon laquelle tout professionnel vendeur de biens ou prestataire de services doit, avant la conclusion d'un contrat, mettre le consommateur en mesure de connaître les caractéristiques essentielles du bien ou du service. Cette obligation apparaît dans un grand nombre de professions.
Oracle America, Inc. vs Google, Inc. est un procès dans lequel l'éditeur du langage de programmation Java accuse la société Google d'avoir enfreint le copyright, détenu par Oracle.
Un paradis des données ou zone refuge (réf.: Instruments juridiques pour lutter contre le racisme sur internet), en analogie au paradis fiscaux, c'est un refuge pour accéder à des données de façon non-réglementée, non-interrompue ou sécurisée,,,. Les paradis de données sont généralement des localisations dont l'environnement légal est favorable à l'idée qu'un réseau informatique détienne des données gratuitement et les protège ainsi que les informations associées. On distingue 3 type de paradis des données : ① un emplacement géographique avec une réglementation faible en matière d'extradition et de lois informatique, ② un emplacement géographique avec une réglementation forte concernant la protection des données et, un espace virtuel conçu pour sécuriser les données par des moyens techniques (tel que le chiffrement) indifférent de l'environnement légal. Le domaine .onion de Tor (service caché), HavenCo (centralizé) et, Freenet (decentralisé) sont trois modèles de paradis de données virtuels contemporains.
Le règlement no 2016/679, dit règlement général sur la protection des données (RGPD), est un règlement de l'Union européenne qui constitue le texte de référence en matière de protection des données à caractère personnel. Il renforce et unifie la protection des données pour les individus au sein de l'Union européenne. Après quatre années de négociations législatives, ce règlement a été définitivement adopté par le Parlement européen le 14 avril 2016. Ses dispositions sont directement applicables dans l'ensemble des 28 États membres de l'Union européenne à compter du 25 mai 2018. Ce règlement remplace la directive sur la protection des données personnelles adoptée en 1995 (article 94 du règlement); contrairement aux directives, les règlements n'impliquent pas que les États membres adoptent une loi de transposition pour être applicables. Les principaux objectifs du RGPD sont d'accroître à la fois la protection des personnes concernées par un traitement de leurs données à caractère personnel et la responsabilisation des acteurs de ce traitement. Ces principes pourront être appliqués grâce à l'augmentation du pouvoir des autorités de régulation.
RDA est l'abréviation de l'expression Rapports (Reporting) des Droits d'Accès. Il se présente sous la forme d'un formulaire dans lequel sont regroupés l'ensemble des accès informatiques et le niveau de droits des employés de la société en question. Elles sont disponibles la plupart du temps pour chaque employé ou parfois par groupe d'employé s'ils disposent exactement tous des mêmes accès. Cette fiche est un gage de sécurité et ce plus particulièrement dans le domaine de la finance dans lequel l'enjeu des accès est un point extrêmement sensible. Pour être efficace, la mise à jour de cette fiche doit s’effectuer en temps réel afin de bien représenter les accès accordés. Cette expression résulte des changements du contexte économique, social et technologique dans lequel les entreprises évoluent. En effet, face à l'évolution du marché, les besoins des entreprises évoluent et afin de s'adapter, les mouvements internes des collaborateurs s'accélèrent. Aussi, les entreprises structurent et utilisent de plus en plus un système d'information étendu, avec de nombreuses applications demandant toujours plus l'attribution de droits et d'accès différenciés. Il est donc difficile pour une entreprise d'obtenir à un moment déterminé une cartographie de ses collaborateurs et de l'ensemble de leurs droits. Afin de pallier ces problèmes de gestion des droits et des accès, les entreprises sont amenées à développer des Rapports des Droits d'Accès. Ces fiches seront utilisées notamment dans le processus d'arrivée et de départ de collaborateurs, dans lequel il faudra accorder puis supprimer les accès des collaborateurs.  Portail de la sécurité informatique  Portail du droit  Portail de la sécurité de l’information
La « résolution de Madrid » définit les principes  qui devraient - de manière urgente, selon les auteurs du texte- renforcer le caractère universel du droit à la protection de la vie privée et des Données personnelles de tous les citoyens (y compris les enfants ou personnes vulnérables, et notamment sur Internet). Elle a été votée en novembre 2009 à l'occasion d'une conférence internationale des autorités de protection de la vie privée (Madrid, du 4 au 6 novembre 2009), mais elle n'est à ce jour pas juridiquement contraignante. Elle vise la rédaction et signature d'une Convention universelle pour la protection des personnes à l’égard du traitement des données personnelles .
Le Safe Harbor, ou la sphère de sécurité, est un ensemble de principes qui permettaient à certaines entreprises américaines − dépendant de l'autorité du département du Commerce des États-Unis, à savoir les principales entreprises sauf les banques et les compagnies d'assurance − de certifier qu'elles respectaient la législation de l'Espace économique européen (EEE) afin d'obtenir l'autorisation de transférer des données personnelles de l'EEE vers les États-Unis.
Début 2003, l'entreprise the SCO Group a intenté un procès contre IBM au prétexte qu'IBM aurait inclus dans le noyau Linux une portion de code source d'Unix, dont SCO aurait eu la propriété intellectuelle. Par ce motif, SCO exigeait aussi le paiement d'une licence par chaque utilisateur de Linux et a contacté de nombreuses entreprises dans ce sens. Ces allégations (et la relative incertitude concernant l'issue du procès) mettaient en cause la légalité du noyau Linux au regard du droit d'auteur. Ce doute a pu constituer un handicap pour les entreprises distribuant ce logiciel (d'où les plaintes déposées en retour par les sociétés Red Hat et Novell contre le groupe SCO). Les grands acteurs du marché Linux, réagirent de différentes manières : IBM en refusant en bloc les allégations de SCO, et déclarant ne pas en tenir compte tant que celles-ci n'auraient pas été prouvées d'une manière ou d'une autre. HP en s'engageant auprès de ses clients à prendre la licence et tous les frais de retard à leur place et à ses frais, si quoi que ce soit se confirmait sérieusement dans cette affaire. Le 4 août 2003, Red Hat intenta un procès contre SCO pour « manœuvres mensongères organisées dans le but de porter préjudice à ses ventes » Novell fut impliqué dans une série d'actions en justice visant à déterminer le propriétaire des droits d'auteurs concernant Unix. Un jugement, rendu le 10 août 2007 par la cour de district de l'Utah, indique que Novell (développeur et distributeur de Linux, en particulier à travers la distribution SuSE) est le véritable propriétaire des copyrights concernant Unix et UnixWare. Ce jugement constitue un sérieux revers pour le groupe SCO, dont le principal motif d'attaque était la revendication de la propriété intellectuelle d'Unix, et lui laisse peu de chances de gagner les suites de son procès contre IBM. Toutefois, en août 2009, une cour fédérale casse le jugement de 2007,. En 2010, Novell gagne définitivement contre SCO.
Sega Enterprises Ltd. v. Accolade, Inc., 977 F.2d 1510 (9th Cir. 1992), est un procès dans lequel la cour d'appel des États-Unis pour le neuvième circuit a appliqué la loi sur la propriété intellectuelle aux États-Unis en faveur de la rétro-ingénierie de logiciels informatiques. Cette décision fait suite à un conflit entre Sega, le fabricant de la console de jeux vidéo Mega Drive (appelée Genesis aux États-Unis), et l'éditeur de jeux vidéo Accolade. Ce dernier a en effet désassemblé le code de la console, afin de publier des jeux en contournant la licence payante de Sega, ce qui entraîne l'affichage d'un message qui fait supposer, à tort, que la commercialisation du jeu a été autorisée par Sega. Ce procès a remis en question plusieurs points techniques, notamment sur le copyright, l'utilisation des marques déposées et les limites du fair use dans le cadre du développement logiciel. Le procès est jugé dans un premier temps par la cour fédérale du district nord de Californie, qui statue en faveur de Sega. Une injonction de la cour oblige alors Accolade à arrêter le développement de nouveaux jeux sur Genesis et demande le retrait de la vente des jeux déjà commercialisés. Accolade fait appel devant la cour d'appel des États-Unis pour le neuvième circuit, arguant que la rétro-ingénierie est protégée par le fair use aux États-Unis. Quelques mois plus tard, la cour d'appel annule le jugement précédent et retourne la situation en condamnant Sega, qui entrave la concurrence en abusant du droit des marques. L'injonction est levée et Accolade peut de nouveau publier des jeux Genesis à partir de janvier 1993. Trois mois plus tard, le 30 avril 1993, un accord commercial est trouvé entre les deux parties. Ce procès est régulièrement cité comme jurisprudence lors de différends concernant la rétro-ingénierie et le fair use. Il protège en effet les éditeurs qui font un usage raisonnable d'un système protégé si l'unique but est de rendre interopérable un produit avec ce système. Il statue également que les principes de fonctionnement d'une console de jeux ne peuvent être protégés par le copyright.
Christopher Soghoian, né à San Francisco en 1981, est un chercheur en sécurité informatique. Il milite en faveur de la protection de la vie privée sur les réseaux informatiques.
Sony Computer Entertainment v. Connectix Corporation, 203 F.3d 596 (2000) est une décision de la cour d'appel des États-Unis pour le neuvième circuit qui statue que la copie d'un BIOS protégé sous copyright pour le développement d'un émulateur ne constitue pas une violation du copyright mais est protégé par la doctrine du fair use. Cette décision note également que la marque déposée Sony n'a pas été terni par Connectix Corp. via la vente de son émulateur, la Virtual Game Station.
Le Stop Online Piracy Act (SOPA), aussi connu sous le nom de H.R.3261, est une proposition de loi déposée à la Chambre des représentants des États-Unis le 26 octobre 2011 par le représentant républicain Lamar S. Smith mais reportée sine die depuis le 20 janvier 2012. Ce projet de loi vise à élargir les capacités d'application du droit d'auteur et des ayants droit pour lutter contre sa violation en ligne et les contrefaçons. Examiné par la commission des affaires juridiques de la Chambre des représentants (en) à la mi-janvier 2012, il s'inscrit dans la lignée d'une disposition antérieure, le PRO-IP Act de 2008. Une proposition similaire a été déposée devant le Sénat, sous le nom de PROTECT IP Act. Le SOPA prévoit une série de mesures à l'encontre des sites contrevenants. Les pénalités prévues incluent notamment la suspension des revenus publicitaires et des transactions en provenance de services comme PayPal, l'interruption du référencement sur les moteurs de recherche, et le blocage de l'accès au site depuis les principaux opérateurs internet. Le SOPA rend également délictuel le streaming de contenu protégés. Les initiateurs du texte affirment qu'il protégerait les secteurs économiques américains liés au copyright et donc nombre d'emplois. Ainsi leur paraît-il nécessaire de renforcer la législation existante (Digital Millennium Copyright Act de 1998, etc.), notamment à l'encontre des sites étrangers. Ses détracteurs la qualifient de « censure numérique ». Elle malmènerait l'ensemble d'Internet et menacerait la liberté d'expression. La commission des affaires juridiques de la Chambre des représentants a tenu une audition à son propos le 16 novembre 2011. Elle devait être présentée devant la Chambre des représentants le 15 décembre 2011 mais le débat a finalement été repoussé à janvier 2012. Entre-temps, de nombreux changements et amendements sont intervenus. Des manifestations, pétitions et boycotts de compagnies qui encouragent la législation ont été engagés et plusieurs sites Internet très fréquentés ont été temporairement coupés en signe de protestation. Le 20 janvier 2012, Lamar S. Smith a annoncé la suspension des travaux de la commission sur ce texte, dans l'attente d'un accord .
Un tiers de confiance est un organisme habilité à mettre en œuvre des signatures électroniques. Cette dénomination récente est employée dans plusieurs domaines différents, l'échange de bien, l'échange d'informations sur internet, les déclarations fiscales française : Sécurisation des transactions de biens contre des paiements: par exemple Paypal, Vérifdeal, PriceMinister ou Amazon. Sécurisation automatique par certificat informatique, dit aussi électronique, lors des échanges sur internet (sécurisation des pages web, des courriers, des fichiers exécutables). Délégation de déclaration fiscale et représentation auprès des services de l'état.
L'apparition de l'informatique a changé la nature des problèmes posés par la notion de vie privée. Si l'informatisation des données a été généralement considérée comme un progrès, elle s'est aussi accompagnée de dangers (en littérature Big Brother, le panoptique et dès l'époque latine la question de Juvénal : Quis custodiet ipsos custodes? (Mais qui gardera ces gardiens ?) liés à la possibilité pour autrui ou un pouvoir institué, d'avoir un accès non contrôlé aux informations de nombreux citoyens. Traditionnellement, la protection de la vie privée repose sur la mise en place de moyens légaux (par exemple loi informatique et libertés), techniques (cryptographie) ou organisationnels (règles internes). Dans le cas d'internet, l'offuscation vient compléter cet arsenal ainsi que le modèle de la lentille de Brunswik dans le dévoilement des données personnelles : l'idée étant de reprendre cette théorie issue de la psychologie cognitive, afin de comprendre un besoin de lancer des indices sur soi-même pour collecter des feedback et savoir quelles données protéger.
Made with Code est une initiative lancée par Google le 19 juillet 2014. Elle vise à valoriser les filles ayant des compétences en programmation dans les collèges et lycées. Made with Code a été créé après que Google ait découvert, via ses recherches, que l'encouragement et l'exposition sont les facteurs les plus importants influençant les jeunes filles à poursuivre dans l'Informatique. Google finance Made with Code à hauteur de 50 millions de dollars, en plus des 40 millions de dollars investis depuis 2010 dans des organisations comme Code.org, Black Girls Code, et Girls Who Code. L'initiative Made with Code implique à la fois des activités en ligne et des événements dans la vie réelle, en collaborant avec des entreprises remarquables telles que Shapeways et App Inventor.
Les algorithmes génétiques appartiennent à la famille des algorithmes évolutionnistes. Leur but est d'obtenir une solution approchée à un problème d'optimisation, lorsqu'il n'existe pas de méthode exacte (ou que la solution est inconnue) pour le résoudre en un temps raisonnable. Les algorithmes génétiques utilisent la notion de sélection naturelle et l'appliquent à une population de solutions potentielles au problème donné. La solution est approchée par « bonds » successifs, comme dans une procédure de séparation et évaluation (branch & bound), à ceci près que ce sont des formules qui sont recherchées et non plus directement des valeurs.
En informatique théorique, un L-système ou système de Lindenmayer est un système de réécriture ou grammaire formelle, inventé en 1968 par le biologiste hongrois Aristid Lindenmayer. Un L-système modélise le processus de développement et de prolifération de plantes ou de bactéries. C'est une forme de grammaire générative. Ces grammaires ont été mises en œuvre graphiquement par de nombreux auteurs, menant à de spectaculaires images. Une étude systématique d'une certaine formulation a été entreprise par Przemyslaw Prusinkiewicz (de) dans les années 1980. Une étude mathématique, débutée dès l'introduction des systèmes par Lindenmayer, a débouché sur une théorie élaborée dont une première synthèse a été faite, en 1980 déjà, par Rozenberg et Salomaa. Au départ, Lindenmayer conçoit sa formalisation comme un modèle de langages formels qui permet de décrire le développement d'organismes multicellulaires simples. À cette époque il travaille sur les levures, les champignons et des algues. Mais sous l'influence des théoriciens et des praticiens de l'informatique, ce système a conduit à des familles de langages formels et aussi à des méthodes pour générer graphiquement des plantes idéalisées très complexes. Formellement, un L-système est un ensemble de règles et de symboles qui modélisent un processus de croissance d'êtres vivants comme des plantes ou des cellules. Le concept central des L-systèmes est la notion de réécriture. La réécriture est une technique pour construire des objets complexes en remplaçant des parties d'un objet initial simple en utilisant des règles de réécriture. Dans l'interprétation biologique, les cellules sont modélisées à l'aide de symboles. À chaque génération, les cellules se divisent, ce qui est modélisé par l'opération consistant à remplacer un symbole par un ou plusieurs autres symboles consécutifs.
La modélisation procédurale est un terme générique pour un certain nombre de techniques, utilisé pour la génération de modèle. Cela peut aller des jeux vidéo (infographie, terrain de jeu, personnages impliqués dans le jeu, etc.), dans l'art génératif (ou art procédural), ou bien encore dans l'ingénierie (organisation, production de formes optimales, comme un profil d'aile, etc.). Parmi les plus connues, on peut citer les fractales (mouvement Brownien fractionnaire) en général pour simuler la nature, les L-Systèmes, une variante des fractales s'appliquant notamment aux plantes, le diagramme de Voronoï permettant une répartition réaliste, telle des bulles à la surface d'un liquide, etc.. Plus récemment, l'apprentissage profond, et ses variantes spécialsiées est une technique permettant d'apprendre les processus de création d'artistes et de les imiter. Les algorithmes génétiques, sont à la fois utilisés pour la simulation, et la modélisation de formes optimales en ingénierie. L'ensemble des règles peut être soit intégré dans l'algorithme, configurable par des paramètres, ou de l'ensemble des règles est séparé du moteur d'évaluation. La sortie est appelé contenu procédural, qui peut être utilisé dans les jeux vidéo, dans le cinéma, ou l'utilisateur peut modifier le contenu manuellement. Les modèles procéduraux présentent parfois la base de données d'amplification, ce qui signifie que de grandes scènes peuvent être générées à partir d'une quantité beaucoup plus faible de règles. Si l'algorithme utilisé produit le même résultat à chaque fois, le résultat ne doit pas être stocké. Souvent, il suffit de lancer l'algorithme avec la même graine (seed) pour y parvenir. Bien que toutes les techniques de modélisation sur un ordinateur nécessitent des algorithmes pour gérer et stocker des données à un moment, la modélisation procédurale met l'accent sur la création d'un modèle à partir d'un ensemble de règles, plutôt que de modifier le modèle via l'action de l'utilisateur. La modélisation procédurale est souvent appliquée quand la création d'un modèle 3D avec modeleurs 3D génériques est trop lourd (en temps et en coût de production), ou lorsque des outils plus spécialisés sont nécessaires. Ceci est souvent le cas pour les plantes, l'architecture ou les paysages.
Nodebox est un outil open source pour la création graphique procédurale statique ou animée. Il utilise une interface graphique basée sur un système de nœuds paramétriques et de la programmation en langage Python. Créé par Frederik De Bleser et Tom De Smedt dans le cadre de l'Experimental Media Group de l'école d'art Saint-Luc d'Anvers. Il est développé en langage java. Une version en HTML5, appelée Nodebox Live, est en cours de développement. Un atelier de NodeBox est dispensé par Zachary Dodson, Rupesh Vyas, dans le cursus de communication visuel à l'Université Aalto, à Helsinki, en Finlande,
En mathématiques, et plus précisément en géométrie, les pavages de Penrose sont des pavages du plan découverts par le mathématicien et physicien britannique Roger Penrose dans les années 1970. En 1984, ils ont été utilisés comme un modèle intéressant de la structure des quasi-cristaux.
Processing (autrefois typographié Proce55ing) est une bibliothèque Java et un environnement de développement libre (sous licence GNU GPL), créé par Benjamin Fry et Casey Reas, deux artistes américains. Processing est le prolongement « multimédia » de Design by numbers, l'environnement de programmation graphique développé par John Maeda au Media Lab du Massachusetts Institute of Technology. Processing est tout particulièrement adapté à la création plastique et graphique interactive. Le logiciel fonctionne sur Macintosh, Windows, Linux, BSD et Android. Il est basé sur la plate-forme Java — il permet d'ailleurs de programmer directement en langage Java. Il existe également une version en JavaScript de Processing, appelée Processing.js, cette version pouvant être exécutée dans un environnement HTML 5 ou via node.js. Une nouvelle version JavaScript est désormais proposée : p5.js. Le projet est rattaché officiellement au site mère de Processing. Les programmes réalisés avec Processing peuvent être lus par les navigateurs internet équipés du plug-in Java, mais aussi sous forme d'applications indépendantes pour Windows, Linux ou Mac OS X (en réalité n'importe quelle machine disposant d'une machine virtuelle Java).
Processing.js est le portage par John Resig du langage de programmation Processing en JavaScript (au lieu de Java). Processing.js repose sur la balise canvas et les applications conçues avec Processing.js ne requièrent donc pas la machine virtuelle Java pour fonctionner. Le code source de Processing.js tient en en seul fichier compressé de moins de 10 Kio. Les programmes réalisés avec Processing.js peuvent être affichés de manière optimum par les navigateurs web compatibles HTML 5, tels que les versions des moteurs de rendu Gecko (version 1.9), WebKit et Presto (version 9.5). Il existe également une version de processing.js appelée Node-processing, pouvant donc être déployé sur un serveur d'application node.js. Processing.js peut permettre de créer et de déployer facilement des applications Internet riches (RIA) multi plates-formes.
Procjam (abréviation de anglais : Procedural Jam (jam procédural)) est un game jam (concours libre de programmation de jeu sur quelques jours ou semaines) spécialisé dans la génération procédurale de contenu,. La finalité n'est pas forcément de faire un jeu, cela peut être une œuvre d'art interactive ou non ou tout autre application qui produit des choses. Procjam publie un magazine appelé Seeds orienté autour de la génération procédurale. Un colloque annuel est également organisé, regroupant les conférences de différents développeurs produisant des œuvres en génération procédurale.
Ansible est une plate-forme logicielle libre pour la configuration et la gestion des ordinateurs. Elle combine le déploiement de logiciels multi-nœuds, l'exécution des tâches ad-hoc, et la gestion de configuration. Elle gère les différents nœuds par-dessus SSH et ne nécessite l'installation d'aucun logiciel supplémentaire à distance sur eux. Les modules fonctionnent grâce à JSON et à la sortie standard et peuvent être écrits dans n'importe quel langage de programmation. Le système utilise YAML pour exprimer des descriptions réutilisables de systèmes. La plate-forme a été créée par Michael DeHaan, l'auteur de l'application serveur de provisioning Cobbler et coauteur du cadre de développement Func pour l'administration à distance. Les utilisateurs de Ansible comprennent le Fedora Project, Hewlett-Packard Allemagne, Basho Technologies, ALE International et l'Université Aristote de Thessalonique, ainsi qu'Airbus, La Poste et la Société générale. Il est inclus dans le cadre de la distribution Linux Fedora, propriété de Red Hat inc., et est également disponible pour Red Hat Enterprise Linux, CentOS et Scientific Linux via les paquets supplémentaires "Extra Packages for Enterprise Linux" (EPEL). Ansible Inc. était la société derrière le développement commercial de l'application Ansible. Red Hat rachète Ansible Inc. en octobre 2015. Le nom Ansible a été choisi en référence au terme Ansible choisi par Ursula Le Guin dans ses romans de science-fiction pour désigner un moyen de communication plus rapide que la lumière.
Apache ZooKeeper est un logiciel open source de la Apache Software Foundation. Il s'agit d'un logiciel de gestion de configuration pour systèmes distribués. ZooKeeper est un sous projet de Hadoop mais il est un projet top-level à part entière. L'architecture de ZooKeeper supporte une haute disponibilité grâce à des services redondants. Les clients peuvent ainsi interroger un autre leader ZooKeeper si le premier ne répond pas. Les nœuds de ZooKeeper stockent leurs données dans un espace de noms hiérarchique, tout comme un système de fichiers ou une structure de données arborescente. Les clients peuvent lire et écrire dans les nœuds et ainsi avoir un service de configuration partagée. Les mises à jour sont totalement ordonnées. ZooKeeper est utilisé par des sociétés comme Rackspace, Yahoo!, Odnoklassniki et eBay ainsi que des systèmes de recherche open source comme Solr.
Cfengine (ou GNU/cfengine) est un logiciel libre de gestion de configuration (ou gestion de parc informatique), écrit en langage C. Il permet de déployer des configurations à travers un parc informatique, de synchroniser des fichiers sur des serveurs hétérogènes (différents Unix, Linux et Windows) et d'envoyer des commandes sur ces derniers. Le projet a été initié en 1993 par Mark Burgess de l'University College d'Oslo (Norvège).
Chef est un logiciel libre de gestion de configuration écrit en Ruby. Il utilise un langage dédié (appelé domain-specific language ou DSL) en pure-Ruby pour l'écriture de configuration du système d'exploitation sous la forme de « recettes » (recipes) ou de « livres de recettes » (cookbook). Chef a été écrit par Opscode et est publié sous licence open source Apache 2.0. Chef peut être utilisé en mode client-serveur, ou dans une configuration consolidée nommée « chef-solo ». La version commerciale du produit est gratuite jusqu’à 5 serveurs, puis commence à 120 $ par mois pour 20 serveurs, et 600 $ par mois pour 100 serveurs.
La gestion de configuration consiste à gérer la description technique d'un système (et de ses divers composants), ainsi qu'à gérer l'ensemble des modifications apportées au cours de l'évolution du système. En d'autres termes, il s'agit de l'ensemble des processus permettant d'assurer la conformité d'un produit aux exigences, tout au long de son cycle de vie. La gestion de configuration est utilisée pour la configuration de systèmes complexes : en informatique, en aéronautique, en automobile, en construction navale, en pharmacie, en systèmes spatiaux, en armement, en construction ferroviaire. En informatique, la gestion de configuration peut être utilisée à plusieurs fins : Pour stocker et tracer les différentes versions ou révisions de toute information destinée à être utilisée par un système (matériel, logiciel, document, donnée unitaire, etc.). Pour déployer des configurations à travers un parc informatique sous formes de fichiers et données. Ceci est réalisé à l'aide de logiciels de gestion de versions, propriétaires ou libres. Utilisée dans le suivi de versions de logiciels, la gestion de configuration permet par exemple de gérer les codes sources. Utilisée dans le suivi de versions de documents, elle permet de tracer toutes les modifications qui sont intervenues sur les informations contenues dans les documents.  
La gestion de configuration logicielle est une discipline du génie logiciel ayant pour objet de répondre à la question : quelqu'un a obtenu un résultat. Comment le reproduire ? Le plus souvent, il ne s'agit pas de reproduire à l'identique, mais de reproduire avec des modifications incrémentales. La question est donc de comparer des résultats et d'analyser leurs différences.
m23 est un programme permettant de déployer et maintenir des distributions Debian, Ubuntu, Kubuntu Linux, Xubuntu, Linux Mint, Fedora, CentOS et openSUSE. m23 peut partitionner et formater les postes clients et installer un système d'exploitation Linux et de nombreux autres paquetages logiciels comme des suites bureautiques, des outils graphiques, des applications serveur ou des jeux grâce au système m23. La totalité de l'administration est réalisée via un navigateur web et peut se faire depuis n'importe quel ordinateur qui a accès au serveur m23. m23 est développé principalement par Hauke Goos-Habermann depuis fin 2002. m23 différencie le serveur et les postes clients. Un server m23 est utilisé pour le déploiement et la maintenance des postes clients. Les ordinateurs qui sont administrés (exemple. un logiciel est installé) grâce au serveur m23 sont les postes clients. Le poste client est démarré depuis le réseau pendant l'installation du système d'exploitation. Il est possible de démarrer le poste client depuis sa carte réseau, une disquette amorçable ou un CD amorçable. La configuration matérielle du poste client est détectée et configurée. Les informations de configuration matérielle et de partition recueillies sont transmises au serveur m23. Ensuite, ces informations sont présentées dans l'interface d'administration du serveur m23. Maintenant l'administrateur doit choisir comment partitionner et formater le poste client. Il y a aussi d'autres paramètres à configurer, comme la distribution Linux à installer sur le poste client. Le poste client m23 peut être installé comme une station de travail avec les interfaces graphique suivantes KDE 3.x, GNOME 2.x, Xfce, Unity, LXDE et natif X11 ou en tant que serveur sans interface graphique. Dans la plupart des configurations de type serveur, le serveur n'a pas besoin d'interface graphique. M23 est publié sous licence GNU GPL.
Puppet est un logiciel libre permettant la gestion de la configuration de serveurs esclaves (GNU/Linux, Mac OS X et Windows). Puppet est écrit à l'aide du langage de programmation Ruby et est diffusé sous licence Apache 2.0 pour les versions récentes de Puppet. Les versions plus anciennes (inférieures à la V2.7.0), sont sous licence GPL. La version libre permet de gérer les déploiements système et applicatif, et accepte les machines virtuelles type Amazon EC2. La version commerciale de Puppet permet en plus, de gérer les machines virtuelles VMware, d'avoir une interface graphique de gestion, d'automatiser et d'orchestrer les déploiements, d'avoir une plateforme de développement pour tous les environnements, de gérer individuellement les droits utilisateurs. Pour utiliser plus facilement Puppet, il est conseillé d'utiliser Geppetto, qui permet d'éviter des soucis de syntaxe. Puppet est utilisé dans la suite Satellite (en) de Red Hat.
Rudder est un logiciel libre de configuration automatique de serveurs (ou gestion de parc informatique). Il se veut simple d'utilisation, orienté web et applique un raisonnement dirigé par les rôles. Il s'appuie sur des agents légers installés localement sur chaque machine gérée. Développé à partir de 2010 et publié en open source en octobre 2011 par la société Normation. Son interface graphique côté serveur est développée en Scala et son agent de configuration en C. Il est publié sous la licence libre GNU General Public License 3.0.
Salt ou SaltStack est un logiciel de gestion de configuration écrit en Python, fonctionnant sur le principe client-serveur. Salt a pour but de rendre la gestion de configuration simple mais flexible. Il s'agit d'une alternative à Puppet, Ansible et Chef. On utilise les langages informatiques YAML, Jinja2 et Python pour configurer Salt.
SD-WAN est l'acronyme de Software-Defined Wide Area Network, soit réseau étendu à définition logicielle, et est présenté comme la nouvelle évolution majeure des télécommunications. Un SD-WAN facilite la gestion en séparant la partie contrôle de la partie réseau.
La gestion des actifs logiciels, équivalent français du terme anglais software asset management (SAM), est une pratique de management qui consiste à gérer et à optimiser l'achat, le déploiement, la maintenance, l'utilisation et l'élimination des actifs logiciels au sein d'une organisation. D'après Information Technology Infrastructure Library (ITIL), elle est définie comme « …all of the infrastructure and processes necessary for the effective management, control and protection of the software assets…throughout all stages of their lifecycle ». Ses objectifs sont de réduire à la fois les risques commerciaux et juridiques d'une part, et les coûts d'autre part tout en maximisant le service rendu. La gestion des actifs logiciels est un sous-ensemble de la gestion des actifs informatiques (IT Asset Management) qui fait maintenant l'objet de la norme ISO/CEI 19770.
le Berkeley Macintosh User Group abrégé BMUG est un groupe d'utilisateurs fondé en septembre 1984 par des étudiants de l'Université de Californie à Berkeley pour échanger des connaissances sur les Macintosh.   Portail d’Apple
Le groupe Mutualisation interministérielle pour un environnement de travail ouvert également appelé MIMO est un groupe de travail interministériel français. Il a pour objectif de mutualiser les moyens et les compétences afin de proposer des solutions pour le poste de travail performantes et interopérables basées sur le respect des formats ouverts et l'utilisation de logiciels libres.
Net’ Entr@ide, orthographié aussi Net' Entraide, est la première association française d’entraide informatique animée uniquement par des jeunes passionnés. Elle a été fondée en janvier 2009 à l'initiative de Yoann Nabat, Valentin Citéra et Matthieu Rupin, alors âgés de 14 ans. Le conseil d'administration de l'association est toujours composé à majorité de jeunes de moins de 18 ans. Elle a pour vocation de développer l'entraide informatique à trois échelles : entre jeunes, entre générations et entre associations. Elle mène cette action dans toute la France grâce à son site internet ainsi que par le biais d'actions ponctuelles comme la création d'une application sur smartphone de manière collaborative (une première en France). Elle a mis en place depuis septembre 2010 des ateliers d'informatique gratuits à Laval en Mayenne à destination de tous. Ces ateliers ont connue une profonde mutation en novembre 2014, avec l'apparition d'une plateforme d'entraide participative sur Internet qui vient compléter l'offre des ateliers. L'association a également pu mettre en place diverses autres activités au cours de son existence : formations personnalisées, cours à domicile, formation en espéranto, etc. L'association est également présente chaque année depuis sa création au salon Laval Virtual. En 2017, l'association a lancé une campagne de financement participatif afin de financer des achats matériels dans le but de développer les ateliers au niveau numérique. En effet, l'objectif est de diffuser les ateliers en direct sur internet, ainsi qu'en rediffusion. La campagne est disponible ici : https://www.helloasso.com/associations/net-entr-ide/collectes/net-entr-ide-a-besoin-de-vous-pour-se-developper
L’histoire de l'informatique est l’histoire de la science du traitement rationnel, notamment par machines automatiques, de l'information considérée comme le support des connaissances humaines et des communications dans les domaines techniques, économiques et sociaux.
L'histoire des ordinateurs commence au milieu du XXe siècle. Si les premiers ordinateurs ont été réalisés après la seconde Guerre mondiale, leur conception héritait de diverses expériences comme l'Harvard Mark I et le Z3, machines électromécaniques programmables commencées en 1939, et surtout de deux calculateurs électroniques : le Colossus du service de cryptanalyse britannique en 1943, l'ENIAC en 1945. À l'arrière-plan on peut mentionner des théories comme la « machine de Turing », ou des combinaisons de techniques bien plus anciennes comme les premières machines à calculer mécaniques (XVIIe siècle) et les premières machines à tisser automatisées par la lecture de cartes et de rubans perforés (XVIIIe siècle).
L'Affaire Bull s'est produite au début des années 1960, lorsque le deuxième constructeur mondial d'informatique s'est effondré en Bourse, ce qui a entraîné 700 licenciement et permis son rachat en 1963 par des actionnaires industriels français puis par le groupe américain General Electric, qui l'a ensuite revendu en 1969 à son compatriote Honeywell.
An Open Letter to Hobbyists (en français : « Une lettre ouverte aux bricoleurs ») est une lettre ouverte écrite en janvier 1976 par Bill Gates, alors cofondateur (General-Partner) de Micro-Soft, adressée aux membres du Homebrew Computer Club (dont Steve Jobs et Steve Wozniak sont encore membres à cette époque), où celui-ci exprime sa consternation face à la généralisation des infractions du droit d’auteur, notamment envers les logiciels de sa compagnie, dans cette communauté. Bill Gates y constate que la majorité des utilisateurs de Altair BASIC ne l’ont pas acheté et déclare qu’une telle violation du droit d’auteur concernant les logiciels (en) a pour effet de décourager les développeurs d’investir du temps et de l’argent pour créer des logiciels de qualité, les documenter et les déboguer. Il met en avant le fait qu’il est injuste que l’on puisse utiliser le temps, l’argent et les efforts des auteurs d’un programme sans les rétribuer en retour. Il s'agit d’une des premières confrontations entre le point de vue mercantile et le point de vue des premiers hackers sur ce que sont les logiciels. L’avis des hackers (voir infra) rappelle en partie celui qui sera formalisé plus tard dans la définition du logiciel libre.
Analyse et traitement informatique de la langue française (ATILF) est une unité mixte de recherche du Centre national de la recherche scientifique (CNRS) et de l'université de Lorraine (ex université Nancy-II).
La bulle Internet ou bulle technologique est une bulle spéculative qui a affecté les « valeurs technologiques », c'est-à-dire celles des secteurs liés à l'informatique et aux télécommunications, sur les marchés d'actions à la fin des années 1990. Son apogée a eu lieu en mars 2000. Le modèle économique du commerce électronique, lié à ce que l'on appelle l'immatériel, est rendu célèbre par Amazon, eBay et AOL des sociétés profitant d'une bulle des capitalisations boursières des jeunes sociétés sans équivalent dans l'histoire, qui finit en krach, phénomène touchant aussi de nombreuses petites sociétés de biotechnologies. En 2012, on parle de seconde bulle Internet, devant le constat d'une répétition des évènements survenus dix ans plus tôt.
Jacques Callies, né en 1894 à Annecy et mort en 1948 à Annecy-le-Vieux, est un homme d'affaires français, qui fut PDG de la société Bull de 1940 à 1948.
Jean Callies (1886-1961) est un ingénieur et homme d'affaires français, ancien dirigeant de la société informatique Bull et des Papeteries Aussedat, directeur-général de l'usine Michelin de Clermont-Ferrand.
Joseph Callies est un ingénieur et homme d'affaires français, ancien dirigeant de la société informatique Bull.
CGI Informatique était une société de services et d'ingénierie informatique (SSII) française créée le 1er juillet 1969 et disparue le 31 décembre 1999, date de son absorption par IBM. Avec 4 000 employés et un chiffre d'affaires annuel de 2 milliards de francs, elle était au début des années 1990 la 6e SSII française. On lui doit notamment l'atelier de génie logiciel Pacbase et le progiciel HR Access.
Le CII 10070 a fait partie de la première série d'ordinateurs de la Compagnie internationale pour l'informatique, dans le cadre du Plan Calcul. En tant que matériel, il s'agissait en fait d'un calculateur SDS (Scientific Data Systems) Sigma 7 - qui deviendra après son rachat par Xerox le XDS (Xerox Data Systems) Sigma 7. En revanche, sur le plan logiciel, il était muni d'un système d'exploitation totalement remanié par les équipes de l'IRIA (qui deviendra l'INRIA) et qui sera acheté par la maison mère. Cet ordinateur destiné au calcul numérique, dit calcul scientifique, disposait de mots de 32 bits, d'un adressage par octets et de 16 registres d'index. Il pouvait gérer à la fois du traitement par lots, appelé aussi batch et du temps partagé. Il disposait également en standard d'une mémoire topographique, similaire à une mémoire virtuelle à ceci près qu'elle ne visait que le remapping instantané de mémoire à mémoire pour des raisons de performance, sans support à la gestion d'échanges avec le disque. Celle-ci était gérée par logiciel dans le moniteur de temps partagé. Cet ordinateur a servi de base à la conception de la série Iris 80, totalement réalisée par la CII.
Le codage des caractères sur carte perforée est la pratique consistant à associer un ensemble de perforations différent d'une carte perforée à chaque caractère, ce qui permet d'y enregistrer un texte.
Le CODASYL (sigle de Conference on Data Systems Languages, en français « Conférence sur les langages de systèmes de traitement de données ») est l'organisme américain de codification des systèmes de bases de données. Il a publié en 1959 les spécifications du langage COBOL. Par la suite, ses travaux entre 1974 et 1981 ont donné naissance au modèle navigationnel de SGBD (opposé au modèle hiérarchique largement promu par IBM). En 1965, le CODASYL a constitué un groupe de travail chargé de développer les extensions du langage COBOL qui permettront de manipuler des données en préservant l'indépendance entre les applications et les périphériques de stockage. La contribution de Charles Bachman revient à la logique de chaines de données et de pointeurs mise notamment en œuvre dans le système IDS. En 1967, ce groupe de travail s'est rebaptisé Data Base Task Group (DBTG) qui publiait notamment COBOL extensions to handle data bases (extensions de COBOL pour manipuler des données). En octobre 1969, le DBTG publie alors les premieres spécifications du modèle de bases de données réseau qui prendra très vite le nom de modèle de données CODASYL. Le CODASYL a défini en 1970 les normes de standardisation des Systèmes de Gestion des Bases de Données (SGBD), uniquement mis en œuvre sur les grands systèmes à cette époque. Le langage de manipulation des données était alors exclusivement le COBOL et le databasic. C'est la première norme de base de données décidée sans l'avis d'IBM.
Le système Community Memory (CM) est le premier bulletin board system (BBS) informatisé public, mis en place en 1973 à Berkeley, en Californie. Il est basé sur un ordinateur à temps partagé SDS 940, placé à San Francisco, connecté à un téléscripteur placé dans un magasin de disques de Berkeley, communicant à 110 bauds. Les utilisateurs pouvaient utiliser le téléscripteur pour envoyer leurs messages vers l'ordinateur, ou parcourir sa mémoire pour lire les réponses ou les anciens messages. Community Memory était initialement pensé pour servir de moyen d'échange de ressources et d'informations pour des organisations économiques, éducatives ou sociales, généralement émanant de la contre-culture. Il devait ainsi les mettre en relation entre elles et avec le public. Community Memory devient vite un «marché aux puces de l'information », qui donne accès à des bases de messages non modérés et bidirectionnels au public, grâce à des terminaux informatiques. Une fois le système mis en place, les utilisateurs en font un mode de communication général, utilisé pour des discussions sur l'art, la littérature, le journalisme, le commerce, les questions de société, ou pour se sociabiliser.
Le Computer Science Network (CSNET) est un réseau informatique créé en 1981 aux États-Unis. Son but était d'étendre l'accès au réseau ARPANET des établissements universitaires et de recherche qui ne pouvaient y être connectés directement. Il a joué un rôle important dans le développement d'Internet. CSNET a été financé par la National Science Foundation (NSF) pour une période initiale de trois ans de 1981 à 1984.
Le Datanet est l'ordinateur le plus vendu par la Compagnie internationale pour l'informatique (CII), avant et après sa fusion avec Honeywell, dans le cadre de sa stratégie réseau New Network Architecture, rebaptisée en 1976 Distributed System Architecture. Il est équipé d'un processeur dit « frontal » et se veut « un processeur de réseau unifié », développé sur le mini-ordinateur Mitra 15 puis sur Mini 6, reliés à l'Iris 80.
Le Délégation générale à l'informatique est une instance créée en septembre 1966 dans le cadre du Plan Calcul pour stimuler la production de logiciels et de matériels informatiques. L'un de ses premiers animateurs fut le gaulliste Robert Galley.
Le Deltar (Delta Getij Analogon Rekenmachine, en néerlandais: calculatrice analogique des marées du plan Delta) était un calculateur analogique, utilisé de 1960 à 1984 pour la conception et la mise en œuvre des travaux du plan Delta. L'ordinateur a été conçu et construit dans le but d'effectuer les calculs complexes nécessaires pour prédire les effets des barrages, digues et barrage anti-tempête sur les marées dans l'estuaire du delta de la Meuse et du Rhin et de l'Escaut. La conception du Deltar était fondée sur l'analogie qui existe entre les propriétés et le comportement de l'eau et de l'électricité, en faisant des analogies avec les éléments tels que la hauteur de l'eau, son débit et son stockage. La conception du calcul utilisait essentiellement les notions électriques, charge, potentiel, inductance et capacité. Après 1984, cet outil a été démonté et en parti détruit. Quelques éléments se retrouvent aujourd'hui dans le Deltapark Neeltje Jans.   Portail des Pays-Bas  Portail de l’informatique
Jusqu'au début des années 1980, la loi de Grosch a semblé s'imposer : disposer de machines puissantes consultables par des terminaux intelligents, mais passifs, semblait la voie d'avenir pour l'informatique. À cette date, on commença pourtant à changer d'avis et à parler de downsizing. Le principe du downsizing était de remplacer au contraire une machine puissante par un réseau de petites machines, souvent UNIX ou VMS, qui bénéficiaient davantage que les ordinateurs centraux des effets d'économie d'échelle, les rendant plus intéressantes économiquement même si cela se traduisait par une efficacité technique légèrement inférieure (nombreux transferts de données, par exemple), celle-ci étant gommée par le débit important des liaisons ethernet entre les machines. Le mot downsizing a été très employé au début des années 1990.
Le Dynabook est un projet d'ordinateur portable à usage créatif pour enfants, conduit au PARC de Xerox en 1972, par Alan Kay. Ce projet a été notamment inspiré par les travaux de Seymour Papert sur le langage LOGO, élaboré au Massachusetts Institute of Technology. Les contraintes de fabrication d'un Dynabook furent l'estimation de la taille, du poids, du coût et des capacités. Moins cher qu'une télévision sera la référence d'Alan Kay. Le Dynabook se perçoit comme un mélange entre un livre, un piano, un outil, un jouet et un moyen d'expression, avec toutes leurs vertus éducatives. Grosso Modo, le coût des manuels scolaires de l'école primaire au lycée peut être estimé entre 100 et 200 euros par élève et par an. Un Dynabook doit pouvoir servir au moins 3 ans. Donc s'il respecte cette contrainte de fiabilité, le coût serait amorti. Mais voir le Dynabook comme un remplaçant du livre scolaire serait une erreur : l'objectif est de faire de l'ordinateur l'objet d'une réflexion pour l'enfant, notamment en l'incitant à le programmer. Aussi, mettre le langage Smalltalk à la portée des plus jeunes sera l'objectif principal du projet d'Alan Kay. Le premier prototype du système d'exploitation du Dynabook s'appelle Bilbo. Les chercheurs du PARC sont bien conscients que les performances prévues pour le Dynabook dépassent encore la puissance disponible dans un portable, aussi ce prototype a la taille d'une machine à laver. Il exécute 6MIPs, dispose de 128K de mémoire vive, interface graphique, et il est muni d'une tablette graphique à stylographe comme GRAIL. La première préoccupation de ces concepteurs est de mettre au point un affichage dynamique de caractères de haute résolution. L'affichage est de 500.000 pixels sur un écran de 8.5" x 11". Pour comparer avec des standards qui nous sont bien connus, cela correspond à une définition de 915 x 546 pixels sur un écran de 14". L'utilisateur pourra choisir sa police de caractère lors de la lecture de n'importe quel document, les menus déroulants et les fenêtres multiples voient le jour, tout ceci grâce à Smalltalk. Le langage de programmation permet de manipuler et de construire toutes sortes de programmes. Les premiers disponibles sont des outils de dessin et de peinture, d'animation et de musique. Le prototype sera soumis aux tests de 200 utilisateurs, essentiellement des étudiants et des écoliers. Mais, tel qu'il fut imaginé par son concepteur, le Dynabook demeura un rêve...
L'Éthique des hackers (titre original : Hackers: Heroes of the Computer Revolution) est un livre écrit par Steven Levy traitant de la culture hacker (en). Il a été publié en 1984 à Garden City (New York) par Doubleday et dont la traduction est parue en France début 2013. Levy y décrit les personnes, les machines et les événements qui vont former la culture des hackers et leur éthique. Une seconde édition a vu le jour en 1994, sous-titrée Afterword: Ten Years After (« Épilogue : dix ans plus tard ») : l'auteur ajouta un épilogue pour l'occasion. Pour le 25e anniversaire en 2010, O'Reilly en publie une version mise à jour.
Free Software, Free Society: Selected Essays of Richard M. Stallman est une collection d'essais écrits par Richard Stallman. La première édition a été publiée en 2002 par les éditions GNU Press selon les termes de la licence de documentation libre GNU. La seconde version, publiée en 2010, contient les versions mises à jour des essais originels ainsi que de nouveaux essais.
Créé en 1958 par la Compagnie des Machines Bull (France), le Gamma 60 était le premier ordinateur multitâches, et un des premiers à embarquer plusieurs processeurs (voir multiprocesseur). Il comportait aussi plusieurs unités d'entrée et de sortie : tambours magnétiques, bandes magnétiques, lecteurs de cartes, perforateurs de cartes, imprimantes, lecteurs de bande papier, perforateurs de bande papier, et un terminal. Au total, 20 unités furent produites.
Le garage Apple est un garage de maison californienne de 1951, du 2066 Crist Drive à Los Altos, dans la baie de San Francisco, près de Palo Alto dans le Silicon Valley en Californie aux États-Unis. C'est dans ce garage de la demeure de la famille Jobs, classée California Historical Landmark en 2013, que débute l'histoire d'Apple, avec les deux amis d'école adolescents Steve Jobs et Steve Wozniak, cofondateurs de la société Apple en 1976,,,,.
Le « garage Google » est le nom donné au garage loué par Larry Page et Sergueï Brin où débute l'histoire officielle de l'entreprise Google le 4 septembre 1998. Attenant à une maison californienne, il est situé au 232 Santa Margarita à Menlo Park, près de l'Université Stanford dans la Silicon Valley en Californie (États-Unis). Actuellement propriété de la société Google, il n'est pas un lieu visitable mais attire à l'extérieur de nombreux curieux.
Le garage Hewlett-Packard est un ancien garage de Palo Alto, dans la baie de San Francisco, en Californie, devenu lieu historique de Californie et des États-Unis. Il s'agit du modeste garage de l'ancien domicile de style géorgien de David Packard et de son épouse Lucile, qui ont vécu au rez-de-chaussée, au 367 Addison Avenue à Palo Alto, à proximité de l'Université Stanford, dans la Silicon Valley. William Hewlett et David Packard y ont développé dans les années 1930 leur premier produit électronique et fondé la société Hewlett-Packard (HP) en 1939, rapidement devenu une des plus importantes entreprises d'électronique, d'instrumentation puis d'informatique du monde (classement mondial des entreprises leader par secteur). Ce garage de « startup », est à ce jour, avec les garage Apple et garage Google un des symboles du rêve américain / mythe fondateur, et est considéré officiellement par l'État de Californie comme le point de départ de la Silicon Valley, par son enregistrement en tant que California Historical Landmark en 1987, protection étendue en 2007 par son placement sur la liste des National Register of Historic Places.
Grif (Grenoble Interactive Formatter) était un logiciel d'édition WYSIWYG et visualisation de documents structurés conformes à la norme SGML.  La technologie dont est issu Grif a été initialement développée au début des années 1980 à l'INRIA/IMAG par Irène Vatton et Vincent Quint dans le cadre du projet Tigre. Industrialisé sous le même nom Grif par la société GIPSI S.A., GIE entre Bull et l'INRIA spécialisée dans les terminaux X Window, Grif a rapidement évolué pour atteindre en 1991 un produit d'édition/visualisation WYSIWYG performant. En 1992, GIPSI S.A. commençant à connaître des tensions fortes sur le marché des terminaux X-Window, le dirigeant de l'équipe Grif et quelques autres ont décidé d'un spin-off et fondé l'entreprise Grif S.A. centrée sur les technologies SGML de GIPSI S.A. En 1991, Grif était donc un moteur de restitution disponible sous X-Window implémentant les technologies suivantes : un validateur/parseur SGML le langage S pour décrire les modèles documentaires d'une manière plus précise et typée que les DTD de SGML, le langage P pour décrire les présentations associées à un modèle documentaire donné ; le langage P n'ayant pas à gérer les contraintes de rendu progressif auxquelles sont soumis les navigateurs Web, P offrait des fonctionnalités supérieures aux CSS, comme la position précise d'un élément par rapport à un élément arbitraire, le langage T pour décrire des transformations/filtres d'exportation d'un document ; cette technologie est comparable à XSLT, un formateur gérant le multi-vues, c'est-à-dire capable d'afficher simultanément plusieurs vues d'un unique document, chaque vue appliquant une feuille de styles (une présentation en langage P) donnée, un éditeur WYSIWYG des tableaux CALS, un modèle de tableaux issu du Department of Defense américain et qui est l'ancêtre du modèle de tableaux de HTML, une implémentation d'EuroMATH, ancêtre de MathML. C'est à cette époque que Tim Berners-Lee et Robert Cailliau ont pris contact avec Grif S.A. pour lui demander de transformer l'éditeur/visualiseur Grif en un éditeur/navigateur Web. Malgré la faible charge de travail requise pour un tel développement, Grif S.A. a souhaité obtenir un financement européen. Tim Berners-Lee a relaté lui-même cet épisode dans son livre "Weaving the Web". L'arrivée lente des financements, le vent du Web et le fait que HTML soit un modèle de documents issu de SGML pousse alors Grif S.A. progressivement à développer l'éditeur de pages HTML Symposia avec toujours la collaboration de l'INRIA/IMAG. Malgré d'excellentes bases, Symposia qui est arrivé trop tard sur le marché, trop cher et ne disposant pas d'une interface homme-machine assez conviviale ne pourra contrer Netscape Gold, GNN Press et surtout HoTMetaL. En 1994, et malgré la qualité de son formateur ou "rendering engine", des technologies sous-jacentes et de la disponibilité du produit également sur Windows et Macintosh, Grif reste un produit à marché de niche essentiellement français (défense, industrie pharmaceutique, énergie) et dont les ventes sont décevantes. La société Grif S.A. dépose son bilan en 1997, une partie de l'équipe technique et les technologies étant alors reprises par l'éditeur de logiciels canadien I4I qui finira par enterrer le produit et disperser l'équipe. Le noyau de Grif a cependant survécu jusqu'à aujourd'hui : Irène Vatton et Vincent Quint, toujours eux, avaient extrait de Grif une bibliothèque centrale, la bibliothèque Thot sur laquelle ils ont conçu l'éditeur de documents pour le Web Amaya. Amaya est depuis devenu un projet commun W3C/INRIA.
L’histoire de l'informatique de gestion est l'histoire de l'ensemble des connaissances, des technologies, et des outils en rapport avec la gestion de données, c'est-à-dire la collecte, la vérification et l'organisation de grandes quantités d'informations. L'informatique de gestion a de nombreuses applications pratiques dans les entreprises : listes de clients, de fournisseurs, de produits, comptabilité, etc. En informatique de gestion, les informations sont souvent placées dans des bases de données et traitées par l'intermédiaire de logiciels spécialisés que sont les Systèmes de gestion de base de données. L’informatique de gestion constitue, avec les utilisations militaires, industrielles et scientifiques, l'un des domaines essentiels d'application qui ont permis le développement rapide de l'informatique. On peut même dire que l'informatique de gestion est en grande partie à l'origine des méthodes modernes de conception et de réalisation.
Cet article évoque les événements majeurs de l'histoire des langages de programmation. Pour une chronologie détaillée de ces événements, consultez la chronologie des langages de programmation.
On peut faire remonter l'histoire de l'ordinateur personnel, comme celle des appareils électroniques grand public du marché de masse, à 1972 avec l'introduction du microcontrôleur HP 9820 PC, ou à de petits calculateurs électroniques des années 1950 comme l'IBM 610, ou à certains mainframes et mini-ordinateurs conçus comme des systèmes mono-utilisateur. Un ordinateur personnel est destiné avant tout à un usage individuel, par opposition à un ordinateur central où les demandes de l'utilisateur final sont filtrés par l'organisation du service et le système d'exploitation, ou un partage du temps système dans lequel un processeur est partagé entre de nombreuses personnes. Après le développement du microprocesseur, les ordinateurs personnels individuels étaient devenus assez peu coûteux pour qu'ils soient finalement devenus des biens de consommation abordables. Les premiers ordinateurs personnels - généralement appelés « micro-ordinateurs » - étaient souvent vendus sous forme de kits et présentaient un intérêt surtout pour les amateurs et les techniciens.
Cet article est consacré à l'histoire des processeurs.
L'histoire du logiciel libre est intimement imbriquée avec celle de l'informatique et celle du génie logiciel. Elle ne commence en tant que telle qu'à partir du moment où est apparu le besoin de distinguer le logiciel libre du logiciel propriétaire, selon le principe énoncé pour Le Cru et le cuit. Ces prémices datant de la fin du XXe siècle, il ne peut s'agir d'une discipline académique. L'histoire du logiciel libre est donc présentée ici de façon informelle.
Le Homebrew Computer Club est un club d'informatique de la Silicon Valley entre 1975 et 1986. Des passionnés d'informatique s'y retrouvaient régulièrement, les plus célèbres d'entre eux sont Steve Jobs et Steve Wozniak, fondateurs d'Apple Inc. C'est à cette association que Bill Gates envoie une lettre ouverte (An Open Letter to Hobbyists) en janvier 1976, accusant ses membres de vol.
Grace Murray Hopper, née le 9 décembre 1906 à New York et morte le 1er janvier 1992 dans le comté d'Arlington, est une informaticienne américaine et Rear admiral (lower half) de la marine américaine. Elle est la conceptrice du premier compilateur en 1951 (A-0 System) et du langage COBOL en 1959.
L'IBM 801 est un ordinateur expérimental d'IBM, le premier à implémenter la technologie RISC (Reduced instruction set computer), qui part de l'idée de la simplification du jeu d'instructions du microprocesseur. Le projet a été mené par George Radin à partir des idées de John Cocke à partir de 1975. Le processeur du 801, de type 32 bits, a été opérationnel à l'été 1980. La fréquence d'horloge était d'environ 15.15 Mhz et avait une puissance de 15 MIPS.
L'informatique ubiquitaire (ou plus succinctement "ubicomp") est la troisième ère de l'histoire de l'informatique, qui succède à l'ère des ordinateurs personnels et celle des ordinateurs centraux.
L'International Federation for Information Processing (IFIP) est une organisation non-gouvernementale reconnue par les Nations unies, liée à plus de 50 sociétés et académies de sciences nationales et internationales, qui regroupe plus d'un million professionnels de l'informatique et des réseaux.
L'histoire de l'iPhone, le smartphone commercialisé par Apple en 2007, débute par la demande de Steve Jobs à ses ingénieurs de travailler sur les écrans tactiles et un ordinateur de type tablette (projet qui s'est concrétisé en 2010 avec l'iPad). L'iPhone présente des similitudes avec le Newton MessagePad de 1993, lequel était déjà « tout écran ».
L'ordinateur Iris 50 est un des ordinateurs commercialisés par la Compagnie internationale pour l'informatique (CII) dans le cadre du plan Calcul à la fin des années 1960. Conçu pour le marché civil, il a pris la suite du CII 10070 (SDS Sigma 7). Une anecdote veut qu'en 1969, la CII soit venue à l'IRIA lui enlever un bloc mémoire de l'Iris 50 de son centre de calcul et le donner à un client privé en difficulté. En même temps que la CII construisait l'Iris 50, elle devait étudier pour l'armée des déclinaisons militaires appelées P0M puis la P2M puis P2MS. La version Iris 35 M, utilisée entre particulier pour traiter les informations nécessaires au tir du missile Pluton, avait une mémoire à tores magnétiques constituée d'éléments de 16 kilooctets chacun; acceptant des conditions d'ambiances sévères, ses principaux périphériques étaient une imprimante, un écran et des modems. La CII a alors conclu à l'impossibilité, pour ses équipes, de créer une autre unité centrale compatible Iris 50. Elle décide alors d'adopter l'architecture Sigma 9, inspiré par le Sigma 7 et commercialisée de la SETI, l'une des 3 sociétés qui avaient fusionné en 1966 pour créer la Compagnie internationale pour l'informatique. L'Iris 50 est donc d'abord fourni avec le système d'exploitation Siris 7, et diffère très peu, pour les premières livraisons, d'une CII 10070. Son successeur, l'Iris 80, a été considérablement transformé et amélioré, tant du côté des composants, passés du DTL au TTL, que du système d'exploitation (Siris 7/8) sur lequel les chercheurs de l'IRIA ont travaillé pour en augmenter la rapidité.
L'ordinateur Iris 80 est l'ordinateur le plus puissant réalisé par la Compagnie internationale pour l'informatique (CII) dans le cadre du Plan Calcul. Conçu pour le marché civil, il a pris la suite du CII 10070 (SDS Sigma 7 importé des États-Unis ou construit sous licence) dans une politique de compatibilité ascendante. Compte tenu d'une politique de préférence nationale que le Plan Calcul imposait au secteur public, cet ordinateur a équipé quatre des quelque vingt centres de calcul universitaires au milieu des années 1970, ainsi que l'I(N)RIA et d'autres organismes de recherche. Il en a été livré une centaine, dont 27 bi-processeurs. Le CS 40 en a été dérivé pour la commutation téléphonique.
ISWIM est un langage de programmation abstrait (ou plus précisément une famille de langages de programmation) conçu par Peter J. Landin et décrit dans un article célèbre intitulé Les 700 prochains langages de programmation et publié dans les Communications of the ACM en 1966. Son acronyme ISWIM signifie « If you See What I Mean » (« Si vous voyez ce que je veux dire »). ISWIM a fortement influencé la conception des langages de programmation qui l'ont suivi, en particulier dans le domaine de la programmation fonctionnelle, de SASL (en) à Haskell, en passant par ML et Miranda (en) et leurs successeurs, et dans le domaine de la programmation dataflow comme Lucid (en) et Lustre. En fait, dans son article, Landin définit les grands principes de ce que devaient être, en 1965, les langages de programmation du futur. À cette époque, ALGOL 60, Lisp et PL/1 venaient de naître. Fortran et Cobol étaient les seuls langages de programmation largement utilisés.
iWoz (iWoz: From Computer Geek to Cult Icon: How I Invented the Personal Computer, Co-Founded Apple, and Had Fun Doing It) est le livre autobiographique de Steve Wozniak, cofondateur d'Apple avec Steve Jobs, paru en 2006 aux États-Unis et en 2011 en France. Le titre débutant en « i » est une référence au titre d'« iCEO » pris par Steve Jobs en 2000. Ce livre est l'occasion pour Steve Wozniak de « mettre certaines choses au clair », de donner sa version d'évènements de l'histoire d'Apple (la genèse de l'Apple I, son départ en 1985), de détailler ses études et sa vie personnelle ainsi que la transition à l'ère des ordinateurs personnels. Gina Smith et Steve Wozniak se rencontrent en 2005 lors d'un concert de rock grâce à une amie commune, et l'idée du livre nait très rapidement. S'en suivent cinquante-six entrevues de deux heures chacune dans deux restaurants de San Francisco et de Campbell en Californie. Divisé en vingt chapitres, iWoz revient sur les nombreuses inventions de Steve Wozniak depuis l'âge de 6 ans, son goût pour les plaisanteries hérité de sa mère et les valeurs morales héritées de son père. La parution en France, 5 ans après la sortie outre-Atlantique, fait suite à la sortie le même mois de la biographie de Steve Jobs, 19 jours après sa mort. La réédition de septembre 2013 intervient un mois après la sortie en salles du film Jobs, où son rôle est interprété par Josh Gad.
Peter de Jager est un ancien employé canadien d'IBM. À partir de 1980 environ, Peter de Jager a alerté en interne chez IBM sur le risque constitué par le passage informatique à l'an 2000, qui fut surnommé par la suite Y2K aux États-Unis (correspondant au bogue de l'an 2000 en France). En 1995, Peter de Jager a fait un discours devant le Sénat des États-Unis, et ouvert un site web (year2000.com), qui fut, entre 1995 et 1999, le site web le plus interconnecté au monde. En l'an 2000, il a revendu les droits sur son site, et démarré une activité de consultant en change management. Peter de Jager est cité sur le site de l'ADELI.
Josef Kates, né le 5 mai 1921 à Vienne est un ingénieur canadien, connu pour ses recherches sur les premiers jeux informatiques avec Bertie the Brain et pour avoir inventé le premier système de contrôle du trafic signalétique automatique.
Peter John Landin (5 juin 1930, Sheffield - 3 juin 2009) est un chercheur en informatique britannique dans le domaine de la conception des langages de programmation. Il a conçu la machine SECD, le langage de programmation ISWIM et la règle d'indentation comme syntaxe (off-side rule en anglais) et a proposé le terme de sucre syntaxique. Landin a souvent été plagié dans les titres d'articles d'informatique après qu'il a intitulé le sien, « Les 700 prochains langages de programmation ». Landin a été un chercheur actif dans le milieu des années 1960, quand il a travaillé avec Christopher Strachey.
Joseph Carl Robnett Licklider (né le 11 mars 1915 - décédé le 26 juin 1990) est un informaticien américain aussi connu sous les noms de J.C.R. ou Lick. Après des premiers travaux en psychoacoustique, il s'intéressa très vite aux technologies de l'information. À l'instar de Vannevar Bush, les idées de J.C.R. Licklider contribuèrent au développement de l'Internet. Il anticipa l'interconnexion en réseau des ordinateurs dotés d'interfaces utilisateurs conviviales. Il imagina bien avant leur naissance l'informatique graphique, les interfaces basées sur des dispositifs de pointage, les bibliothèques numériques, le commerce électronique, la banque en ligne, et même l'idée des programmes distants qui migreraient via le réseau où l'on aurait besoin d'eux. On le dénomma « le pionnier de l'informatique », pour avoir semé les « graines informatiques » de l'ère numérique. Licklider eut un rôle particulièrement important dans la conception, le financement et la gestion de la recherche qui a conduit à l'ordinateur personnel et à l'Internet. Son papier sur la symbiose Homme-Machine (Man-Computer Symbiosis) préfigure l'informatique interactive et il continua de financer les premiers efforts sur le temps partagé et le développement d'application, le plus notable étant le travail de Douglas Engelbart qui fonda le Augmentation Research Center au Stanford Research Institute et créa le fameux On-Line System. Il joua un rôle similaire en concevant et finançant les premières recherches sur les réseaux et notamment ARPAnet. Son papier de 1968 sur L'Ordinateur comme outil de communication (The Computer as a Communication Device) prédit l'utilisation de réseaux d'ordinateurs pour des groupes ayant les mêmes centres d'intérêts ainsi que la collaboration sans critères de localisation. En 2013, il a été admis, à titre posthume, au temple de la renommée d'Internet, dans la catégorie des pionniers.
Voici une liste de publications importantes en informatique, organisés par domaine. Quelques raisons pour lesquelles une publication peut être considérée comme importante : sujet créateur – Une publication qui a créé un nouveau sujet ; découverte – Une publication qui a changé de manière significative les connaissances scientifiques ; influence – Une publication qui a considérablement influencé le monde, ou qui a eu un impact massif sur l'enseignement de l'informatique.
LTR (ou Langage Temps Réel) est un langage de programmation développé en France au sein de la société Thomson-CSF. Il permet de programmer des séquences propres à être exécutées par un moniteur temps réel préemptif (Chronos),.
La machine analytique (analytical engine en anglais) est une machine à calculer programmable imaginée en 1834 par le mathématicien anglais Charles Babbage. Il ne la réalisera jamais (sauf pour un prototype inachevé), mais il passera le reste de sa vie à la concevoir dans les moindres détails. Le plus jeune de ses fils, Henry Babbage, en construira l'unité centrale (le moulin) et l'imprimante de 1880 à 1910. C'est pendant le développement d'une machine à calculer destinée au calcul et à l'impression de tables mathématiques (machine à différences) que Babbage eut l'idée d'y incorporer des cartes du métier Jacquard, dont la lecture séquentielle donnerait des instructions et des données à sa machine, et donc imagina, malgré des différences notables de fonctionnement (elle fonctionnait à la vapeur avec des roues et engrenages mécaniques), l'ancêtre des ordinateurs modernes. C'est au cours du développement de cette machine que Ada Lovelace formalise les idées de Babbage et développe la notion de programme pour la première fois dans l'histoire.
Micral est une gamme d'ordinateurs conçus et commercialisés par le bureau d'études R2E depuis 1973. Le tout premier modèle de cette gamme, nommé « Micral », possède la caractéristique d'être basé sur un microprocesseur et d'avoir des dimensions réduites. Il est ainsi reconnu comme étant le premier micro-ordinateur de l'histoire.
Le Mini 6 est l'un des mini-ordinateurs commercialisés par la Compagnie internationale pour l'informatique (CII), qui permettait de fonctionner en relation avec un grand système et fut souvent commercialisé dans des applications de teleprocessing. Il était combiné avec le Mitra 15 pour relayer les informations au niveau régional chez EDF, avant de connaitre des usages plus larges.
Le Mitra 15 est, avec l'Iris 80, l'un des ordinateurs réalisés par la Compagnie internationale pour l'informatique (CII) dans le cadre du plan Calcul. Il a été commercialisé de 1971 à 1985 et permettait de fonctionner en relation avec un grand système. Il fut fabriqué à un millier d'exemplaires pour la CII jusque 1975 dans l'usine de Toulouse, puis à Crolles dans la banlieue de Grenoble. Au total plus de 7 000 exemplaires ont été fabriqués.
The Mother of All Demos ((en) littéralement « la mère de toutes les démos ») est le nom souvent donné à la démonstration de Douglas Engelbart qui s’est déroulée le 9 décembre 1968 au Bill Graham Civic Auditorium (en) à San Francisco.
La Musique sur internet a beaucoup évolué, de la création du format MP3 jusqu'aux baladeurs (iPod et autres), le site YouTube de Google, etc. La gestion des droits numériques a eu un impact important sur l'histoire de la musique sur Internet.
Chuck Peddle (né en 1937 à Bangor (Maine), États-Unis) est un ingénieur américain connu pour avoir mis au point la famille des processeurs MOS Technology 6502 dans les années 1970.
La Peugeot Quasar est le premier concept car historique du constructeur automobile français Peugeot. Baptisé du nom des galaxies Quasar, il est présenté au mondial de l'automobile de Paris 1984, et exposé au musée de l'Aventure Peugeot de Sochaux.
Le plan Calcul était un plan gouvernemental français lancé en 1966 par le président Charles de Gaulle sur l'impulsion de Michel Debré et d'un groupe de hauts fonctionnaires et d'industriels, destiné à assurer l'autonomie du pays dans les technologies de l'information, et à développer une informatique européenne.
Le Plan Composants, qui a pris la suite du Plan Calcul, était un plan gouvernemental français lancé en 1978 par le ministère de l'industriels, destiné à accélérer le développement de l'industrie française des circuits intégrés
Le project Cybersyn ou en espagnol, proyecto Synco a été un projet chilien visant à créer une économie planifiée contrôlée par ordinateur en temps réel durant les années 1970–1973 (sous le gouvernement du président Salvador Allende). Il s'agissait essentiellement d'un réseau de télex qui reliait les entreprises à un ordinateur central situé à Santiago qui était contrôlé suivant les principes de la cybernétique. Le principal architecte de ce système était le scientifique britannique Stafford Beer.
QDOS ou 86-DOS est un système d'exploitation développé et vendu par la société Seattle Computer Products pour leurs kits basés sur le processeur Intel 8086. Sa structure de commandes et son Interface de programmation étaient compatibles avec ceux de CP/M ce qui facilitait le portage des applications d'un système à l'autre. Appelé QDOS qui signifie Quick-and-dirty Operating System, soit « système d'exploitation rapide et sale » pendant sa phase de développement, il fut renommé 86-DOS une fois commercialisé. Acheté par Microsoft, 86-DOS est l'ancêtre de MS-DOS.
Le retrocomputing, ou rétro-informatique, est une activité consistant à utiliser du matériel et des logiciels informatiques obsolètes. Il s'agit d'un loisir ludique et/ou intellectuel. Concrètement, il peut s'agir de se divertir avec des jeux vidéo anciens (retrogaming) ou de s'amuser à faire fonctionner des ordinateurs généralement jugés inutilisables.
Richard Stallman et la révolution du logiciel libre (ISBN 978-2-212-12609-9) est un récit biographique à propos de Richard Stallman publié sous licence libre le 21 janvier 2010 par la maison d’édition Eyrolles et le projet Framasoft, écrit par Sam Williams (en) (auteur de la version anglaise), Richard Stallman et Christophe Masutti (Framasoft).
Seattle Computer Products (SCP) était une entreprise qui fabriquait des ordinateurs à Seattle et qui fut l'une des premières à fabriquer des systèmes informatiques basés sur le processeur Intel 8086. La SCP était composé en partie de lycéens venant des communautés voisines qui soudaient et assemblaient les ordinateurs. Certains d'entre eux travaillèrent plus tard pour Microsoft. C'est à l'âge de vingt-deux ans, en 1978, que Tim Paterson fut embauché par Rod Brock qui possédait à l'époque la société SCP. En 1980, Tim Paterson créa, en quatre mois à peine, le système d'exploitation QDOS, connu plus tard sous le nom de 86-DOS. En décembre 1980, la société Microsoft cherchait un système d'exploitation pour le futur PC d'IBM. Elle offrit donc la possibilité à une entreprise tierce de fabriquer ce système, pour 50 000 $. La société SCP fut donc retenue et avant même le lancement de QDOS, Microsoft racheta les droits pour 50 000 $ ; SCP l'accusa donc plus tard de fraude, car Microsoft aurait oublié de préciser qu'IBM était le client de la commande (bien qu'il semble que ce fût un droit de Microsoft lié à l'accord commercial). Microsoft arrangea donc un compromis avec la société SCP, en 1986, en payant 1 million de dollars supplémentaires. Par la suite la société SCP disparut, car le marché des processeurs Intel 8086 fut dominé par les ordinateurs compatibles PC.
Sperry Univac fut d'abord la division informatique de Remington Rand, formée à la suite du rachat en 1950 de Eckert-Mauchly Computer Corporation. UNIVAC est l'acronyme de UNIVersal Automatic Computer.
Christopher Strachey (né le 16 novembre 1916, décédé le 18 mai 1975) est un informaticien britannique. Il est surtout connu comme l'un des premiers à avoir prôné une approche mathématique dans l'étude des programmes.
Symbolics (Symbolics, Inc) est une entreprise informatique américaine fondée en 1979 par Russell Noftsker dont l'objectif était de commercialiser les machines Lisp du laboratoire d'intelligence artificielle du MIT.
Le terme systèmes ouverts peut désigner à la fois des systèmes informatiques : - qui fournissent un ensemble d'avantages en interopérabilité, portabilité, et standards ouvert de logiciel; - ou configurés pour permettre des accès non restreints par des personnes et/ou des ordinateurs ; Cet article ne traite que du premier sens. Le terme trouve son origine à la fin des années 1970 et au début des années 1980, principalement pour décrire les systèmes basés sur le système d'exploitation Unix, spécialement par opposition aux systèmes dits propriétaires, ordinateurs centraux et micro-ordinateurs en usage à cette époque. Au contraire de ces anciens systèmes propriétaires, la nouvelle génération de systèmes Unix s'est caractérisée sur des interfaces de programmation standardisées et sur des interconnexions de périphériques ; le développement du matériel et du logiciel par des tiers fut encouragé, en partant de la norme de cette époque, qui voyait des compagnies comme Amdahl et Hitachi se tourner vers la justice pour obtenir le droit de vendre des systèmes et des périphériques qui étaient compatibles avec les ordinateurs centraux d'IBM. On peut dire que la définition de "système ouvert" s'est formalisée dans les années 1990, avec l'émergence de standards de logiciels administrés de façon indépendante comme Single UNIX Specification de l'Open Group. Bien que les utilisateurs d'ordinateurs aujourd'hui soient habitués à un haut niveau d'interopérabilité entre le matériel informatique et le logiciel, avant l'an 2000, les systèmes ouverts purent faire l'objet d'une promotion de la part de fournisseurs Unix, car ils avaient un avantage compétitif significatif. IBM et d'autres compagnies résistèrent à cette tendance pendant des décennies, dont un exemple significatif est donné par la déclaration d'un commercial d'IBM en 1991, selon lequel on devrait être « prudent de ne pas rester enfermé dans les systèmes ouverts ». Cependant, dans la première partie du XXIe siècle, beaucoup de ces fournisseurs de systèmes propriétaires, particulièrement IBM et Hewlett-Packard, commencèrent à adopter Linux comme une partie de leur stratégie commerciale d'ensemble, avec open source markété comme un atout de « système ouvert ». En conséquence, un ordinateur central IBM avec open source Linux on zSeries est markété comme étant plutôt un système ouvert que des serveurs utilisant Microsoft Windows — ou même ceux utilisant Unix, en dépit de leur héritage de systèmes ouverts. En réaction, de plus en plus de compagnies ouvrent le code source de leurs produits, avec l'exemple notable de Sun Microsystems et la création de Openoffice.org et des projets OpenSolaris, basés sur leurs produits logiciels dont le code source était préalablement fermé, StarOffice et Solaris.
Le T2000 était un mini-ordinateur fabriqué à Echirolles par la société Télémécanique et destiné aux applications temps réel. Introduit en 1968, cet ordinateur avait été conçu par Verjus et Sempé, après le MAT01 de Mors. Il reçut un prix de design industriel. Il utilisait un mot de 19 bits, en bloc de 4000 mots, et, pour un accès rapide, un disque à têtes fixes. Cet ordinateur eut un certain succès, mais une conception trop centralisée, un code saturé rendait difficile d'en dériver une gamme. Le successeur du T2000 fut le T1600, défini sur de nouvelles bases.
Idek Tramielski, dit Jack Tramiel, né le 13 décembre 1928 à Łódź en Pologne et mort le 8 avril 2012 à Stanford aux États-Unis, est un homme d'affaires américain. Il a fondé Commodore International en 1954 avant de prendre la tête d'Atari en 1984.
Transiris est le premier logiciel de routage pour le transport des données entre ordinateurs à avoir été installé au sein même du système d'exploitation dans une logique de réseau et de partage des données appelée téléinformatique. Fonctionnalité importante de l'Iris 80 de la Compagnie internationale pour l'informatique , il a ensuite donné naissance à Datanet.
L'expression tyrannie des nombres désigne l'un des problèmes que devaient affronter les ingénieurs informaticiens dans les années 1960. Ils étaient dans l'impossibilité de perfectionner les circuits électroniques qu'ils concevaient à cause du nombre très élevé de composants à ajouter et à connecter pour chaque amélioration des performances. Chaque composant devait être relié à plusieurs autres au moyen de soudures effectuées à la main : les fils prenant de plus en plus de place, il devenait en pratique impossible de progresser ainsi. Cette expression a été pour la première fois utilisée par le vice-président des Laboratoires Bell en 1957 dans un article célébrant le 10e anniversaire de « naissance » du transistor :  « Depuis quelque temps, l'homme électronique sait comment « en principe » prolonger grandement ses facultés visuelle, tactile et mentale grâce aux transmissions numériques et aux traitements de toutes sortes d'informations. Cependant, toutes ses fonctions souffrent de ce qui est appelé la « tyrannie des nombres ». Ces systèmes, à cause de leur nature numérique complexe, exigent des centaines, des milliers et parfois des dizaines de milliers de composants électroniques »  — (en) Jack Morton, The Tyranny of Numbers
Unipress Software Inc était une SSII américaine située à Edison dans l'État du New Jersey, États-Unis. Unipress Software est connu pour avoir distribué Unipress Emacs le 6 mai 1983. sous la forme d'un logiciel privateur. Il s'agissait du code source de Gosling Emacs racheté à son auteur, James Gosling. L'ancien président de la société Mark Krieger annonça le port d'Unipress Emacs sur des machines Sun386i/150 et Sun386i/250 le 2 mai 1988. Les développements d'Unipress Emacs se poursuivront jusqu'en 1994. Unipress Software Inc fut racheté par la société Numara Software le 13 septembre 2006. Unipress Software représente avec l'épisode Symbolics un évènement majeur dans l'histoire du mouvement du logiciel libre.
Georges Vieillard est un homme d'affaires français, ancien dirigeant de la société Bull.
Thomas J. Watson (Né le 17 février 1874 à Campbell, États-Unis et mort le 19 juin 1956 à New York). Nommé à la tête de la compagnie C-T-R, en 1914, il la renomme IBM en 1924 et la préside jusqu'en 1956.
Konrad Zuse (22 juin 1910 – 18 décembre 1995) est un ingénieur allemand qui fut l'un des pionniers du calcul programmable qui préfigure l'informatique.
Issu du monde numérique et plus précisément du Web 2.0, un badge numérique (aussi appelé insigne numérique, macaron numérique) est un dispositif numérique qui se présente sous forme d’icône et qui est utilisé pour confirmer l'acquisition d’aptitudes, de connaissances ou de compétences. Ce concept déjà reconnu sous des formes plus traditionnelles peut servir comme outil de motivation et de récompense et même d’attestation électronique pour un travail ou une activité accomplis par un internaute.
Les badges ouverts Mozilla sont un système de Badges numériques sur lequel la fondation Mozilla a travaillé en collaboration avec la Fondation MacArthur. Appelé en anglais Open Badges ce dispositif a trois fonctions : décerner aux apprenants des badges pour les connaissances, habiletés et compétences acquises, permettre aux institutions ou aux enseignants de conférer une reconnaissance pour les cours enseignés, certifier le titre délivré et l'afficher sur un CV ou sa page personnelle. Constatant que certaines connaissances et compétences ne s'acquièrent pas forcément à travers un cursus scolaire, Mozilla a choisi de les valoriser par le biais de la ludification.
Une favicon \fa.vi.kon\ est une icône informatique symbolisant un site web. Ces icônes sont créées et/ou utilisés par les concepteurs des sites internet. Les navigateurs web peuvent utiliser la favicon dans la barre d'adresse, les signets, les onglets ou encore les autres raccourcis.
Le format de fichier ICO est le format de fichier gérant les icônes sur le système d'exploitation Microsoft Windows. Les fichiers de ce format contiennent généralement des images de format carré : 16×16, 32×32, 48×48, 256×256 pixels, etc.
Une icône est un petit pictogramme représentant une action, un objet, un logiciel, un type de fichier, etc. Les icônes ont dans un premier temps servi d'outils pour rendre les interfaces utilisateur graphiques plus simples d'utilisation. Une icône est une image comme n'importe quelle autre. Cependant, le terme « icône » désigne le plus souvent une image de taille relativement petite et dont le but est de représenter ou illustrer quelque chose. Un favicon est une utilisation d'une icône pour représenter un site web, un CD ou un DVD. Il s'affiche en général sur un carré de 16 pixels de côté.
Une icône de partage (en anglais, share icon) est une icône d'interface utilisateur permettant à un utilisateur de partager un contenu avec d'autres personnes.
Nuvola (mot italien qui veut dire nuage) est un ensemble d'icônes pour logiciels libres / logiciels open source produit par David Vignoni. Elles ont été créées à la base pour les environnements graphiques comme KDE et GNOME, mais elles ont aussi été regroupées dans divers thèmes pour Windows XP et Mac OS. La version finale 1.0 comprend environ 600 icônes (la version 2 est en cours de création). L'ensemble de base regroupe les icônes au format PNG mais il existe également une version au format SVG. Les icônes (celles des applications en particulier) sont faites de couleurs vives pour être agréables à l'œil. Elles représentent une grande variété d'objets basiques facilement reconnaissables. La couleur dominante des icônes est le bleu mais d'autres couleurs sont aussi présentes. Ces icônes sont sous la licence GNU LGPL 2.1. C'est l'un des avantages qui explique qu'elles soient beaucoup utilisées pour illustrer l'encyclopédie Wikipédia. Elles sont également utilisées par des interfaces graphiques (GNOME, KDE…) et des logiciels libres (Pidgin (ex-Gaim), amarok…). Le projet Nuvola 2.0+, spécialement conçu pour Wikipédia, vise à recréer et à enrichir ce set d'icône prévus à l'origine pour KDE.
Oxygen est le thème graphique de l’environnement de bureau KDE4. Il est composé d'un thème d'icônes, d'un thème de sons système, d'un thème de curseur et d'un thème de fenêtre. Il vise à concurrencer Tango Desktop. Il utilise les standards établis par freedesktop.org contrairement aux anciens thèmes d'icônes KDE car le standard freedesktop n'existait pas à ce moment.
Tango Desktop est un projet (lancé en 2005) visant à proposer une unité d'interface graphique au monde informatique.
L'informatique durable, la green computing, ou green IT, ou green information technology,, ou informatique verte est un concept qui vise à réduire l'empreinte écologique, économique, et sociale des technologies de l'information et de la communication (TIC). Il s'agit d'une manière globale et cohérente de réduire les nuisances rencontrées dans le domaine des équipements informatiques et ce, durant l'ensemble de la durée de vie de chaque équipement : soit aux différents stades de fabrication, d'utilisation (consommation d'énergie) et de fin de vie (gestion/récupération des déchets, pollution, épuisement des ressources non renouvelables). Ce concept s'inscrit plus largement dans la notion d'informatique éco-responsable » ou développement durable. Le Journal officiel français du 12 juillet 2009 donne « éco-TIC » comme équivalent de « Informatique verte ». Selon la définition qu'il en donne, les « écotechniques de l'information et de la communication » sont des techniques de l'information et de la communication dont la conception ou l'emploi permettent de réduire les effets négatifs de l'activité humaine sur l'environnement. Le Journal officiel précise que « la réduction des effets négatifs de l'activité humaine sur l'environnement tient à la diminution de la consommation d'énergie et des émissions de gaz à effet de serre qui résulte du recours aux écotechniques ou à la conception même de ces techniques, qui s'attache à diminuer les agressions qu'elles pourraient faire subir à l'environnement au cours de leur cycle de vie ».
80 PLUS est une initiative lancée en 2004 par Ecos Consulting pour promouvoir le rendement électrique des blocs d'alimentation des ordinateurs. Elle certifie qu'au moins 80 % de l'énergie reçue en entrée est effectivement transmise à la machine (c'est-à-dire moins de 20 % de pertes par effet Joule). De nombreuses entreprises ont adhéré à ce programme, comme Dell, HP et Lenovo. Depuis 2004, plusieurs évolutions du label ont vu le jour, augmentant à chaque fois le rendement exigé. La liste des certifications « 80 Plus » est disponible ci-dessous.
La certification TCO est une certification concernant le bureau en général (et plus particulièrement les moniteurs d’ordinateurs), attribuée par la compagnie TCO Development, filiale de la Swedish Confederation of Professional Employees. Bien qu’associée généralement avec les écrans, cette certification définit également des standards pour les ordinateurs, les claviers, les imprimantes, les téléphones portables et les fournitures de bureau.
Le Climate Savers Computing Initiative est un groupe à but non lucratif de consommateurs, d'entreprises et d'organisations non gouvernementales qui cherchent à promouvoir les techniques propres de nature à améliorer l'efficacité énergétique et à réduire la consommation d'énergie des ordinateurs. L'inititiave date de 2007, et les organismes qui en sont à l'origine sont Google et le WWF. Les constructeurs informatiques qui participent à cette initiative s'engagent à développer des produits qui atteignent des objectifs d'efficacité énergétique, et leurs membres s'engagent à acheter des produits informatiques éco-efficaces. L'initiative s'est donné pour objectif de réduire vers 2010 la consommation d'énergie des ordinateurs de 50 %, et de réduire les émissions globales de CO2 des opérations des ordinateurs de 54 millions de tonnes par an.
Les déchets d'équipements électriques et électroniques (DEEE, D3E ou PEEFV — produits électriques et électroniques en fin de vie —, en anglais Waste Electronic and Electrical Equipment WEEE) sont une catégorie de déchets constituée des équipements en fin de vie, fonctionnant à l'électricité ou via des champs électromagnétiques, ainsi que les équipements de production, de transfert et de mesure de ces courants et champs (ce sont surtout des ordinateurs, imprimantes, téléphones portables, appareils photos numériques, réfrigérateurs, jeux électroniques, télévisions, etc.). En Europe, une directive visant un meilleur recyclage des produits électriques et électroniques limite cette catégorie aux matériels fonctionnant avec des tensions inférieures à 1 000 V en courant alternatif et 1 500 V en courant continu. Au-delà, ils sont considérés comme des déchets industriels. Diverses lois et taxes sont mises en place dans les années 2000 pour gérer ces déchets et limiter l'utilisation des substances dangereuses (RoHS et DEEE en Europe, en Californie, China RoHS en Chine).
La directive 2005/32/CE, communément appelée directive EuP (energy-using products), est une directive de l'Union européenne sur la consommation électrique des produits. Votée en 2007, elle est en application en Europe depuis janvier 2008. Son objectif est de réduire la consommation des appareils électriques et électroniques grâce à une meilleure conception (éco-conception). Elle a depuis été remplacée par la directive 2009/125/CE dite d'écoconception ou ErP (energy-related products).
La Directive européenne RoHS (2002/95/CE) vise à limiter l'utilisation de six substances dangereuses, RoHS est l'acronyme de Restriction of Hazardous Substances. Elle est complétée par la directive de 2008 sur les déchets (Directive 2008/98/CE) qui vise à homogénéiser les règlementations nationales, encore divergentes en Europe, sur le « principe de responsabilité du producteur » (intégrant le principe pollueur-payeur) qui « peuvent entraîner des disparités considérables dans la charge financière pesant sur les opérateurs économiques. Les différences entre les politiques nationales sur la gestion des DEEE compromettent l'efficacité des politiques de recyclage. Pour cette raison, les critères essentiels doivent être fixés au niveau de l'Union et des normes minimales pour le traitement des DEEE doivent être développées ». Révision, directive no 2011/65/UE : le nouveau texte adopté le 8 juin 2011 par le Conseil et publié le 1er juillet 2011 au Journal officiel de l'Union européenne (JOUE) a notamment élargi le nombre d'appareils électriques concernés et encourage l'écoconception, le tri sélectif et le recyclage de certains composants plutôt que leur mise au rebut.
L'éco-informatique, ou informatique environnementale, désigne l'application de l'informatique dans les sciences de l'environnement.
Energy Star est le nom d'un programme gouvernemental américain chargé de promouvoir les économies d'énergie aux États-Unis et utilisé au Canada, Australie et Union Européenne. Il a été initié par l'EPA (Environmental Protection Agency) en 1992 pour réduire les émissions de gaz à effet de serre. Il prend la forme d'un label apposé sur différents produits qui respectent les normes environnementales tels que les ordinateurs ou encore les éclairages. Il peut être décerné à des bâtiments industriels, des bureaux ou des maisons particulières : ainsi, en 2005, 2 000 bâtiments et 350 000 nouvelles maisons ont reçu cette marque de qualité aux États-Unis.
Electronic Product Environmental Assessment Tool (EPEAT) est un écolabel permettant au consommateur d'évaluer l'effet d'un produit informatique sur l'environnement. Il distingue les produits certifiés en trois catégories répondant à différents critères de performance environnementale. EPEAT fut créé et est toujours géré par le GEC (Green Electronics Council), un programme de l'ISDF (International Sustainability Development Foudation), organisation à but non lucratif qui « imagine un monde où le commerce, les communautés et la nature se développent en harmonie ». Pour qualifier des produits électroniques à la norme IEEE 1680, famille de produits électroniques écoconçus, le GEC a signé un Mémorandum d'entente avec un groupe d'organismes d'évaluation technique et environnemental.
Le G8-HORCs (Heads of Research Councils of G8 Countries, ou Conseils de recherche des pays du G8) est un groupe créé en 1979 sur la suggestion du professeur Heinz Maier-Leibnitz, alors à la tête de la Fondation pour la Recherche allemande (DFG) . Sur le modèle du G8, il rassemble les directeurs des instituts nationaux de recherche des États-Unis, du Royaume-Uni, de France, d'Allemagne, d'Italie, du Canada, de Russie et, depuis la sixième rencontre, en 1987, du Japon. Ceux-ci discutent de stratégies de la recherche à venir, en particulier concernant des problèmes transversaux ou/et globaux. Les discussions sont confidentielles.
La gestion énergétique des centres de calcul est la mise en œuvre de moyens permettant d'optimiser les besoins importants en énergie que ces centres nécessitent. Elle fait partie du concept d'informatique durable et permet de répondre à un certain nombre de problématiques. C'est une gestion intelligente qui s'appuie sur des indicateurs de performance énergétique obtenus par une évaluation de chaque site. Pour améliorer ces indicateurs, une étude préalable est nécessaire dès la phase de conception du centre, afin de tirer parti des avantages géographiques du lieu d'implantation (climat, accessibilité à des sources d'énergie) et de l'architecture même du site, liée notamment au choix de la solution de refroidissement. L'optimisation des logiciels, tant au niveau du traitement des données qu'au niveau du fonctionnement du centre, et l'amélioration continuelle de la performance de l'ensemble des équipements contribuent également à la réduction de la consommation et donc des coûts énergétiques. Cette efficacité est devenue aujourd'hui un enjeu majeur, tant au niveau économique qu'au niveau environnemental. Les sociétés ont besoin de plus en plus de ressources informatiques. Celles-ci leur permettront d'atteindre leurs objectifs et surtout d'accroître leur rentabilité. En conséquence, le nombre de data centers ne cesse d'augmenter et génère un besoin croissant en énergie. Il devient nécessaire de privilégier des énergies de type renouvelable et de valoriser la chaleur produite par les serveurs. Gérer efficacement ces centres de calculs, optimiser leur fonctionnement permettront de limiter l'impact environnemental et le coût financier que ceux-ci engendrent. C'est aussi un moteur de recherche pour le développement de nouvelles technologies dans les domaines de l'alimentation électrique, la climatisation, la virtualisation et la fabrication des composants électroniques.
Green 500 est une liste des superordinateurs du TOP500 classée selon un critère d'efficacité énergétique. Le projet a été lancé à l’initiative de Kirk Cameron et de Wu Feng, professeurs associés travaillant pour le département des sciences informatiques de Virginia Tech. L'efficacité énergétique est évaluée par la performance par watt, mesurée en FLOPS par watt. Il existe une mise à jour semestrielle de la liste. La première liste a été publiée en novembre 2007.
The Green Grid est un consortium industriel à but non lucratif qui rassemble 501 membres (sociétés et représentants issus pour la plupart du monde de l'informatique) début 2015 appartenant à « plus de 200 entités membres à l'échelle mondiale ». L'organisation accueille aussi des utilisateurs finaux du monde industriel, des décideurs ou représentants du monde politique, des entreprises de services publics, des architectes/designers et propriétaires/gestionnaires de data centers pour travailler au "verdissement" du secteur des réseaux informatiques et des technologies de l'information et des communications (TIC).
L'indicateur d'efficacité énergétique (en anglais PUE ou Power Usage Effectiveness) est utilisé pour qualifier l'efficacité énergétique d'un centre d'exploitation informatique. C'est un des éléments de l'informatique éco-responsable (green IT).
L'informatique quantique est le sous-domaine de l'informatique qui traite des calculateurs quantiques utilisant des phénomènes de la mécanique quantique, par opposition à ceux de l'électricité exclusivement, pour l'informatique dite « classique ». Les phénomènes quantiques utilisés sont l'intrication quantique et la superposition. Les opérations ne sont plus basées sur la manipulation de bits dans un état 1 ou 0, mais de qubits en même temps dans un état 1 et 0.
En informatique quantique, l’algorithme d'estimation de phase quantique est un algorithme quantique (en) permettant d'estimer la valeur propre (ou sa phase, ce qui, dans ce cas précis, est équivalent) d'un opérateur unité associée à un vecteur propre donné.
L'algorithme de Deutsch-Jozsa est un algorithme quantique, proposé par David Deutsch et Richard Jozsa (en) en 1992 avec des améliorations de R. Cleve, A. Ekert, C. Macchiavello, et M. Mosca en 1998,. Bien qu'il ne soit pas d'un grand intérêt pratique, il s'agit d'un des premiers algorithmes quantiques qui est plus efficace qu'un algorithme classique.
En informatique quantique, l’algorithme de Grover est un algorithme de recherche, permettant de rechercher un ou plusieurs éléments qui répondent à un critère donné parmi                         N                 {\displaystyle N}    éléments non classés en temps proportionnel à                                                 N                                     {\displaystyle {\sqrt {N}}}    et avec un espace de stockage proportionnel à                         log         ⁡         (         N         )                 {\displaystyle \log(N)}   . Il a été découvert par Lov Grover en 1996. Dans les mêmes conditions (recherche parmi des éléments non classés), un algorithme classique ne peut faire mieux qu'une recherche dans un temps proportionnel à                         N                 {\displaystyle N}   , en testant successivement le critère sur chaque élément. L'étendue de la gamme de critères pouvant être utilisée par cet algorithme lui donne un caractère universel, ce qui en fait un des algorithmes les plus importants et potentiellement le plus utile de l'informatique quantique. L'exemple classique d'utilisation de cet algorithme est la recherche, dans un annuaire téléphonique ordinaire classé alphabétiquement, du nom qui correspond à un numéro de téléphone donné. L'algorithme de Grover fonctionne toujours en lui présentant les nombres entiers de 1 à N, représentant dans le cas de l'annuaire une position dans ce dernier. Le critère de sélection est dans ce cas : la position correspond à un numéro de téléphone donné. La position étant connue, on en déduit le nom ou toute autre information liée à la position. Plus généralement, l'ensemble des nombres entiers de 1 à N peut indexer un ensemble de solutions possibles à un problème. Dans ce cas, s'il est possible de vérifier rapidement qu'une solution résout un problème (ce qui est généralement le cas, et ce qui définit même toute une classe importante de problèmes dits de complexité NP), alors il est possible à l'aide de cet algorithme d'accélérer notablement la recherche des solutions de ces problèmes par rapport à une « recherche brute ». Parmi les problèmes NP (et même NP-Complet) qui pourraient être résolus par cet algorithme se trouvent notamment : le problème SAT ; la coloration de graphe ; la détermination d'un chemin hamiltonien dans un graphe ; des puzzles, cryptographiques comme « SEND+MORE=MONEY », ou numériques comme le sudoku. Malgré tout, ce genre de problème est souvent de complexité exponentielle par rapport au nombre d'éléments du problème (typiquement,                         N         =                    2                        n                                     {\displaystyle N=2^{n}}    si n est le nombre d'éléments mis en jeu dans le problème). Même si cet algorithme apporte une optimisation non négligeable (quadratique) par rapport à une recherche brute, il transforme un problème de complexité                                    2                        n                                     {\displaystyle 2^{n}}    en                                                                2                                n                                                         =                    2                        n                            /                          2                                     {\displaystyle {\sqrt {2^{n}}}=2^{n/2}}   , qui demeure exponentielle. Certains algorithmes classiques, adaptés à un problème particulier, peuvent faire mieux, surtout si on tolère une solution approximative, ou probabiliste. Mais cet algorithme présente le double avantage d'être généraliste (dès que l'on a l'algorithme pour vérifier une solution, on a automatiquement l'algorithme pour trouver la solution), et de garantir de trouver la ou les solution(s) optimale(s). Il a été prouvé en 1999, par Christof Zalka, que l'algorithme de Grover est l'algorithme quantique le plus efficace pouvant traiter le problème de la recherche non structurée. Il est impossible de faire mieux qu'une amélioration quadratique de la complexité, en utilisant le parallélisme du calcul quantique.
En arithmétique modulaire et en informatique quantique, l’algorithme de Shor est un algorithme quantique pour factoriser un entier naturel N en temps O                        (         (         log         ⁡         N                    )                        3                             )                 {\displaystyle ((\log N)^{3})}    et en espace                         O         (         log         ⁡         N         )                 {\displaystyle O(\log N)}   , nommé en l'honneur de Peter Shor. Beaucoup de cryptosystèmes à clé publique, tels que le RSA, deviendraient vulnérables si l'algorithme de Shor était un jour implémenté dans un calculateur quantique pratique. Un message chiffré avec RSA peut être déchiffré par factorisation de sa clé publique N, qui est le produit de deux nombres premiers. En l'état actuel des connaissances, il n'existe pas d'algorithme classique capable de faire cela en temps                         O         (         (         log         ⁡         N                    )                        k                             )                 {\displaystyle O((\log N)^{k})}    pour n'importe quel k, donc, les algorithmes classiques connus deviennent rapidement impraticables quand N augmente, à la différence de l'algorithme de Shor qui peut casser le RSA en temps polynomial. Il a été aussi étendu pour attaquer beaucoup d'autres cryptosystèmes à clé publique. Comme tous les algorithmes pour calculateur quantique, l'algorithme de Shor est probabiliste : il donne la réponse correcte avec une haute probabilité et la probabilité d'échec peut être diminuée en répétant l'algorithme. L'algorithme de Shor fut utilisé en 2001 par un groupe d'IBM, qui factorisa 15 en 3 et 5, en utilisant un calculateur quantique de 7 qubits.
En informatique quantique et en informatique théorique, un automate fini quantique est une généralisation des automates finis où un mot est accepté selon le résultat d'une certaine mesure. Il existe plusieurs modèles des automates finis quantiques ; le plus restrictif est celui des automates dits « measure-once » de Moore et Crutchfield 2000 ; un autre est celui des automates « measure-many » de Kondacs et Watrous 1997. Ces deux modèles sont très différents l'un de l’autre ; le modèle « measure-once » se rapproche plus de la théorie classique des automates finis. D'autres définitions, plus générales, ont été proposées, en généralisation des diverses variantes des automates, par exemple par Yakaryilmaz et Say 2011. Un des objectifs de l’étude des automates fini quantiques est l’étude des langages formels qu'ils acceptent ; un autre est d'étudier les problèmes de décidabilité pour ces classes d'automates et de langages. Les automates finis quantiques sont similaires aux automates probabilistes, mais la classe des langages reconnus par automates quantiques est strictement contenue dans la classe des langages reconnus par automates probabilistes. Les automates finis quantiques peuvent aussi être vus comme une version quantique des sous-shifts de type fini, ou comme une variante quantique des chaînes de Markov. Les automates finis quantiques sont, à leur tour, des cas particuliers des automates finis dit géométriques ou des automates finis dit topologiques. Un automate fini quantique opère sur des mots finis                         w         =                    a                        1                                        a                        2                             ⋯                    a                        m                                     {\displaystyle w=a_{1}a_{2}\cdots a_{m}}   , dont les lettres                                    a                        i                                     {\displaystyle a_{i}}    sont dans un alphabet donné                         Σ                 {\displaystyle \Sigma }   . Il attribue à chaque mot                         w                 {\displaystyle w}    une probabilité; en fonction de cette probabilité, le mot est accepté ou rejeté par l'automate. Les langages acceptés par les automates finis quantique ne coïncident pas avec les langages rationnels acceptés par les automates finis, ni avec les langages stochastiques acceptés par les automates finis probabilistes.
En théorie de la complexité des algorithmes BQP (bounded error quantum polynomial time) est la classe des problèmes de décision qui peuvent être résolus par un calculateur quantique en un temps polynomial, avec une probabilité d'erreur d'au plus 1/3 dans tous les cas. Elle est le pendant quantique de la classe classique de complexité BPP.
Le calcul quantique adiabatique (en anglais, adiabatic quantum computation ou AQC) est une méthode de calcul quantique reposant sur le théorème adiabatique, qui peut être vu comme une sous-classe des méthodes de recuit simulé quantique,,,,.
Un calculateur quantique (anglais quantum computer parfois traduit ordinateur quantique, ou système informatique quantique), utilise les propriétés quantiques de la matière, telle que la superposition et l'intrication afin d'effectuer des opérations sur des données. À la différence d'un ordinateur classique basé sur des transistors qui travaille sur des données binaires (codées sur des bits, valant 0 ou 1), le calculateur quantique travaille sur des qubits dont l'état quantique peut posséder plusieurs valeurs. De petits calculateurs quantiques ont été construits à partir des années 1990. Jusqu'en 2008, la difficulté majeure concerne la réalisation physique de l'élément de base : le qubit. Le phénomène de décohérence (perte des effets quantiques en passant à l'échelle macroscopique) freine le développement des calculateurs quantiques. Le premier processeur quantique est créé en 2009 à l'université Yale : il comporte deux qubits composés chacun d'un milliard d'atomes d'aluminium posés sur un support supraconducteur. Ce domaine est soutenu financièrement par plusieurs organisations, entreprises ou gouvernements en raison de l'importance de l'enjeu : au moins un algorithme conçu pour utiliser un circuit quantique, l'algorithme de Shor, rendrait possible de nombreux calculs combinatoires hors de portée d'un ordinateur classique en l'état actuel des connaissances. La possibilité de casser les méthodes cryptographiques classiques est souvent mise en avant.
La cryptographie quantique consiste à utiliser les propriétés de la physique quantique pour établir des protocoles de cryptographie qui permettent d'atteindre des niveaux de sécurité qui sont prouvés ou conjecturés non atteignables en utilisant uniquement des phénomènes classiques (c'est-à-dire non-quantiques). Un exemple important de cryptographie quantique est la distribution quantique de clés, qui permet de distribuer une clé de chiffrement secrète entre deux interlocuteurs distants, tout en assurant la sécurité de la transmission grâce aux lois de la physique quantique et de la théorie de l'information. Cette clé secrète peut ensuite être utilisée dans un algorithme de chiffrement symétrique, afin de chiffrer et déchiffrer des données confidentielles.
D-Wave (D-Wave Systems) se présente comme première entreprise d'informatique quantique au monde, fondée en 1999 et basée en Colombie-Britannique (Canada). Elle annonce en 2007 avoir construit le prototype d'un processeur de 28 qubits permettant de faire du recuit simulé quantique. Le 11 mai 2011, elle annonce son système D-Wave One comme le premier calculateur quantique commercial. C'est un processeur de 128 qubits basé sur la méthode du recuit simulé quantique. En septembre 2016, elle communique sur sa prochaine génération de processeurs contenant 2000 qubits. Plusieurs experts rappellent que le système D-Wave n'est pas un calculateur quantique général, et qu'hormis les calculs de recuit simulé, il ne présente pas d'avantage particulier sur un ordinateur classique. Les ingénieurs de Google qui ont testé D-Wave avouent qu'il n'a pour l'instant pas d'applications, ce qui a conduit Google à développer ses propres circuits quantiques à partir de 2014.
L'échange de clé quantique (ou distribution de clé quantique, ou négociation de clé quantique), souvent abrégé QKD (pour l'anglais : quantum key distribution) est un échange de clé, c'est-à-dire un protocole cryptographique visant à établir un secret partagé entre deux participants qui communiquent sur un canal non sécurisé. Ce secret sert généralement à générer une clé cryptographique commune (c'est pourquoi il s'agit d'échange de clé, au singulier), permettant ensuite aux participants de chiffrer leurs communications au moyen d'un algorithme de chiffrement symétrique. L'échange de clé quantique se caractérise en ce qu'il fonde sa sécurité non pas sur la difficulté calculatoire supposée de certains problèmes, comme c'est le cas pour les protocoles cryptographiques utilisés aujourd'hui, mais sur l'impossibilité supposée de violer les principes de la physique quantique : c'est un cas particulier de cryptographie quantique. Parmi les propriétés fondamentales sur lesquelles s'appuie l'échange de clé quantique, il y a notamment le théorème de non clonage, qui garantit qu'il est impossible pour un adversaire de créer une réplique exacte d'une particule dans un état inconnu. Ainsi il est possible sous certaines conditions de détecter une tentative d'interception des communications.
L'entropie de Rényi, due à Alfréd Rényi, est une fonction mathématique qui correspond à la quantité d'information contenue dans la probabilité de collision d'une variable aléatoire.
En probabilités et en théorie de l'information, l'entropie min d'une variable aléatoire discrète X prenant n valeurs ou sorties possibles 1... n associées au probabilités p1... pn est :                                    H                        ∞                             (         X         )         =                    min                        i             =             1                                   n                             (         −         log         ⁡                    p                        i                             )         =         −         (                    max                        i                             log         ⁡                    p                        i                             )         =         −         log         ⁡                    max                        i                                        p                        i                                     {\displaystyle H_{\infty }(X)=\min _{i=1}^{n}(-\log p_{i})=-(\max _{i}\log p_{i})=-\log \max _{i}p_{i}}    La base du logarithme est juste une constante d'échelle. Pour avoir un résultat en bits, il faut utiliser le logarithme en base 2. Ainsi, une distribution a une entropie min d'au moins b bits si aucune sortie n'a une probabilité plus grande que 2-b. L'entropie min est toujours inférieure ou égale à l'entropie de Shannon; avec égalité si toutes les valeurs de X sont équiprobables. L'entropie min est importante dans la théorie des extracteurs de hasard. La notation                                    H                        ∞                             (         X         )                 {\displaystyle H_{\infty }(X)}    vient d'une famille paramétrée d'entropies appelée entropie de Rényi,                                    H                        k                             (         X         )         =         −         log         ⁡                                                                                                         ∑                                            i                                                           (                                        p                                            i                                                                                )                                            k                                                                                                                 k               −               1                                                  {\displaystyle H_{k}(X)=-\log {\sqrt[{k-1}]{\begin{matrix}\sum _{i}(p_{i})^{k}\end{matrix}}}}     Portail des probabilités et de la statistique
En quantique, le fait de mesurer un état détruit cet état. C'est un phénomène qui est en opposition au modèle classique (celui qui nous est familier dans notre expérience quotidienne) dans lequel le fait de prendre une mesure n'affecte pas l'objet mesuré. En particulier, en théorie des codes classique, il est d'usage de mesurer l'information transmise afin de déterminer s'il y a eu des erreurs et dans ce cas, comment s'en occuper. Le fait que les mesures soient destructives pour les états quantiques rend la tâche d'élaborer un code quantique difficile par rapport aux codes classiques.
Le projet It from Qubit ("IfQ") réunit plusieurs fois par an, sur plusieurs sites géographiques en parallèle, des centaines de chercheurs dans une organisation transdisciplinaire internationale. Ces chercheurs sont pour la plupart physiciens et/ou informaticiens spécialisés dans ou intéressés par la physique quantique. Le it représente l'espace-temps des physiciens. Le qubit représente la plus petite quantité d'information quantique (C'est l'analogue quantique du bit de l'informatique). L'objectif du projet est de réfléchir à de nouvelles idées relatives à la définition d'un théorie quantique de la gravitation compatible avec la physique quantique et avec la relativité générale.
Le théorème d'impossibilité du clonage quantique est un résultat de mécanique quantique qui interdit la copie à l'identique d'un état quantique inconnu et arbitraire. Il a été énoncé en 1982 par Wootters, Zurek, et Dieks. Ce théorème a d'importantes conséquences en informatique quantique. Par exemple, il fait en sorte qu'il est impossible d'adapter un code quantique directement du code de répétition de la théorie des codes classique. Ceci rend la tâche d'élaborer un code quantique difficile par rapport aux codes classiques. Dans ce cas dit classique, le « clonage » est trivialement réalisable. C'est d'ailleurs la façon dont l'information de cet article est transmise à son lecteur.
En mécanique quantique, l'intrication quantique, ou enchevêtrement quantique, est un phénomène dans lequel deux particules (ou groupes de particules) ont des états quantiques dépendant l'un de l'autre quelle que soit la distance qui les sépare. Un tel état est dit « intriqué » ou « enchevêtré » parce qu'il existe des corrélations entre les propriétés physiques observées de ces particules distinctes : cet état semble contredire le principe de localité. Ainsi, deux objets intriqués O1 et O2 ne sont pas indépendants même séparés par une grande distance, et il faut considérer {O1+O2} comme un système unique. Cette observation est au cœur des discussions philosophiques sur l'interprétation de la mécanique quantique. Elle est en effet contraire au principe de réalisme local défini par Albert Einstein. L'intrication quantique a des applications potentielles dans les domaines de l'information quantique, tels que la cryptographie quantique, la téléportation quantique ou l'ordinateur quantique.
Libquantum est une bibliothèque logicielle visant à faire des simulations de mécanique quantique, et en particulier de calculateurs quantiques, écrite en C par Björn Butscher et Hendrik Weimer de l'université de Stuttgart.
Les matrices de Pauli, développées par Wolfgang Pauli, forment, au facteur i près, une base de l'algèbre de Lie du groupe SU(2). Elles sont définies comme l'ensemble de matrices complexes de dimensions 2 × 2 suivantes :                                    σ                        1                             =                    σ                        x                             =                                 (                                                                0                                                     1                                                                                   1                                                     0                                                          )                                     {\displaystyle \sigma _{1}=\sigma _{x}={\begin{pmatrix}0&1\\1&0\end{pmatrix}}}                                       σ                        2                             =                    σ                        y                             =                                 (                                                                0                                                     −                   i                                                                                   i                                                     0                                                          )                                     {\displaystyle \sigma _{2}=\sigma _{y}={\begin{pmatrix}0&-i\\i&0\end{pmatrix}}}                                       σ                        3                             =                    σ                        z                             =                                 (                                                                1                                                     0                                                                                   0                                                     −                   1                                                          )                                     {\displaystyle \sigma _{3}=\sigma _{z}={\begin{pmatrix}1&0\\0&-1\end{pmatrix}}}    (où i est l’unité imaginaire des nombres complexes). Ces matrices sont utilisées en mécanique quantique pour représenter le spin des particules, notamment dès 1927 dans l'étude non-relativiste du spin de l'électron : l'équation de Pauli.
Les matrices gamma forment des ensembles de matrices conventionnelles respectant des relations de commutations spécifiques.
L'architecture de Kane pour la réalisation de l'ordinateur quantique est une proposition théorique suggérant l'utilisation des spins nucléaires d'atomes donneurs uniques de 31P dans un cristal de silicium isotopiquement pur (28Si) comme composants de base (qubits).
En informatique, la porte de Toffoli, est une porte logique. Elle est réversible et universelle, ce qui signifie que n'importe quel circuit réversible peut être construit à partir de portes de Toffoli. Elle agit comme une porte NON à double contrôle, d'où le nom qu'on lui donne également de « controlled-controlled-not gate » (CCNOT). Elle est due à Tommaso Toffoli.
En informatique quantique, et plus précisément dans le modèle de circuit quantique (en) de calcul, une porte quantique (ou porte logique quantique) est un circuit quantique élémentaire opérant sur un petit nombre de qubits. Les portes quantiques sont les briques de base des circuits quantiques, comme le sont les portes logiques classiques pour des circuits numériques classiques. Contrairement à de nombreuses portes logiques classiques, les portes logiques quantique sont « réversibles ». Cependant, il est possible d'effectuer un calcul classique en utilisant uniquement des portes réversibles. Par exemple, la porte de Toffoli réversible peut implémenter toutes les fonctions Booléennes, souvent au prix de devoir utiliser des bits auxiliaires (en). La porte de Toffoli a un équivalent quantique direct, montrant que des circuits quantiques peuvent effectuer toutes les opérations effectuées par les circuits classiques. Les portes logiques quantique sont représentés par des matrices unitaires. Les portes quantiques les plus courantes fonctionnent sur des espaces d'un ou deux qubits, tout comme les portes logiques classiques fonctionnent sur un ou deux bits. Les portes quantiques peuvent être décrites comme des matrices unitaires de taille                                    2                        n                             ×                    2                        n                                     {\displaystyle 2^{n}\times 2^{n}}   , où                         n                 {\displaystyle n}    est le nombre de qubits sur lesquels la porte agit. Les variables sur lesquelles les portes agissent, les états quantiques, sont des vecteurs dans                                    2                        n                                     {\displaystyle 2^{n}}    dimensions complexes, où                         n                 {\displaystyle n}    est, à nouveau, le nombre de qubits de la variable: Les vecteurs de base sont les résultats possibles de la mesure, si elle est effectuée, et un état quantique est une combinaison linéaire de ces résultats.
Le Q Sharp (ou Q#) est un langage de programmation utilisé pour simuler des algorithmes quantiques. Développé par Microsoft, une bêta est lancée le 11 décembre 2017 (the Microsoft Quantum Development Kit Preview), ainsi il n'est pas nécessaire d'être expert en informatique quantique pour programmer en Q#.
En informatique quantique, un qubit ou qu-bit (quantum + bit ; prononcé /kju.bit/), parfois écrit qbit, est l'état quantique qui représente la plus petite unité de stockage d'information quantique. C'est l'analogue quantique du bit.
Un registre quantique est un registre composé de plusieurs qubits , il est l'équivalent quantique d'un registre classique.
Un simulateur quantique est une classe restreinte d'ordinateur quantique qui contrôle les interactions entre des bits quantiques de manière à pouvoir simuler certains problèmes quantiques difficiles à modéliser autrement,. Les simulateurs quantiques permettent l'étude de systèmes quantiques difficiles à étudier en laboratoire et impossible à modéliser avec des supercalculateurs non quantique. Les simulateurs sont des dispositifs spécifiques conçu pour étudier un sujet spécifique de la physique, ils ne sont donc pas généralistes,. Un simulateur quantique universel est un calculateur quantique tel que proposé par Richard Feynman en 1982. Feynman a montré qu'une machine de Turing classique simulant des phénomènes quantiques connaîtrait une croissance exponentielle de son temps de calcul tandis que un calculateur quantique hypothétique ne ferait pas l'expérience de cette augmentation. David Deutsch , en 1985, a repris les idées de Feynman et a proposé un calculateur quantique universel. En 1996, Seth Lloyd a montré qu'un ordinateur quantique standard pourrait être programmé pour simuler des systèmes locaux quantiques efficacement. Des simulateurs quantiques ont été réalisés à l'aide notamment de systèmes de gaz quantiques ultra froids, d'ions piégés, de systèmes photoniques, de points quantiques et de circuits supraconducteurs
La suprématie quantique désigne le nombre de qbits au-delà duquel aucun superordinateur classique n'est capable de gérer la croissance exponentielle de la mémoire et la bande passante de communication nécessaire pour simuler son équivalent quantique. Les superordinateurs de 2017 peuvent reproduire les résultats d'un ordinateur quantique de 5 à 20 qubits, mais à partir de 50 qubits cela devient physiquement impossible. Le seuil d'environ 50 qubits correspond à la limite de la suprématie quantique. D’après Harmut Neven, responsable des recherches en calcul quantique chez Google, son équipe est sur le point de construire un système de 49 qubits d’ici la fin de l’année 2017. En novembre 2017, IBM réussit à faire fonctionner un calculateur quantique de 50 qbits pendant 90 microsecondes atteignant donc le seuil théorique de la suprématie quantique. Cependant, une équipe de recherche d'IBM a montré en octobre 2017 que ce seuil n'était pas aussi fixe que pensé dans un premier temps et qu'il est possible de modéliser le comportement d'un ordinateur quantique avec des ordinateurs conventionnels au-delà de 49 qbits grâce à des techniques mathématiques,. Pour certains chercheurs notamment chez IBM, le nombre de qubits seul ne permet pas de capturer la complexité inhérente aux calculateurs quantiques et ils suggèrent que la puissance d'un calcul quantique sur un appareil donné soit exprimée par un nombre appelé « volume quantique », qui regrouperait tous les facteurs pertinents, à savoir, le nombre et la connectivité des qubits, la profondeur de l'algorithme utilisé ainsi que d'autres mesures telles que la qualité des portes logiques, notamment le bruit du signal. L'utilisation du mot « suprématie » est également mise en cause en raison de ces connotations raciales et politiques.
La transformée d´Hadamard (aussi connue sous le nom de « transformée de Walsh-Hadamard ») est un exemple d'une classe généralisée d'une transformée de Fourier. Elle est nommée d'après le mathématicien français Jacques Hadamard et effectue une opération linéaire et involutive avec une matrice orthogonale et symétrique sur 2m nombres réels (ou complexes, bien que les matrices utilisées possèdent des coefficients réels). Ces matrices sont des matrices de Hadamard. La transformée de Hadamard peut être vue comme étant issue d'une transformée de Fourier discrète et s'avère être en fait l'équivalent d'une transformée de Fourier discrète multidimensionnelle d'une taille de 2×2×...×2×2. Elle décompose un vecteur arbitraire en entrée en une superposition de fonctions de Walsh.
En informatique quantique, un transmon est un type de qubit de charge (en) supraconducteur qui a été conçu pour réduire la sensibilité au bruit de charge. Le transmon a été développé à l'université Yale en 2007,. Son nom est une abréviation de transmission line shunted plasma oscillation qubit.
L'informatique théorique est l'étude des fondements logiques et mathématiques de l'informatique. C'est une branche de la science informatique. Plus généralement, le terme est utilisé pour désigner des domaines ou sous-domaines de recherche centrés sur des vérités universelles (axiomes) en rapport avec l'informatique. L'informatique théorique se caractérise par une approche par nature plus mathématique et moins empirique de l'informatique et ses objectifs ne sont pas toujours directement reliés à des enjeux technologiques. De nombreuses disciplines peuvent être regroupées sous cette dénomination diffuse dont la théorie de la calculabilité, l'algorithmique et la théorie de la complexité, la théorie de l'information, l'étude de la sémantique des langages de programmation, la logique mathématique, la théorie des automates et des langages formels.
Un algorithme émergent est un processus de résolution de problème qui s'appuie sur un ensemble de règles simples pour faire émerger un comportement global plus complexe sans que ce dernier n'ait été explicitement détaillé. Un réseau de neurones artificiels est par exemple constitué d'agents simples, mais c'est leur nombre et leurs interconnexions qui leur permettent de résoudre des problèmes d'ordres de complexité bien supérieurs.
L'analyse de la complexité d'un algorithme consiste en l'étude formelle de la quantité de ressources (par exemple de temps ou d'espace) nécessaire à l'exécution de cet algorithme. Celle-ci ne doit pas être confondue avec la théorie de la complexité, qui elle étudie la difficulté intrinsèque des problèmes, et ne se focalise pas sur un algorithme en particulier.
En mathématiques, l'arité d'une fonction, ou opération, est le nombre d'arguments ou d'opérandes qu'elle requiert. Une fonction ou un opérateur peuvent donc être décrits comme unaires, binaires, ternaires, etc. Des termes comme 7-aire ou n-aire sont aussi utilisés. L'addition de deux nombres, par exemple, est une fonction binaire, ou opération binaire. La fonction inverse, qui associe à un élément son inverse, est une fonction unaire. En calcul propositionnel on considère aussi l'arité des connecteurs qui sont des fonctions des booléens dans un booléen. Quelquefois il est commode de considérer les constantes comme des opérateurs nullaires, c'est-à-dire des fonctions d'arité 0. On parle aussi de l'arité d'un prédicat ou d'une relation : ainsi l'égalité [=] est une relation binaire, de même l'inégalité stricte [<] et l'appartenance [∈]. Une opération, plus généralement appelée fonction, peut aussi être considérée comme une relation ; ainsi l'addition peut être considérée comme une relation ternaire entre les deux termes et leur somme. Ceci se généralise : toute fonction n-aire est en même temps une relation (n+1)-aire fonctionnelle. Il suffit de définir : R(x1, x2, ..., xn, y) si et seulement si f(x1, x2, ..., xn) = y.
En informatique théorique, une bisimulation est une relation binaire entre systèmes de transition d'états, associant les systèmes qui se comportent de la même façon au sens qu'un des systèmes simule l'autre et vice-versa. Intuitivement deux systèmes sont bisimilaires s'ils sont capables de s'imiter l'un l'autre. Dans cette optique, chaque système ne peut être distingué de l'autre par un observateur.
Une carte combinatoire est un objet combinatoire qui intervient dans la modélisation de structures topologiques subdivisées en objets. La version la plus simple en est la carte planaire, structure combinatoire pour la représentation de graphes planaires dans le plan.
De façon générale un codage permet de passer d'une représentation des données vers une autre.
En sciences et techniques, notamment en informatique et en théorie de l'information, un code est une règle de transcription qui, à tout symbole d'un jeu de caractères (alphabet source) assigne de manière univoque un caractère ou une chaîne de caractères pris dans un jeu de caractères éventuellement différent (alphabet cible). Un exemple est le code morse qui établit une relation entre lettres de l'alphabet latin et des séquences de sons courts et longs. En sciences de la communication, un code au sens le plus large fait référence à un langage. Toute communication est basée sur l'échange d'informations engendrées par un émetteur selon un code spécifique et que le destinataire interprète selon le même code. Les données transmises ou conservées électroniquement consistent en une longue série de zéros et de uns (les bits) dont la combinaison définit nombres, symboles ou (par exemple, 11000001 = 'A'). Ce sont des codages de caractères. Dans un autre code, la même combinaison de bits pourrait signifier le nombre 193. D'autres types de données aussi, par exemple, les séquences des bases nucléiques dans les brins d'ADN sont exprimés par des codes. En théorie des codes on appelle les éléments composant un code les « mots du code » ; les symboles qui composent les mots du code sont pris dans un alphabet (l'alphabet cible). On distingue l'alphabet source, sur lequel est formé le texte en clair, de l'alphabet cible, dans lequel est exprimé le texte codé.
La complexité est une notion utilisée en philosophie, épistémologie (par exemple par Anthony Wilden ou Edgar Morin), en physique, en biologie (par exemple par Henri Atlan), en théorie de l'évolution (par exemple Pierre Teilhard de Chardin), en écologie, en sociologie, en ingénierie, en informatique ou en sciences de l’information. La définition connaît des nuances importantes selon ces différents domaines.
La complexité de Kolmogorov (nommée d'après le mathématicien Andreï Kolmogorov, mais parfois nommée complexité de Kolmogorov-Solomonoff), ou complexité aléatoire, ou complexité algorithmique, est une fonction permettant de quantifier la taille du plus petit algorithme nécessaire pour engendrer un nombre ou une suite quelconque de caractères. Cette quantité peut être vue comme une évaluation d'une forme de complexité de cette suite de caractères.
La complexité de la communication ou complexité de communication est une notion étudiée en informatique théorique. Le dispositif abstrait classique est le suivant : Alice et Bob ont chacun un message, et ils veulent calculer un nouveau message à partir de leurs messages, en se transmettant un minimum d'information. Par exemple, Alice et Bob reçoivent un mot chacun, et ils doivent décider s'ils ont reçu le même mot ; ils peuvent bien sûr s'envoyer leur mot l'un à l'autre et comparer, mais la question est de minimiser le nombre de messages. Utiliser une ressources en communication est le fait d'envoyer une information à l'autre. Alice peut ainsi envoyer la première lettre de son mot à bob, ceci a un coût de 1. La théorie de la complexité de communication a donc pour but de calculer quelles sont les ressources en communication nécessaires pour effectuer certaines tâches dans un contexte de calcul distribué. Contrairement à la théorie de la complexité classique, on ne compte pas les ressources nécessaires aux calculs (temps et espace par exemple). Cette notion a été définie en 1979 par Andrew Yao et est utilisée notamment pour l'étude des algorithmes en ligne et le design des circuits VLSI.
La complexité de Lempel-Ziv est présentée pour la première fois dans l'article On the Complexity of Finite Sequences (IEEE Trans. On IT-22,1 1976), par deux informaticiens israéliens, Abraham Lempel et Jacob Ziv. Cette mesure de complexité est liée à la complexité de Kolmogorov, mais elle n'utilise comme seule fonction que la copie récursive. Le mécanisme mis en œuvre dans cette mesure de complexité est à l'origine des algorithmes de compression de données sans perte LZ77, LZ78 et LZW. Bien que ne reposant que sur un principe élémentaire de recopie de mots, cette mesure de complexité n'est pas trop restrictive en ce sens qu'elle respecte les principales qualités attendues d'une telle mesure : elle n'octroie pas aux séquences avec certaines régularités une grande complexité et cette complexité est d'autant plus grande que la séquence est longue et peu régulière.
En informatique théorique, la complexité des preuves ou complexité des démonstrations est le domaine qui étudie les ressources nécessaires pour prouver ou réfuter un énoncé mathématique. Le démarche classique du domaine est de fixer une sorte de preuve, puis de montrer des bornes sur la longueur des preuves pour certains énoncés. La sorte de preuve peut être d'origine logique, comme la déduction naturelle, le calcul des séquents, des systèmes basés sur la règle de résolution, ou plus combinatoire, comme l'algorithme DPLL et la méthode des plans sécants. Pour chacun il faut définir une notion de longueur pertinente. Le domaine est lié à la théorie de la complexité et aux assistants de preuve. Il a aussi de fortes relations avec les circuits booléens.
La complexité générique est un domaine particulier de la complexité algorithmique qui concerne l’étude de la complexité de problèmes algorithmiques pour « la plupart des données ». La complexité générique est une façon de mesurer la complexité d'un problème algorithmique en négligeant un petit ensemble d'entrées non représentatives et en considérant la complexité dans le pire des cas sur les entrées restantes. La concept de « petit ensemble » est définie en termes de densité asymptotique. L'efficacité apparente des algorithmes mesurée par leur complexité générique provient du fait que, pour une grande variété de problèmes informatiques concrets, les cas les plus difficiles semblent être rares, alors que les cas typiques sont, eux, relativement faciles. Cette approche de la complexité prend son origine dans la théorie combinatoire des groupes qui a une tradition dans l'étude de problèmes algorithmiques qui remonte au début du XXe siècle. La notion de complexité générique a été introduit en 2003 dans des articles de Kapovich, Miasnikov, Schupp et Shpilrain,. Les auteurs montrent que, pour une large classe de groupes finiment engendrés, la complexité générique de quelques problèmes de décision classiques de la théorie combinatoire des groupes, à savoir le problème du mot (en), le problème de conjugaison (en), ou le problème d'appartenance, est en fait linéaire. Une introduction et un survol de la complexité générique sont donnés dans un texte de R. Gilman, A. G. Miasnikov, A. D. Myasnikov, and A. Ushakov. Les livres de Miasnikov, Shpilrain et Ushakov, traitent principalement des résultats des auteurs relativement à la cryptographie.
En mathématiques, ou en informatique, la confluence d'une relation binaire                                    →                        R                                     {\displaystyle \rightarrow _{R}}    est définie comme la propriété suivante : Pour tous éléments                         M         ,                    M                        1                             ,                    M                        2                                     {\displaystyle M,M_{1},M_{2}}    tels que                         M                    →                        R                                   ∗                                        M                        1                                     {\displaystyle M\rightarrow _{R}^{*}M_{1}}    et                         M                    →                        R                                   ∗                                        M                        2                                     {\displaystyle M\rightarrow _{R}^{*}M_{2}}   , il existe un élément                                    M           ′                          {\displaystyle M'}    tel que                                    M                        1                                        →                        R                                   ∗                                        M           ′                          {\displaystyle M_{1}\rightarrow _{R}^{*}M'}    et                                    M                        2                                        →                        R                                   ∗                                        M           ′                          {\displaystyle M_{2}\rightarrow _{R}^{*}M'}   . La confluence est équivalente à la propriété de Church-Rosser.
La conjecture d'Ehrenfeucht ou, maintenant qu'elle est démontrée, le théorème de compacité est un énoncé concernant la combinatoire du monoïde libre ; c'est à la fois un théorème d'informatique théorique et un théorème d'algèbre.
En informatique théorique, et notamment en théorie des automates, l'algorithme appelé la construction par sous-ensembles, en anglais « powerset construction » ou « subset construction », est la méthode usuelle pour convertir un automate fini non déterministe (abrégé en « AFN ») en un automate fini déterministe (abrégé en « AFD ») équivalent, c'est-à-dire qui reconnaît le même langage rationnel. L'existence même d'une conversion, et l'existence d'un algorithme pour la réaliser, est remarquable et utile. Elle est remarquable parce qu'il existe peu de modèles de machines pour lesquels les versions déterministe et non déterministe ont la même puissance de reconnaissance : ainsi, les automates à pile déterministes sont moins puissants que les automates à pile généraux. Elle est utile dans la pratique parce que la reconnaissance par un automate déterministe est bien plus facile à mettre en œuvre que la reconnaissance par automate non déterministe. Toutefois, la construction par sous-ensembles peut produire un automate dont la taille, mesurée en nombre d'états, est exponentielle par rapport à la taille de l'automate de départ. La construction par sous-ensembles a été publiée pour la première fois par Michael O. Rabin et Dana S. Scott dans un article fondateur paru en 1959. Ils ont obtenu le prix Turing pour leurs travaux autour du non-déterminisme en 1976.
En informatique, la continuation d'un système désigne son futur, c'est-à-dire la suite des instructions qu'il lui reste à exécuter à un moment précis. C'est un point de vue pour décrire l'état de la machine.
Un algorithme est correct s'il fait ce qu'on attend de lui. Plus précisément, rappelons qu'un algorithme est décrit par une spécification des données sur lesquelles l'algorithme va démarrer son calcul et une spécification du résultat produit par l'algorithme. Démontrer la correction de l'algorithme consiste à démontrer que l'algorithme retourne, quand il calcule en partant des données, un objet qui est un des résultats escomptés et qui satisfait la spécification du résultat comme énoncé dans la description de l'algorithme.
Le déployeur universel a été conçu théoriquement dans le cadre du mécanisme digital, une théorie supposant que tout est mécanisme,que le cerveau est le siège de la pensée, que les milliards de neurones (individuellement dépourvus de pensée) dont il est constitué ne sont capables que de calculs très simples .Selon cette théorie, le cerveau n'est rien d'autre qu'un dispositif mécanique et la partie irrationnelle ou émotive ne sont que des états particuliers de ce mécanisme. Cela revient à assimiler le cerveau à un ordinateur et la conscience au fonctionnement de ce mécanisme
L'adjectif diophantien (du nom de Diophante d'Alexandrie) s'applique à tout ce qui concerne les équations polynomiales à coefficients entiers, également appelées équations diophantiennes. Les notions qui suivent ont été développées pour venir à bout du dixième problème de Hilbert. Il s'agit de savoir s'il existe un algorithme général permettant de dire si, oui ou non, il existe une solution à une équation diophantienne. Le théorème de Matiyasevich prouve l'impossibilité de l'existence d'un tel algorithme.
DEVS (de l'anglais Discrete Event System Specification) est un formalisme modulaire et hiérarchique pour la modélisation, la simulation et l'analyse de systèmes complexes qui peuvent être des systèmes à événements discrets décrits par des fonctions de transitions d'états et des systèmes continus décrits par des équations différentielles, par exemple et des systèmes hybrides (continus et discrets).
Le dixième problème de Hilbert fait partie de la liste des 23 problèmes posés par David Hilbert en 1900 à Paris, lors de sa conférence au congrès international des mathématiciens. Il énonce :  X - De la possibilité de résoudre une équation de Diophante. On donne une équation de Diophante à un nombre quelconque d'inconnues et à coefficients entiers rationnels : On demande de trouver une méthode par laquelle, au moyen d'un nombre fini d'opérations, on pourra distinguer si l'équation est résoluble en nombre entiers rationnels.  En termes modernes, il demande de trouver une méthode algorithmique générale permettant de décider, pour n'importe quelle équation diophantienne (c'est-à-dire équation polynomiale à coefficients entiers), si cette équation possède des solutions entières. En 1970, Youri Matiiassevitch démontre qu'il n'existe pas de tel algorithme. Le théorème de Matiiassevitch établit que les ensembles diophantiens, qui sont les ensembles de solutions entières positives ou nulles d'une équation diophantienne avec paramètres, sont exactement tous les ensembles récursivement énumérables, ce qui entraîne qu'un tel algorithme ne peut exister.
L'École d'été de Marktoberdorf est une école d'été internationale annuelle, d'une durée de deux semaines, en informatique et en mathématiques qui s'adresse à des étudiants titulaires d'une maîtrise ou jeunes chercheurs en thèse. Elle a lieu chaque année depuis 1970 à Marktoberdorf, près de Munich. Les étudiants sont logés dans l'internat du lycée local de Marktoberdorf. Des comptes-rendus sont publiés le cas échéant.
La consommation en énergie d'un logiciel est définie par l'ordinateur en fonction des systèmes qui le composent. Plusieurs principes de développement peuvent être utilisés pour limiter la consommation d'un logiciel. Aujourd'hui, avec le nombre d'appareils informatiques utilisés chaque jour, du centre de données au smartphone, l'économie d’énergie est devenue plus importante qu'auparavant.
Un encodage one-hot consiste à représenter des états en utilisant pour chacun une valeur dont la représentation binaire n'a qu'un seul chiffre 1. On peut définir une fonction d'encodage one-hot comme étant la fonction qui prend en entrée un vecteur                         z                 {\displaystyle z}    et qui redéfinit en sortie la plus grande valeur de                         z                 {\displaystyle z}    à 1 et toutes autres valeurs de                         z                 {\displaystyle z}    à 0. L'avantage principal de cet encodage est que pour passer d'un état à un autre, seules deux transitions sont nécessaires : un chiffre passe de 1 à 0, un autre de 0 à 1. Son inconvénient est qu'il faut au minimum n bits pour représenter n états, ce qui conduit à une augmentation linéaire du nombre de chiffres par rapport au nombre d'états. Un encodage utilisant toutes les valeurs binaires existantes a quant à lui une augmentation logarithmique du nombre de chiffres.  Par exemple, dans un encodage one-hot, pour trois états possibles, on pourra utiliser les valeurs binaires 001, 010, et 100.
L'épistémologie de l'informatique est la branche de l'épistémologie, parmi les épistémologies disciplinaires, qui prend pour objet d'étude l'informatique en tant que science pour en déterminer son épistémologie, c'est-à-dire, d'une part son ou ses objet(s), ses principes, ses concepts fondamentaux, ses théories et résultats, i.e. ce qui la constitue ; d'autre part ses modes de construction de nouvelles connaissances, ses processus d'inférence et d'émergence de nouveaux concepts, les éléments à l'origine de ses évolutions, i.e. ce qui la fait progresser ; et enfin, ses fondements, son origine, sa portée objective, i.e. ce qui la justifie dans le concert des sciences. L'épistémologie de l'informatique cherche donc à répondre à plusieurs questions, en reprenant la démarche de Jean-Louis Le Moigne : à quoi l'informatique s'intéresse-t-elle ? (question gnoséologique) comment l'informatique procède-t-elle pour connaitre ou engendrer son objet ? (question méthodologique) de quelle manière l'informatique valide-t-elle ses résultats ?
L'European Association for Theoretical Computer Science, en abrégé EATCS, en français Association européenne d'informatique théorique,, est une organisation européenne fondée en 1972. Son objectif est de faciliter l'échange des idées et les résultats dans la communauté des chercheurs en informatique théorique. Elle vise aussi à stimuler la coopération entre les communautés qui font de l'informatique théorique et celles faisant de l'informatique « pratique ».
En développement logiciel, les Feature model sont une technique de modélisation des Software product line. Ils ont été introduits pour la première fois dans la méthode de feature-oriented domain analysis par Kang en 1990.
En informatique, une fonction déterministe est une fonction qui pour le même argument renverra toujours le même résultat. En mathématiques, la définition de la fonction, dans la théorie des ensembles, est implicitement déterministe, c'est pourquoi le terme est essentiellement utilisé en informatique ou le terme fonction est utilisé dans une définition moins formelle. Le terme fonction déterministe correspondant à la définition mathématique classique d'une fonction, nous donnerons quelques exemples de fonctions non déterministes. Le propre des fonctions non déterministes est de renvoyer des résultats différents pour plusieurs appels utilisant les mêmes arguments. Ce cas recouvre deux cas de figure principaux: soit la fonction comprend des arguments cachés, i.e. appelle des arguments qui ne figurent pas dans les arguments entrés par l'utilisateur, par exemple une fonction qui donne l'âge en fonction de la date de naissance, prendra comme argument la date de naissance mais appellera également, de façon "cachée", la date en cours au moment de l'appel de la fonction, depuis l'horloge de l'ordinateur par exemple soit la fonction comprend une composante aléatoire (ou, en général, plus précisément pseudo-aléatoire)
Une fonction pseudo-aléatoire (ou PRF pour pseudorandom function) est une fonction dont l'ensemble des sorties possibles n'est pas efficacement distinguable des sorties d'une fonction aléatoire. Il ne faut pas confondre cette notion avec celle de générateur de nombres pseudo-aléatoires (PRNG). Une fonction qui est un PRNG garantit seulement qu'une de ses sorties prise seule semble aléatoire si son entrée a été choisie aléatoirement. En revanche, une fonction pseudo-aléatoire garantit cela pour toutes ses sorties, indépendamment de la méthode de choix de l'entrée. En cryptographie, ce genre de fonction est extrêmement important : il sert de brique de base à la conception de primitives cryptographiques, en particulier pour les algorithmes de chiffrement.
Une grammaire contextuelle est une grammaire formelle dans laquelle les substitutions d'un symbole non terminal sont soumises à la présence d'un contexte gauche et d'un contexte droit. Elles sont plus générales que les grammaires algébriques. Les langages formels engendrés par les grammaires contextuelles sont les langages contextuels. Ils sont reconnus par les automates linéairement bornés. Les grammaires contextuelles ont été décrites par Noam Chomsky. Ce sont les grammaires de type 1 dans la hiérarchie de Chomsky. Elles peuvent servir à décrire la syntaxe de langages naturels où il apparaît qu'un mot est approprié dans un certain contexte, mais ne l'est pas par ailleurs.
En informatique théorique, et notamment en théorie des langages, on appelle grammaire linéaire une grammaire algébrique dont tous les membres droits de règles contiennent au plus un symbole non terminal. Un langage linéaire est un langage qui est engendré par une grammaire linéaire. Les langages rationnels sont une sous-famille stricte des langages linéaires. Les langages linéaires sont une sous-famille stricte des langages algébriques.
En mathématiques, un graphe aléatoire est un graphe qui est généré par un processus aléatoire. Le premier modèle de graphes aléatoires a été popularisé par Paul Erdös et Alfréd Rényi dans une série d'articles publiés entre 1959 et 1968.
IEEE Transactions on Neural Networks and Learning Systems est une revue scientifique mensuelle révisée par les pairs publiée par l'IEEE Computational Intelligence Society. Il couvre la théorie, la conception et les applications des réseaux de neurones artificiels et des systèmes d'apprentissage connexes. Le rédacteur en chef est Haibo He (Université du Rhode Island). Selon le Journal Citation Reports, la revue avait un facteur d'impact 2013 de 4.370.
L’Information Partielle Linéaire (LPI) est une méthode de modélisation linéaire pour les décisions pratiques, basée sur les informations précédemment floues. Edward Kofler, un mathématicien suisse, développa sa théorie sur l'information partielle linéaire (LPI) en 1970 à Zurich (Suisse).
L'interprétation abstraite est une théorie d'approximation de la sémantique de programmes informatiques fondée sur les fonctions monotones pour ensembles ordonnés, en particulier les treillis (en anglais : lattice). Elle peut être définie comme une exécution partielle d'un programme pour obtenir des informations sur sa sémantique (par exemple, sa structure de contrôle, son flot de données) sans avoir à en faire le traitement complet. Sa principale fonction concrète est l'analyse statique, l'extraction automatique d'informations sur les exécutions possibles d'un programme ; ces analyses ont deux usages principaux : pour les compilateurs, afin d'analyser le programme pour déterminer si certaines optimisations ou transformations sont possibles ; pour le débogage, ou pour prouver l'absence de certains types de bugs dans le programme. L'interprétation abstraite a été formalisée par le professeur Patrick Cousot et le docteur Radhia Cousot.
Le Join-Calcul est un calcul de processus inspiré de la machine abstraite chimique. Il a servi de base pour le développement de plusieurs langages de programmation, tels que JoCaml (basé sur OCaml) et Cω (basé sur C#).
Un langage congruentiel est un langage formel qui est la réunion d'un nombre fini de classes d'une congruence sur l'alphabet donné. Un cas important est celui où la congruence est engendrée par un système de réécriture fini. Selon le type du système de réécriture, la complexité algorithmique du problème du mot peut être linéaire en temps, PSPACE-complet ou indécidable. Les classes de langages congruentiels forment une autre hiérarchie de langages, incomparable à la hiérarchie de Chomsky.
En théorie de la complexité, un langage creux est un langage où le nombre de mots de longueur n est borné par un polynôme en n. Ils sont utiles dans l'étude de la classe NP. L'adjectif creux illustre bien de tel langage ː sur un alphabet à deux lettres, comme il y a 2n mots de longueur n, alors s'il n'y a qu'un nombre polynomial de mots de longueur n, cela signifie que la proposition de mots de longueur n dans un langage creux tend vers 0, quand n tend vers l'infini.
Le lemme local de Lovász (parfois abrégé LLL) est un résultat de théorie des probabilités discrètes, dû à László Lovász et Paul Erdős. De façon informelle, il établit qu'une certaine propriété associée aux événements indépendants peut être généralisée à des événements dépendants, si les dépendances sont petites. Il existe plusieurs versions de ce résultat. Le lemme local est utilisé dans plusieurs domaines, notamment en combinatoire et en informatique théorique. Dans ces domaines il est parfois défini de la manière suivante : étant donné un ensemble de mauvais événements, n'ayant pas de grande dépendances les uns avec les autres, il est possible d'éviter tous ces événements à la fois.
La linguistique informatique est un champ interdisciplinaire basé sur une modélisation symbolique (à base de règles) ou statistique du langage naturel établie dans une perspective informatique.
Voici une liste de publications importantes en informatique théorique, organisés par domaine. Quelques raisons pour lesquelles une publication peut être considérée comme importante: Sujet créateur – Une publication qui a créé un nouveau sujet Découverte – Une publication qui a changé de manière significative les connaissances scientifiques Influence – Une publication qui a considérablement influencé le monde, ou qui a eu un impact massif sur l'enseignement de la informatique théorique.
En logique mathématique, la logique combinatoire est une théorie logique introduite par Moses Schönfinkel en 1920 lors d'une conférence et développé dès 1929 par Haskell Brooks Curry pour supprimer le besoin de variables en mathématiques, pour formaliser rigoureusement la notion de fonction et pour minimiser le nombre d'opérateurs nécessaires pour définir le calcul des prédicats à la suite de Henry M. Sheffer. Plus récemment elle a été utilisée en informatique comme modèle théorique de calcul et comme base pour la conception de langages de programmation fonctionnels. Le concept de base de la logique combinatoire est celui de combinateur qui est une fonction d'ordre supérieur ; elle utilise uniquement l'application de fonctions et éventuellement d'autres combinateurs pour définir de nouvelles fonctions d'ordre supérieur. Chaque combinateur simplement typable est une démonstration à la Hilbert en logique intuitionniste et vice- versa . On appelle cela la correspondance de Curry-Howard
Les logiques de description aussi appelé logiques descriptives (LD) sont une famille de langages de représentation de connaissance qui peuvent être utilisés pour représenter la connaissance terminologique d'un domaine d'application d'une manière formelle et structurée. Le nom de logique de description se rapporte, d'une part à la description de concepts utilisée pour décrire un domaine et d'autre part à la sémantique basée sur la logique qui peut être donnée par une transcription en logique des prédicats du premier ordre. La logique de description a été développée comme une extension des langages à cadres, une famille de langage de programmation pour l'intelligence artificielle, et des réseaux sémantiques, qui ne possédaient pas de sémantique formelle basée sur la logique.
En architecture informatique, la loi d'Amdahl donne l'accélération théorique en latence de l'exécution d'une tâche à charge d'exécution constante que l'on peut attendre d'un système dont on améliore les ressources. Elle est énoncée par l'informaticien Gene Amdahl à l'AFIPS Spring Joint Computer Conference en 1967. La loi d'Amdahl peut être formulée de la façon suivante :                                    S                        latence                             (         s         )         =                                 1                            1               −               p               +                                                   p                   s                                                                          ,                 {\displaystyle S_{\text{latence}}(s)={\frac {1}{1-p+{\frac {p}{s}}}},}    où Slatence est l'accélération théorique en latence de l'exécution de toute la tâche ; s est l'accélération en latence de l'exécution de la partie de la tâche bénéficiant de l'amélioration des ressources du système ; p est le pourcentage du temps d'exécution de toute la tâche concernant la partie bénéficiant de l'amélioration des ressources du système avant l'amélioration. De plus,                                                 {                                                                                     S                                            latence                                                           (                   s                   )                   ≤                                                               1                                                1                         −                         p                                                                                                                                                                       lim                                            s                       →                       ∞                                                                                S                                            latence                                                           (                   s                   )                   =                                                               1                                                1                         −                         p                                                                                                                                                              {\displaystyle {\begin{cases}S_{\text{latence}}(s)\leq {\frac {1}{1-p}}\\\lim _{s\to \infty }S_{\text{latence}}(s)={\frac {1}{1-p}}\end{cases}}}    montrent que l'accélération théorique de l'exécution de toute la tâche augmente avec l'amélioration des ressources du système et que, quelle que soit l'amélioration, l'accélération théorique est toujours limitée par la partie de la tâche qui ne peut tirer profit de l'amélioration. La loi d'Amdahl est souvent utilisée en calcul parallèle pour prédire l'accélération théorique lors de l'utilisation de plusieurs processeurs. Par exemple, si un programme a besoin de 20 heures d'exécution sur un processeur uni-cœur et qu'une partie du programme qui requiert une heure d'exécution ne peut pas être parallélisée, même si les 19 heures (p = 95 %) d'exécution restantes peuvent être parallélisées, quel que soit le nombre de processeurs utilisés pour l'exécution parallèle du programme, le temps d'exécution minimal ne pourra passer sous cette heure critique. Ainsi, l'accélération théorique est limitée au plus à 20 (1/(1 − p) = 20). On en déduit deux règles : premièrement, lors de l'écriture d'un programme parallèle, il faut limiter autant que possible la partie sérielle ; deuxièmement, un ordinateur parallèle doit être un excellent ordinateur sériel pour traiter le plus rapidement possible la partie sérielle.
La loi de Grosch est une loi (au sens scientifique) concernant l'informatique, en particulier les ressources, le coût et la puissance des ordinateurs. Elle vient de l'observation que la combinaison des ressources informatiques sur une même machine a un effet multiplicatif et non pas additif. Elle s'énonce généralement de la manière suivante: la puissance réelle d'un ordinateur croît généralement bien plus vite que son coût. Elle suggère donc que d'importantes économies d'échelle sont possibles en allant vers le gigantisme. Ainsi, une machine dont on augmente la vitesse permet la résolution de nouveaux problèmes. Une autre machine dont on augmente la taille mémoire aussi. Si on effectue les deux opérations sur la même machine, alors cela permet de surcroit de résoudre des classes de problèmes additionnels qui n'auraient été abordables par aucune des deux machines modifiées prises séparément. Herbert Grosch pense pouvoir en déduire une loi implacable et dont l'énoncé influencera toute l'informatique des années 1960 et 1970 : il faut procéder à toutes les améliorations possibles sur une même machine centrale et en partager les ressources.
En architecture informatique, la loi de Gustafson donne l'accélération théorique en latence de l'exécution d'une tâche à temps d'exécution constant que l'on peut attendre d'un système dont on améliore les ressources. Elle est énoncée par l'informaticien John L. Gustafson et son collègue Edwin H. Barsis dans l'article Reevaluating Amdahl's Law en 1988. La loi de Gustafson peut être formulée de la façon suivante :                                    S                        latence                             (         s         )         =         1         −         p         +         s         p         ,                 {\displaystyle S_{\text{latence}}(s)=1-p+sp,}    où Slatence est l'accélération théorique en latence de l'exécution de toute la tâche ; s est l'accélération en latence de l'exécution de la partie de la tâche bénéficiant de l'amélioration des ressources du système ; p est le pourcentage de la charge d'exécution de toute la tâche concerné par la partie bénéficiant de l'amélioration des ressources du système avant l'amélioration. La loi de Gustafson est souvent utilisée en calcul parallèle pour prédire l'accélération théorique lors de l'utilisation de plusieurs processeurs dans le cas où il est possible d'augmenter la quantité de données traitées, contrairement à la loi d'Amdahl, qui suppose une quantité de données constante. Elle traduit le fait que l'on peut traiter plus de données dans le même temps en augmentant le nombre de processeurs.
En informatique, une machine à états abstraits (en anglais abstract state machine ou ASM), est un automate fini dont les états ne portent pas simplement des noms, mais des structures au sens de la logique mathématique, c'est-à-dire des ensembles non vides munis de fonctions, d'opérations et de relations. Les structures peuvent être vues comme des algèbres, ce qui explique le nom d'algèbres évolutives donné initialement aux ASM.
La mémoire temporelle et hiérarchique (en anglais Hierarchical temporal memory (HTM)) est un modèle d'apprentissage automatique développé par Jeff Hawkins et Dileep George de la compagnie Numenta. Il modélise certaines propriétés structurelles et algorithmiques du néocortex. C'est un modèle biomimétique fondé sur le paradigme mémoire-prédiction, une théorie du fonctionnement du cerveau élaborée par Jeff Hawkins dans son livre On Intelligence. Ce modèle permet de découvrir et d'inférer les causes à haut niveau des motifs et séquences observés dans les données, bâtissant ainsi un modèle complexe du monde.
En informatique, un modèle a pour objectif de structurer les informations et activités d'une organisation : données, traitements, et flux d'informations entre entités.
Un modèle booléen est une méthode ensembliste de représentation du contenu d'un document. C'est l'un des premiers modèles utilisés en recherche d'information, permettant de fouiller automatiquement les grands corpus de bibliothèques. Il en existe un modèle étendu qui généralise également le modèle vectoriel.
Le MCT ou Modèle des croyances transférables est un modèle non probabiliste de « raisonnement incertain » reposant sur la théorie des fonctions de croyance. Il a été proposé et développé par Philippe Smets au début des années 1990 Quand un système réel utilise plusieurs capteurs pour détecter une même information, la combinaison de ces sources d'information permet d'améliorer la détection : on parle alors de fusion d'information. Des cadres formels ont été proposés pour, d'une part, représenter l'imperfection des informations issues de plusieurs capteurs (s'appuyant sur les théories précédemment citées) et, d'autre part, de combiner ces informations afin d'améliorer la détection. Ces cadres formels sont nombreux et l'objet de cet article est de présenter le « Modèle des Croyances Transférables » (MCT), basé sur les fonctions de croyance et étendant la théorie Dempster-Shafer.
Le modèle probabiliste de pertinence est une méthode probabiliste de représentation du contenu d'un document, proposée en 1976 par Robertson et Jones. Elle est utilisée en recherche d'information pour exprimer une estimation de la probabilité de pertinence d'un document par rapport à une requête, et ainsi classer une liste de documents dans l'ordre décroissant d'utilité probable pour l'utilisateur. L'une des applications directes de ce modèle est la méthode de pondération Okapi BM25, considérée comme l'une des plus performantes dans le domaine.
Un modèle vectoriel (parfois nommé sémantique vectorielle) est une méthode algébrique de représentation d'un document visant à rendre compte de sémantique, proposé par Gerard Salton dans les années 1970 . Elle est utilisée en recherche d'information, notamment pour la recherche documentaire, la classification ou le filtrage de données. Ce modèle concernait originellement les documents textuels et a été étendu depuis à d'autres types de contenus. Le premier exemple d'emploi de ce modèle est le système SMART.
La modélisation d’entreprise se situe au cœur du domaine de la productique, traitant de problèmes allant de la représentation du système industriel en vue de son analyse et de sa conception, jusqu’à des problèmes d’intégration et d’interopérabilité des systèmes industriels.
En théorie des langages fonctionnels typés, une monade est une structure permettant de manipuler des langages fonctionnels purs avec des traits impératifs. Il s'agit alors d'avoir une représentation simulant exactement des notions telles que les exceptions ou les effets de bords, tout en conservant la pureté des langages fonctionnels. Il existe plusieurs intérêts à l'usage des monades : analyses statiques et preuves de programmes plus simples, usage de l'appel par nécessité, optimisations (déforestation, mémoïsation de valeurs, parallélisation, réduction forte (en)).
En combinatoire, et notamment en combinatoire des mots, un carré est un mot composé de deux parties égales consécutives, comme bonbon ou papa. En bio-informatique, un carré est appelé une répétition en tandem. Un mot sans facteur carré ou plus simplement un mot sans carré est un mot qui ne contient pas de facteur carré. Par exemple, le mot répétition contient le carré titi ; en revanche, le mot consécutivement est un mot sans carré. L'étude des mots sans carré fait partie, plus généralement, de l'étude des répétitions dans les mots, et de la possibilité de les éviter. On parle alors de répétitions évitables ou inévitables. Il existe des mots infinis sans carré sur tout alphabet d'au moins trois lettres, comme l'a prouvé Axel Thue,. Sur un alphabet à deux lettres, un tel mot n'existe pas. Le mot de Prouhet-Thue-Morse contient des carrés, en revanche il est sans cube.
En informatique théorique, en combinatoire, et notamment en combinatoire des mots, un motif inévitable est un motif (au sens défini ci-dessous) qui apparaît dans tout mot assez long. Un motif est évitable sinon. Par exemple, le motif xx est inévitable sur deux lettres et inévitable sur trois lettres, parce que tout mot assez long sur deux lettres contient un carré (composé de deux facteurs consécutifs égaux), et qu'il existe des mots arbitrairement longs sans carré sur trois lettres. Les motifs évitables et inévitables généralisent la notion de répétition dans les mots, et leur étude s'inscrit dans celle des régularités dans les mots.
En logique mathématique et en informatique théorique, le mu-calcul (ou logique du mu-calcul modal) est l'extension de la logique modale classique avec des opérateurs de points fixes. Le mu-calcul (propositionnel et modal) a d'abord été introduit par Dana Scott et Jaco de Bakker puis a été étendu dans sa version moderne par Dexter Kozen. Cette logique permet de décrire les propriétés des systèmes de transition d'états et de les vérifier. De nombreuses logiques temporelles (telles que CTL* ou ses fragments très usités comme CTL (en) ou LTL) sont des fragments du mu-calcul. Une manière algébrique de voir le mu-calcul est de le considérer comme une algèbre de fonctions monotones sur un treillis complet, les opérateurs étant une composition fonctionnelle plus des points fixes ; de ce point de vue, le mu-calcul agit sur le treillis de l'algèbre des ensembles. La sémantique des jeux du mu-calcul est lié aux jeux à deux joueurs à information parfaite, notamment les jeux de parité.
Neural Computation est une revue scientifique mensuelle révisée par les pairs qui examine tous les aspects des réseau de neurones artificiels, y compris la modélisation du cerveau et la conception et la construction de systèmes de traitement de l'information à inspiration neurale.
Le nombre cyclomatique, la complexité cyclomatique ou la mesure de McCabe est un outil de métrologie logicielle développé par Thomas McCabe en 1976 pour mesurer la complexité d'un programme informatique. Cette mesure reflète le nombre de décisions d'un algorithme en comptabilisant le nombre de « chemins » linéairement indépendants au travers d'un programme représenté sous la forme d'un graphe.
L'optimisation linéaire en nombres entiers (OLNE) (ou programmation linéaire en nombres entiers (PLNE) ou encore Integer Programming (IP)) est un domaine des mathématiques et de l'informatique théorique dans lequel on considère des problèmes d'optimisation d'une forme particulière. Ces problèmes sont décrits par une fonction de coût et des contraintes linéaires, et par des variables entières. La contrainte d'intégralité sur les variables, qui différencie l'OLNE de l'optimisation linéaire classique est nécessaire pour modéliser certains problèmes, en particulier des problèmes algorithmiques. Mais cette contrainte supplémentaire rend le problème plus complexe et demande des techniques particulières.
Un ordre partiel complet (complete partial order ou CPO) est un ensemble partiellement ordonné dont toutes les chaînes ont une borne supérieure. Cette définition équivaut à celle d'ensemble inductif strict avec plus petit élément. La notion de CPO est utilisée pour résoudre les équations aux domaines, notamment quand on cherche une sémantique dénotationnelle pour un langage en informatique.
Un pangramme autodescriptif est une phrase contenant toutes les lettres de l'alphabet (pangramme) et exprimant une proposition vraie sur elle-même (autodescriptif).  « Ce pangramme autodescriptif en hommage à Douglas Hofstadter, Lee Sallows, Jacques Pitrat, Nicolas Graner et Éric Angelini contient exactement dix-sept a, un b, onze c, huit d, trente-cinq e, cinq f, neuf g, six h, vingt-quatre i, deux j, un k, sept l, six m, vingt-six n, onze o, huit p, huit q, onze r, quinze s, vingt-sept t, dix-sept u, quatre v, deux w, neuf x, un y, et cinq z. »  — OuLiPo
En informatique théorique un préordre de simulation est une relation entre systèmes de transition d'états associant des systèmes qui se comportent de la même façon au sens qu'un système simule l'autre. Intuitivement, un système simule l'autre s'il peut imiter toutes ses actions. La définition basique relie les états à l'intérieur d'un système de transition, mais c'est facilement adaptable pour mettre en relation deux systèmes de transition séparés en construisant un système consistant en l'union disjointe des composants correspondants.
Preuves, Programmes et Systèmes (PPS) est une unité mixte de recherche (UMR 7126) de logique informatique appartenant à l'université Paris-Diderot et au Centre national de la recherche scientifique dont le directeur est Thomas Ehrhard. Elle est rattachée à l'Institut des sciences informatiques et de leurs interactions (INS2I) et à l'Institut de sciences mathématiques et de leurs interactions (INSMI) du CNRS. En janvier 2016, le Laboratoire d'informatique algorithmique: fondements et applications et l'unité de recherche Preuves, Programmes et Systèmes fusionnent pour former l'Institut de Recherche en Informatique Fondamentale (IRIF).
Le prix Knuth récompense les scientifiques ayant apporté une contribution exceptionnelle en informatique théorique. Il porte le nom de Donald E. Knuth, l'un des plus grands contributeurs à l'informatique théorique. Le prix Knuth est attribué tous les dix-huit mois, depuis 1996 ; il inclut une récompense de 5 000 USD. Le prix est attribué par le groupe d'intérêt SIGACT de l'ACM et le comité technique d'informatique théorique de l'IEEE. Les prix sont attribués en alternance au colloque ACM STOCS (Symposium on Theory of Computing) et à la conférence IEEE FOCS (Symposium on Foundations of Computer Science) qui sont parmi les conférences les plus prestigieuses d'informatique théorique. Contrairement au prix Gödel qui récompense les articles exceptionnels, le prix Knuth est attribué aux individus.
Le prix EATCS est un prix, remis par l'European Association for Theoretical Computer Science (EATCS) à un chercheur pour honorer sa brillante carrière en informatique théorique.
Le prix Gödel est une distinction créée en 1992 par l'European Association for Theoretical Computer Science (EATCS) et le Special Interest Group on Algorithms and Computation Theory (SIGACT) de l'Association for Computing Machinery (ACM) pour honorer des travaux remarquables d'informatique théorique. Il est nommé en l'honneur du logicien Kurt Gödel.
En informatique théorique, un problème est un objet mathématique qui représente une question ou un ensemble de questions auxquelles un ordinateur devrait être en mesure de répondre. Le plus souvent, ces problèmes sont de la forme : étant donné un objet (l'instance), effectuer une certaine action ou répondre à telle question. Par exemple, le problème de la factorisation est le problème suivant : étant donné un nombre entier, trouver un facteur premier de cet entier. On distingue en particulier deux types de problèmes : les problèmes de décision, qui consistent à répondre oui ou non à une question (par exemple, cet entier est-il premier ?) ; les problèmes d'évaluation ou de construction, qui consistent à produire un objet spécifié par l'énoncé du problème. Les problèmes algorithmiques jouent un rôle central en informatique théorique et forment un domaine à part entière, à côté de celui des algorithmes qui étudient les méthodes efficaces de résolution de problèmes décidables et de celui de l'analyse de la complexité des algorithmes qui cherche à comprendre les performances de ces algorithmes.
Les problèmes de satisfaction de contraintes ou CSP (Constraint Satisfaction Problem) sont des problèmes mathématiques où l'on cherche des états ou des objets satisfaisant un certain nombre de contraintes ou de critères. Les CSP font l'objet de recherches intenses à la fois en intelligence artificielle et en recherche opérationnelle. De nombreux CSP nécessitent la combinaison d'heuristiques et de méthodes d'optimisation combinatoire pour être résolus en un temps raisonnable. Ils sont notamment au cœur de la programmation par contraintes, un domaine fournissant des langages de modélisation de problèmes et des outils informatiques les résolvant.
La programmation génétique est une méthode automatique inspirée par le mécanisme de la sélection naturelle tel qu'il a été établi par Charles Darwin pour expliquer l'adaptation plus ou moins optimale des organismes à leur milieu. Elle a pour but de trouver par approximations successives des programmes répondant au mieux à une tâche donnée.
En informatique théorique et en logique mathématique, la propriété de Church-Rosser est une propriété des systèmes de réécriture. Elle est nommée ainsi d'après les mathématiciens Alonzo Church et John Barkley Rosser.
La recherche d'information (RI) est le domaine qui étudie la manière de retrouver des informations dans un corpus. Celui-ci est composé de documents d'une ou plusieurs bases de données, qui sont décrits par un contenu ou les métadonnées associées. Les bases de données peuvent être relationnelles ou non structurées, telles celles mises en réseau par des liens hypertexte comme dans le World Wide Web, l'internet et les intranets. Le contenu des documents peut être du texte, des sons, des images ou des données. La recherche d'information est historiquement liée aux sciences de l'information et à la bibliothéconomie qui visent à représenter des documents dans le but d'en récupérer des informations, au moyen de la construction d’index. L’informatique a permis le développement d’outils pour traiter l’information et établir la représentation des documents au moment de leur indexation, ainsi que pour rechercher l’information. La recherche d'information est aujourd'hui un champ pluridisciplinaire, intéressant même les sciences cognitives. La recherche d'information sur le web à l'aide d'un moteur de recherche est une technique de l'information et de la communication, désormais massivement adoptée par les usagers.
En informatique théorique, la réécriture (ou récriture) est un modèle de calcul dans lequel il s’agit de transformer des objets syntaxiques (mots, termes, lambda-termes, programmes, preuves, graphes, etc.) en appliquant des règles bien précises. La réécriture est utilisée en informatique, en algèbre, en logique mathématique et en linguistique. La réécriture est utilisée en pratique pour la gestion des courriers électroniques (dans le logiciel sendmail, les entêtes de courrier sont manipulées par des systèmes de réécriture) ou la génération et l'optimisation de code dans les compilateurs.
En combinatoire des mots, une répétition est une suite de symboles qui se répète plusieurs fois consécutivement. Par exemple, le mot répétition lui-même contient la répétition titi, formé de deux occurrences consécutives du mot ti. L'étude des répétitions a des applications importantes en biologie, où on les retrouve sous le terme général de répétition en tandem, en compression des données à l’aide de dictionnaires, comme dans Lempel-Ziv-Welch. Une répétition évitable est une répétition que l’on peut éviter dans certains cas, une répétition inévitable est une répétition que apparaîtra nécessairement à un moment donné. Une généralisation de cette notion est celle de motif inévitable. Par exemple, un cube xxx est évitable sur 2 lettres, un carré xx est inévitable sur 2 lettres, mais est évitable sur 3 lettres : ce sont les mots sans carré. Le motif xyx est inévitable, même sur 2 lettres
Le codage parcimonieux (ou sparse coding en anglais) est un type de réseau de neurones à apprentissage non supervisé. Il est basé sur le principe qu'une entrée est approximativement la somme de plein de petits motifs élémentaires.
Le gaz neuronal, est un réseau de neurones artificiel, inspiré des cartes auto-adaptatives, et introduites en 1991 par Thomas Martinetz et Klaus Schulten. Le gaz neuronal est un algorithme simple pour trouver une représentation optimale de données à partir de vecteurs principaux. La méthode fut appelée "gaz neuronal" parce que l'évolution des vecteurs principaux durant l'étape d'apprentissage fait penser à un gaz qui occupe un espace de façon uniforme. Cet algorithme est appliqué à la compression de données ou à la quantification vectorielle, par exemple en reconnaissance des langues naturelles, en traitement des images ou à la reconnaissance de motifs. En tant qu'alternative robuste et convergente à l'algorithme K-moyennes, il peut être utilisé pour le partitionnement de données.
Une machine à état liquide (MEL, liquid state machine en anglais, LSM) est un genre particulier de réseau de neurones dopé. Une MEL consiste en un large ensemble d'unités (appelées nœuds, ou neurones). Chaque nœud reçoit une entrée dépendant du temps depuis une source externe (les entrées) aussi bien que depuis d'autres nœuds. Les nœuds sont aussi connectés aléatoirement les uns aux autres. La partie récurrente de ces liens, ainsi que les entrées qui dépendent du temps, génèrent un motif d'activation spatio-temporel des nœuds. Ces activations spatio-temporelle sont lues par les unités via des discriminants linéaires. L'ensemble de nœuds connectés de façon récurrente va finir par réussir à calculer une grande variété de fonctions non-linéaires des entrées. Si on se donne un nombre assez grand de telles fonctions non-linéaires, alors il est théoriquement possible d'obtenir des combinaisons linéaires (en utilisant les unités de lecture) qui vont permettre de calculer n'importe quelles opérations mathématiques nécessaires à la réalisation de certaines tâches, comme la reconnaissance de la parole ou la vision automatique. Le terme liquide dans le nom de cette méthode vient de l'analogie avec le fait de lâcher une pierre dans un réservoir d'eau (ou d'un autre liquide). La pierre qui chute va générer des ondes de choc dans le liquide. Le signal d'entrée (le mouvement de la pierre en train de tomber) a ainsi été converti en un motif spatio-temporel de déplacement du liquide (les ondes de choc). Les MEL ont été mises en avant comme une façon d'expliquer certains aspects du cerveau. Certains défendent les MEL comme étant une amélioration de la théorie des réseaux neuronaux artificiels parce que : Les circuits ne sont pas conçus pour effectuer une tâche précise. Les signaux d'entrées continus et dépendant du temps sont pris en compte de façon "naturelle". Les calculs sur de diverses échelles de temps peuvent être effectués par le même réseau. Et le même réseau peut être utilisé pour calculer différentes opérations. Certains critiquent aussi les MEL telles qu'elles sont utilisées en neuroscience computationnelle parce que : Les MEL n'expliquent pas comment le cerveau fonctionne. Au mieux elles ne font qu'imiter certaines de ses fonctions. Il n'y a aucun moyen d'ouvrir un réseau qui fonctionne bien et d'espérer apprendre comment ou quelles opérations sont calculées (une MEL agit comme une boite noire). Il n'y a pas assez de contrôle sur le processus d'apprentissage des MEL.
En apprentissage automatique, la machine de Boltzmann restreinte est un type de réseau de neurones artificiels pour l'apprentissage non supervisé. Elle est couramment utilisée pour avoir une estimation de la distribution probabiliste d'un jeu de données. Elle a initialement été inventée sous le nom de Harmonium en 1986 par Paul Smolenski.
Un réseau de neurones artificiels, ou réseau neuronal artificiel, est un système dont la conception est à l'origine schématiquement inspirée du fonctionnement des neurones biologiques, et qui par la suite s'est rapproché des méthodes statistiques. Les réseaux de neurones sont généralement optimisés par des méthodes d’apprentissage de type probabiliste, en particulier bayésien. Ils sont placés d’une part dans la famille des applications statistiques, qu’ils enrichissent avec un ensemble de paradigmes  permettant de créer des classifications rapides (réseaux de Kohonen en particulier), et d’autre part dans la famille des méthodes de l’intelligence artificielle auxquelles ils fournissent un mécanisme perceptif indépendant des idées propres de l'implémenteur, et fournissant des informations d'entrée au raisonnement logique formel (voir Deep Learning). En modélisation des circuits biologiques, ils permettent de tester quelques hypothèses fonctionnelles issues de la neurophysiologie, ou encore les conséquences de ces hypothèses pour les comparer au réel.
Le retour de pertinence est un domaine particulier de la recherche d'information qui a pour but de prendre en compte le point de vue des utilisateurs sur les documents présentés. Le principe, simple, repose sur le fait que l'utilisateur est le seul à savoir exactement ce qu'il cherche et donc est le plus à même de juger de la pertinence des informations retournées par un système de recherche. Partant de cette idée, les systèmes exploitant le retour de pertinence permettent de récupérer ces « jugements » utilisateurs et les exploitent pour améliorer les résultats de recherche. Les techniques de retour de pertinence trouvent leur origine dans les années 1970 avec notamment les travaux de Rocchio sur le retour explicite de pertinence. L'idée essentielle était alors qu'il était possible d'affiner la requête initiale des utilisateurs au fur et à mesure que ceux-ci fournissaient des jugements de pertinence sur des documents consultés. Le système classique de recherche d'information était donc doté d'une boucle de retour qui permettait au système de reformuler la requête posée et donc d'avancer dans la recherche.
La sémantique axiomatique est une approche basée sur la logique mathématique qui sert à prouver qu'un programme informatique est correct.
En informatique, la sémantique dénotationnelle est une des approches permettant de formaliser la signification d'un programme en utilisant les mathématiques. Parmi les autres approches, on trouve la sémantique axiomatique et la sémantique opérationnelle. Cette discipline a été introduite par Christopher Strachey et Dana Scott. En général, la sémantique dénotationnelle utilise des techniques de programmation fonctionnelle pour décrire les langages informatiques, les architectures et les programmes. Les mathématiques utilisées en sémantique dénotationnelle font partie de ce qu'on appelle maintenant la théorie des domaines.
En informatique théorique, la sémantique formelle (des langages de programmation) est l’étude de la signification des programmes informatiques vus en tant qu’objets mathématiques.
En informatique, la sémantique opérationnelle est l'une des approches qui servent à donner une signification aux programmes informatiques d'une manière rigoureuse, mathématiquement parlant (voir Sémantique des langages de programmation). Une sémantique opérationnelle d'un langage de programmation particulier décrit comment chaque programme valide du langage doit être interprété en termes de suite d'états successifs de la machine. Cette suite est la signification du programme. Dans le cas d'un programme fonctionnel, l'état final d'une suite qui termine donne la valeur de retour du programme. (Dans le cas général, il peut y avoir plusieurs suites de calculs et plusieurs valeurs de retour pour un seul programme, parce que celui-ci pourrait être non déterministe.) Un des moyens les plus courants pour définir rigoureusement une sémantique opérationnelle est de fournir un système de transition d'états rendant compte du comportement attendu du programme considéré. Une telle définition autorise une analyse formelle du langage, permettant l'étude de relations entre les programmes. Parmi les relations importantes, on trouve : les pré-ordres de simulation et les bisimulations, qui sont très utiles dans le cadre du parallélisme. Définir une sémantique opérationnelle au travers d'un système de transition se fait habituellement en donnant une définition inductive de l'ensemble des transitions possibles. Habituellement, cela prend la forme d'un ensemble de règles d'inférence définissant les transitions valides du système. La sémantique opérationnelle est reliée à la sémantique dénotationnelle au travers du concept d'abstraction.
En mathématiques, en informatique théorique, et notamment en combinatoire des mots, une sesquipuissance ou mot de Zimin est un mot sur un alphabet qui possède un préfixe propre qui est aussi un suffixe propre. En d'autre termes, une sesquipuissance est un mot avec bord. Une sesquipuissance est un motif inévitable, en ce sens que tout mot assez long en contient une en facteur. On définit par récurrence des sesquipuissances d'ordre n>1 : ce sont des mots qui ont un bord qui lui-même est une sesquipuissances d'ordre n-1.
Le Special Interest Group on Algorithms and Computation Theory (SIGACT) est le pôle d'intérêt commun de l'ACM consacré au soutien de la recherche sur l'informatique théorique. Il a été créé en 1968 par Patrick C. Fischer (en).
En mathématiques, et notamment en combinatoire des mots, la suite de pliage de papier régulière, connue aussi sous le nom de suite de la courbe du dragon, est une suite automatique composée de 0 et de 1. Les premiers termes de la suite sont : 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, . . . (c'est la suite A014707 de l'OEIS) Le nom provient de la construction suivante : si l'on prend une bande de papier que l'on plie en deux, toujours dans le même sens (à gauche par exemple), la forme résultante présente une suite de changements de direction que l'on peut coder par G pour gauche et D pour droite. On obtient alors la suite : G G G D G G D G G D D G G D G G D D G G G D D G D D G G D G G D D G G G D D G D D G G G D G G D D D G G D D G D D etc. . . . Chaque ligne est obtenue, à partir de la précédente, en commençant par écrire la ligne précédente, puis en lui ajoutant un G au bout (extrémité droite) et enfin en recopiant la ligne précédente en la lisant de droite à gauche et en échangeant systématiquement chaque G et chaque D.  La régularité dans le nom réfère au fait que l'on plie la bande toujours dans le même sens. Lorsque l'on déplie la bande, la figure s'approche de la courbe du dragon. La suite de 0 et 1 donnée plus haut s'obtient en replaçant G par 0 et D par 1. Si on remplace au contraire G par 1 et D par 0, on obtient la suite « duale » : 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, ... (c'est la suite A014577 de l'OEIS)
En mathématiques, et notamment en combinatoire des mots, la suite de Rudin-Shapiro, aussi connue sous le nom suite de Golay–Rudin–Shapiro est une suite automatique, nommée ainsi d'après Marcel Golay, Walter Rudin et Harold Shapiro, qui ont étudié indépendamment ses propriétés.
Un système complexe est un ensemble constitué d'un grand nombre d'entités en interaction qui empêchent l'observateur de prévoir sa rétroaction, son comportement ou évolution par le calcul.
Contrairement au système de variable continue, un système d'événement discret est un système qui satisfait les deux propriétés suivantes : l'espace d'état est discret, la transition d'état est déclenchée par l'événement.
En informatique théorique, plus précisément en calculabilité, un système de tague (tag system en anglais) est un modèle de calculabilité défini par Emil Leon Post en 1943 comme système de réécriture. C'est un cas particulier de système de Post. Ce modèle est Turing-complet : toute fonction calculable peut être émulée[Quoi ?] par un système de tague de Post.
Dans la théorie des automates et en logique séquentielle, une table de transition d'état est un tableau montrant dans quel état (ou états dans le cas d'un automate fini non déterministe) d'un automate fini se déplacer, sur la base de l'état actuel et des autres entrées. Une table d'état est essentiellement une table de vérité, dans laquelle certaines des entrées sont l'état actuel, et les sorties comprennent l'état suivant, en même temps que les autres sorties. Une table d'état est l'un des moyens de spécifier un Automate fini, d'autres moyens étant un Diagramme états-transitions, et une équation caractéristique.
Un terme est une expression de base du calcul des prédicats, de l'algèbre, notamment de l'algèbre universelle, et du calcul formel, des systèmes de réécriture et de l'unification. C'est l'objet produit par une analyse syntaxique. Sa principale caractéristique est d'être homogène (il n'y a que des opérations de base et pas d'opérations logiques) et de décrire l'agencement des opérations de base. Un terme est parfois appelé une formule du premier ordre. Par exemple, (x + f(x,y)) * 3 et *(+(x,f(x,y)),3) et *+xfxy3 et la figure à droite représentent le même terme sous quatre formes externes différentes.
La terminaison est une propriété fondamentale des algorithmes. Elle stipule que les calculs décrits par l'algorithme s'arrêteront. En général cet arrêt doit avoir lieu quelles que soient les données initiales que l'on fournit à l'algorithme. Si l'on veut insister sur ce point on parle alors souvent de terminaison uniforme, mais le plus généralement « terminaison » couvre aussi bien l'arrêt sur une donnée que l'arrêt sur toutes les données et c'est le contexte qui décide. La terminaison forme avec la correction partielle un des aspects de la correction des algorithmes, la correction partielle et la terminaison forment ce que l'on appelle la correction totale. Dans le cas spécifique des machines de Turing, la terminaison s'appelle l'arrêt (halting en anglais dans le vocabulaire de Turing). Un cas particulier d'étude de la terminaison est la terminaison d'un système de réécriture.
La terminaison d'un système de réécriture                                    →                        R                                     {\displaystyle \rightarrow _{R}}    porte sur un système de réécriture abstrait et affirme que toute chaîne de réduction de termes de la forme                                    t                        0                                        →                        R                                        t                        1                                        →                        R                                        t                        2                                        →                        R                             …                 {\displaystyle t_{0}\rightarrow _{R}t_{1}\rightarrow _{R}t_{2}\rightarrow _{R}\ldots }    est finie. Elle est souvent présentée en disant qu'il n'y a aucune chaîne de réduction infinie.
En informatique théorique, un test de propriété (ou property testing en anglais) est un test probabiliste qui a pour objectif de déterminer si un objet donné, par exemple un graphe ou une fonction, possède une propriété fixée ou bien s'il est « loin » de l'avoir. Ces algorithmes ont l'avantage d'être très rapides. Les testeurs de propriété sont notamment utiles pour tester que des grands graphes ont certaines propriétés.
Le théorème de Cobham est un théorème de combinatoire des mots qui a des connexions importantes avec la théorie des nombres, et notamment des nombres transcendants, et avec la théorie des automates. Le théorème stipule que si les écritures, en deux bases multiplicativement indépendantes, d'un ensemble d'entiers naturels S sont des langages rationnels, alors l'ensemble S est une union finie de progressions arithmétiques. Le théorème a été prouvé par Alan Cobham en 1969. Depuis, il a donné lieu à de nombreuses extensions et généralisations.
En combinatoire et notamment en combinatoire des mots le théorème de Dejean, appelé la conjecture de Dejean avant d'avoir été démontré, concerne les répétitions d'exposant fractionnaire dans les mots infinis. La conjecture a été formulée en 1972 par Françoise Dejean qui alors travaillait avec Marcel-Paul Schützenberger, et elle a été démontrée en 2009, presque simultanément, par James D. Currie et Narad Rampersad d'une part, par Michaël Rao d'autre part.
En informatique théorique, et plus précisément en théorie de la complexité, le théorème de Mahaney dit que s'il existe un langage creux NP-complet, alors P = NP. Un langage creux est un langage où le nombre de mots de longueur n du langage est polynomial en n.
La théorie algorithmique des jeux ou théorie des jeux algorithmique est un domaine entre les mathématiques, l'informatique théorique et l'économie. Plus précisément, ce domaine est une étude de certains aspects de l'économie et de la théorie des jeux d'un point de vue quantitatif et algorithmique.
La théorie de la complexité est le domaine des mathématiques, et plus précisément de l'informatique théorique, qui étudie formellement la quantité de ressources (temps, espace mémoire, etc.) dont a besoin un algorithme pour résoudre un problème algorithmique. Il s'agit donc d'étudier la difficulté intrinsèque de problèmes, de les organiser par classes de complexité et d'étudier les relations entre les classes de complexité.
En informatique théorique, l'objectif de la théorie des automates est de proposer des modèles de mécanismes mathématiques qui formalisent les méthodes de calcul. Cette théorie est le fondement de plusieurs branches importantes de l'informatique théorique, comme : La calculabilité, par le modèle des machines de Turing ; Les automates finis, et leurs variantes, qui sont utilisés dans l'analyse des langues naturelles, la traduction des programmes par les compilateurs, divers algorithmes de manipulation de textes comme les algorithmes de recherche de sous-chaîne, ou la vérification automatique du fonctionnement de circuits logiques. C'est la partie sur laquelle on insiste le plus dans les écoles d'ingénieurs, car elle est immédiatement exploitable avec profit ; La théorie de la complexité des algorithmes, visant à classifier les algorithmes en fonction des ressources temporelles et en mémoire nécessaires à leur exécution ; La vérification de modèle qui sert à établir la conformité de programmes à leurs spécifications. Voir par exemple Coq. Les automates n'ont pas d'existence physique, mais sont un modèle abstrait.
La théorie des domaines est une branche des mathématiques dont le principal champ d'application se trouve en informatique théorique. Cette partie de la théorie des ensembles ordonnés a été introduite par Dana Scott pendant les années 1960, afin de fournir le cadre théorique nécessaire à la définition d'une sémantique dénotationnelle du lambda-calcul. Les domaines sont des ensembles partiellement ordonnés. Dans la sémantique dénotationnelle du lambda-calcul, les éléments des domaines représentent les lambda-termes et le plus petit élément (quand on en munit le domaine) représente le résultat d'un calcul ne finissant pas, c'est l'élément dit « indéfini », noté ⊥ (prononcer « bottom »). L'ordre du domaine définit, dans l'idée, une notion de quantité d'information : un élément du domaine contient au moins toute l'information contenue dans les éléments qui lui sont inférieurs. L'idée est ensuite de se ramener à des domaines particuliers où toute fonction monotone (croissante) a un plus petit point fixe. En général, on utilise des ordres partiels complets (complete partial order, ou CPO), c'est-à-dire des domaines qui possèdent un plus petit élément et où toute chaîne (partie strictement ordonnée) a une borne supérieure. Ainsi, il devient aisé d'associer une sémantique au combinateur de point fixe Y, en le représentant par une fonction totale qui à une fonction associe un de ses points fixes s'il en existe et ⊥ sinon. Par là-même, donner un sens à une fonction définie « récursivement » (c'est-à-dire en fait, en tant que point fixe d'une fonctionnelle G) devient possible : si f est la fonction qui à 0 associe 1 et à n > 0 associe n * f(n – 1), on peut aussi définir f comme ceci : f = Y(G) (point fixe de G) où G est la fonction qui prend une fonction φ en entrée et rend la fonction qui à 0 associe 1 et à n > 0 associe n * φ(n – 1) (et à ⊥ associe ⊥, par définition de ⊥). G est monotone sur le domaine des fonctions de ℕ⊥ dans ℕ⊥ et, à ce titre, admet un point fixe (la fonction factorielle) alors, on a un moyen de calculer f : en itérant G sur la fonction f0 = ⊥, c'est-à-dire la fonction qui à tout entier naturel et à ⊥ associe ⊥. f est la limite de la suite ainsi obtenue (et le plus petit point fixe de G). La théorie des domaines permet aussi de donner un sens aux équations de domaine de type A = A → A (A est l'ensemble des fonctions de A dans A). Dans les mathématiques habituelles, ceci est absurde, à moins de donner un sens particulier à cette flèche. Par exemple ℝ = ℝ → ℝ paraît impossible, ne serait-ce que pour des raisons de cardinalité (Dans la théorie des cardinaux, ℝ est un infini strictement plus petit que ℝ → ℝ) ; pourtant, si cette flèche ne représente que les applications continues de ℝ dans ℝ, on garde bien le même cardinal que ℝ (en effet, une application continue de ℝ dans ℝ peut être définie par sa restriction à l'ensemble dénombrable ℚ, donc cet ensemble a le cardinal de ℝℕ, donc de ℝ). En théorie des domaines, la notion de continuité sur un ensemble A aura son équivalent : la continuité selon Scott (en) sur un domaine A. Une fonction est Scott-continue ssi elle est monotone sur A et si pour toute partie filtrante (partie où toute paire d'éléments a un majorant) B de A admettant une borne supérieure, on a sup(f(B)) = f(sup(B)). Cette définition sera souvent simplifiée pour le cas où A est un CPO : la fonction est continue si et seulement si elle est monotone et si, pour toute chaîne B, on a sup(f(B)) = f(sup(B)).  Portail de la logique  Portail de l'informatique théorique
Une U-matrix (unified distance matrix en anglais) est un concept d'apprentissage automatique, pour certains réseaux de neurones artificiels. Plus précisément, c'est une représentation d'une carte auto-adaptative où les distances euclidiennes entre les poids associés aux neurones voisins sont représentées par une image en tons de gris. Les U-Matrix sont utilisées pour visualiser des données exprimées dans un espace de grandes dimensions sur image 2D.
En informatique et en logique, l'unification est un processus algorithmique qui, étant donnés deux termes, trouve une substitution qui appliquée aux deux termes les rend identiques. Par exemple,                         x         +         3                 {\displaystyle x+3}    et                         2         +         y                 {\displaystyle 2+y}    peuvent être rendus identiques par la substitution                         x         :=         2                 {\displaystyle x:=2}    et                         y         :=         3                 {\displaystyle y:=3}   , qui donne quand on l'applique à chacun de ces termes le terme                         2         +         3                 {\displaystyle 2+3}   . Dit autrement, l'unification est la résolution d'une équation dans l'algèbre des termes (unification syntaxique) ou dans une algèbre quotient par un ensemble d'identités (unification modulo une théorie) ; la solution de l'équation est la substitution qui rend les deux termes identiques et que l'on appelle l'unificateur. L'unification a des applications en inférence de types, programmation logique, en démonstration automatique de théorèmes, en système de réécriture, en traitement du langage naturel. Souvent, on s'intéresse à l'unification syntaxique où il faut que les termes obtenus par application de l'unificateur soient syntaxiquement égaux, comme dans l'exemple ci-dessus. Par exemple, le problème d'unification syntaxique ayant pour données                         x         +         3                 {\displaystyle x+3}    et                         5                 {\displaystyle 5}    n'a pas de solution. Le filtrage par motif (ou pattern matching) est une restriction de l'unification où l'unificateur n'est appliquée qu'à un seul des deux termes. Par exemple,                         x         +         3                 {\displaystyle x+3}    et                         2         +         3                 {\displaystyle 2+3}    sont rendus égaux par la substitution                         x         :=         2                 {\displaystyle x:=2}   . La fin de l'article présente aussi l'unification modulo une théorie, qui est le cas où on dispose de connaissances supplémentaires sur les fonctions (par exemple,                         +                 {\displaystyle +}    est commutatif).
La théorie de Vapnik-Chervonenkis (également connue sous le nom de théorie VC) est une théorie mathématique et informatique développée dans les années 1960-1990 par Vladimir Vapnik et Alexey Chervonenkis. C'est une forme de théorie de l'apprentissage automatique, qui tente d'expliquer l'apprentissage d'un point de vue statistique.
En mathématiques, et dans d'autres disciplines comprenant des langages formels dont la logique mathématique, une variable libre ou variable parlante[réf. nécessaire] est une notation qui spécifie à quelles places dans une expression mathématique (en) une substitution peut avoir lieu. Cette idée est liée à celle de marque substitutive (un symbole qui sera plus tard remplacé par une chaîne de caractères), ou de caractère joker qui tient lieu de symbole non spécifié, qui s'oppose à la notion de variable muette ou variable liée. En programmation informatique une variable libre est une variable référencée dans une fonction et qui n'est pas une variable locale, ni un paramètre de cette fonction.
Les interactions Homme-machines (IHM) définissent les moyens et outils mis en œuvre afin qu'un humain puisse contrôler et communiquer avec une machine. Les ingénieurs en ce domaine étudient la façon dont les humains interagissent avec les ordinateurs ou entre eux à l'aide d'ordinateurs, ainsi que la façon de concevoir des systèmes qui soient ergonomiques, efficaces, faciles à utiliser ou plus généralement adaptés à leur contexte d'utilisation. L'amélioration de l'ergonomie de l'interface Homme-machine a notamment pour objectif d'optimiser l'aménagement du poste de travail et de limiter ainsi les risques du travail sur écran (troubles musculosquelettiques, fatigue oculaire, syndrome d'épuisement professionnel, stress, stress numérique...).
Un actuateur d'impulsions neuronales (en anglais NIA : neural impulse actuator) est un dispositif BCI (Interface Neuronale Directe) développé par OCZ Technology. Les dispositifs BCI tentent de remplacer les dispositifs de saisie numérique classiques comme le clavier et la souris d'ordinateur en lisant l'activité électrique du cerveau de l'utilisateur. Le nom neural impulse actuator, implique que le signal prenne son origine dans l'activité neuronale, pourtant, ce qui en réalité capturé, est un mélange d'activités nerveuse et musculaires incluant des composants orthosympathiques et parasympathiques qui doivent être rentrés comme des signaux biopotentiels plus que comme de simples signaux neuraux. Ce nom est tout de même justifiable étant donné que ces signaux sont sous contrôle neuronal. Ces biopotentiels sont dispersés en plusieurs spectres de fréquences pour autoriser la séparation en différent groupes de signaux électriques. Les signaux individuels qui sont isolés peuvent être des ondes cérébrales alpha, bêta, des électromyogrammes ou des électro-oculogrammes. La version actuelle du NIA utilise des fibres de carbone injectées dans du plastique pour le bandeau et pour les capteurs ce qui provoque une meilleure sensibilité par rapport à la version précédente qui utilisait des capteurs à base de chlorure d'argent. Un prototype a été présenté au CeBIT en 2007.
Le but de l'analyse des tâches est de mettre en évidence et de décrire les tâches à effectuer pour accomplir un travail. Une analyse très fine des tâches peut également servir à prédire ou à expliquer les performances d'un utilisateur dans un environnement donné.
L'audit ergonomique se distingue des tests utilisateurs par l'absence de participation des utilisateurs. L'audit se base généralement sur une des méthodes d'inspection des interfaces.
En informatique, une boîte combinée (en anglais combo-box ou combobox) est un élément d'interface graphique qui réunit une zone de texte et une liste déroulante. Cet élément est couramment utilisé par des sites Web et des logiciels. À l'état initial seulement, une des valeurs possibles de la liste est affichée. Dans la grande majorité des cas, une manière de montrer les autres options est placée juste à côté de la zone de modification de texte. La valeur de la zone est changée en sélectionnant à partir de la liste déroulante. De plus, l'utilisateur peut saisir directement ses propres valeurs en utilisant la zone de texte. La valeur initiale peut être changée programmatiquement, ce n'est pas nécessairement le premier élément de la liste. En programmation, on peut généralement ajouter et enlever des items un par un à la boîte combinée, ou en groupe.
On appelle les bonnes pratiques pour l'IHM (ou en anglais, human interface guidelines, HIG) les lignes directrices que doivent suivre les développeurs dans la production d'un logiciel pour respecter la cohérence de l'aspect et des fonctions des différentes interfaces graphiques destinées à un certain système d'exploitation ou environnement graphique. Ces règles de conduites sont exposées sous forme de recommandations par l'éditeur du système d'exploitation. Elles visent à améliorer la prise en main du logiciel par les utilisateurs en exploitant des ressources qu'ils peuvent retrouver dans d'autres logiciels. En général, les bonnes pratiques définissent d'abord les règles à respecter en termes d'aspect visuel (choix des polices de caractères, emplacement des menus, conventions sur les icônes, les boutons, les boîtes de dialogue et les fenêtres…). Selon le cas, elles peuvent concerner l'utilisation de la couleur, les effets de transparence et d'ombrage ou encore les animations. Elles s'étendent aussi à tous les mécanismes liés à l'interaction humain-machine, l'utilisabilité, aux fonctions d'aide ou à la présentation et à la saisie d'informations. L'objectif est d'offrir aux utilisateurs le même environnement visuel et le même type de comportement de chaque élément quel que soit le logiciel qu'ils emploient. Ces règles sont établies en fonction d'études d'ergonomie et de conventions esthétiques. Elles diffèrent selon le système ou l'environnement : les bonnes pratiques de Windows ou de GNOME présentent des particularités par rapport à celles de Mac OS X ou de Symbian OS.
Une borne interactive est un terminal informatique mis à la disposition du public pour fournir un accès à des réseaux d'information. Elle peut offrir de nombreux services ciblés tels que la billetterie. La borne interactive est donc très utilisée pour remplacer les caisses et guichets dans les administrations, les commerces, les lieux culturels, etc.
BrainGate est un système d'implants neuronaux mis au point par la société de bio-technologie Cyberkinetics en 2003, en collaboration avec le département de Neurosciences à l'université Brown.
La cartographie d'information est une discipline qui trouve ses fondamentaux dans une pluralité de connaissances à la croisée de la gestion des connaissances, de la sémiologie graphique et des sciences cognitives. Elle fait appel à une grande variété de concepts issus de domaines et sous domaines parfois très spécialisés. Ce sont des outils privilégiés d'exploration et d'analyse de la complexité et des espaces de représentation (dimensions des données, réduction, projection/spatialisation),. Les cartes d'informations diffèrent des cartes géographiques, des cartes ou schémas heuristiques, des cartes éditoriales ou infographies, des cartes d'investigation et s'apparentent aux graphes, aux cartes sémantiques et dynamiques.
Un compte d'utilisateur est l'ensemble des ressources informatiques attribuées à un utilisateur ou à un appareil (ordinateur, périphérique...). Un compte d'utilisateur ne peut être exploité qu'en s'enregistrant auprès d'un système à l'aide de son identifiant (généralement un nom d'utilisateur ou user name en anglais) et de son authentifiant tel qu'un mot de passe (password). Cette procédure se dit « s'enregistrer » et il arrive que dans ce domaine on utilise parfois abusivement le terme anglais login pour désigner le compte proprement dit.
La conception centrée sur l'utilisateur ou conception orientée utilisateur (UCD, user-centered design en anglais) est une philosophie et une démarche de conception surtout présente en ergonomie informatique, où les besoins, les attentes et les caractéristiques propres des utilisateurs finaux sont pris en compte à chaque étape du processus de développement d'un produit. La norme ISO 9241-210 qui annule et remplace la norme ISO 13407 définit sept ensembles de pratique de base pour mettre en œuvre le processus de conception centrée sur l'utilisateur. La conception centrée sur l'utilisateur est principalement utilisée en conception informatique et s'appuie sur des critères d'ergonomie et d'utilisabilité. Cette démarche se distingue fortement d'autres démarches de conception en cherchant à adapter le produit (généralement l'interface utilisateur) à l'utilisateur final plutôt que de lui imposer un mode d'utilisation choisi par les concepteurs.
La conception de logiciel interactif (ou conception d'interface homme-machine [IHM], ou conception d'interface) est l'activité qui vise à définir, pour le premier des termes, le fonctionnement d'un logiciel destiné à être utilisé par une ou plusieurs personnes, quand aux deux autres, ils peuvent également s'appliquer à l'interface d'une machine-outil, d'un instrument de musique numérique, d'un véhicule ou un aéronef, ou à tout un tas d'autres types de machines. C'est un partie de la conception de produit liée à l'ergonomie.
La conception participative est une méthode de travail utilisée principalement en conception de logiciel interactif. Sa principale caractéristique est la participation active des utilisateurs au travail de conception. Il s'agit donc d'une méthode de conception centrée sur l'utilisateur où l'accent est mis sur le rôle actif des utilisateurs. La conception participative prend ses racines dans le mouvement syndicaliste, particulièrement en Scandinavie. Il s'agissait principalement d'inciter les employeurs à obtenir l'adhésion des travailleurs quand de nouveaux outils étaient conçus pour eux, en les faisant participer aux choix. Dès les années 1970, des chercheurs ont étudié comment appliquer la conception participative pour réaliser des logiciels ayant une meilleure utilisabilité et une meilleure acceptabilité. Le domaine a ensuite été abondamment étudié dans les années 1980 et 1990 par des chercheurs et professionnels américains, en perdant sa connotation syndicaliste. Depuis le début des années 2000, les communautés scientifiques françaises de l'ergonomie et du génie logiciel ont commencé à s'y intéresser, et des sociétés spécialisées commencent à apparaître en Europe. Il existe dans la littérature de nombreuses variantes de la méthode et de nombreuses techniques utilisées pour impliquer efficacement les utilisateurs. On peut noter particulièrement : l'observation et entretiens la production de scénarios le brainstorming le prototypage papier le prototypage vidéo
Dasher est un logiciel d’accessibilité informatique qui permet à des personnes d’écrire du texte sans utiliser de clavier d’ordinateur mais à l’aide d’un écran et un dispositif de pointage comme une souris, un touchpad, un écran tactile, un trackball, un joystick, un bouton, une Wiimote ou même une souris commandée par les pieds ou la tête. De tels dispositifs peuvent servir de prothèse pour les personnes handicapées qui sont dans l’impossibilité d’utiliser des claviers standards, ou dans toute situation qui rend leur usage difficile.
Pouvoir analyser, comprendre et décoder les informations transitant dans le cerveau humain est un défi auquel de plus en plus de monde parmi la communauté scientifique mais aussi industrielle s'intéressent. En effet, la compréhension d'un phénomène aussi complexe que celui du fonctionnement d'un cerveau pourrait révolutionner complètement la façon dont nous appréhendons le monde. Dans cet objectif, un grand nombre de scientifiques s'intéressent plus particulièrement au lobe occipital. Cette région du cerveau, permettant la reconnaissance des orientations et des contours, est la plus propice à ce genre d'analyse car idéalement située. Elle offre ainsi aux scientifiques l'opportunité de peut-être un jour, en comprendre le fonctionnement. Ouvrant ainsi la porte à la compréhension d'autres régions du cerveau aujourd'hui trop complexes pour les appréhender.
Le dock est une fonctionnalité d'interface graphique qui permet en général à l'utilisateur de lancer ses logiciels et de passer de l'un à l'autre. Les dock les plus connus sont ceux d'Android (pour les petits terminaux) et de Mac OS X (pour les ordinateurs), cependant, cette fonctionnalité est également disponible sur la plupart des autres systèmes d'exploitation. Le dock est le plus souvent situé en bas de l'écran et contient des raccourcis tel que les icônes des applications, dossiers ou fichiers favoris de l'utilisateur. Parfois, cette zone contient également la corbeille. Ce concept du dock a été popularisé par NeXTSTEP, le système d'exploitation ayant notamment servi de base à Mac OS X. Toutefois, son origine remonte au système d'exploitation Arthur sorti en 1987 et ancêtre de RISC OS. Le dock s'appelait alors "Iconbar". Les systèmes de fichiers montés et d'autres choses (comme l'imprimante) étaient iconifiés à gauche de l'Iconbar. La partie droite était, quant à elle, réservée aux applications en exécution. Des fichiers pouvaient être glissés-déplacés sur cette barre.
L'empathie artificielle décrit le fait que des robots et interfaces soient développés de telle sorte qu'ils aient ou développent certains aspects de l'empathie humaine. Les tentatives de mettre au point de tels robots remontent aux années 1990. Ces systèmes posent de nombreux challenges technologiques. Ils posent également des questions éthiques et philosophiques.
Un environnement virtuel pour l'apprentissage humain (EVAH) est un EIAH particuliers, qui a pour objectifs de mettre l'apprenant en situation d'apprentissage dans un environnement de réalité virtuel.  Portail de l’informatique  Portail de l’éducation  Portail des TIC pour l'enseignement
Les environnements informatiques pour l'apprentissage humain (EIAH) sont des environnements informatiques qui ont pour objectifs de favoriser ou susciter des apprentissages, de les accompagner et de les valider. Les réalisations sont nombreuses, mais les succès très inégaux. La recherche dans ce domaine est née avec l'informatique mais s'est surtout développée dans le sillage de l'intelligence artificielle dans les années 1970. De nombreuses dénominations ont été utilisées. Le terme EIAH est né dans les années 1990, avec le souhait de souligner l'interaction entre les deux pôles source de la complexité du projet technologique et scientifique : l'informatique (avec la modélisation computationnelle qu'elle exige et son inscription matérielle) et l'apprentissage humain (pour lequel on ne dispose encore que de modèles très partiels). La recherche sur les EIAH est fondamentalement pluridisciplinaire, en appelant à la coopération de différents secteurs de l'informatique (génie logiciel, réseau, la modélisation des connaissances et des interactions, etc.), et des sciences de l'homme et de la société (psychologie, didactique, ergonomie, sciences des langages, sciences de la communication, etc.). Liste de quelques thèmes concernés : Étude des systèmes d'organisation d'information (voir ontologie, web sémantique) Techniques de l'apprentissage en ligne (E-learning) Modélisation de l'utilisateur Tuteur intelligent Logo Dématérialisation des examens Micromondes Environnements Virtuels pour l'Apprentissage Humain et Environnement Virtuels pour la Formation. Analyse des données de l'apprentissage De la gamification à la ludicisation de l'apprentissage
L'ergonomie informatique (ou ergonomie des interfaces) est une branche de l'ergonomie, qui a pour objectif d'améliorer l'interaction homme-machine, la facilité d’utilisation et d’apprentissage des produits interactifs. Cette pratique cherche à concevoir ou modifier des interfaces afin qu'elles soient en adéquation avec les caractéristiques physiologiques, perceptives et cognitives de leurs utilisateurs potentiels. Elle s'appuie sur différentes méthodes de conception et d’évaluation permettant d’obtenir le logiciel ou le site web le mieux adapté aux utilisateurs visés.
Eugène Goostman est une intelligence artificielle russe née en 2001 à Saint-Pétersbourg, et plus précisément un agent conversationnel. Ce programme, créé par Vladimir Veselov, Eugene Demchenko et Sergey Ulasen, simule un jeune garçon ukrainien de 13 ans avec une personnalité et un niveau de connaissances crédibles pour les interlocuteurs.
L'expérience du magicien d'Oz (ou technique) est une expérience dans le domaine de l'interaction homme-machine et de l'ergonomie informatique dans laquelle les sujets interagissent avec un système informatisé qu'ils croient autonome, mais qui est en fait totalement ou partiellement contrôlé par un humain. En ergonomie, la technique du magicien d'Oz est pratiquée dans le cadre de tests utilisateur. Par exemple, un sujet d'expérience croit interagir avec une interface vocale d'ordinateur, alors que la "voix" est créée par une personne dissimulée dans une autre pièce (le "magicien") par traitement de texte transformé en un flux audio. La fonctionnalité fournie par le "magicien" peut s'implémenter dans des versions ultérieures du système, mais ses détails d'implémentation ne sont pas pertinents dans l'étude. En général, le but de ces tests est d'étudier l'utilisabilité - efficacité, efficience, satisfaction - d'une interface utilisateur en observant et mesurant l'utilisation par les utilisateurs plutôt que les qualités du système dans son ensemble. La technique du magicien d'Oz est aujourd'hui peu utilisée avec l'avènement de nouvelles techniques de maquettage papier et surtout les logiciels de prototypage d'interface, permettant de simuler les interactions entre l'utilisateur et le système. Le nom de l'expérience vient de l'histoire du Magicien d'Oz, dans laquelle un homme ordinaire caché derrière un rideau se fait passer pour un puissant magicien.
Un fil d’Ariane (en anglais, breadcrumb, « miette de pain », voir section « étymologie » ci-dessous) est, en ergonomie et plus particulièrement de nos jours dans le domaine de la conception d’interfaces informatiques, une aide à la navigation sous forme de signalisation de la localisation du lecteur dans un document (très souvent, une page d’un site web). Leur but est de donner aux utilisateurs un moyen de garder une trace de leur emplacement à l'intérieur de programmes, documents ou pages Web.
Paul Morris Fitts, né le 6 mai 1912 à Martin (Tennessee) et mort le 2 mai 1965 à Ann Arbor (Michigan), est un psychologue, professeur d'université et officier américain.
Gaia est une interface utilisateur, développée par Mozilla pour fonctionner avec leur propre système d'exploitation appelé Firefox OS (anciennement Boot to Gecko).
GOMS (acronyme anglais pour goals, operators, methods, and selection rules, littéralement buts, opérateurs, méthodes et règles de sélection) est un modèle de description du comportement qui prend comme hypothèse le caractère adaptatif du sujet humain. GOMS permet de modéliser le comportement à différents niveaux d'abstraction, depuis la tâche jusqu'aux actions physiques. Le modèle a été développé en 1983 par Stuart Card, Thomas P. Moran et Allen Newell et décrit dans leur ouvrage The Psychology of Human Computer Interaction.
Her, ou Elle au Québec, est une comédie dramatique de science-fiction américaine écrite et réalisée par Spike Jonze sortie en salles en 2013.
HTC Sense est une interface utilisateur non libre, basée sur l'interface utilisateur TouchFLO 3D, développée par HTC pour les appareils mobiles fonctionnant sous Android et précédemment sous Windows Mobile.
L'IHM par genre (gender HCI en anglais) est une branche de l'interface humain-machine (IHM) focalisant sur les différences dans la manière dont les femmes et les hommes interagissent avec les ordinateurs.
L'interactivité est une activité nécessitant la coopération de plusieurs êtres ou systèmes, naturels ou artificiels, qui agissent en ajustant leur comportement. Elle est souvent associée aux technologies permettant des échanges homme-machine. Toutefois, l'interactivité est présente dans toutes les formes de communication et d'échange où la conduite et le déroulement de la situation sont liées à des processus de rétroaction, de collaboration, de coopération entre les acteurs qui produisent ainsi un contenu, réalisent un objectif, ou plus simplement modifient et adaptent leur comportement. Une communication interactive s'oppose à une communication à sens unique, sans réaction du destinataire, sans rétroaction.
Une interface neuronale directe - abrégée IND ou BCI ou encore ICM (interface cerveau-machine, ou encore interface cerveau-ordinateur) est une interface de communication directe entre un cerveau et un dispositif externe (un ordinateur, un système électronique…). Ces systèmes peuvent être conçus pour assister, améliorer ou réparer des fonctions humaines de cognition ou d'action défaillantes. L'IND peut être unidirectionnelle ou bidirectionnelle. Ce type de périphérique est fondamentalement différent de toute autre interface homme-machine : une telle liaison ne requiert en effet aucune transformation préalable du signal électrique émis par l’activité cérébrale en activité musculaire (psychomotrice), cette dernière étant usuellement traduite en signal d’entrée pour la machine. En s’affranchissant de la chaîne de réaction « cerveau, nerfs, muscles, interface conventionnelle homme-machine », les temps de réponse peuvent être écourtés de plusieurs dixièmes de seconde dans le cas d’interaction urgente. De plus, ils laissent les organes vitaux (mains, pieds, yeux, etc.) libres et disponibles pour d’autres types de commandes simultanées.
En mécanique, un levier de commande est un petit dispositif, une manette, qui se présente sous la forme d'une tige rigide pivotant à une extrémité, et permettant de transmettre un signal de commande simple. Un levier constitue une interface homme-machine mécanique simple.
Le lifelog ou lifeblog (lit. journal/carnet de vie) désigne le fait d'enregistrer et d'archiver toutes les informations de sa vie. Cela comprend tous les textes, toutes les informations visuelles, tous les fichiers audio, toute l'activité médias, ainsi que toutes les données biologiques provenant de capteurs sur le corps. Les informations seraient archivées pour le bénéfice du lifelogger, et partagées avec d'autres à des degrés divers, et contrôlées par lui seul. L'un des précurseurs du lifelog est Richard Buckminster Fuller, avec son Dymaxion Chronofile (en).
En informatique, une liste déroulante (anglais drop-down list) est un élément d'interface graphique qui permet à l'utilisateur de sélectionner une ou plusieurs options. Il en existe deux types : celles qui affichent et permettent de choisir une seule option, et celles qui permettent de sélectionner plusieurs options et qui en montrent au moins deux. Cet élément est couramment utilisé par des sites Web et des logiciels. Sous certaines interfaces utilisateur (notamment avec Windows, avant l'Aero de Vista), une liste déroulante qui affiche une seule option a la même apparence qu'une boîte combinée (ou combo-box en anglais). Sous d'autres, comme GNOME et Mac OS X, les deux types sont facilement différenciés.   Portail de l’informatique
En psychologie expérimentale, en ergonomie et en interaction Homme-machine, la loi de Fitts est un modèle du mouvement humain, indice de la difficulté d'une tâche. La formulation la plus courante actuelle exprime le temps requis pour aller rapidement d'une position de départ à une zone finale de destination, en fonction de la distance à la cible et de la taille de la cible. La loi de Fitts est utilisée pour modéliser l'acte de « pointer », à la fois dans le vrai monde, par exemple avec une main ou un doigt, et sur les ordinateurs, par exemple avec une souris. Publiée par Paul Fitts en 1954, elle ne s'applique pas nécessairement aux interface tactiles actuelles (2015).
Magic Leap est une startup américaine travaillant sur une technologie de réalité augmentée, fondée en 2010 par Rony Abovitz. Basée à Dania Beach, la société a levé depuis sa création 2 milliards de dollars auprès d'investisseurs de premier rang, parmi lesquels Google, Alibaba, Andreessen Horowitz et Qualcomm. Le casque de réalité augmentée Magic Leap One sera lancé en 2018 pour les développeurs.
Magic Leap One est un visiocasque de réalité augmentée créé par Magic Leap et dont la sortie est prévue pour 2018.
Une manette est une poignée allongée, souvent en métal ou en plastique rigide, qui sert à actionner certains organes mécaniques. Elle fonctionne suivant le principe du levier. Exemples : manette de jeu, manette de frein (ou de dérailleur) d'une bicyclette, manette de démarrage à froid d'une motocyclette, etc.
Le modèle de l'arche est un patron d'architecture logicielle introduit en 1992 pour structurer les logiciels interactifs. Comme le modèle de Seeheim, il vise essentiellement à fixer un vocabulaire pour analyser les architectures des applications et réfléchir à leurs évolutions : comment, par exemple, remplacer une interface graphique par une interface vocale en laissant inchangée la plus grande partie possible du logiciel.
Le modèle de Seeheim est un patron d'architecture logicielle introduit en 1983 pour structurer l'interface homme-machine dans un logiciel interactif. Seeheim est une ville d'Allemagne où s'est tenu en novembre 1983 un atelier de travail organisé par Eurographics sur le thème "User Interface Management Systems" (Systèmes de Gestion d'Interface Utilisateur). Le modèle de Seeheim a été proposé collectivement au cours de cette réunion.
Le modèle du processeur humain est un modèle issu de la psychologie cognitive qui permet d'évaluer l'utilisabilité d'un produit. Il a été décrit en 1983 par Stuart Card, Thomas P. Moran et Allen Newell. Il représente le sujet humain comme un système de traitement d'informations régi par des règles. Ce dernier comprend trois sous-systèmes interdépendants (sensoriel, moteur et cognitif) munis chacun d'un processeur et d'une mémoire. Le Modèle du Processeur Humain constitue un cadre général de pensée et un point de départ culturel simple dans le domaine de l'ergonomie cognitive. Sa représentation de l'individu sous forme d'un système de traitement de l'information sert de paradigme à la psychologie contemporaine. Cette vision et la théorie des systèmes de production sur laquelle le modèle s'appuie, ont contribué pour une large part à rénover les techniques de laboratoire de la psychologie traditionnelle. La simplicité de ce modèle a sa contrepartie. il ne fournit pas d'indication sur les représentations mentales alors que ces structures ont une incidence directe sur le comportement ou la prédiction de ce comportement. Bien qu'il se situe à un haut niveau d'abstraction, il se limite à la prédiction ou à l'explication de phénomènes de performance de bas niveau. Le Modèle du Processeur Humain est donc trop réducteur pour assister l'informaticien dans sa tâche de conception de systèmes interactifs. Il ne véhicule aucune méthode de conception. Aucun élément du modèle n'indique comment satisfaire les contraintes de performance qu'il permet cependant de déduire. D'autres modèles comme GOMS ou Keystroke tentent de combler cette lacune.
Le modèle-vue-vue modèle (en abrégé MVVM, de l'anglais Model View ViewModel) est une architecture et une méthode de conception utilisée dans le génie logiciel. Apparu en 2004, MVVM est originaire de Microsoft et adapté pour le développement des applications basées sur les technologies Windows Presentation Foundation et Silverlight via l'outil MVVM Light par exemple. Cette méthode permet, tel le modèle MVC (modèle-vue-contrôleur), de séparer la vue de la logique et de l'accès aux données en accentuant les principes de binding et d’événement.
Les modèles théoriques de l'expérience utilisateur sont généralement mobilisés dans les travaux scientifiques et peuvent concerner plusieurs disciplines comme les sciences cognitives, les sciences de gestion, ou les sciences de l'information et de la communication. Ces modèles sont également utilisés par des professionnels dans le cadre de la conception des interface homme machines ou plus récemment dans la réalisation de pages internet. Il existe de nombreux modèles dans la littérature néanmoins trois modèles principaux sont fréquemment cités : le modèle d’ Hassenzahl, le modèle de Mahlke et le modèle de Karapanos.
Un moniteur d'activité physique (aussi appelé moniteur d'activité ou moniteur sportif ou traqueur d'activité ; en anglais, activity tracker) est un appareil électronique ou une application qui permet de mesurer l'intensité et la quantité d'activité physique effectuée par un individu tels que la distance marchée ou courue, la consommation et la dépense de calories, et dans certains cas, le rythme cardiaque et la qualité du sommeil. Le but du moniteur est d'augmenter la motivation de son utilisateur et de lui permettre d'améliorer ses performances. Le terme est maintenant utilisé principalement pour désigner des appareils électroniques de surveillance dédiés qui sont synchronisés, dans de nombreux cas sans fil, à un ordinateur ou à un téléphone intelligent pour le suivi des données à long terme. Il s'agit alors d'un exemple de technologie portable et d'objet connecté. Le terme est aussi utilisé pour désigner des applications sur téléphones intelligents et des applications Facebook. En 2017, selon une enquête internationale (189 770 répondeurs) lancé par Mozilla, c'est aux États-Unis que ces objets sont proportionnellement les plus répandus (Chez les personnes ayant répond aux Etats-Unis à ce sondage, les traqueurs y constituaient 22% des objets connectés (contre 12 % en moyenne globale)).
La navigation au clavier est un type d'interface homme-machine permettant de parcourir des documents (textes ou autres) avec l'aide d'un clavier, sans l'usage de la souris. On parle aussi d'interface en ligne de commande ou Interpréteur de commandes dans un environnement graphique. Une commande au clavier est une combinaison de touches qui une fois enfoncée permet d'exécuter un programme ou une fonction d'un programme et ainsi de naviguer dans un document. Cette navigation est couramment utilisée dans des éditeurs de texte tels Emacs ou Vim mais aussi dans des navigateurs web tel que Microsoft Internet Explorer, ou bien Firefox avec le plugin Vimperator. C'est une alternative ou un complément à la navigation à l'aide de la souris. Son but est de rendre la navigation plus efficace. Un raccourci clavier est une séquence de touches ayant une action particulière (page suivante, sous-menu, ...).
Le nombre magique sept, plus ou moins deux : quelques limites à nos capacités de traitement de l'information (Miller, 1956) est un des articles les plus cités en psychologie,,. Il a été publié en 1956 par le psychologue cognitif George A. Miller du département de psychologie de l'université de Princeton dans le journal Psychological Review. En résumé, il soutient que le nombre d'objets pouvant tenir dans la mémoire de travail d'un humain moyen est de 7 plus ou moins 2. Des recherches récentes démontrent que la loi du nombre magique sept est fondée sur une interprétation erronée de l'article de Miller. Le nombre correct de nouveaux éléments pouvant tenir dans la mémoire courante est probablement de trois ou quatre.
L’oculométrie (en anglais « Eye-tracking » ou « Gaze-tracking ») regroupe un ensemble de techniques permettant d'enregistrer les mouvements oculaires. Les oculomètres les plus courants analysent des images de l'œil humain enregistrées par une caméra, souvent en lumière infrarouge, pour calculer la direction du regard du sujet. En fonction de la précision souhaitée, différentes caractéristiques de l'œil sont analysées. D'autres techniques sont basées sur les variations de potentiels électriques à la surface de la peau du visage ou encore sur les perturbations induites par une lentille spéciale sur un champ magnétique. Par le biais de l’eye-tracking, on peut analyser l'activité oculaire d'un individu, mettre en évidence où se porte son regard, et ainsi savoir ce qu'il voit et ne voit pas. C'est un domaine utilisé notamment dans l’ergonomie. Appliquée au domaine du web, l'ergonomie est utile pour le confort de navigation des internautes mais aussi pour rendre un site attractif, attrayant et efficace. L’eye-tracking est une mesure objective du parcours visuel sur un écran ou sur un support écrit. Les systèmes d’eye-tracking permettent de mettre en évidence les déplacements effectués par les yeux sur l’écran et de faire apparaître, sous la forme de cartes, les zones où se concentre le regard. Très utilisée dans la création des écrans Internet, cette technique apporte aux concepteurs d’écrans ou aux ergonomes des informations sur la navigation au sein d’un écran ou entre plusieurs écrans et sur la pertinence de l’implantation de telle ou telle information pour attirer le regard et donc l’attention.
Oxatis est une entreprise française basée à Marseille créée en 2001 par Marc Schillaci et Marc Heurtaut. La société édite et développe une solution de création de site e-commerce en mode SaaS.
Le partage d'écran est appellation couramment utilisée par les technologies et les produits qui permettent un accès et la collaboration à distance au bureau de l'ordinateur d'une personne par un émulateur de terminal graphique. Les deux scénarios de partage d'écran les plus courants sont : la connexion à distance (surnommé ː bureau à distance) la collaboration en temps réel (exː Groupware) Plusieurs logiciels permettent le partage d'écran, certains payants, d'autres gratuits ou avec une version gratuite (Skype, Chrome Remote Desktop)  Portail de l’informatique
La pédagogie à faible encadrement ou pédagogie à faible intrusion (Mini-invasive Pedagogy) est une forme d'apprentissage selon laquelle les enfants évoluent dans des environnements non surveillés. La méthodologie est née d'une expérience réalisée par Sugata Mitra travaillant au National Institute of Technology de Tiruchirappalli (NIIT) en 1999. Cette expérience, dénommée le Trou dans Le Mur est ensuite devenue un programme de formation porté par l'organisation Hole-in-the-Wall Education Ltd. (HiWEL), en coopération avec le NIIT, l'International Finance Corporation ainsi qu'une société indienne dédiée à l'informatique et la pédagogie. La pédagogie à faible encadrement serait utilisée dans plus de 300 « stations d'apprentissage » à destination de plus de 300 000 enfants en Inde et dans plusieurs pays Africains. Ce programme a été récompensé par le prix Digital Opportuny décerné par la World Information Technology and Services Alliance (WITSA).
Un ou une pop-up (de l'anglais pop-up window ou pop-up tout court), parfois appelée fenêtre intruse ou fenêtre surgissante, est une fenêtre secondaire qui s'affiche, sans avoir été sollicitée par l'utilisateur (fenêtre intruse), devant la fenêtre de navigation principale lorsqu'on navigue sur Internet. Le mot incrustation a été jadis recommandé par une commission de terminologie informatique francophone. Ce moyen est communément utilisé pour afficher des messages publicitaires ou un avertissement comme la réponse à un message privé dans un forum. Néanmoins, certains sites sont conçus selon le principe d'une page classique ne contenant qu'une image de fond et le lien du style « entrer » qui ouvre le site en lui-même est en fait conçu dans une fenêtre de type pop-up. Certains sites contenant des fonctionnalités annexes (souvent en Adobe Flash), comme un lecteur audio ou vidéo, nécessitent l'utilisation de ce système de fenêtre pop-up, car ces fonctionnalités se lancent dans une fenêtre pop-up.
Le patron de conception PAC (présentation, abstraction, contrôle) a été introduit par la chercheuse en informatique grenobloise Joëlle Coutaz à la fin des années 1980 en tant que modèle abstrait d'architecture logicielle pour les interfaces homme-machine. Il énonce qu'un logiciel interactif peut être organisé comme une hiérarchie de composants constitués chacun de trois facettes : la Présentation prend en charge l'interaction avec l'utilisateur, par exemple en affichant un dessin et en gérant les entrées effectuées avec la souris. L'ensemble des facettes de Présentation constitue la partie du programme purement dédiée à l'IHM. l'Abstraction gère les données à représenter, ou les fonctions à interfacer. C'est par exemple, dans un téléphone portable, le carnet d'adresses et la gestion des communications téléphoniques. L'ensemble des Abstractions est la partie qui autrefois constituait la totalité des programmes, et qu'on appelle maintenant le noyau fonctionnel. le Contrôle gère la correspondance entre les deux autres facettes : cohérence des représentations avec les données internes, conversion des actions de l'utilisateur en opérations du noyau fonctionnel. Les facettes de contrôle servent aussi à créer une hiérarchie de composants logiciels pour organiser le programme : la facette de contrôle du composant parent, communique avec celle du composant fils. PAC est un modèle abstrait qui ne décrit pas sous quelle forme doivent être réalisées et connectées les différentes facettes. Diverses solutions ont été mises en œuvre par les programmeurs, en utilisant par exemple l'héritage ou des constructions à base de pointeurs. PAC est parfois confondu, par erreur, avec le patron modèle-vue-contrôleur (MVC) introduit en 1979 autour du langage Smalltalk. PAC est beaucoup plus orienté vers la séparation entre IHM et noyau fonctionnel, donc vers des objectifs de génie logiciel. MVC est plus focalisé sur l'organisation des entrées et des sorties dans les composants logiciels. Noter d'ailleurs que le contrôle de PAC est très différent du contrôle de MVC : le premier a pour rôle de piloter (« control ») la cohérence des facettes logicielles entre elles, le second gère les entrées de l'utilisateur et de ce fait lui permet de piloter (« control ») l'exécution du programme. Certaines interprétations récentes de MVC sont plus proches de PAC que des travaux d'origine sur MVC.
Un profil utilisateur est un ensemble de données qui influencent le comportement d'un dispositif informatique en fonction de l'utilisateur. Un profil peut être relatif à une personne seule, ou à un groupe de personnes ayant des points communs, tels que par exemple les membres d'un groupe de travail,.
En informatique, la programmation événementielle est un paradigme de programmation fondé sur les événements. Elle s'oppose à la programmation séquentielle. Le programme sera principalement défini par ses réactions aux différents événements qui peuvent se produire, c'est-à-dire des changements d'état de variable, par exemple l'incrémentation d'une liste, un mouvement de souris ou de clavier. La programmation événementielle peut également être définie comme une technique d'architecture logicielle où l'application a une boucle principale divisée en deux sections : la première section détecte les événements, la seconde les gère. Elle est particulièrement mise en œuvre dans le domaine des interfaces graphiques. À noter qu'il n'est pas ici question d'interruptions logicielles : le traitement d'un événement ne peut pas être interrompu par un autre, à part en des points précis explicitement prédéterminés du code logiciel (points qui, en fait, créent une seconde boucle événementielle au sein de la première). La programmation événementielle peut être réalisée dans n'importe quel langage de programmation, bien que la tâche soit plus aisée dans les langages de haut niveau (comme Java). Certains environnements de développement intégrés (par exemple Qt Software) permettent de générer automatiquement le code des tâches récurrentes dans la gestion des événements.
On qualifie de retour de force la deuxième phase d'une rétroaction, qui est le retour d'un effet sur le dispositif qui lui a donné naissance, et donc, ainsi, sur elle-même. C’est-à-dire que la valeur de sortie (à une date antérieure) fait partie des éléments de la commande du dispositif. Un retour de force est une interaction. L'étude du retour de force, comme l'action de résistance mécanique (à un solide, un liquide, un gaz) est un sujet fondamental de la mécanique en physique. La discipline qui étudie systématiquement les rétroactions est nommée automatique. On utilise couramment le terme de retour de force en économie, finance, modélisme et robotique. Les logiciels de temps réel et les langages de programmation orientée objet sont des technologies qui sont en train de faire progresser ces différents domaines. La théorie du chaos est une illustration extrême d'un retour de force disproportionné.
Salto est un logiciel de création d'applications interactives, développé par la société Alterface. Salto est un moteur d'interaction temps réel, il n'est absolument pas un outil de création de contenus. Il utilise des contenus existants (images, sons, films, scènes 3D) créés au moyen d'outils existants sur le marché (3D Studio Max, Adobe Photoshop, Adobe Premiere...). Salto est un moteur logiciel qui permet de relier des senseur et des actuateurs et ainsi réaliser des applications interactives telles des bornes ou des spectacles. D'un premier abord austère pour les non-développeurs, Salto est un outil permettant de mettre en œuvre rapidement des interactions sans devoir tout re-créer. [non neutre]
SensoMotoric Instruments (SMI) est une entreprise allemande dédiée aux applications de vision artificielle tout en mettant principalement l’accent sur la technologie de suivi de l’œil. SMI est fondée en 1991 comme une spin-off du département de recherche universitaire et médicale de l’Université Libre de Berlin. Le siège de la société se situe à Teltow, prés de Berlin en Allemagne. SMI possède des bureaux à Boston, au Massachusetts, et à San Francisco aux États-Unis, ainsi que des partenaires et des distributeurs à travers le monde entier. SMI propose des solutions de suivi de regard pour différents domaines, de la recherche scientifique aux applications professionnelles en passant par des solutions OEM. Les systèmes proposés par SMI peuvent être couplés avec un EEG, des systèmes de détection de mouvements du corps et/ou de tête, ou d’autres type de données biométriques. Ces systèmes de suivi de regard peuvent également être intégrés dans des systèmes de réalité virtuelle CAVE, des simulateurs, des voitures, des systèmes médicaux, ou utilisés avec des ordinateurs comme outils de mesure, ou même d’interaction homme-machine,,,.
Sketchpad (alias Robot Draftsman) est un programme informatique écrit par Ivan Sutherland en 1963 dans le cadre de sa thèse de doctorat au MIT ; thèse pour laquelle il a reçu un Turing Award en 1988 et le prix de Kyoto en 2012. Sketchpad est considéré comme le précurseur des logiciels de conception assistée par ordinateur (CAO) et a ouvert la voie aux interfaces homme-machine (IHM).
Une smartwatch, littéralement « montre intelligente », est une montre bracelet informatisée avec des fonctionnalités allant au-delà du simple affichage de l'heure et du chronométrage, présentant des caractéristiques comparables à celles d'un PDA. Il faut les considérer comme des ordinateurs de poignet. On utilise aussi le terme de montre connectée pour les montres disposant de connectivité sans-fil avec des technologies telles que le Bluetooth et le Wi-Fi. Alors que les premiers modèles, apparus dans les années 1980, étaient capables d'effectuer des tâches de base comme les calculs, l'agenda-planning puis les traductions ou les jeux, les smartwatches modernes sont devenues des ordinateurs à part entière. Certaines smartwatches peuvent exécuter des applications mobiles, fonctionnent sur un système d'exploitation mobile, ou encore proposent des fonctions de téléphonie mobile. Parmi les autres applications courantes, on trouve la lecture de fichiers audio ou vidéo, la réception radio FM, la connectivité Bluetooth, ou encore la mesure de l'activité physique du porteur. En 2014, tandis que la variété de ces produits sur le marché n'a jamais été aussi grande, selon une étude du bureau d'analyse Jackdaw : « le public ne s'intéresse pas aux montres connectées. Deux raisons à cela : les fonctionnalités proposées ne sont pas assez utiles et les appareils ne relèvent pas tous les défis technologiques qu'ils suscitent. » L'horlogerie suisse lance aussi ses smartwatches.
Le Special Interest Group on Computer-Human Interaction (SIGCHI) est le pôle d'intérêt commun de l'ACM dans le domaine de l'interface homme-machine. Ce pôle participe aux conférences MobileHCI organisées par le SIGMOBILE.
SpeechMagic est une plateforme à grande échelle permettant la capture d’informations sous format numérique. Elle a été développée par Philips Speech Recognition Systems à Vienne, Autriche. SpeechMagic dispose de nombreux vocabulaires de reconnaissance vocale et propose de nombreux services visant à une capture d’informations médicales « précise, conviviale et efficace ». Bien que la technologie soit surtout utilisée dans le secteur de la santé, les différentes applications sont également disponibles pour les secteurs juridique et fiscal. SpeechMagic prend en charge 25 langues de reconnaissance et fournit plus de 150 ConTexts (vocabulaires spécialisés). Plus de 8 000 établissements de soins de santé dans 45 pays utilisent SpeechMagic pour la capture d’informations et la création de documents professionnels. Le plus important site utilisant les applications powered by SpeechMagicTM se trouve aux États-Unis avec 60 000 auteurs, plus de 3 000 éditeurs et quelque 400 millions de lignes dictées par an. La société de conseil Frost & Sullivan a décerné à SpeechMagic le « Market Leadership Award in European Healthcare » en 2005 et le « Global Excellence Award » à Philips Speech Recognition Systems en 2007 pour s’être illustré en matière d’intégration et de stratégies, reconnaissant ainsi son excellent travail.
Sony Ericsson UI est une interface utilisateur développée par Sony Ericsson et reprise en 2012 par Sony pour les appareils mobiles de la gamme Xperia fonctionnant sous Android.
TouchWiz est une interface utilisateur pour appareils mobiles, appelée surcouche, développée par Samsung depuis 2009. Elle fonctionne en surcouche des systèmes d'exploitations mobiles Android, Bada, Windows Mobile et le système d'exploitation propriétaire de Samsung.
Dans les systèmes répartis, la transparence fait référence au fait d'adhérer à une interface externe, autant que possible, qui change alors un comportement interne. La transparence dans les systèmes répartis possède huit dimensions, qui ne sont pas indépendantes les unes avec les autres : Au plus bas niveau : Accès : L'accès à une composante du système réparti, se fait d'une façon unique et uniformisée pour tout le système ; Localisation : L'utilisateur ne connait pas la localisation d'une composante dans le système. Au niveau intermédiaire : Migration : Les composantes peuvent changer de place dans le système, sans que l'utilisateur n'en soit conscient ; Réplication : Une composante peut être répliquée plusieurs fois, mais l'utilisateur ne la voit comme une seule et unique composante ; Concurrence : L'utilisateur pense qu'il est seul dans le système, même si une composante est utilisée simultané par plusieurs utilisateurs. À un niveau plus haut : Extensibilité : L'utilisateur ne sait pas comment l'extensibilité est utilisée dans le système réparti ; Performance : L'utilisateur ne sait pas comment la performance est maintenue dans le système ; Défaillance : L'utilisateur ne sait pas comment le système traite les erreurs et assure une qualité de services.  Portail de l’informatique
Sherry Turkle, née le 18 juin 1948 à New York, est professeur d'études sociales en science et technologie au Massachusetts Institute of Technology.
L’utilisabilité, ou encore aptitude à l'utilisation est définie par la norme ISO 9241-11 comme « le degré selon lequel un produit peut être utilisé, par des utilisateurs identifiés, pour atteindre des buts définis avec efficacité, efficience et satisfaction, dans un contexte d’utilisation spécifié ». C'est une notion proche de celle d'affordance, ou même d’ergonomie qui est cependant plus large. Les critères de l’utilisabilité sont : l’efficacité : le produit permet à ses utilisateurs d’atteindre le résultat prévu. l’efficience : atteint le résultat avec un effort moindre ou requiert un temps minimal. la satisfaction : confort et évaluation subjective de l’interaction pour l’utilisateur.
En informatique, le terme utilisateur (anciennement un opérateur ou un informaticien) est employé pour désigner une personne qui utilise un système informatisé (ordinateur ou robot) mais qui n'est pas nécessairement informaticien (par opposition au programmeur par exemple). L'utilisateur peut aussi être une machine automatique (essentiellement représenté par un bot informatique) pouvant disposer de différents degrés d'autonomie.
En informatique, dans le cadre du développement d'un logiciel, l'utilisateur final est la personne qui va utiliser ledit logiciel.
La vallée dérangeante, ou vallée de l'étrange (de l'anglais uncanny valley) est une théorie scientifique du roboticien japonais Masahiro Mori, publiée pour la première fois en 1970, selon laquelle plus un robot androïde est similaire à un être humain, plus ses imperfections nous paraissent monstrueuses. Ainsi, beaucoup d'observateurs seront plus à l'aise en face d'un robot clairement artificiel que devant un robot doté d'une peau, de vêtements et d'un visage visant à le faire passer pour humain. Ce n'est qu'au-delà d'un certain degré de réalisme dans l'imitation, selon cette théorie, que les robots humanoïdes seront mieux acceptés. C'est pour cela qu'est utilisé le terme de vallée : il s'agit d'une zone à franchir dans laquelle chaque progrès fait vers l'imitation humaine amènera au départ plus de rejet, mais passé un certain seuil de réalisme une acceptation plus grande.
Fernanda Bertini Viégas (née en 1971) est une designer et scientifique brésilienne, dont le travail concerne surtout les aspects sociaux, collaboratifs et artistiques de la visualisation de l'information.
Une zone de liste (ou list box ou listbox) est un widget qui permet à l'utilisateur de sélectionner un ou plusieurs éléments d'une liste statique. L'utilisateur doit cliquer dans la liste sur un élément pour le sélectionner, parfois en combinaison avec des raccourcis claviers (touche Majuscule ou Control pour faire des sélections multiples). Cliquer sur un élément de liste déjà sélectionné le désélectionne. Une zone de liste est intitulée select ou select1 dans le standard XML XForms. Select est utilisé pour permettre à l'utilisateur de sélectionner plusieurs éléments de liste tandis que select1 ne permet à l'utilisateur de sélectionner qu'un seul élément de liste.
Internet est le réseau informatique mondial accessible au public. C'est un réseau de réseaux, à commutation de paquets, sans centre névralgique, composé de millions de réseaux aussi bien publics que privés, universitaires, commerciaux et gouvernementaux, eux-mêmes regroupés en réseaux autonomes (il y en avait 47 000 en 2014). L'information est transmise par Internet grâce à un ensemble standardisé de protocoles de transfert de données, qui permet l'élaboration d'applications et de services variés comme le courrier électronique, la messagerie instantanée, le pair-à-pair et le World Wide Web. L'internet ayant été popularisé par l'apparition du World Wide Web (WWW), les deux sont parfois confondus par le public non averti. Le World Wide Web n'est pourtant que l'une des applications d'internet. L'accès à internet peut être obtenu grâce à un fournisseur d'accès via divers moyens de communication électronique : soit filaire (réseau téléphonique commuté (bas débit), ADSL, fibre optique jusqu'au domicile), soit sans fil (WiMAX, par satellite, 3G+, 4G). Un utilisateur d'internet est désigné par le néologisme « internaute ».
1&1 Internet, fondée en 1988, est une société d’hébergement Web. 1&1 Internet fait partie du groupe coté en bourse United Internet AG. Elle revendique plus de 13 millions de clients à travers le monde et plus de 19 millions de noms de domaines enregistrés.
L'accès à Internet est une désignation des moyens mis à la disposition d'un particulier ou d'une petite entreprise (dans le cadre de cet article) d'accéder à l'internet. Les organismes commercialisant ces offres de service sont appelées fournisseur d'accès à Internet. L'accès à Internet a beaucoup varié depuis sa création dans les années 1970. Un fournisseur d'accès à Internet permet cet accès soit grâce à une connexion filaire quasiment jusqu'à l'utilisateur, pour un accès peu mobile (réseau téléphonique commuté (bas débit), ADSL, fibre optique jusqu'au domicile...), soit grâce à une connexion sans fil pour un accès mobile ( Wi-Fi, WiMAX, Internet par satellite) ou via un réseau de téléphonie mobile : GSM/EDGE, 3G/UMTS, 3G+ ou LTE, 4G ou encore 4G+.
Une adresse IP virtuelle (virtual IP address, VIP ou VIPA) est une adresse IP non connectée à un ordinateur ou une carte réseau (NIC) spécifiques. Les paquets entrants sont envoyés à l'adresse IP virtuelle, mais en réalité ils circulent tous via des interfaces réseau réelles. Les VIP sont surtout utilisées pour la redondance de connexion : si un ordinateur ou une carte réseau tombe en panne, un autre ordinateur ou une autre NIC peuvent alors reprendre la connexion.
L'agrégation web, aussi connue sous le terme francisé de « curation », est l'action de regrouper, sélectionner et, éventuellement, valider des pages concernant un sujet précis et de les présenter, mises en forme, dans un blog ou un outil dédié. Le mot curation semble un phénomène de mode exploité par un marketing efficace. Mais il demeure que l'Agrégation Web répond a des besoins forts des internautes : disposer de pages (URL) consacrées à un sujet et éclairées, éventuellement, par de réels spécialistes ; réduire l'investissement nécessaire à la publication construite sur le web ; commenter ou critiquer ces pages.
Algeria Web Awards, couramment abrégé en AWA, est une compétition annuelle dont le but est de récompenser le meilleur contenu algérien sur internet. Créée en 2012 à l'initiative de Faiçal Azouaou, enseignant de l'École nationale supérieure d'informatique (ESI), AWA est organisée par le Club scientifique de cette même école jusqu'en 2014. La compétition reprend en 2016 sous l'impulsion de l'agence digitale Creativinno, nouvel organisateur de l'évènement.
Animateur de communauté ou CM, l'abrégé de community manager, est un métier qui consiste à animer et à fédérer des communautés sur Internet pour le compte d'une société, d'une marque, d’une célébrité ou d’une institution. Profondément lié au web 2.0 et au développement des réseaux sociaux, le métier est aujourd'hui encore en évolution. Le cœur de la profession réside dans l'interaction et l'échange avec les internautes (animation, modération) ; mais le gestionnaire de communauté peut occuper des activités diverses selon les contextes. L'appellation community manager est intégrée au Larousse 2016, confirmant l'utilisation de cet anglicisme dans la langue française.
BabelNet, est un réseau sémantique multilingue et une ontologie lexicalisée. BabelNet a été créé en intégrant automatiquement la plus grande encyclopédie multilingue - c’est-à-dire Wikipédia - avec le lexique de la langue anglaise le plus connu - WordNet. L’intégration a été réalisée par correspondance automatique. Les entrées manquantes dans d'autres langues ont été obtenues par des techniques de traduction automatique. Le résultat est un “dictionnaire encyclopédique” qui fournit des entrées lexicalisées dans la plupart des langues, reliées entre elles par une grande quantité de relations sémantiques. Comme WordNet, BabelNet regroupe les mots de différentes langues par groupes de synonymes appelés Babel synsets. Pour chaque Babel synset, BabelNet fournit des définitions textuelles (appelées gloses) en plusieurs langues, obtenues à partir de WordNet et Wikipédia.
La bulle de filtres, ou bulle de filtrage, est un concept développé par le militant d'Internet Eli Pariser. Selon Eli Pariser, la « bulle de filtres » désigne à la fois le filtrage de l'information qui parvient à l'internaute par différents filtres ; et l'état d' « isolement intellectuel » et culturel dans lequel il se retrouve quand les informations qu'il recherche sur Internet résultent d'une personnalisation mise en place à son insu. Selon cette théorie médiatisée depuis les années 2000, à partir des différentes données collectées sur l'internaute (y compris à partir du big data), des algorithmes vont « discrètement » sélectionner les contenus qui lui seront visibles ; faisant que chaque internaute accéderait à une version différente du web, installé dans une « bulle » unique et optimisée pour sa personnalité supposée et in fine en partie construite par le système et le choix de ses "amis" et sources d'informations sur le net par chaque internaute. Eli Pariser estime que ce phénomène est devenu commun sur les réseaux sociaux et via les moteurs de recherche. Des sites tels que Google, Facebook ou Yahoo! n'affichent pas toutes les informations, mais seulement celles sélectionnées pour l'utilisateur, et de manière hiérarchisée selon ses prédispositions supposées (y compris idéologiques et politiques. À partir de différentes données (historique, clics, interactions sociales) ces sites prédisent ce qui sera le plus pertinent pour lui. Ils lui fournissent ensuite l'information la plus pertinente (y compris du point de vue commercial et publicitaire), en omettant celle qui l'est moins selon eux. Si les algorithmes considèrent qu'une information n'est pas pertinente pour un internaute, elle ne lui sera simplement pas présentée.
Une carte réseau est matérialisée par un ensemble de composants électroniques soudés sur un circuit imprimé. L'ensemble constitué par le circuit imprimé et les composants soudés s'appelle une carte électronique, d'où le nom de carte réseau. La carte réseau assure l'interface entre l'équipement ou la machine dans lequel elle est montée et connectés sur le même réseau. Aujourd'hui on peut trouver des cartes réseau un peu partout, dans les ordinateurs, imprimantes, téléphones portables, consoles de jeux, télévisions… On n'utilise le terme « carte réseau » que dans le cas d'une carte électronique autonome prévue pour remplir ce rôle d'interface réseau. Ainsi, un ordinateur muni d'une interface réseau assurée par des composants soudés sur sa carte mère ne comporte pas, à proprement parler, de carte réseau. Les équipements communiquent sur le réseau au moyen de signaux qui doivent absolument respecter des normes.  
Le certificat informatique et internet (C2i) est une certification française délivrée par les établissements supérieurs français certifiés LMD au sens du Processus de Bologne, tels les universités, les écoles spécialisées ou encore les centres d'enseignement à distance. Il est institué dans le but de développer, de certifier la possession de compétences dans l'usage des technologies de l’information et de la communication par les étudiants dans les établissements d’enseignement supérieur. Le C2i se décline en un niveau 1 généraliste destiné aux étudiants du cycle Licence, et des niveaux 2 spécialisés orientés "Métiers" destiné aux étudiants du cycle Master. Prenant la suite du brevet d'informatique et internet (B2i), une formation et certification dispensée au sein de l’enseignement secondaire, le C2i constitue l'étape finale d'un système de certification relative aux usages raisonnés du numérique (Informatique et Internet) en France. Il ne s'agit pas d'un enseignement d'informatique en tant que discipline, même si certains concepts informatiques sont traités dans le cadre du C2i.
Le C2i2mead (C2i niveau 2 métiers de l'environnement et de l'aménagement durables) est un certificat Informatique et Internet instauré par le ministère de l’enseignement supérieur français, destiné aux métiers de l'environnement et de l'aménagement durables. Il est l'un des Certificats Informatique et Internet (C2I), certification française délivrée par les établissements supérieurs français certifiés LMD au sens du Processus de Bologne, tels les universités, au cours de la licence et du master pour attester de compétences dans la maîtrise des outils informatiques et réseaux.
Cibox Corp. est une société internationale d'origine française, basée à Paris, dont le métier est le design, le développement et la construction de produits d'électronique grand public (Consumer Electronics) le plus souvent connectés et permettant un accès, par Internet, aux technologies et services du cloud. Initialement constructeur d'ordinateurs personnels comme Dell ou Apple, puis fournisseur d'accès à internet (FAI), Cibox s'est spécialisée dans le domaine des terminaux connectés à Internet (smartphones et tablettes tactiles) et de solutions tant filaires que sans fil de stockage externe.
Le Clearnet est un terme qui se réfère au non-darknet, non-Tor,. Ce world wide web traditionnel possède des base d'anonymat relativement faible, car la plupart des sites Web identifient systématiquement leurs utilisateurs par leur adresse IP. Le terme a été utilisé comme synonyme du moteur de recherche du web surfacique non-traditionnel, en raison du chevauchement historique où le darknet étaient une partie du web profond.
Creativity and Generosity in a Connected Age Cognitive Surplus: Creativity and Generosity in a Connected Age est un essai de Clay Shirky publié en 2010. Il est une suite indirecte de Here Comes Everybody, qui traitait de l'impact des médias sociaux sur la société. Il y aborde le concept de surplus cognitif, qui expliquerait l'émergence d'activités telles, entre autres, les sciences participatives.
Cindy Cohn est une avocate américaine spécialiste d'Internet et des droits civiques,.
Le crosspostage (ou encore publication croisée, X-postage, crosspost ou crossposting) est le fait d'envoyer un message simultanément à plusieurs forums, listes de diffusion, ou groupes Usenets. Autrement dit on charge le "facteur" (le système de diffusion des messages) de faire le tour des destinataires (les forums) et de leur délivrer à chacun le même message. Il ne faut pas confondre le crosspostage avec le multipostage, qui correspond à l'envoi de multiples messages identiques - et séparés - chacun à un seul forum, liste ou groupe. Le crosspostage peut être utile si le message peut intéresser plusieurs publics. Toutefois, crossposter à des groupes qui n'ont rien à voir avec le message posté pourrait être considéré comme du spam. En outre, un crosspostage étendu aux réponses du message initial est généralement mal considéré, car il multiplie le trafic sans ajouter de nouveau contenu. Dans le cas extrême, si toutes les réponses était ainsi crosspostées aux mêmes groupes, alors chaque groupe aurait la même apparence. Un crossposteur peut minimiser ce problème en précisant que toutes les réponses doivent être dirigées vers un seul groupe dit groupe de suivi (follow-up en anglais).
Le Computer Science Network (CSNET) est un réseau informatique créé en 1981 aux États-Unis. Son but était d'étendre l'accès au réseau ARPANET des établissements universitaires et de recherche qui ne pouvaient y être connectés directement. Il a joué un rôle important dans le développement d'Internet. CSNET a été financé par la National Science Foundation (NSF) pour une période initiale de trois ans de 1981 à 1984.
Cyber-base est un label français d'espace public numérique, géré par la Caisse des dépôts et consignations. La fin de ce dispositif national est annoncé pour le 31 décembre 2014. L'objet d'une cyber-base est d'initier, des débutants aux plus confirmés, aux technologies de l'information et de la communication (TIC), dans le cadre d'ateliers en groupe ou individuel : découverte de l'environnement de l'ordinateur, maîtrise d'internet, initiation des logiciels de bureautique, initiation à l'image, la vidéo et au son... Des créneaux horaires sont également réservés à l'accès libre. Il existe plus de 800 Cyber-bases en France (DOM TOM y compris). La première Cyber-base de France fut inaugurée rue d'Alsace-Lorraine à Vesoul, dans la Haute-Saône, le 24 juin 1999, à l'initiative du sénateur-maire de Vesoul, Alain Joyandet. Presque dix ans plus tard, le 28 mai 2009, la première en milieu carcéral fut inaugurée au Centre pénitentiaire de Bordeaux-Gradignan.
Une cybermanifestation (ou cyber-riposte), est une manifestation qui se déroule sur internet, généralement à l'initiative d’activistes ou de groupes de pression.
Le cyberterrorisme est un terme controversé. Certains auteurs choisissent une définition très étroite : relatif aux mutations, par des organisations terroristes connues, d'attaques contre la désorganisation des systèmes d'information dans le but principal de la création d'alarme et de panique. Selon cette définition limitée, il est difficile d'identifier toutes les instances du cyberterrorisme. Le cyberterrorisme peut aussi être défini beaucoup plus généralement, par exemple, comme "l'utilisation préméditée des activités perturbatrices, ou la menace de celle-ci, contre des ordinateurs et / ou réseaux, dans l'intention de causer un préjudice de nature sociale, idéologique, religieuse, politique, ou autres objectifs. Ou pour intimider toute personne dans la poursuite de tels objectifs". Cette définition large a été créée par Kevin G. Coleman, de l'Institut Technolytics. Le terme a été inventé par Barry C. Collin. Les autorités chinoises voient le cyberterrorisme au travers de la guerre informatisée.
Un débrideur est un service sur Internet permettant de contourner les limites de téléchargement des comptes gratuits des sites d'hébergement de fichiers tel que RapidShare, free, etc. Ils peuvent également permettre de contourner les limitations d'utilisation des sites de streaming. Le verbe débrider vient de verbe brider synonyme de limiter auquel on a ajouté le préfixe dé pour former son contraire. Le système de ces sites gratuits ou payants est de passer par des proxy afin de mettre à disposition des serveurs permettant d'accéder à des comptes premium. Cela s'avère très utile dans le téléchargement de gros fichiers. L'avantage de l'abonnement à un débrideur plutôt qu'aux services de l'hébergeur est que généralement les offres des débrideurs sont plus attractives et qu'elles englobent de nombreux hébergeurs .
La Déclaration d'indépendance du cyberespace est un document rédigé le 8 février 1996 à Davos en Suisse par John Perry Barlow, un des fondateurs de l'Electronic Frontier Foundation. Il soutient l'idée qu'aucun gouvernement (ou qu'aucune autre forme de pouvoir) ne peut s'imposer et s'approprier Internet, alors en pleine extension. Il a été écrit en partie en réponse à l'adoption de la Loi sur les télécommunications de 1996 aux États-Unis.
La dépendance à Internet (également nommée cyberdépendance, cyberaddiction, netaholisme, usage problématique d'Internet (UPI) ou trouble de dépendance à Internet (TDI)) désigne un trouble psychologique entraînant un besoin excessif et obsessionnel d'utiliser un ordinateur et interférant sur la vie quotidienne. La dépendance à Internet peut être considérée comme pathologique ; elle est par ailleurs diagnostiquée dans le manuel diagnostique et statistique des troubles mentaux (DSM-IV) en tant que modèle de description du TDI. Des activités en ligne, telles que l'achat en ligne, si faites en personne, peuvent être considérées comme problématiques si celles-ci sont compulsives. D'autres activités, comme les jeux vidéo, sont également considérées comme problématiques si celles-ci interfèrent sur la vie quotidienne. Plusieurs classifications ont été souvent divisées en sous-types vis-à-vis des activités comme notamment la dépendance à la pornographie, aux jeux en ligne, au réseautage social en ligne, aux courriels, ou aux achats sur Internet. L'expression anglophone (Internet addiction) est employée pour la première fois par la psychologue américaine Kimberly Young (en), lors d'un colloque de l’American Psychological Association (APA) à Toronto (Canada) en 1996. Otto Fenichel (mort en 1946) soulève le premier la question des « toxicomanies sans drogues ». Selon le docteur Ivan K. Goldberg, la dépendance à Internet est un symptôme et non un trouble à proprement parler. Il s'inspire de la dépendance aux jeux d'argent pour sa description de l'hypothétique « dépendance à Internet ». Pour Marc Valleur, Médecin-chef au Centre Marmottan à Paris, la cyber-addiction n'est pas à proprement parler une maladie en tant que telle, mais peut être analysée avec les mêmes approches que celles utilisées pour les dépendances aux drogues, à l'alcool ou au tabac.
La dépendance ou l'addiction au smartphone est un phénomène qui apparaît depuis la large diffusion des smartphones. Il relève, du moins en partie, de la cyberaddiction (dépendance à Internet) qui peut se développer, notamment dans le cadre du nomadisme numérique, ou révèle souvent d'autres addictions. Il peut ajouter à la dépendance au téléphone et à l'information disponible sur l'Internet une autre dépendance, à certains des réseaux sociaux qui se sont développés grâce à l'internet. Ce trouble est classé dans les pathologies communicationnelles ; troubles psychologiques entraînant chez le « mobinaute », un besoin excessif, incontrôlable voire obsessionnel d'utiliser un téléphone au point d'y consacrer tant de temps et d'énergie, que l'objet et son utilisation finissent par interférer négativement avec la vie quotidienne, professionnelle ou affective du sujet qui peut développer une anxiété, parfois phobique ou une dépression qui vont indirectement aussi affecter son entourage. Cette dépendance se résout parfois d'elle-même, et dans ce cas, à la différence des dépendances chimiques elle n’entraînerait pas ou peu de séquelles physiques et psychiques pour la santé, affirmation que seules des études épidémiologiques de long terme pourront confirmer. Cette nouvelle addiction semble tendre, pour partie au moins, à se substituer à l'addiction à la télévision. Elle touche davantage les jeunes ; selon une étude parue en 2013, 7 % des 50 millions de Sud-Coréens (dans l'un des pays les plus « câblés » au monde), présentent « un risque élevé » d'addiction à l'internet, mais ce taux triple en grimpant à près de 20 % chez les adolescents (génération née et ayant grandi avec l’internet), les étudiants de haut niveau ne sont pas les moins touchés,. 240 000 adolescents seraient susceptibles d'être touchés par ce phénomène en Corée rien qu'en 2013.
Digitalcourage – connu sous le nom de FoeBuD (Verein zur Förderung des öffentlichen bewegten und unbewegten Datenverkehrs) jusqu’en novembre 2012 – est une organisation allemande d’activisme pour la protection de la vie privée et les droits fondamentaux dans l’internet. Dans le but de préserver « un monde habitable dans l’âge digitalisé », Digitalcourage fait campagne pour les droits de l’homme et du citoyen, le droit de la consommation, la protection de la vie privée, la liberté de l’information et des sujets liés. La groupe maintient des liens avec des organisations comme le AK Vorratsdatenspeicherung (Groupe de travail contre la conservation des données) et le Chaos Computer Club. En plus, Digitalcourage est membre de l’organisation faîtière European Digital Rights (EDRi).
Dohop est un moteur de recherche de voyage. Dohop est né en 2004. En 2005, Dohop a lancé le premier planificateur de vol au monde pour les compagnies low-cost. Plus tard, il est étendu pour inclure tous les vols réguliers à travers le monde, avec plus de 660 compagnies aériennes. En 2006, l'entreprise remporte le Travelmole Web Awards dans la catégorie du meilleur site de technologie, et a été sélectionné parmi les 100 meilleurs sites de voyage par le Times magazine en 2007. Le moteur de recherche Dohop référence également plus de 200000 hôtels à travers le monde. Dohop est disponible en plusieurs langues. Plusieurs branches de la famille Dohollou (dont de France), sont actionnaires. C'est ce nom qui a inspiré le nom de la société Dohop.
Le domaining est l'activité qui consiste à étudier, rechercher, créer, acheter ou revendre des noms de domaine. Le mot vient de l'anglais par l'ajout du suffixe « ing » au terme domain, qui en anglais signifie communément nom de domaine. Une traduction française littérale donnerait « domainage ». Le domaining est une activité commerciale florissante dans le monde anglophone mais beaucoup moins en francophonie, où les entreprises et les individus semblent moins conscients de la valeur intrisèque d'un nom de domaine de qualité, via sa brandabilité, sa mémorabilité, son exclusivité ou ses potentialités. Le marché des adresses web francophones restent ainsi très limité comparé au marché anglophone ou même germanophone. De très nombreux blogs, journaux en ligne, forums ou place de marché se consacrent entièrement au domaining.
Le don en ligne, est un moyen de faire un don via Internet, typiquement par carte bancaire. Ce modèle est basé sur trois acteurs : l’association bénéficiaire du don, le donateur et une plateforme qui met en relation les deux acteurs précédents.
Le dual play est, dans l'industrie des télécommunications, une offre commerciale dans laquelle un opérateur propose à ses abonnés (à l'ADSL, au câble, ou plus récemment à la fibre optique) un ensemble de deux services dans le cadre d'un contrat unique : l'accès à l'Internet à haut voire très haut débit et la téléphonie fixe (de nos jours le plus souvent sous forme de voix sur IP). Il peut s'agir également des deux services suivants : l'accès à l'Internet à haut voire très haut débit et la télévision (par ADSL ou par câble) avec parfois des services de vidéo à la demande.
Le e-G8 est un forum participatif réunissant en France des leaders de l'Internet, programmé pour la veille du 37e sommet du G8.
En informatique de réseau, l'erreur 451 (« Unavailable For Legal Reasons ») est un code d'erreur du protocole de communication HTTP, renvoyé par un serveur HTTP pour indiquer que la ressource demandée (généralement une page web) est inaccessible pour des raisons d'ordre juridique,. Il s'agit donc d'une erreur similaire à l'erreur 403 (« Forbidden ») (avec laquelle elle partage le premier « 4 » indiquant l'indisponibilité de la ressource), apportant à celle-ci la précision du motif de l'indisponibilité. L'erreur pourrait servir à indiquer qu'une page est censurée sur un réseau par un gouvernement ou une décision de justice. Une éventuelle page d'erreur permettrait de donner des informations quant à la décision de justice rendant la page inaccessible légalement, décision pouvant toucher à la sécurité nationale, au droit d'auteur, à la vie privée, au blasphème… Cependant, afin de masquer la censure, certains gouvernements, notamment le Royaume-Uni, interdisent l'usage de ce code, en utilisant une obligation de silence. Proposé en 2012 peu après la mort de Ray Bradbury, ce code HTTP fait directement référence à son roman de science-fiction dystopique Fahrenheit 451, dont le thème est la violente censure des œuvres littéraires reposant sur l'autodafé. Le nombre 451 fait référence à la température en degrés Fahrenheit à laquelle le papier s'enflamme sans besoin d'une flamme extérieure. Le code HTTP 451 est approuvée par l'IESG le 18 décembre 2015, mais était déjà utilisé avant son approbation officielle. Son utilisation, par les différents acteurs de l'internet, est facultative. GitHub a annoncé le 17 mars 2016 son implémentation sur son site pour les ressources inaccessibles à cause d'une requête DMCA. Mark Nottingham, président du groupe de travail HTTP de l'IETF, indique : « Je pense que des gouvernements restrictifs vont refuser l'utilisation du code 451 pour cacher ce qu'ils font. Nous ne pouvons pas stopper ça, mais si des gouvernements vont dans ce sens, ils enverront un signal fort à leurs citoyens sur leur intention. »
FidoNet est un réseau d'échange de messages à l'échelle mondiale qui a pris source aux États-Unis.
FileHippo est un site de téléchargement qui propose des logiciels informatiques pour Windows. Les logiciels présents sur le site internet sont regroupées par catégories : freeware, shareware et open-source. FileHippo App Manager est un programme gratuit qui recherche les logiciels obsolètes sur l'ordinateur de l'utilisateur et offre des liens vers des versions plus récentes,,. FileHippo n'accepte pas les soumissions de logiciel de de la part des éditeurs. FileHippo a été estimée à plus de 13,000,000 US $ en novembre 2015.
Un Flux de clics (« Clickstream ») est l'enregistrement de ce sur quoi clique un utilisateur lorsqu'il navigue sur Internet ou lorsqu'il utilise un autre outil informatique. Lorsqu'il clique n'importe où sur une page Web ou sur un formulaire d'application, l'action est enregistrée sur le poste client ou sur un serveur Web, avec d'autres informations telles que le navigateur Internet utilisé, les routeurs, les serveurs de proxy, etc. L'analyse des flux de clics est utile pour l'analyse de l'activité Internet,, les tests de logiciels, la recherche de marché et pour l'analyse de la productivité des employés.
La fracture numérique générationnelle ou plutôt culturelle est un clivage fort dans la société sur les usages d'Internet. Cette fracture est due au fait que la nouvelle génération est née avec Internet. Ils sont appelés "natif numérique" (ou "digital native" en anglais), tandis que l'ancienne génération est appelée immigrant du numérique. La fracture numérique générationnelle peut être aussi appelée l'autre fracture numérique ou la nouvelle fracture numérique par opposition à la fracture numérique géographique et sociale qui concernait l'accès à Internet et non ses usages.  
La fracture numérique est la disparité d'accès aux technologies informatiques, notamment Internet. Elle recouvre parfois le clivage entre « les info-émetteurs et les info-récepteurs ». Cette disparité est fortement marquée d'une part entre les pays riches et les pays pauvres, d'autre part entre les zones urbaines denses et les zones rurales. Elle existe également à l'intérieur des zones moyennement denses. Cette notion est calquée sur celle de fracture sociale. Ceux qui sont « du bon côté » disposent en principe d’un accès à internet pour défendre leurs droits et leurs idées, pour s'informer, pour communiquer et même pour augmenter leur pouvoir d'achat (comparateurs de prix, sites de ventes privées à tarifs préférentiels, bons plans, etc.). Les autres connaissent un désavantage supplémentaire à ceux qu'ils subissaient déjà : ils se retrouvent exclus d'une nouvelle dimension de la société, qui leur échappe chaque jour un peu plus. Une cartographie des laissés pour compte de l'internet a été publiée en Septembre 2016 dans la revue Science. Elle confirme que la pauvreté et/ou l'éloignement limitent l'accès en ligne, et démontre aussi (ce qui est nouveau) que le fait d'appartenir à un groupe politiquement marginalisés (minorités ethniques et religieuses) peut se traduire par un moindre accès. Ceci est dû au fait que ce sont les gouvernements des pays qui organisent et construisent l'infrastructure qui relie les citoyens à l'Internet. Certaines minorités sont ainsi systématiquement exclues de l'Internet mondial.
La fuite des photos de personnalités d'août 2014 (également connue sous fappening) est une diffusion de photos intimes volées sur des terminaux mobiles ou comptes de stockage en ligne. Certaines victimes témoignent de l'apparition de photos effacées depuis plusieurs années. Ce délit pourrait être qualifié, au pénal, de crime à caractère sexuel, ce qui justifie une enquête du FBI des États-Unis.
GovDex est une initiative du gouvernement de l'Australie qui promeut des méthodes et des outils destinés à développer le e-Gouvernement.
Le Guidelines International Network (G-I-N) est un réseau international d'organisations et d'individus évoluant dans le domaine des recommandations pour la pratique clinique (développement et mise en œuvre). Fondée en novembre 2002, G-I-N est une association sans but lucratif (Scottish Guarantee Company SC243691 et Scottish Charity SC034047). En février 2011, 93 organisations et 77 individus sont membres de l'organisation. Ils représentent environ 45 pays. L'objectif du réseau est d'améliorer la qualité des soins de santé en favorisant le développement et la mise en œuvre systématique des recommandations pour la pratique clinique par le biais de la collaboration internationale,. Avec plus de 7000 références (février 2011) la G-I-N International Guideline Library est la plus large bibliothèque de recommandations pour la pratique clinique. Les documents qui y sont contenus émanent des membres du réseau et sont mis à jour régulièrement.
La Guinéenne de Large Bande (GUILAB) est le premier opérateur d’infrastructures de télécommunications de la Guinée et l’unique passerelle des communications internationales du pays.
L'Habeas Corpus Numérique est un concept désignant les normes qui ont vocation à concilier les nouvelles technologies avec le respect des libertés individuelles. À la suite des polémiques concernant les règles de confidentialité de Facebook et Google, la Ministre chargée de l’Économie Numérique, Fleur Pellerin, a annoncé son souhait de déposer un projet de loi encadrant la protection des données personnelles.
HETIC (Hautes études des technologies de l'information et de la communication) est un établissement privé d'enseignement supérieur spécialisé dans l'internet et l'infographie tridimensionnelle, situé à Montreuil, en France. Elle a été fondée en 2002. HETIC fait partie du groupe européen d’enseignement supérieur Studialis.
The Power of Organizing Without Organizations Here Comes Everybody: The Power of Organizing Without Organizations est un livre de Clay Shirky publié aux éditions Penguin Press en 2008. L'auteur analyse l'effet d'Internet sur la dynamique des groupes, en prenant pour exemple les sites internet Wikipédia et MySpace. C'est son sixième ouvrage. Le titre fait allusion à HCE, un personnage de James Joyce. L'auteur dit que son livre décrit « ce qui arrive lorsque l'on donne aux gens des outils permettant de travailler ensemble, sans avoir besoin d'une structure organisationnelle classique. »
Depuis quelques années, le piratage de fichiers vidéo sur Internet gagne en popularité. Face à la montée de ce phénomène de diffusion, Mark Pesce – professeur et spécialiste de la réalité virtuelle - propose dans sa conférence intitulée « Piracy is Good ? », une façon de tirer un avantage économique en ce qui a trait à la diffusion libre de fichiers vidéo.
Un hypertexte est un document ou un ensemble de documents contenant des unités d'information liées entre elles par des hyperliens. Ce système permet à l'utilisateur d'aller directement à l'unité qui l'intéresse, à son gré, d'une façon non linéaire. Le terme, créé en 1965 par Ted Nelson, désigne, dans les premières années, un champ de recherche d'orientation littéraire fonctionnant sur un système fermé. En parallèle, les grandes industries commencent à définir un langage de balisage standardisé (GML / SGML) afin de gérer leurs documents sur un ordinateur central. C'est cette technologie que retiendra Tim Berners-Lee pour relier entre elles les ressources de l'Internet dans le World Wide Web. Étymologiquement, le préfixe « hyper » suivi de la base « texte » renvoie au dépassement des contraintes de la linéarité du texte écrit. Lorsque les unités d'information ne sont pas uniquement textuelles, mais aussi audiovisuelles, on peut parler de système et de documents hypermédias. Le terme « hypertexte » est polysémique. Dans les années 1990, lorsque ce terme était peu répandu dans son sens informatique, les dictionnaires lui donnaient le sens du néologisme créé par Gérard Genette pour la théorie littéraire : « texte littéraire dérivé par rapport à un autre qui lui est antérieur et lui sert de modèle ou de source, d'où des phénomènes de réécriture possibles comme le pastiche ou la parodie ». Pour représenter cette idée de liens entre les textes, Genette cite «Pierre Ménard, auteur de Don Quichotte» de Borges, ou l'Ulysse de Joyce. Certains critiques, particulièrement dans le milieu anglo-saxon, n'hésitent pas à compter Borges comme un des précurseurs de l'hypertextualité.
IMIX ou Internet MIX est un ensemble de profils de trafic réseau correspondants à une navigation courante sur Internet. Un profil particulier (appelé 'simple IMIX') est notamment utilisé pour mesurer les performances des pare-feu dans des conditions proches des conditions réelles. En effet, le protocole IP utilise des trames pour communiquer. La longueur de ces trames n'est pas constante (comme c'est le cas en ATM) et peut varier de 40 à 1 500 octets. La performance de traitement de flux dépend donc de la capacité à traiter aussi rapidement que possible le contenu de ces trames.
L'indice de développement des technologies de l'information et de la communication (ICT development index, IDI) est un indice synthétique publié par l'Union internationale des télécommunications des Nations unies sur la base d'indicateurs convenus au niveau international. Cela en fait un outil précieux pour comparer les indicateurs les plus importants pour « mesurer » la société de l'information. L'IDI est un outil standard que les gouvernements, les opérateurs, les agences de développement, les chercheurs et d'autres peuvent utiliser pour mesurer la fracture numérique et comparer les performances en matière de TIC des divers pays. L'indice de développement des TIC repose sur onze indicateurs, regroupés en trois sections : accès, utilisation et compétences.
L'infolisme est une forme de cyberdépendance caractérisée par le besoin obsessionnel de recherche de l'information sur Internet. Décrit dès 2010 par Dan Véléa et Michel Hautefeuille, psychiatres addictologues, dans leur livre Les addictions à Internet,, cette dépendance se traduit par une quête permanente d'informations, soit dans un domaine particulier, soit dans tous les domaines.
L'Interface Message Processor (IMP) était le nœud de commutation de paquets utilisé pour connecter les ordinateurs à l'ARPANET à la fin des années 1960 et pendant les années 1970. C'était la première génération de ce qui s'appelle aujourd'hui routeur,,. Les IMP étaient au cœur d'ARPANET pendant 20 ans, jusqu'en 1989.
L'expression internet citoyen est apparue en France en 1995 à l'occasion d'un appel au G7 lancé par un collectif de sociologues et de philosophes sous le nom de VECAM (Veille européenne et citoyenne sur les autoroutes de l’information et le multimédia).
Cet article présente l'utilisation du réseau mondial Internet dans le monde.
La situation d'Internet en Afrique est marquée par un important retard de développement, et un accès à un réseau lent. Malgré cette situation difficile, le déploiement de réseau progresse et en 2015 29 % des africains sont internautes.
Les Internet Experiment Notes (IENs) étaient une série de notes techniques publiées au début de la création du réseau Internet (ensemble des réseaux qui utilisent la suite des protocoles TCP/IP) alors que les RFC étaient, à l'origine, dévolues au réseau informatique ARPANET développé aux États-Unis par la DARPA.
L'Internet interplanétaire est un projet de protocole de communication développé pour fonctionner dans l'espace sur des distances interplanétaires, là où un protocole traditionnel tel qu'Internet ne peut pas fonctionner. Cette technologie est basée sur le protocole IPN (InterPlaNet), développé par Vint Cerf et la NASA depuis 1998, dont le but premier était d'établir des liaisons entre la Terre et Mars. Les télécommunications entre les stations situées sur la Terre et les engins spatiaux lancés à la découverte d'autres planètes ne s'effectuent pas dans les conditions des télécommunications terrestres. Alors que l'Internet dispose de câbles sous-marin comme structure principale, l'Internet interplanétaire repose entièrement sur les ondes radio, avec des temps de latence beaucoup plus importants. Les délais de communication peuvent être très longs (jusqu'à 22 minutes pour Mars), les télécommunications peuvent être interrompues par le mouvement respectif des planètes (rotation) et des engins spatiaux sur leur orbite, et les débits sont faibles (planètes externes). Compte tenu de ce cadre de fonctionnement les hypothèses sur lesquelles reposent les protocoles terrestres ne s'appliquent pas : présence permanente d'une route entre émetteur et récepteur, lien bidirectionnel, délai aller-retour faible, pertes faibles, mécanismes de répétition intégrés au protocole suffisants... Les télécommunications spatiales ne bénéficient pas actuellement des avancées récentes des télécommunications terrestres. Le projet d'Internet interplanétaire vise à transposer celles-ci pour permettre leur application aux télécommunications spatiales. Ce projet est notamment porté par le centre Jet Propulsion Laboratory de la NASA. Des expérimentations sont en cours avec des engins placés en orbite terrestre.
L'Internet Watch Foundation (IWF ; en français fondation pour la surveillance d'Internet) est, au Royaume-Uni, une organisation qui se veut indépendante et qui cherche à lutter contre les contenus illégaux sur Internet. L'IWF travaille en collaboration avec les services de police et les fournisseurs d'accès à Internet, à qui elle transmet une liste noire (blacklist) de contenus potentiellement illégaux, qui est ensuite utilisée par de nombreux FAI anglais pour en censurer l'accès à leurs clients. Originellement créée pour lutter contre la pornographie enfantine, l'IWF a étendu ses actions à la lutte contre les contenus racistes et criminels sur Internet.
Le 5 décembre 2008, l’Internet Watch Foundation — une organisation britannique de veille citoyenne — met sur liste noire l’article de la Wikipédia en anglais concernant l’album Virgin Killer des Scorpions, en raison de la présence dans celui-ci de sa couverture controversée, représentant une jeune fille posant nue, un effet de verre brisé cachant son sexe. L’image a été considérée comme « un contenu potentiellement illégal » aux termes de la loi britannique, qui prohibe la possession ou la création de photographies indécentes d’enfants. La liste noire de l’IWF est utilisée par des systèmes de filtrage comme Cleanfeed, systèmes dont l’utilisation est imposée aux fournisseurs d'accès à Internet britanniques par la loi. L’URL de la page de description, qui présente l’image, a aussi été mise dans la liste noire ; cependant, tant la miniature que l’image elle-même restent accessibles. La couverture avait déjà été l’objet de controverses au moment de sa sortie, et avait été remplacée sur certains marchés par une couverture alternative représentant une photographie des membres du groupe. L’IWF décrit l’image comme « une image potentiellement illégale représentant de manière indécente un enfant de moins de 18 ans »,. La politique de Wikipédia veut que l’on ne censure pas les contenus « que certains lecteurs trouvent inconvenantes ou choquantes, même extrêmement », et donc ne retire que les contenus non pertinents, vandales ou illégaux aux termes de la loi de l’État américain de Floride, où ses serveurs sont situés,. Wikipédia retire aussi les contenus qui violent ses règles à propos des biographies de personnes vivantes, ou qui violent d’autres de ses règles. En plus des conséquences directes de la censure — qui peut cependant être contournée — de l’article et de l’image pour les lecteurs britannique de la Wikipédia en anglais au travers des FAI touchés, et quand bien même cette image est facilement accessible sur d’autres sites majeurs dont Amazon.co.uk (duquel elle a été retirée ultérieurement) et en vente libre dans le pays, l’action a eu plusieurs conséquences pour Wikipédia, notamment le fait d’empêcher temporairement tous les éditeurs anonymes utilisant ces FAI de contribuer à quelque page que ce soit de l’encyclopédie. Ceci a été décrit par l’IWF comme un « dommage collatéral » involontaire. Ceci est dû au faible nombre de proxies utilisés pour accéder à Wikipédia, étant donné que Wikipédia a une politique de blocage des contributeurs qui vandalisent l’encyclopédie. Ainsi, tout vandalisme venant d’un de ces FAI sera effectué au travers d’un de ces proxies et tous les autres utilisateurs du même FAI seront eux aussi empêchés d’éditer Wikipédia. Après avoir lancé sa procédure d’appel et analysé la situation, l’IWF a annulé sa mise sur liste noire de la page le 9 décembre 2008,, et a annoncé qu’elle ne mettrait plus sur liste noire d’autres copies de l’image hébergées hors du Royaume-Uni.
Internet2 ou University Corporation for Advanced Internet Development, abrégé sous le sigle UCAID, est un consortium à but non lucratif conduit par plus de 200 universités des États-Unis et par des partenaires commerciaux issus du domaine des réseaux et de l'informatique, afin de développer les technologies permettant de faire atteindre de très hauts débits au réseau Internet. Internet2 est une marque déposée,,.
Une ligne dédiée (on dit aussi parfois connexion dédiée) est un contrat de service liant un opérateur de télécommunication à son client et par lequel il s'engage à fournir un lien de télécommunication symétrique entre deux sites distants.
Le marketing interactif est une notion de marketing qui peut être définie comme une « déclinaison du marketing qui consiste à ne plus considérer le consommateur comme un élément passif, mais bien comme un partenaire potentiel avec lequel il est possible d’interagir » (d'après Jean-Marc Lehu).
Les Maroc Web Awards (MWA) sont une compétition annuelle qui récompense, grâce aux votes d'internautes et d'un jury, le meilleur du web marocain ou lié au Maroc, amateur ou professionnel, et se clôture par une cérémonie de remise de prix par catégorie, portant sur le web de l'année précédente. Cette compétition s'inscrit dans la continuité des Maroc Blog Awards (MBA), qui honoraient essentiellement le meilleur de la blogosphère et ont connu quatre éditions. Après un lancement en décembre 2007, la première cérémonie des MBA s'était tenue en février 2008. L'événement est organisé par Synergie Media.
Le média-planneur élabore un plan média en vue de prévoir et de coordonner les différents passages d'une campagne publicitaire dans les grands médias traditionnels et le web. En fonction du public-cible, il détermine les stratégies optimales et les moins coûteuses afin d'atteindre au mieux les cibles de la campagne ou d'améliorer l'image médiatique d'un produit ou de l'entreprise. En relation permanente avec d'autres secteurs publicitaires (annonceurs, agences de vente d'espaces, etc.), il est véritablement le lien entre les annonceurs et la publicité. Cependant, le média-planneur se voit de plus en plus supplanté par l'informatique avec le développement du Big Data.
La notion de médias, images et technologies de l'information et de la communication, ou MITIC, créée en Suisse à Genève par le service chargé du domaine concerné au Département de l'instruction publique, tend à élargir le concept de TIC (Technologies de l'information et de la communication) pour, d’une part, prendre en compte les convergences que le numérique a amené entre le domaine de l’informatique et celui des médias (presse, radio, télévision, édition multimédia, web), aussi bien que la banalisation d’internet comme environnement de travail quotidien et, d’autre part, dans le champ scolaire, rapprocher les domaines de l’informatique et de la critique de l’information (éducation aux médias) dont le développement s’est fait pendant longtemps de manière indépendante. Le domaine des MITIC est l’une des cinq thématiques transversales de la formation générale intégrée dans le Plan d’étude romand (PER) de la scolarité obligatoire qui a été introduit à partir de 2011 dans les cantons de Suisse romande. Les trois principaux objectifs d’apprentissage sont les suivants: Exercer un regard sélectif et critique (cycle 1), décoder la mise en scène de divers types de messages (cycle 2) et exercer des lectures multiples dans la consommation et la production de médias et d’informations (cycle 3).
Le Metablog est un concept d'internet fondé sur le principe du Blog et l'ouvrant aux perspectives du Web 3.0 via la mise en place de plusieurs environnements participatifs permettant le travail collaboratif en mode simultané, et l'utilisation des réseaux sociaux agrégatifs de savoir, tels que les entend par exemple James Surowiecki. Proche du principe d'hypertexte philosophique, le Metablog permet de coupler l'utilisation virtuelle des informations sur internet à l'action opérationnelle concrète des organisations et individus, dans un contexte interactif. Il facilite la mise en place de systèmes d'informations stratégiques intelligents, à haute valeur sémantique ajoutée. Il constitue également un outil d'étude et d'expérimentation pour les sciences cognitives et l'intelligence artificielle (Avec cette réserve que cette étude se limite à la définition purement mécanique des facultés intellectuelles, telles que les ont par exemple décrites René Descartes, Blaise Pascal ou, dans une moindre mesure, Cajetan - or, le Web 3.0 place par définition l'internaute au cœur des processus metacognitifs). On peut schématiser cette révolution paradigmatique en disant que dans les périodes précédentes (Web0.0 à Web2.0) l'internaute s'est approprié l'information - désormais il est l'information, jusque dans ses aspects métaphysiques. Le principe du MetaBlog a été introduit en 2008 via le projet Emergenc'y porté par l'association Développement Sans Frontières. Il correspond au nom donné à l'espace communautaire du site internet de cette association de solidarité internationale et permet la gestion participative des projets, la communication et la création d'un réseau social convivial autour des initiatives solidaires, basées sur les valeurs de citoyenneté responsabilisée, des Droits de l'homme et du partage. Malgré les difficultés à circonscrire encore ce que sera le Web 3.0 - l'absence d'une définition rigoureuse entrainerait la conceptualisation prématurée d'un hypothétique Web 4.0 - le MetaBlog propose une sérieuse piste d'optimisation conceptuelle des applications sur internet. Il emprunte aux domaines de l'Intelligence collective sur Internet (définie par le philosophe canadien Pierre Lévy comme «le projet d'une intelligence variée, partout distribué ; sans cesse valorisée, coordonnée et mise en synergie en temps réel ; et qui aboutit à une mobilisation effective des connaissances») et de la bio-inspiration. Il intègre le principe d'émergence auto-organisée des internautes via des applications individuelles comme collectives, il permet l'intégration d'ontologies contextuelles au sein même des connaissances partagées, et s'adapte facilement aux technologies du Web sémantique basées sur l’interopérabilité des architectures. Le metablog peut être une réponse à ce que dénonçait Tardy dès 1984 : « la taxonomie l'emporte sur l'arthrologie », à savoir que les principes sont nommés et catégorisés, au détriment de l'étude de leur articulation. À cet égard, le métablog est une application concrète du remplacement du paradigme nomothétique par une épistémologie herméneutique, que Marc Weisser appelle de ses vœux. Un aperçu a été donné lors des récentes élections américaines comme iraniennes. Avec les premières préapplications "metablogish" c'est bien du rapport entre Internet et la démocratie qu'il s'agit : la révolte citoyenne des Iraniens en 2009 répondant à la fièvre mobilisatrice des Américains en 2008, pour montrer que pouvoir de construction et pouvoir d'opposition sont les deux faces d'une même pièce. Par le regard mutuel qu'il implique, le MetaBlog affecte aussi la sécurité inter-relationnelle, ce qu'ont montré de récents travaux de psychologie clinicienne, autour du thème de l'achat sur portail.
Le meurtre sur Internet, ou cybermeurtre, désigne un meurtre dans lequel la victime et son assassin ont été impliqués par des communications sur Internet avant de se rencontrer dans la vie active,,. Le premier meurtre connu d'une victime rencontrée en ligne date de 1996.
Un modem affilié (en anglais tethered modem) est un équipement qui donne à un autre appareil l'accès à Internet. Généralement portatif (mobile), ce dispositif agit comme un modem et connecte un ordinateur à Internet au moyen d'une communication par câble (par exemple USB) ou sans fil (en bluetooth ou Wi-Fi). Le plus souvent, le modem attaché est constitué d'un logiciel utilisé sur un téléphone mobile qui partage son accès mobile à Internet avec un ordinateur portable. Le partage de la connexion Internet avec un modem attaché est communément appelée « tethering » en anglais.
Le multipostage (ou encore publication multiple, multipost ou multiposting) est le fait d'envoyer plusieurs messages identiques, chacun à un forum, une liste de diffusion, ou un groupe Usenet. Si on considère l'analogie du courrier postal, le multipostage consiste donc à copier plusieurs fois le même message et à envoyer chaque copie dans une enveloppe séparée aux différents destinataires que sont les forums. Le multipostage est absolument à éviter, car il multiplie les stockages et transmissions. Les administrateurs Usenet lui préférent le crosspostage avec suivi (ou follow-up), sachant qu'un crosspostage seul - sans suivi adéquat - est lui aussi à éviter.
Netocratie (Netocracy en anglais) est un terme inventé par le comité de rédaction du magazine américain Wired au début des années 1990. Mot-valise entre Internet et aristocratie, qui fait référence à une perception globale de classe supérieure qui fonde son pouvoir sur un fort avantage technologique et la mise en réseau des compétences, par rapport à ce qui est dépeint comme une bourgeoisie progressivement sur le déclin. Un mot plus approprié serait retocracy (du latin rete, net) ou dictycracy (du grec dikty, réseau). Le concept a par la suite été repris par les philosophes suédois Alexander Bard et Jan Söderqvist pour leur livre Les Netocrates publié originellement en suédois en 2000 sous le titre Nätokraterna puis publié en anglais en 2002 par Reuters/Pearsall sous le titre The New Power Elite et Life After Capitalism. Le concept de netocratie a été comparé au concept de Richard Florida de « classe créative ». Bard et Söderqvist ont également défini une sous-classe en opposition à la netocracy, qu'ils appellent le « consumtariat ».
Das Netz (La Toile ou Voyage en cybernétique) est un film indépendant allemand réalisé par Lutz Dammbeck (de) et sous-titré Unabomber, le LSD et l'Internet, sorti en 2005. Le film explore les idées et l'histoire des divers personnalités et mouvements ayant créé internet, et la société technologique actuelle : les artistes révolutionnaires Marshall McLuhan et Nam June Paik les hippies idéalistes comme Timothy Leary et Ken Kesey les artistes de la contre-culture tels que John Brockman (en) et Stewart Brand et les cybernéticiens et systémiciens tels que Robert Taylor et Heinz von Foerster. En toile de fond surgissent les idées du néo-luddite Theodore Kaczynski (alias Unabomber), dans un échange par lettres avec Lutz Dammbeck.
Le partage généralisé du processeur ou PGP (en anglais : Generalized processor sharing, abrégé GPS) est un algorithme d'ordonnancement idéal pour les ordonnanceurs de paquets. Il est en rapport avec le principe de file d'attente équitable, qui consiste à grouper les paquets dans des classes, et à partager les capacités du serveur entre elles. Le PGP distribue cette capacité en fonction de certains coefficients de pondération fixés. En ordonnancement, le partage généralisé du processeur est « un algorithme  d'ordonnancement idéalisé qui permet d'obtenir une équité parfaite. Tous les ordonnanceurs, en pratique, approximent un PGP et l'utilise comme référence pour mesurer l'équité. » Le partage généralisé du processeur suppose que le trafic est fluide (que la taille des paquets est infinitésimale) et donc peut être arbitrairement divisé. Il existe plusieurs techniques qui permettent de quasiment atteindre les performances du PGP, comme la théorie des files d'attente pondérées équitables (weighted fair queueing, WFQ), également connu comme le partage généralisé paquet par paquet du processeur (packet-by-packet generalized processor sharing, PGPS).
Le Parvis des gentils (italien : Cortile dei gentili) est une structure créée par le Vatican en 2011 afin d'encourager le dialogue entre les croyants et les non croyants. En fait, il s'agit de plusieurs événements qui se tiennent dans différentes villes. Il s'agit également d'un site Internet où sont notamment diffusés en direct ces événements.
Une pétition en ligne est une pétition qui utilise le canal de l'internet. Les pétitions en ligne sont apparues dans les années 2000. Les pétitions en ligne sont souvent utilisées pour faire pression sur les décideurs dans des domaines comme les droits de l'homme ou l'environnement. Il existe de nombreuses organisations qui sont spécialisées dans les pétitions en ligne, comme Avaaz.org, SumOfUs, Change.org, etc. Les pétitions en ligne sont parfois associées à d'autres techniques de mobilisation de masse pour faire pression sur les décideurs, comme on l'observe par exemple dans le cadre des marches mondiales pour le climat initiées par Avaaz.org qui rassemblent des centaines de milliers de personnes dans de nombreuses villes du monde entier.
La peur d’Internet est une technophobie liée aux techniques informatiques en réseau.
Le Plan Ordinateur Portable (POP) est mis en place par la Région de La Réunion en 2010. À destination des lycéens réunionnais pour lutter contre la fracture numérique.
Project Brillo (ou Projet Brillo) est un nom de code pour une version avant-première d'un Système d'exploitation embarqué basé sur Android fait par Google, annoncé lors de la Google I/O de 2015. Ce projet a pour but d’être utilisé sur les appareils de l'Internet des Objets, c'est pour cela qu'il a la contrainte d'utiliser le moins de mémoire possible et d’être peu coûteux en ressources énergétiques. Il supporte le Bluetooth low energy et le Wi-Fi. Parallèlement à cela, Google a annoncé le protocole Weave, qui permettra aux dispositifs Brillo de pouvoir communiquer entre eux et certainement avec d'autres appareils.
Le quadruple play est, dans l'industrie des télécommunications, une offre commerciale dans laquelle un opérateur propose à ses abonnés (à l'ADSL, au câble, ou plus récemment à la fibre optique) un ensemble de quatre services dans le cadre d'un contrat unique : • l'accès à l'Internet à haut voire très haut débit ; • la téléphonie fixe (de nos jours le plus souvent sous forme de voix sur IP) ; • la télévision (télévision par ADSL ou télévision par câble) avec parfois des services de vidéo à la demande ; • la téléphonie mobile (de nos jours le plus souvent sous forme de voix, mais aussi l'accès à Internet). Ce service est fourni au moyen de set-top box spécifiques, les box via la ligne filaire téléphonique, et les liaisons hertziennes (2G : GSM, GPRS, EDGE ; 3G : UMTS ; 4G : LTE, etc.) ou de proximité (Wifi, Wimax, ...) pour la téléphonie mobile.
Le « real -time web » (rtw) est un ensemble de technologies et de pratiques (un paradigme) dans lequel l'utilisateur reçoit des informations au moment où elles sont publiées, il se distingue donc des paradigmes dans lesquels c'est l'usager ou l'une de ses applications qui va vérifier régulièrement les mises à jour d'une source basée sur le web. Les informations transmises sont souvent de courts messages, des mises à jour de statut, des messages de news de type alertes, et des liens vers un document plus volumineux. Cela concerne le plus souvent des contenus dits « softs » (commentaires, réflexions personnelles, coups de cœur) par opposition aux contenus « hard » (faits bruts, nouvelles émanant de sources traditionnelles et/ou validées.)On pourrait adapter au rtw le célèbre aphorisme de Sacha Guitry ([1]: sur le rtw des gens communiquent, sur le web traditionnel ils se sont exprimés." Le flux d'informations défilant sur la "time line" de Twitter ou le "fil d'actualité" de Facebook fait des blogs et des sites statiques l'équivalent des quotidiens des années 1930 assistant impuissants à la montée en force de l'information radiophonique. La radio n'a pas tué les journaux mais les a forcés à adopter une stratégie de commentaires, d'éclairage des informations, que les radios avaient annoncée la veille. C'est aussi là que réside l'importance du rtw. Son intrusion inopinée dans le monde d'Internet forcera blogueurs et sites d'information à se positionner en commentateurs d'une information dont ils ne sont plus les émetteurs les plus prompts. Les exemples les plus connus de ce nouvel univers sont Twitter et les publications de Facebook. Cette approche est présente dans des réseaux sociaux, des moteurs de recherches, des sites d'information qui tendent à se comporter comme des correspondants de messagerie. Parmi les premiers effets positifs du rtw : une meilleure interaction avec l'internaute, et une diminution du nombre des accès aux serveurs.
Le rédacteur web est un des nouveaux métiers liés à l'internet. Ce métier consiste à produire des contenus rédactionnels adaptés au web. Les règles d'écriture diffèrent de celles de la presse écrite : l'écriture doit être concise, rapidement compréhensible et le style dynamique. De plus, l'avantage réside dans le fait de pouvoir modifier son contenu à n'importe quel moment grâce à un outil de gestion de contenu. Cette écriture doit également prendre en compte la dimension interactive du web avec les commentaires et les liens hypertextes par exemple. Elle doit également correspondre aux exigences des moteurs de recherche en prenant compte de la densité de mots-clés. C'est un métier où il faut allier technique et écriture. L'orthographe et la syntaxe se doivent d'être irréprochables. Une maîtrise de l'outil informatique est donc indispensable. Le rédacteur web peut exercer en externe, dans une agence Web ou en indépendant. Il peut occuper un poste en interne au sein d'une rédaction spécialisée à cet effet.
La redécentralisation d'Internet consiste à mettre les terminaux, les services et l'information à la périphérie du réseau Internet. Internet a été conçu comme un réseau de réseaux interconnectés dans lequel n'importe quel nœud du réseau peut communiquer avec tous les autres. Dans le contexte actuel, on observe cependant une centralisation d'Internet en ce qui concerne son infrastructure, ses services, son information. On observe la centralisation de l'information, des services vers les géants d'Internet comme Google, Facebook dans les centres de traitement de données. Des protocoles de messageries originaux comme le courriel, pour lesquels un particulier peut devenir son propre fournisseur d'adresse électronique, s'ils le souhaitent, sont dans une certaine mesure complétés par certains services comme Facebook, un réseau massivement utilisé mais pour lesquels l'utilisateur doit nécessairement passer par les serveurs de la compagnie pour utiliser ce service en lui-même. Des alternatives existent, mais elles ne sont généralement pas interconnectées.
La redirection d'URL est une technique permettant à une page web d'être disponible sous plusieurs URL. Les sites web qui proposent des services de redirection d'URL ne doivent pas être confondus avec les sites web qui proposent la réduction d'URL (au moyen de techniques de redirection d'URL).   Portail d’Internet
La réduction d'URL (ou lien court) est une technique utilisée sur Internet qui permet de rendre une page accessible par l'intermédiaire d'une très courte URL.
La Règle 34 (en anglais : Rule 34) — « Si ça existe, il y a du porno à ce sujet » est un mème qui suggère que sur n'importe quel sujet, il existe un équivalent pornographique.
Une relation par Internet est une relation humaine, voir une relation amoureuse, entre deux personnes par l'intermédiaire d'internet.  Portail de la sociologie
Rena Tangens est une artiste allemande née à Bielefeld, militante et pionnière de l'Internet.
Le délit de Sextorsion consiste en l'extorsion via internet de faveurs sexuelles ou monétaires. Il se double le plus souvent de celui de chantage à la webcam. Ce délit est réprimé en France depuis août 2014 avec la création de l’article 222-33-2-2 du Code pénal qui punit le cyber harcèlement et l'article 312-1 relatif à l'extorsion (qui est le fait d'obtenir par violence, menace de violences ou contrainte soit une signature, un engagement ou une renonciation, soit la révélation d'un secret, soit la remise de fonds, de valeurs ou d'un bien quelconque)
Une shoutbox (anglicisme pour « boîte à commentaires », littéralement « boîte à cris ») est un système de messagerie instantanée, généralement codé en JavaScript, intégré dans une page Web qui permet aux gens de laisser des messages sur un site Internet sans avoir besoin de s'enregistrer. C'est aussi un outil très rapide, compte tenu du fait que la boîte est déjà prête, il suffit d'écrire le message et de cliquer sur le bouton d'envoi. L'administrateur du site peut supprimer les messages injurieux et les « floods », identifier l'adresse IP des personnes ayant posté les messages et, éventuellement, prendre des mesures de bannissement pour leurs auteurs.
Smallest Federated Wiki, en français « le plus petit wiki fédéré », est un projet de développement d'une nouvelle vision du wiki. L'idée vient de Ward Cunningham, le créateur du WikiWikiWeb, qui, ne s'estimant pas satisfait des wiki tels qu'ils le sont actuellement, a commencé à développer une nouvelle vision du wiki sur github.
Un social bot (autres appellations : socialbot, socbot) est un type particulier d'agent conversationnel utilisé sur les médias sociaux afin de générer des messages automatiques (ex. : des tweets), de faire office de « suiveur fantôme (en) » ou même d'entretenir un compte utilisateur. Les social bots sont ainsi généralement utilisés lors de campagnes publicitaires ou par des firmes de relations publiques,,.
Le web solidaire correspond à l’ensemble des initiatives solidaires mises en place sur le web.
La souveraineté numérique désigne l'application des principes de souveraineté au domaine des technologies de l'information et de la communication (TIC), c'est-à-dire à l'informatique et aux télécommunications.
Le suivi-vers (ou simplement suivi, ou encore suivi-à, follow-up, followup-to ou en abrégé fu2) est le fait de préciser sur quel groupe Usenet (ou forum, ou liste de diffusion) les réponses à un message crossposté doivent être publiées. Ce groupe sera appelé dans ce qui suit : groupe de suivi.
Summly est une application facilitant l'agrégation d'informations et la lecture créée par un adolescent Britannique, Nick D'Aloisio. L'application a reçu l'Apple's Best Apps of 2012 award. Le succès des téléchargements sur les smartphones (plus d'un million en quelques mois) en a fait un choix évident pour le groupe Yahoo! qui a annoncé l'avoir acheté pour un montant estimé à 30 millions de dollars (25 millions d'euros) par Yahoo! le 26 mars 2013,.
SUP’Internet est une école supérieure privée formant aux métiers de l’Internet établie à Paris, France.
Le taux de clics (abrégé TDC) est un rapport entre le nombre de clics qu'un élément reçoit et le nombre d'affichages de celui-ci. Il s'exprime en pourcentage. Si une bannière publicitaire est affichée 1 000 fois et reçoit 10 clics, le taux de clics sera de 1 %. Le taux de clic est très utilisé en marketing électronique pour mesurer l'efficacité de la publicité en ligne. On peut aussi parler de taux de clic pour les clics effectués dans les pages de résultats d'un moteur de recherche (SERP).  Le taux de clics uniques désigne le rapport entre le nombre d'affichages d'une publicité et le nombre de personnes. Selon le livre blanc publié par ComScore, « 3 % des internautes effectuent 66 % des clics » et 85 % de tous les clics sur les publicités ne sont le fait que de 8 % des internautes.
Le TEAMS est un câble sous-marin de l'océan Indien reliant les Émirats arabes unis au Kenya. TEAMS (The East African Marine System) est une entreprise émanant du gouvernement du Kenya pour relier le pays au reste du monde à travers un câble sous-marin en fibre optique. Il a d'abord été proposé comme alternative à l'EASSy (l'East African Submarine Cable System). Le gouvernement kényan étant frustré par le modèle de propriété favorisé par l'Afrique du Sud, la durée du projet et la perception de la volonté de l'Afrique du Sud de contrôler le câble. En conséquence, en novembre 2006, le gouvernement kényan a décidé d'établir une coopération avec Emirates Telecommunication Establishment (Etisalat) pour construire son propre câble de fibre optique. Bien que le gouvernement ait son propre câble de fibre optique il est toujours partie prenante de l'EASSy. Cinq sociétés — Alcatel-Lucent, Tyco Telecommunication, Fujitsu Corporation, NEC Corporation et Huawei Technologies — ont concouru pour la construction du câble sous-marin TEAMs. Le 11 octobre 2007, Alcatel-Lucent a gagné le contrat de 82 millions de dollars pour poser le câble. La construction commença en janvier 2008 du côté des Émirats arabes unis. Le 12 juin 2009 le câble est arrivé à Mombasa ville portuaire du Kenya et a été inauguré par le Président du Kenya Mwai Kibaki, le Premier ministre du Kenya Raila Odinga et d'autres oligarques. Le 25 février 2012 le câble TEAMS a été accidentellement coupé par un bateau ancré à côté de Mombasa. Le 27 février 2012, Joel Tanui, General Manager de TEAMS a publié une déclaration déclarant que TEAMS avait déjà averti ses actionnaires et la société de maintenance du câble sous-marin E-Marine pour commencer la réparation du câble endommagé. Les réparations devaient durer trois semaines.
Les principes directeurs de règlement des litiges relatifs aux noms de domaine, plus connu sous sa forme anglophone Uniform Domain-Name Dispute-Resolution Policy (UDRP) s’appliquent à divers types de litiges entre les registrants et les tiers concernant l’enregistrement et l’utilisation des noms de domaine. Cette méthode de résolution a été introduite par l'ICANN et s'appliquent à l’ensemble des domaines de premier niveau générique .
L’unité de formation et de recherche d'ingénierie de l'internet et du multimédia (ou UFR Ingémédia) est une unité de formation et de recherche en sciences de l’information et de la communication interne à l'université du Sud Toulon-Var. Créée en 2002, elle est située sur le campus de La Garde. Elle propose des formations dans le secteur des technologies de l'information et de la communication (TIC) de niveau Bac+3 (licence professionnelle), à Bac +5 (master).
Vigilance sur le Net est une campagne de sensibilisation mise sur pied par Vidéotron pour attirer l’attention des Québécois sur les menaces liées à l’utilisation d’Internet et pour leur fournir des outils et des trucs concrets afin de s’en prémunir. La campagne agit par l'entremise de son site officiel, d'un microsite pour les jeunes, d'une Tournée Vigilance sur le Net et d'une trousse qui permet aux enseignants d’organiser leurs propres activités de sensibilisation aux dangers d’Internet avec leurs élèves. Le programme Vigilance sur le Net a pris fin en juillet 2016.  Portail du Québec  Portail d’Internet
WebCL est un standard développé par le Khronos Group, dont le but est l'intégration d'OpenCL 1.1, bibliothèque de calcul parallèle, dans l'ensemble des interfaces de programmation de HTML5. Cette adaptation est inspirée par le succès du port d'OpenGL ES au HTML5 avec WebGL. Pour que WebCL fonctionne sur un système, celui-ci doit supporter OpenCL 1.1. Actuellement (2013), les navigateurs ne gèrent pas WebCL en natif, mais utilisent des extensions ; Nokia et Mozilla ont développé des extensions pour Firefox,. Samsung pour Webkit et Motorola pour Node.js.
Le webécrivain public est un écrivain public spécialisé dans le maniement du web, bénéficiant d'une formation en recherche documentaire (documentaliste ou recherchiste). Il intervient comme tierce personne entre les administrations, qui proposent de plus en plus leurs prestations via des sites web, et les personnes qui y recourent mais n'ont pas d'ordinateur, ou pratiquent insuffisamment ce média.
Le World Internet Project (WIP) est un projet international collaboratif de recherche associant un certain nombre d'institutions universitaires. il est né au Center for Communication Policy (en) de l'UCLA et a été fondé avec l'École NTU (études de la communication) de Singapour et avec l'observatoire Osservatorio Internet Italia de l'Université Bocconi de Milan (Italie).
YouNow (en français : « Toi Maintenant ») est un service d'hébergement de vidéos de l'entreprise américaine YouNow Inc. qui a son siège a New York. Les utilisateurs peuvent regarder, apprécier et faire des livestreams. L'inscription est gratuite.
YouPass est un service de paiement mettant à disposition de la monnaie électronique en monétisant le crédit téléphonique. YouPass est un Établissement de Monnaie Électronique proposant des services de micro-paiement par mobile. L’activité de la société consiste à convertir des unités téléphoniques en monnaie électronique. Grâce à cette conversion, l’utilisateur des services de la société dispose d’un crédit de monnaie électronique lui permettant de procéder à des achats via son smartphone ou sur internet, sans utiliser de carte bancaire ,.
Zeronet est une plateforme en ligne décentralisée basée sur la chaine de blocs Namecoin et le protocole P2P BitTorrent. Cette application consiste en un script open-source écrit en langage Python. Les sites de ce réseau sont hébergés et partagés par les pairs ce qui rend théoriquement impossible la censure ou le blocage des contenus ainsi hébergés. ZeroNet intègre Tor, permettant ainsi l'anonymat des utilisateurs.
On appelle langage informatique un langage formel non nécessairement Turing-complet utilisé lors de la conception, la mise en œuvre, ou l'exploitation d'un système d'information. Le terme est toutefois utilisé dans certains contextes dans le sens plus restrictif de langage de programmation.
Architecture Analysis and Design Language (AADL) est un langage de description d'architecture système destiné aux systèmes embarqués (contextes : automobile, aéronautique et spatial notamment). Sa standardisation est en cours sous l'autorité de SAE International. Une première version stable, AADL 1.0, a été publiée en novembre 2004. La première signification de AADL était « Avionics Architecture Description Language ». AADL est dérivé de MetaH, un langage de description d'architecture inventé par Advanced Technology Center de Honeywell. Le principe est de décrire l'architecture pour mieux maîtriser sa complexité, et pouvoir vérifier quelques propriétés de ce système comme l'ordonnançabilité, la bonne transmission des messages, le bon dimensionnement du matériel (capacité mémoire ...). Une application d'AADL est UML pour la description de système qui permet également de générer automatiquement la conception. AADL décrit plusieurs composants qui modélisent une partie du système. Certains composants sont matériels (bus, processor, memory ...), d'autres logiciels (process, thread, subprogram, ...). Chaque composant peut avoir des propriétés (champ properties), et peut contenir des sous-composants (un processus - process - pouvant par exemple contenir plusieurs fils d'exécution - threads). On peut également décrire plusieurs machines et les relier entre elles pour simuler des connexions réseaux entre elles.
Asymptote est un langage de description de graphismes vectoriels, particulièrement adapté pour le dessin technique. Par opposition à un logiciel classique de dessin vectoriel, il s'agit d'un langage de programmation, comme MetaPost. Asympote possède une syntaxe proche de celle de C++, interagit naturellement avec LaTeX, étend les chemins de Metapost à la troisième dimension et gère les nombres flottants. On peut donc le considérer comme un successeur de MetaPost, mais avec une syntaxe beaucoup plus propre[non neutre]. Le compilateur est un logiciel libre sous licence GNU Lesser General Public License.
Le filtre BPF (Berkeley Packet Filter) est un langage permettant de filtrer les paquets échangés sur un réseau. Il est maintenu et a été conçu par les chercheurs du Laboratoire national Lawrence-Berkeley (États-Unis). Ce langage est utilisé entre autres par des logiciels spécialisés dans l'analyse d'échanges réseau (exemples : ngrep, tcpdump, snort, ntop) et sert à sélectionner des données transitant sur un réseau selon des critères précis, par exemple : Qui (quel est l'hôte qui) envoie les données ? Qui (quel est l'hôte qui) reçoit les données ? Quel est le protocole (Protocole réseau) utilisé ? Vers et depuis quel port sont envoyées les données ? etc.
Le langage Caméléon est un langage de programmation fonctionnelle graphique libre et open source (licence MIT).
En informatique et en télécommunications, un caractère d'échappement est un caractère qui déclenche une interprétation alternative du ou des caractères qui le suivent. Le caractère d'échappement peut : retirer sa valeur spéciale à un caractère. Par exemple, la combinaison \espace dans un nom de fichier permet de traiter l'espace comme un caractère ordinaire et non comme un séparateur ; ajouter une valeur spéciale à un caractère ordinaire. Par exemple \n signifie une nouvelle ligne. La difficulté augmente quand on veut afficher le caractère d'échappement sans qu'il ait une action particulière. Souvent, il faut l'échapper avec lui-même : \\ pour afficher \, et ainsi de suite (\\\\ pour afficher \\). L'ensemble du caractère d'échappement et des caractères dont il modifie la signification forme une séquence d'échappement. Les caractères d'échappement et leur signification dépendent des langages informatiques et des appareils qui les utilisent.
Le CODASYL (sigle de Conference on Data Systems Languages, en français « Conférence sur les langages de systèmes de traitement de données ») est l'organisme américain de codification des systèmes de bases de données. Il a publié en 1959 les spécifications du langage COBOL. Par la suite, ses travaux entre 1974 et 1981 ont donné naissance au modèle navigationnel de SGBD (opposé au modèle hiérarchique largement promu par IBM). En 1965, le CODASYL a constitué un groupe de travail chargé de développer les extensions du langage COBOL qui permettront de manipuler des données en préservant l'indépendance entre les applications et les périphériques de stockage. La contribution de Charles Bachman revient à la logique de chaines de données et de pointeurs mise notamment en œuvre dans le système IDS. En 1967, ce groupe de travail s'est rebaptisé Data Base Task Group (DBTG) qui publiait notamment COBOL extensions to handle data bases (extensions de COBOL pour manipuler des données). En octobre 1969, le DBTG publie alors les premieres spécifications du modèle de bases de données réseau qui prendra très vite le nom de modèle de données CODASYL. Le CODASYL a défini en 1970 les normes de standardisation des Systèmes de Gestion des Bases de Données (SGBD), uniquement mis en œuvre sur les grands systèmes à cette époque. Le langage de manipulation des données était alors exclusivement le COBOL et le databasic. C'est la première norme de base de données décidée sans l'avis d'IBM.
Cython est un langage de programmation et un compilateur qui simplifient l'écriture d'extensions compilées pour Python. La syntaxe du langage est très similaire à Python mais il supporte en plus un sous-ensemble du langage C/C++ (déclarations de variables, appel de fonctions, ...). Le premier intérêt de Cython est qu'il produit du code nettement plus performant. Dans des programmes qui nécessitent par exemple la manipulation de grands tableaux, le gain peut aller jusqu'à un facteur 100. Par ailleurs, Cython permet d'écrire des interfaces Python à des bibliothèques externes écrites en C ou C++. Cython est disponible pour la plupart des systèmes d'exploitation.
En typographie, une feuille de style désigne l'ensemble d'attributs de caractères et de formats de paragraphes pouvant être appliqués en une seule opération à un ou plusieurs paragraphes, à un livre, etc. De façon extensive, c'est l'ensemble des contraintes typographiques, de maquette, couleur, reliure, etc. qui définissent l'unité d'une collection littéraire, d'une revue périodique, etc. C'est aussi un document permettant de mettre en forme un autre document, comme un document rédigé dans un langage de balisage, comme SGML, HTML ou XML. Pour ce faire, les feuilles de style sont rédigées dans un langage spécifique. Les langages de feuilles de style les plus courants sont : CSS XSL XSL-FO DSSSL
Les feuilles de style en cascade, généralement appelées CSS de l'anglais Cascading Style Sheets, forment un langage informatique qui décrit la présentation des documents HTML et XML. Les standards définissant CSS sont publiés par le World Wide Web Consortium (W3C). Introduit au milieu des années 1990, CSS devient couramment utilisé dans la conception de sites web et bien pris en charge par les navigateurs web dans les années 2000.
fread est une fonction qui lit l'entrée de la mémoire tampon d'un fichier. Elle est incluse dans l'en-tête stdio.h de la bibliothèque standard du C.
Gallina est le langage uniforme de description mathématique utilisé par Coq. À noter que gallina signifie poule en latin.
Le Gentee est un langage de haut niveau dont la syntaxe ressemble au C et C++. Il permet de compiler le code source sous une machine virtuelle pour pouvoir l'exécuter sans être obligé de le transformer en exécutable. Mais on peut le compiler sous la forme « *.exe » par la suite pour pouvoir l'exécuter directement sous Windows sans la présence du langage sur le disque dur. Les fichiers portant l'extension « *.g » sont des fichiers sources et ceux portant l'extension « *.ge » sont des bibliothèques.
Le standard IEEE 1076 définit le langage de description de matériel VHSIC-HDL ou VHDL. Il a été développé par CLSI sous contrat F33615-83-C-1003 pour la United States Airforce en 1983. Le langage a subi de nombreuses révisions et a divers sous-standards qui lui sont associés et qui l'étendent de manière importante. 1076 a été et continue à être un milestone dans le design de systèmes électroniques.
In silico (dont la traduction littérale en français est en silice) est un néologisme d'inspiration latine désignant une recherche ou un essai effectué au moyen de calculs complexes informatisés ou de modèles informatiques. Cette expression est surtout utilisée dans les domaines de la génomique et la bioinformatique.
L'index TIOBE mesure la popularité des langages de programmation sur base du nombre de pages web retournées par un moteur de recherche lorsqu'on lui soumet le nom du langage de programmation. Il est mis à jour une fois par mois et donne l'historique depuis 2002. Le langage ayant la plus forte croissance dans l'année est nommé « langage de l'année ».
Kermeta est un langage de métamodélisation exécutable dont le format et le code source est ouvert. Il dispose d’un environnement de développement de métamodèles basé sur EMOF dans un environnement Eclipse. Il permet non seulement de décrire la structure des métamodèles, mais aussi leur comportement. Il permet ainsi de définir et d’outiller de nouveaux langages en améliorant la manière de spécifier, simuler et tester la sémantique opérationnelle des métamodèles. Il permet en outre d’appliquer plus facilement les techniques de l’ingénierie dirigée par les modèles aux outils IDM eux-mêmes. Kermeta workbench se compose d’outils s’interfaçant facilement avec les outils existants dans la communauté des logiciels libres d’Éclipse. Ces outils sont libres et sont distribués selon les termes de la licence Éclipse.
Koala est une application multiplateforme permettant la compilation des langages dynamique suivant : Less, Sass, Compass et CoffeeScript. L'application en est à sa version 2.0.4 (release), et est traduit en 10 langues (donc le français et l'anglais).  Portail de l’informatique
Un langage dédié (Domain specific language) est un langage de programmation dont les spécifications sont conçues pour répondre aux contraintes d’un domaine d'application précis. Il s'oppose conceptuellement aux langages de programmation classiques (ou généralistes) comme le Java ou le C, qui tendent à traiter un ensemble de domaines. Néanmoins, aucun consensus ne définit précisément ce qu'est un langage dédié. Ce manque de définition précise sur la nature d'un langage dédié rend délicate la tâche d'établir un historique clair sur l'origine du concept. En informatique, les langages dédiés traitent divers domaines informatiques (pilotes informatiques, le calcul scientifique, les bases de données) ou traite de différents autres domaines où intervient l'informatique (médecine, aéronautique). L'utilisation de langage dédié n'est pas propre à l'informatique, il existe des langages dédiés aux domaines de la médecine, de la cuisine, etc. La construction des langages dédiés diffère fondamentalement de celle d'un langage classique. Le processus de développement peut s’avérer très complexe. Sa conception nécessite une double compétence sur le domaine à traiter et en développement informatique. Qu'il soit langage dédié interne ou externe, la mise en œuvre de ce type de langage requiert l'utilisation de patron de conception. Il existe des patrons de conceptions décrivant l’implémentation comme le décrit Spinellis et décrivant plutôt les phases de développement du langage. Ces deux approches, bien qu'étant fondées sur des idées différentes, se complètent l'une et l'autre. L'utilisation de langages dédiés présente des avantages et des inconvénients par rapport à l'utilisation de langages généralistes. La justification de l'utilisation de langages dédiés peut se faire en fonction de considérations d'ordre technologique, ou d'ordre financier. D'autre part, les avantages et les inconvénients ne sont pas les mêmes qu'il s'agisse d'un langage dédié interne ou externe.
Un langage de programmation graphique ou visuel est un langage de programmation dans lequel les programmes sont écrits par assemblage d'éléments graphiques. Sa syntaxe concrète est composée de symboles graphiques et de textes, qui sont disposés spatialement pour former des programmes. De nombreux langages visuels se basent sur les notions « de boîtes et de flèches » : les boîtes (ou d'autres d'objets) sont traités comme des entités, reliées par des flèches ou des lignes qui représentent des relations. Plus précisément, un langage est défini par une syntaxe abstraite, à laquelle sont associées une ou plusieurs syntaxes concrètes, parmi lesquelles une ou plusieurs peuvent être graphiques. Généralement ces langages sont associés à un environnement graphique de programmation. Il n'est pas toujours possible de les dissocier. Il faut également faire la distinction entre le langage au sens "normalisé" et son implémentation au sens "logiciel".
Less est un langage dynamique de génération de CSS conçu par Alexis Sellier. Initialement inspiré par Sass, il l'influence à son tour avec l'apparition de la syntaxe "SCSS" par laquelle Sass reprend des éléments de la syntaxe CSS classique. Le principe de Less est en effet de ne pas rompre avec la syntaxe CSS : tout code CSS est aussi du code Less valide et sémantiquement équivalent. Less y ajoute notamment les mécanismes suivants : variables, imbrication, mixins, opérateurs et fonctions. Less est diffusé en open source. Sa première version a été écrite en Ruby, les versions ultérieures en JavaScript. Par rapport aux autres préprocesseurs CSS, il présente la particularité de pouvoir être converti à la volée, soit par le serveur, soit par le navigateur. Il peut également être traduit automatiquement en CSS classique à l'écriture.
M est un langage de programmation développé par Microsoft et spécialement conçu pour la création de langages dédiés et la modélisation de données avec le XAML. M faisait partie du défunt projet Oslo et aucune date de publication n'a été annoncée.
PGF/TikZ est une combinaison de deux langages informatiques pour la création de graphiques vectoriels. PGF est un langage de bas niveau, tandis que TikZ est un ensemble de macros qui fournit une syntaxe plus simple comparée à celle de PGF. Il a été créé en 2005 par Till Tantau qui est aussi le développeur principal de l'interpréteur de PGF et TikZ qui est écrit en TeX. PGF est le sigle de Portable Graphics Format, tandis que TikZ est un acronyme récursif de TikZ ist kein Zeichenprogramm (« TikZ n'est pas un programme de dessin »). L'interpréteur de PGF/TikZ peut être utilisé depuis les paquets LaTeX et ConTeXt. Les fonctionnalités de TikZ sont comparables à celles de PSTricks, cependant TikZ prend en charge le format PostScript ainsi que le format PDF mais ne peut utiliser certaines fonctionnalités avancées de PostScript. Plusieurs programmes de création de graphiques vectoriels comme GeoGebra, Inkscape, Blender, MATLAB ou R permettent d'exporter leur fichiers au format PGF/TikZ. Il existe aussi des éditeurs du type WYSIWYG, tels que KtikZ, qui permettent un visionnage en direct de ce que la compilation donne. Des bibliothèques additionnelles permettent de réaliser des schémas particuliers, par exemple en sciences de l'ingénieur. Le projet est toujours actif à la date de janvier 2012 ; la plupart des développements sont réalisés par Till Tantau.
PL/pgSQL (Procedural Language/PostgreSQL Structured Query Language) est un langage procédural géré par PostgreSQL. Ce langage est très similaire au PL/SQL d'Oracle, ce qui permet de porter des scripts de ou vers Oracle au prix de quelques adaptations.
Prime data language (ou PDL) est un langage de description de données d'instruments financiers. C'est un « méta-langage » à la XML avec une syntaxe différente.  Portail de l’informatique
En informatique, le principe de Huffman est une règle informelle souvent invoquée par Larry Wall, le concepteur du langage Perl, pour concevoir les constructions syntaxiques d'un langage informatique et pour écrire des programmes en un langage informatique donné. Le but du principe de Huffman est d'écrire des programmes concis. Son domaine d'application n'est pas spécifique à Perl, mais peu de concepteurs de langages ont prêté autant d'attention que Larry Wall aux propriétés souhaitables de la syntaxe concrète d'un langage. Le principe de Huffman fait référence au principe qui sous-tend le codage de Huffman et d'autres algorithmes de compression de données : l'encodage d'un symbole souvent utilisé doit être plus court que celui d'un symbole moins utilisé.
printf (pour l'anglais print formatted, soit « imprimer formaté ») est une commande Unix permettant de faire afficher une chaîne de caractères à l'écran. C'est aussi un nom de fonction du langage C, et de nombreux autres langages informatiques permettant d'afficher une ou plusieurs variables de façon formatée dans le flux de sortie.
PROMELA (PROtocol MEta LAnguage) est un langage de spécification de systèmes asynchrones, ce qui en d'autres termes veut dire que ce langage permet la description de systèmes concurrents, comme les protocoles de communication. Il autorise la création dynamique de processus. La communication entre ces différents processus peut se faire en partageant les variables globales ou alors en utilisant des canaux de communication. On peut ainsi simuler des communications synchrones ou asynchrones. En PROMELA, il n'y a pas de différence entre les instructions et les conditions. Une instruction ne peut être passée que si elle est exécutable, une condition que si elle est vraie. Sinon le processus est bloqué jusqu'à ce que la condition devienne vraie. Promela est associé à l'outil de validation Spin.  Portail de la programmation informatique
Le Property Specification Language (PSL) (en français : Langage de spécification par propriétés) est basé sur le langage Sugar d’IBM. Il a été approuvé par l’organisme Accellera en mai 2003, et par l’IEEE en septembre 2004. C'est un langage formel qui permet de réaliser une spécification matérielle à l'aide de propriétés et d'assertions. Du fait de la haute précision mathématique du langage, l'opération de description retire toute ambiguïté à la spécification résultante. C'est un langage rapide à assimiler, basé sur une syntaxe relativement simple.
ProvideX (en abrégé, PVX) est un langage « BASIC ». Issu des Business Basic, il a su non seulement être un outil permettant de migrer les applications en mode texte vers des logiciels graphiques, mais surtout, il reste en évolution constante et permet, entre autres choses, la programmation orientée objet (OOP), l'utilisation de composants COM, DLL… Le langage ProvideX a le double avantage d'être simple, et très complet. Il est donc facile à apprendre. Un développeur non initié peut rapidement réaliser ses premières applications, mais l'expert a accès aux couches parmi les plus basses du système pour programmer dans le détail des logiciels très pointus. Un des principaux avantages de ce langage est qu'il est portable sur plusieurs plates-formes (Windows, Unix-Linux, MAC) ; ainsi, les applications réalisées en ProvideX pourront être distribuées « universellement », sans obliger l'utilisateur à adopter un système d'exploitation plutôt qu'un autre. Il faut néanmoins noter que cet aspect "multiplateformes" n'est pas une caractéristique du langage même mais plutôt de son portage sous Java Le système de données géré nativement par ProvideX permet la création de fichiers multi-clefs (jusqu'à 255 clefs de 255 segments chacune), d'une capacité de plusieurs téraoctets. Ces fichiers permettent un accès de type ISAM (séquentiel indexé) aux données, gèrent les transactions (commit, rollback), et permettent donc de restituer leur état initial en cas d'abandon de procédure de modification de données. Il est ouvert sur les technologies relatives aux bases de données relationnelles, au Web, aux serveurs d'applications… En France, la communauté de développeurs PVX est nouvelle et restreinte. Les utilisateurs sont relativement nombreux, mais les applications de gestion qu'ils utilisent sont suffisamment indépendantes et fonctionnelles pour leur permettre d'ignorer que leurs logiciels sont développés avec ProvideX. Aux États-Unis, l'utilisation de ce langage dans des progiciels renommés de comptabilité et de finances en fait un des plus efficaces dans le monde des logiciels dédiés à la gestion.[réf. nécessaire] Le site : http://www.pvxaddict.net est en français. Il constitue une bonne base de connaissance et d'accès à l'arsenal des outils de développement que ProvideX propose.  Portail de la programmation informatique
Pyrex est un langage de programmation destiné à l'écriture de modules Python.
roff est un langage de formatage de texte historiquement lié à UNIX et à ses dérivés (BSD, Linux…).
Sass (Syntactically Awesome Stylesheets) est un langage de génération de feuilles de style initialement développé par Hampton Catlin et Nathalie Weizenbaum. Sass est un langage de feuilles de style en cascade (CSS). C'est un langage de description qui est compilé en CSS. SassScript est un langage de script pouvant être utilisé à l’intérieur du code Sass. Deux syntaxes existent. La syntaxe originale, nommée « syntaxe indentée », est proche de Haml. La nouvelle syntaxe se nomme SCSS. Elle a un formalisme proche de CSS. Sass peut être étoffé avec Compass (pratique pour les préfixes des différents navigateurs).
SQR (Structured Query Report) est un langage de développement informatique destiné à créer des rapports à partir d'une base de données.
Squirrel est un langage de programmation haut niveau impératif et orienté objet, conçu pour être un langage de script léger qui soit compatible en taille, occupation mémoire, et exigence temps réel d'applications comme les jeux vidéo. MirthKit, une boîte à outil pour faire et distribuer des jeux 2D open sources, multiplateformes, utilise Squirrel. Il est utilisé intensivement par Code::Blocks pour les scripts ainsi que dans la quasi-totalité du code source du jeu Final Fantasy Crystal Chronicles: My Life As a King publié par Square Enix, et également dans le jeu Hard Reset de Flying Wild Hog et dans tous les jeux Source récents de Valve .
Standard Portable Intermediate Representation ou SPIR est un Langage intermédiaire, créé par le Khronos Group, à l'origine pour le calcul parallèle intensif et faciliter l'interprétation d'OpenCL, et dans son API Vulkan, avec SPIR-V pour la transmission des shaders aux pilotes des processeurs graphiques. Initialement basée sur LLVM, la version SPIR-V utilise sa propre représentation. Étant donné la proximité de SPIR-V avec SPIR (et donc la représentation intermédiaire de LLVM), ce dernier pourra être utilisé pour son interprétation.
SystemC est souvent présenté comme un langage de description de matériel, au même titre que VHDL ou Verilog. En fait, SystemC est un langage de description de plus haut niveau, puisqu'il permet une modélisation de systèmes au niveau comportemental. SystemC n'est pas un langage à part entière mais un ensemble de classes C++ qui introduisent les concepts nécessaires à la modélisation du matériel (par exemple la notion de processus concurrents). Conservant les fonctionnalités du C++, il reste possible de décrire des fonctions purement logicielles. SystemC permet donc de modéliser des systèmes matériels, logiciels, mixtes ou même non-partitionnés. Il est donc particulièrement approprié à la conception de systèmes de type SoC. SystemC intègre également la possibilité de simuler le modèle conçu, puis, par raffinements successifs, d'aboutir à une représentation implémentable. SystemC a été développé en commun par plusieurs entreprises. À cette fin, l'OSCI (Open SystemC Initiative) a été créé, chargé de diffuser, promouvoir et rédiger les spécifications de SystemC. Depuis décembre 2005, SystemC est standardisé auprès de l'IEEE sous le nom de IEEE 1666-2005.  #include "systemc.h" SC_MODULE(and3) {   sc_in<bool> e1;   sc_in<bool> e2;   sc_in<bool> e3;   sc_out<bool> s;   void compute_and()   {     s = e1 & e2 & e3;   };   SC_CTOR(and3)   {     SC_METHOD(compute_and);     sensitive << e1;     sensitive << e2;     sensitive << e3;   } }
SystemVerilog est un langage de description et de vérification de matériel basé sur Verilog.
Le Verilog, de son nom complet Verilog HDL est un langage de description matériel de circuits logiques en électronique, utilisé pour la conception d'ASICs (application-specific integrated circuits, circuits spécialisés) et de FPGAs (field-programmable gate array). Le sigle anglais HDL -Hardware Description Language- signifie Langage de Description du Matériel. « Verilog HDL » ne doit pas être abrégé en VHDL, ce sigle étant utilisé pour le langage concurrent VHSIC Hardware Description Language.
Verilog-AMS est un dérivé du langage de description matériel Verilog. Il comprend des extensions analogiques et des signaux mixtes (en anglais analog and mixed-signal, AMS) afin de définir le comportement des systèmes à signaux analogiques et mixtes. La norme Verilog-AMS a été instaurée dans l'intention de permettre aux concepteurs de systèmes à signaux analogiques et mixtes et de circuits intégrés de pouvoir créer et d'utiliser des modules qui encapsulent les descriptions de comportement de haut niveau, aussi bien que des descriptions structurelles de systèmes et de composants Verilog-AMS définit un langage de modélisation standardisé par l'industrie pour les circuits à signaux mixtes. Il fournit à la fois le temps-continu et les sémantiques de modélisation d'événements. Il est donc approprié pour les circuits analogiques, numériques et mixtes. Il important de noter que le Verilog ne constitue pas un langage de programmation. Il s'agit d'un langage de description du matériel.
VHDL est un langage de description de matériel destiné à représenter le comportement ainsi que l'architecture d’un système électronique numérique. Son nom complet est VHSIC Hardware Description Language. L'intérêt d'une telle description réside dans son caractère exécutable : une spécification décrite en VHDL peut être vérifiée par simulation, avant que la conception détaillée ne soit terminée. En outre, les outils de conception assistée par ordinateur permettant de passer directement d'une description fonctionnelle en VHDL à un schéma en porte logique ont révolutionné les méthodes de conception des circuits numériques, ASIC ou FPGA.
VHDL-AMS est un dérivé du langage de description matériel VHDL (norme IEEE 1076-1993). Il comprend des extensions analogiques et des signaux mixtes (en anglais analog and mixed-signal, AMS) afin de définir le comportement des systèmes à signaux analogiques et mixtes (IEEE 1076.1-1999). La norme VHDL-AMS a été instaurée dans l'intention de permettre aux concepteurs de systèmes à signaux analogiques et mixtes et de circuits intégrés de pouvoir créer et d'utiliser des modules qui encapsulent les descriptions de comportement de haut niveau, aussi bien que des descriptions structurelles de systèmes et de composants. VHDL-AMS définit un langage de modélisation standardisé par l'industrie pour les circuits à signaux mixtes. Il fournit à la fois le temps-continu et les sémantiques de modélisation d'événements. Il est donc approprié pour les circuits analogiques, numériques et mixtes. Il est particulièrement bien adapté pour la vérification de circuits intégrés complexes qui allient des signaux analogiques, mixtes et des fréquences radios. Il est important de noter que le VHDL-AMS ne constitue pas un langage de conception ou synthèse. Il s'agit seulement d'un langage de description du matériel.
Le Web Services Flow Language est un langage inventé par IBM. Ce langage s'attaque à un problème clé des services Web : la description de l'enchaînement des traitements sur de multitudes d'applications réparties. Une fois qu'un business process définissant les diverses opérations élémentaires à exécuter pour mener une tâche est défini, l'enchaînement des opérations est alors traduit dans ce dialecte XML. À l'instar de WSDL, WSFL ne sert donc pas à définir le processus mais sa mise en œuvre.  Portail de l’informatique
WebML (Web Modeling Language) est un langage graphique et une méthode pour le développement d'applications Web complexes. Il fournit des moyens de spécifications et des graphiques, le tout intégré dans un processus de conception.  
XBLite est un compilateur open source d'un langage de programmation proche du BASIC. C'est une spécialisation pour Microsoft Windows du langage de programmation multi-plateforme XBasic, créée en 2001 par David Szafranski. XBLite et son environnement de développement sont publiés sous licence GNU GPL, ses bibliothèques sous licence GNU LGPL. Sa syntaxe est quasiment celle de XBasic, mais on peut y voir des similitudes avec celle de QuickBASIC de Microsoft en ce sens qu'XBLite est aussi un langage procédural avec sous-routines et fonctions. XBLite possède un type numérique de 64 bits, permet de définir de nouveaux types de donnée et autorise un découpage en modules pour créer une application ou un jeu.
XHTML Friends Network (XFN) est un microformat destiné à indiquer les relations existant entre les individus via les liens établis entre leurs sites. Par exemple, pour indiquer que je suis un ami de Paul (friend), et je l’ai déjà rencontré (met), je vais ajouter dans le code du lien les deux attributs : <a href="paul.example.com" rel="friend met">le site de Paul</a> Ce microformat a été créé par Tantek Çelik, Matthew Mullenweg et Eric Meyer.
YAWL (acronyme de Yet Another Workflow Language) est un langage graphique de programmation de workflow.
En informatique, un logiciel est un ensemble de séquences d’instructions interprétables par une machine et d’un jeu de données nécessaires à ces opérations. Le logiciel détermine donc les tâches qui peuvent être effectuées par la machine, ordonne son fonctionnement et lui procure ainsi son utilité fonctionnelle. Les séquences d’instructions appelées programmes ainsi que les données du logiciel sont ordinairement structurées en fichiers. La mise en œuvre des instructions du logiciel est appelée exécution, et la machine est appelée ordinateur ou calculateur. Un logiciel peut être classé comme système, applicatif, standard, spécifique, ou libre, selon la manière dont il interagit avec le matériel, selon la stratégie commerciale et les droits sur le code source des programmes. Le terme logiciel propriétaire est aussi employé. Les logiciels sont créés et livrés à la demande d'un client, ou alors ils sont créés sur l'initiative du producteur, et mis sur le marché, parfois gratuitement. En 1980, 60 % de la production et 52 % de la consommation mondiale de logiciels est aux États-Unis. Les logiciels sont également distribués illégalement et la valeur marchande des produits ainsi distribués est parfois supérieure au chiffre d'affaires des producteurs. Les logiciels libres sont créés et distribués comme des commodités produites par coopération entre les utilisateurs et les auteurs. Créer un logiciel est un travail intellectuel qui prend du temps. La création de logiciels est souvent le fait d'une équipe, qui suit une démarche logique et planifiée en vue d'obtenir un produit de bonne qualité dans les meilleurs délais. Le code source et le code objet des logiciels sont protégés par la convention de Berne concernant les œuvres littéraires.
Analyst's Notebook (ANB) est un logiciel d'analyse de données récoltées dans le cadre d'une enquête, développé par i2. En France, il est utilisé par le Service central de renseignement criminel sous le nom ANACRIM.
Boutons de barre d'outils pour GNOME  L'annulation est, en informatique, une fonctionnalité proposée par la plupart des logiciels modernes, qui permet à l'utilisateur d'annuler la ou les dernières actions qu'il a effectuées, généralement sur un document, afin de le faire revenir à un état antérieur. Cette fonctionnalité existe aussi dans les wikis. L'annulation est souvent associée à la restauration, c'est-à-dire la commande inverse, qui permet d'annuler l'annulation, et de revenir à un état plus récent.
Une applet Java est une applet, fournie aux utilisateurs sous la forme de bytecode Java. Une applet Java peut fonctionner dans un navigateur web, grâce à une machine virtuelle Java (JVM), ou dans l'AppletViewer de Sun, un outil permettant de tester les applets Java. Les applets Java ont été introduites dans la première version du langage, de 1995. Les applets Java sont la plupart du temps écrites en langage Java, mais elles peuvent également être écrites dans n'importe quel langage qui se compile en bytecode, comme Jython, Groovy ou encore Scala. Les applets sont utilisées pour fournir au sein d'applications Web des fonctionnalités interactives qui ne peuvent pas être fournies par le langage HTML. Étant donné que le bytecode Java est multiplate-forme, les applets Java peuvent être exécutées sur différentes plates-formes, dont Windows, Unix, Mac OS et Linux. Il existe des outils open-source, comme applet2app, qui peuvent être utilisés pour convertir une applet en une application riche Java, ou en application native pour Windows ou Linux. Ce qui donne l'avantage de pouvoir lancer une applet Java en mode hors-ligne, ou en dehors d'un navigateur web. Beaucoup de développeurs Java influents, blogs et magazines recommandent l'utilisation de la technologie Java Web Start au lieu des applets Java,.
Une application, un applicatif ou encore une appli, une app est, dans le domaine informatique, un programme (ou un ensemble logiciel) directement utilisé pour réaliser une tâche, ou un ensemble de tâches élémentaires d'un même domaine ou formant un tout. Typiquement, un éditeur de texte, un navigateur web, un lecteur multimédia, un jeu vidéo, sont des applications. Les applications s'exécutent en utilisant les services du système d'exploitation pour utiliser les ressources matérielles.
Une application en flux est une application qui réside sur un serveur et dont les parties sont transmises à un utilisateur lorsque l'utilisateur en a besoin. Dans ce mode de distribution logicielle sur demande, seule les parties utilisées et non l'ensemble de l'application sont transmises à l'utilisateur. Les bases d'applications en flux se trouvent dans la façon dont les langages informatiques et les systèmes d'exploitation modernes produisent et exécutent le code des applications. Seulement quelques parties d'un logiciel doivent être disponibles à un moment donné pour que l'utilisateur final puisse effectuer une certaine fonction. Au lieu d'une installation complète sur son ordinateur, des parties peuvent être transmises par le réseau en fonction des besoins. Les applications en flux sont généralement associées à la virtualisation d'applications afin de ne pas installer les applications de la manière traditionnelle.
En informatique, une application web (aussi appelée web application, de l'anglais) est une application manipulable directement en ligne grâce à un navigateur web et qui ne nécessite donc pas d'être installée. De la même manière que les sites web, une application web est généralement placée sur un serveur et se manipule en actionnant des widgets à l'aide d'un navigateur web, via un réseau informatique (Internet, intranet, réseau local, etc.). Exemples : Des messageries web, les systèmes de gestion de contenu, les wikis et les blogs sont des applications web. Les moteurs de recherches, les logiciels de commerce électronique, les jeux en ligne, les logiciels de forum peuvent être sous forme d'application web. Des appareils réseau tels que les routeurs sont parfois équipés d'une application web dans leur micrologiciel. Les applications web font partie de l'évolution des usages et de la technologie du Web appelée Web 2.0.
Un autorépondeur est un logiciel qui répond automatiquement à un message électronique qui lui est envoyé. Le logiciel peut être très simple ou très complexe. Un logiciel autorépondeur est souvent inclus dans un logiciel d'envoi de courrier électronique de masse. Le logiciel est aussi souvent inclus dans un logiciel de liste de diffusion pour confirmer une souscription, un désabonnement ou une autre activité de gestion de la liste. Les autorépondeurs sont souvent utilisés comme outils de marketing pour répondre immédiatement à un message d'un client, puis relancer le client à intervalles prédéterminés.
Babylon.js est un moteur 3D temps réel sous forme de bibliothèque JavaScript permettant l'affichage de scènes 3D dans un navigateur web via HTML5. Le code source est disponible sur github et distribué sous licence Apache 2.0. Il a été développé initialement par des employés de Microsoft, sur leur temps libre et le projet compte en 2016 plus de 90 contributeurs.
Le brevet logiciel désigne à l'échelle d'un pays le fait de posséder des réglementations et une jurisprudence claires permettant l'octroi de brevets sur les logiciels, c'est-à-dire un droit d'interdiction de l'exploitation par un tiers de l'invention brevetée, à partir d'une certaine date et pour une durée limitée (20 ans en général). La jurisprudence aux États-Unis est traditionnellement favorable à la protection des logiciels par le brevet. Les décisions récentes paraissent cependant relativiser cette position. En Europe, l'article 52(2) de la Convention sur le brevet européen exclut la brevetabilité des programmes d'ordinateur. Mais, dans la pratique des brevets y sont accordés pour des « inventions mises en œuvre par logiciel » c'est-à-dire liant un logiciel ayant un « effet technique » (par exemple certains systèmes ABS), l'on évoque à ce propos la brevetabilité des inventions mises en œuvre par ordinateur. L'on définit en général la notion d'effet technique comme une transformation de la Nature par l'Homme, autrement comme la production d'un effet matériel. Il existe donc différentes positions à l’échelle internationale, et la pertinence de la possibilité de dépôt de brevets dans le domaine des logiciels fait l'objet d'un débat politique et technique opposant diverses parties dans lequel les lobbies industriels jouent un rôle de premier plan. Des débats au parlement européen ont ainsi eu lieu dans les années 2000, conclus par le maintien de la non-brevetabilité des logiciels « en tant que tels » en 2005.
Le brevet logiciel en Europe est encadré par l'office européen des brevets (OEB) selon la convention sur le brevet européen (CBE). Elle a été établie à l'origine à la Convention de Munich du 5 octobre 1973, mais c'est une révision (CBE 2000) qui s'applique depuis 2007. La brevetabilité du logiciel n'y est pas accepté « en tant que telle » d’après l'article 52. Du fait de la non-correspondance avec d'autres acceptations comme celle des États-Unis ou du Japon le sujet a fait l'objet de débats, en particulier au Parlement européen jusqu'en 2005 où la modification de cette exception a été définitivement rejetée.
La cartographie logicielle est la représentation d'une information statique ou dynamique d'un programme informatique et de ses processus à l'aide d'une carte en 2D ou 3D. Elle constitue un concept fondamental et un outil de visualisation et d'analyse logicielle. Les objectifs de la cartographie logicielle sont : l'analyse de la qualité du code source ; l'évaluation de l'activité de l'équipe et de la progression du développement du logiciel ; l'amélioration de l'efficacité de l'ingénierie logicielle dans le respect des processus de développement et de maintenance logicielle.
Cloudant est un produit de type "cloud computing" vendu par IBM. Il s'agit d'une base de données non-relationnelle et distribuée, basée sur le projet CouchDB d'Apache, ainsi que sur le projet BigCouch.
La rétrocompatibilité, ou compatibilité descendante, est la compatibilité d'un produit vis-à-vis de ses anciennes versions ; la compatibilité ascendante est la compatibilité d'un produit vis-à-vis des versions plus récentes, voire encore en phase de conception. Pour appréhender ce vocabulaire, il faut imaginer ce produit sur une échelle de temps verticale, avec ses versions antérieures en dessous de lui et ses versions postérieures au-dessus de lui ; à partir du produit, l'accès aux versions plus anciennes se fait alors en descendant, et l'accès aux versions plus récentes en montant. Les problèmes de compatibilité, tant ascendante que descendante, sont fréquents en informatique en raison de la rapidité de l'évolution du matériel et des logiciels.
Un configurateur est un logiciel applicatif permettant de guider un consommateur acheteur vers la définition exacte ou approchée du bien ou du service censé répondre à ses besoins et attentes. Les configurateurs sont proposés en magasin, sur Internet, ou disponibles par l'intermédiaire d'un délégué commercial lors d'un contact client.
La distribution est l'action de mettre un logiciel à disposition des usagers. Les logiciels peuvent être distribués dans le commerce de détail, téléchargés en libre-service, incorporés dans un appareil informatique, ou mis en ligne sur un ordinateur du fournisseur. La distribution peut être gratuite, peut faire l'objet de commerce et peut être complétée par des contrats de service concernant par exemple de la maintenance ou de l'assistance technique. En plus de la distribution publique, des techniques permettent la distribution automatisée de logiciels aux employés d'une entreprise. Le droit d'auteur s'applique aux logiciels. La majorité des logiciels continuent d'appartenir à leur producteur après avoir été distribués. Un contrat de licence établi par le producteur fixe les droits accordés aux utilisateurs et les responsabilités du producteur. les conditions sous lesquelles un logiciel est distribué dépendent de choix stratégiques effectuées par leur producteur, en fonction de la nature du producteur (indépendant bénévole ou entreprise commerciale), de la fonctionnalité du logiciel, et de l'utilisateur cible. Selon les modalités de distribution, l'utilisateur aura plus ou moins de possibilités de collaborer à l'amélioration et la correction du logiciel. Dans le procédé classique de développement d'un logiciel, celui-ci est distribué après avoir été programmé et vérifié. Il peut être distribué avant la fin des vérifications, et peut également être construit en plusieurs étapes donnant à chaque fois lieu à la distribution d'une version différente du logiciel. Le procédé de développement open source consiste à distribuer le logiciel avec le code source plus ou moins terminé, permettant au public de collaborer à sa programmation.
DocuSign est une société informatique créée en 2003 qui propose des technologies dans le domaine de la signature électronique (signature numérique) et de la gestion des transactions digitales, en vue de faciliter les échanges et validation électroniques de contrats et de documents. Son siège est basé à San Francisco. La filiale française de DocuSign est issue du rachat de la société OpenTrust, déjà présente sur le marché de la signature électronique en France, qui était un spécialiste de la sécurité numérique. La société, à l’origine de 3 brevets dans le domaine de la signature et de l’identité numérique, proposait déjà des certificats électroniques permettant de réaliser des transmissions de données sécurisées utilisées essentiellement par des groupes industriels, des gouvernements, des banques et des assurances.
DV1 Manager  est une application française de pilotage à distance (CAT) pour récepteur de technologie Radio Logiciel tel que l'AOR AR-DV1. C'est un logiciel propriétaire de Télépilotage et de transfert de fichiers (au format Microsoft Office Excel .XLS).
En informatique et en technologies de l'information, un fichier exécutable, parfois (par métonymie) un programme, ou simplement un exécutable est un fichier contenant un programme et identifié par le système d'exploitation en tant que tel. Le chargement d'un tel fichier entraîne la création d'un processus dans le système, et l'exécution du programme, par opposition au fichier de données qui doit d'abord être interprété par un programme pour prendre sens. Traditionnellement, le programme contenu dans un fichier exécutable contient lui-même des instructions codées dans un langage compréhensible par un processeur. Cependant, dans une acceptation plus générale, n'importe quel fichier contenant des instructions destinée à un interpréteur ou une machine virtuelle, comme des scripts ou du bytecode, peut aussi être considéré comme un exécutable.
Dans un système informatique, un firmware (ou micrologiciel, microcode, logiciel interne, logiciel embarqué ou encore microprogramme) est un programme intégré dans un matériel informatique (ordinateur, photocopieur, automate (API, APS), disque dur, routeur, appareil photo numérique, etc.) pour qu'il puisse fonctionner.
Un fork (terme anglais signifiant « fourche », « bifurcation », « embranchement » ) est un nouveau logiciel créé à partir du code source d'un logiciel existant lorsque les droits accordés par les auteurs le permettent : ils doivent autoriser l'utilisation, la modification et la redistribution du code source. C'est pour cette raison que les forks se produisent facilement dans le domaine des logiciels libres. En français québécois on emploie le terme de fourche. Un fork peut être bénéfique pour un projet donné lorsque sa gouvernance actuelle conduit à une impasse, sa reprise par un nouveau groupe pouvant le relancer. Il peut aussi être néfaste en provoquant un éparpillement des ressources. Un fork peut naître à la suite de divergences de points de vue ou d'objectifs parmi les développeurs, ou encore à la suite de conflits personnels (Ex : Iceweasel sous Debian est un fork de Mozilla Firefox). Les projets parents et dissidents peuvent avoir des rapports amicaux (fork amical) ou bien conflictuels (fork inamical). Quels que soient les rapports entre les deux projets, les licences de logiciels libres permettent l'emprunt de code d'un projet par l'autre. Ainsi, les différents BSD s'empruntent régulièrement du code car ils partagent la même licence. Une illustration de la régulation des projets libres par ce moyen est l'apparition d'au moins trois forks libres pour le projet SourceForge à l'issue de la « dérive de SourceForge » : Gforge Debian SF GNU Savannah Codendi et d'autres qui ne sont plus maintenus.
En informatique, un format de données est la façon dont est représenté (codé) un type de données, sous forme d'une suite de bits. Par commodité, on interprète cette suite de bits comme un nombre binaire, et on dit par raccourci que la donnée est représentée comme un nombre. Par exemple, le caractère C est généralement codé comme une suite dont 3 bits sont activés, ce que l'on écrit 0100 0011, soit 67 en décimal. Un format de données est ainsi une convention (éventuellement normalisée) utilisée pour représenter des données — des informations représentant un texte, une page, une image, un son, un fichier exécutable, etc. Lorsque ces données sont stockées dans un fichier, on parle de format de fichiers. Une telle convention permet d'échanger des données entre divers programmes informatiques ou logiciels, soit par une connexion directe, soit par l'intermédiaire d'un fichier. On appelle interopérabilité cette possibilité d'échanger des données entre différents logiciels.
GameRanger est un logiciel qui fournit un service internet destiné aux ordinateurs de type Macintosh (Mac OS et OS X) et PC sous Windows, développé par GameRanger Technologies. GameRanger permet de jouer en ligne à des jeux multijoueurs et propose également plusieurs fonctionnalités comme un chat et un système de communication. Il a été développé par Scott Kevill et a d'abord été disponible sur Macintosh en juillet 1999. La même année, GameRanger reçoit un award Best Internet Gaming Achievement de la part de Macworld Magazine. GameRanger est adapté pour Windows en 2008,. En 2015, GameRanger supporte au total plus de 700 jeux, démos jouables et extensions.
Gatling est un outil open-source de test de charge et de performance pour applications web. Il utilise les technologies Scala, Akka et Netty. La première version a été publiée le 13 Janvier 2012. En 2015, le créateur de Gatling a créé la société Gatling Corp, dédiée au développement de ce projet open-source. Selon le blog officiel de Gatling Corp, Gatling a été téléchargé plus de 800 000 fois (août 2017). En juin 2016, Gatling a officiellement présenté Gatling FrontLine, un Version Entreprise avec de nouvelles fonctionnalités. Gatling a été mentionné deux fois par ThoughtWorks dans son Technology Radar, en 2013 et 2014, évoquant notamment la possibilité de manipuler les tests de performance comme du code. La dernière version stable est Gatling 2.3. Elle a été publiée le 30 août 2017.
Knockin est une plateforme développée par la start-up française Hatis pour accompagner l'organisation des campagnes électorales. Elle permet de gérer un fichier de contacts militants, une cartographie électorale ainsi que des campagnes de porte-à-porte reliées à une application smartphone. Similaire aux outils américains Organizer ou Votebuilder (en) utilisé par Hillary Clinton en 2016, considérés comme une « révolution des méthodes de campagne », Knockin reçoit le soutien de la CNIL, le 9 février 2017, qui reconnaît que l'outil satisfait les exigences de la loi Informatique et Libertés et n'effectue aucune recoupe de données sensibles. Surnommé le « Pokémon Go des sarkozystes » en référence à l'application de réalité augmentée développée par Niantic ayant fait le buzz à l'été 2016, l'outil Knockin, développé par Paul Hatte, aussi membre de l'équipe de campagne de Nicolas Sarkozy, fut en particulier utilisé par l'ex-président dans le cadre de sa campagne pour la primaire de la droite et du centre en France.
Liste de services cachés du Portail Tor décrit aussi des services Darknet. Des services hors fonction sont compris aussi.
Un logiciel auto-école est un logiciel de gestion conçu pour les auto-écoles.
On appelle logiciel d’occasion un logiciel qui a déjà été utilisé et qui est revendu après sa première utilisation. Dans ce contexte, c’est plus le droit d’exploitation d’un programme que le logiciel en lui-même qui importe. Si la faillite demeure une des raisons de cession des licences, tout comme les restructurations et les réductions d’effectifs, il peut s’agir également de changements de systèmes, du déploiement d’un nouveau logiciel, etc. L’objectif de l’entreprise concédante peut également être de vendre les excédents de licences en volume. Outre les ventes directes, entre entreprises, il existe également des distributeurs spécialisés qui proposent ces logiciels à la revente. Les entreprises sont intéressées par les logiciels d’occasion aussi bien pour des raisons financières (ils sont généralement meilleur marché que les « neufs ») ou parce que les éditeurs ne proposent plus les versions certes toujours utilisées, mais dépassées. Le commerce des logiciels d’occasion a également lieu entre des personnes privées. Bien que juridiquement infondé, la revente des logiciels est souvent interdite dans les contrats de logiciels, aussi bien vis-à-vis des particuliers que les sociétés.
En technologies de l'information et de la communication, un logiciel de gestion de la formation (en anglais Training Resource Management System) est un système de gestion d’apprentissage à destination du monde professionnel. S’adressant aux responsables formation d’entreprises et aux organismes de formation professionnelle, ce genre de système propose des solutions dites de back-office incluant la gestion logistique, financière, et administrative des activités de formation,.
Un logiciel d'écriture de scénario est un logiciel de traitement de texte spécialisé dans l'écriture des scénarios. Le besoin d'un tel type de logiciel spécifique est lié aux particularités du format standard des scénarios qui ne sont pas ou mal pris en charge par les traitements de texte classiques. Notamment, la gestion des sauts de page imposés par le format des scénarios est difficile à implémenter.
Un logiciel tiers est un logiciel qui peut être ajouté à un autre logiciel, bien qu'il n'ait pas été développé par l'éditeur du logiciel auquel il est ajouté. Le logiciel tiers n'engage pas la responsabilité de l'éditeur du logiciel principal. Dans le cas d'un système d'exploitation tel que Windows XP, Vista ou Seven, il existe des applications installées par défaut telles que le lecteur Windows Média ou encore Internet Explorer. Lorsque ces applications ne vous conviennent pas, il existe ce qu'on appelle des logiciels tiers que l'on peut traduire par logiciels alternatifs. Vous trouverez ces différents logiciels sur Internet ou dans le commerce sous forme de cd.  Portail de l’informatique
Un logiciel utilitaire (aussi appelé programme utilitaire ou simplement utilitaire) est un logiciel conçu pour analyser, configurer, optimiser ou entretenir une pièce d'équipement informatique, un système d'exploitation, un logiciel ou les informations enregistrées sur un support informatique. Un logiciel utilitaire est utilisé pour gérer un système informatique contrairement aux logiciels d'application, qui visent à exécuter des tâches qui profitent aux utilisateurs du système informatique. Un certain nombre de logiciels utilitaires sont généralement livrés avec les systèmes d'exploitation. Cependant, ces programmes ne sont pas considérés comme faisant partie du système d'exploitation. Bien que les logiciels utilitaires livrés avec les systèmes d'exploitation soient de plus en plus complets et sophistiqués, les utilisateurs installent souvent des logiciels utilitaires tiers en remplacement ou en complément de ceux fournis avec le système d'exploitation.
La logithèque est une bibliothèque de logiciels. À l'origine, ce terme désignait une collection de logiciels présente chez un particulier ou disponible pour un ordinateur personnel. Le terme a été popularisé sur le World Wide Web francophone par le site Léa-Linux en l'an 2000 pour sa liste de logiciels pour GNU/Linux. Depuis la démocratisation de l'accès à internet à haut débit, le terme désigne également le téléchargement de paquets permettant d'installer des logiciels. En termes techniques, ce type de logithèque est un Serveur HTTP qui fonctionne habituellement sous le service Apache avec beaucoup d'espace disque, sur lequel sont copiés des logiciels dont la licence permet la redistribution. Ainsi, la Logithèque Ubuntu est le nom français du logiciel Ubuntu Software Centre de Canonical, installé par défaut dans les distributions GNU/Linux Ubuntu et variantes telles que Xubuntu, qui permet de choisir et installer très facilement des milliers d'applications. Il est également possible d'y accéder via Internet.
Un programme potentiellement indésirable (PPI ; aussi appelé logiciel potentiellement indésirable ou LPI ; en anglais, Potentially unwanted program ou PUP, ou bien encore Potentially unwanted application ou PUA) est un programme qu'un utilisateur peut percevoir comme indésirable. Il s'agit d'un terme utilisé par les produits de sécurité et de contrôle parental pour désigner subjectivement certains logiciels. Un tel logiciel peut compromettre la confidentialité ou affaiblir la sécurité d'un ordinateur. Les entreprises incluent souvent dans un programme souhaité des fonctions non désirées. Lors du téléchargement d'un programme désiré, elles peuvent aussi proposer une application indésirable et, dans certains cas, sans fournir une option claire de refus de l'application indésirable. Les entreprises antivirus définissent comme programmes potentiellement indésirables des programmes qui affichent de la publicité intrusive ; surveillent les sujets consultés sur l'Internet par l'utilisateur pour vendre cette information à des annonceurs ; injectent leur propre publicité dans des pages web qu'un utilisateur regarde,,,. Ces pratiques sont largement considérées comme contraires à l'éthique parce qu'elles violent les intérêts de sécurité des utilisateurs sans leur consentement éclairé. Un nombre croissant de projets de logiciels libres ont exprimé leur consternation à l'égard de sites web tiers qui groupent le téléchargement de leurs logiciels libres avec des programmes potentiellement indésirables, sans les informer et sans leur consentement. Presque tous les sites tiers de téléchargement gratuit font de tels groupements. Les développeurs de logiciels et les experts en sécurité recommandent de toujours télécharger un logiciel du site officiel du projet, d'un intermédiaire de confiance ou d'un magasin d'applications.
Un nettoyeur de registre (aussi appelé correcteur de registre ; en anglais, registry cleaner) est une classe de logiciel utilitaire tiers conçu pour le système d'exploitation Microsoft Windows, dont le but est de supprimer des éléments inutiles de la base de registre Windows. L'utilisation de nettoyeurs de registre n'est pas recommandée par Microsoft, mais les fournisseurs de nettoyeurs affirment qu'ils sont utiles pour réparer les incohérences découlant des modifications aux applications, en particulier pour applications utilisant la technologie Component Object Model (COM). L'efficacité des nettoyeurs de registre est un sujet controversé et les experts sont en désaccord sur leurs avantages. De plus, l'utilité des nettoyeurs est entachée par le fait que des logiciels malveillants et des scarewares sont souvent associés à des utilitaires de ce type.
La New Network Architecture désigne l'un des premiers ensemble de produits pour le transport des données entre ordinateurs à avoir été conçus à des fins de téléinformatique, pour le compte de la Compagnie internationale pour l'informatique dans les années 1970.
Nokia Sensor était dans les années 2000 une application mobile développée par Nokia et intégrée dans plusieurs téléphones de la marque. Il permet de créer une sorte de mini "site Web" appelé "album" ou "folio" destiné à être partagé avec les plus proches utilisateurs de l'application. La technologie utilisée pour l'échange des données entre les différents Sensor est Bluetooth. Les albums sont composés de quelques pages (souvent quatre) dont une page d'accueil, avec le titre du folio, un message de bienvenue et une image, ainsi qu'un livre d'or. Les autres pages peuvent contenir plusieurs champs (titre, contenu et éventuellement image). On peut imaginer utiliser l'application comme un blog simplifié, bien que la taille des champs est limitée.
ODM ou Operational Decison Manager est une solution logicielle développée par l'entreprise IBM dans le domaine des Business Rules Management System (BRMS). Il s'agit d'un système permettant de prendre des décisions automatiquement à l'aide de règles métiers.
OpenPass est une méthodologie pour enregistrer des données sur une carte RFID pour un système de contrôle d'accès, par des logiciels propriétaires réalisés par différents fournisseurs. OpenPass est réalisé selon la licence GPL
Petit computer est un logiciel de programmation payant téléchargeable développé par Gamebridge sorti sur la console DS de Nintendo le 25 juillet 2013, cependant, il n'est plus disponible depuis février 2015. Le langage de programmation qu'utilise ce dernier est une variante du BASIC. Il y a la possibilité de transformer ses programmes en QR codes, afin que d'autres personnes détenant le logiciel puissent les scanner. Ils auront alors à leur tour le jeu fait par la communauté et seront libres d'y jouer (mais aussi de le modifier, etc.). Un livre a également été écrit pour apprendre à créer un jeu (Je programme ma DS avec petit computer, de Grégoire et Marie-Noëlle de Woot).
En sciences, on appelle réduction de données la transformation de données issues d'observations, dites données brutes, en données qui peuvent servir directement pour une analyse, dites données réduites ou produits de la réduction. Aujourd'hui, on utilise souvent un logiciel de réduction de données pour ce faire.
Transiris est le premier logiciel de routage pour le transport des données entre ordinateurs à avoir été installé au sein même du système d'exploitation dans une logique de réseau et de partage des données appelée téléinformatique. Fonctionnalité importante de l'Iris 80 de la Compagnie internationale pour l'informatique , il a ensuite donné naissance à Datanet.
Un utilitaire de compression de disque est un logiciel utilitaire qui augmente la quantité d'informations pouvant être stockées sur un disque dur. Contrairement à un utilitaire de compression de fichier, qui compresse uniquement les fichiers spécifiés et qui oblige l'utilisateur à désigner les fichiers à compresser - un utilitaire de compression de disque fonctionne automatiquement sans que l'utilisateur soit conscient de son existence. Lorsque des informations doivent être stockées sur le disque dur, l'utilitaire compresse ces informations. Lorsque les informations doivent être lues, l'utilitaire les décompresse. Un utilitaire de compression de disque remplace les routines de lecture et d'écriture sur disque du système d'exploitation standard. Les utilitaires de compression de disque étaient populaires surtout au début des années 1990, lorsque les disques durs des micro-ordinateurs étaient encore relativement petits (20 à 80 mégaoctets). Les disques durs étaient également assez coûteux à l'époque, environ 10 dollars américains par mégaoctet. Les utilitaires de compression de disque permettaient alors d'augmenter la capacité d'un disque dur à faible coût. Un bon utilitaire de compression de disque pouvait, en moyenne, doubler l'espace disponible avec une perte de vitesse négligeable. La compression du disque est tombée en désuétude à la fin des années 1990 lorsque les progrès de la technologie et de la fabrication du disque dur ont entraîné une augmentation des capacités et une baisse des prix.
Le terme racketiciel désigne un logiciel vendu de force avec du matériel informatique. Cette vente liée qui est illégale dans la plupart des pays, est malgré tout souvent pratiquée, au détriment de la liberté de choix des consommateurs.
Un logiciel de Video Management System (VMS) est un système de gestion vidéo. Il permet de configurer, contrôler et gérer les équipements de vidéosurveillance, la gestion des flux de vidéo-protection de l’affichage à l'exploitation. Le logiciel de gestion vidéo (VMS) permet d'enregistrer et d'afficher la vidéo en direct à partir de plusieurs caméras de surveillance que ce soit des caméras IP ou analogiques avec une alarme codeur-moniteur, caméras de contrôle et permet d'afficher en live et récupérer des enregistrements archive. Certains équipements de vidéosurveillance sont disponibles avec leur propre VMS pré-intégré, Mais il est également possible de choisir son logiciel. Les paramètres sont à prendre en compte pour dimensionner l'installation : la taille de l'installation (nombre de caméras) pour déterminer le nombre de licences à acquérir. les fonctionnalités nécessaires, reconnaissance faciale, comptage, tracking, etc. (ces fonctionnalités peuvent faire partie intégrante de la caméra. Les traitements analytiques comprennent les fonctionnalités suivantes : segmentation d’objets en mouvement suivi et classification d’objets détection et reconnaissance de plaques d’immatriculation reconnaissance de comportements activation de caméras motorisées (auto-tracking) détection d’intrusion objet abandonné comptage d’objets
ZeroMQ (également écrit ØMQ, 0MQ ou ZMQ) est une bibliothèque de messagerie asynchrone haute performance, destinée à être utilisée dans des applications distribuées ou concurrentes. Il fournit une file d'attente de messages, mais contrairement au Message-oriented middleware, un système ZeroMQ peut fonctionner sans message broker. L'API de la bibliothèque est conçue pour ressembler à celle des sockets BSD. ZeroMQ est développé par une importante communauté de contributeurs. Il a été fondé par iMatix qui détient le nom de domaine et la marque. Il existe des liaisons tierces pour de nombreux langages de programmation populaires.
Le Clarifying Lawful Overseas Use of Data Act ou CLOUD Act (H.R. 4943) est une loi fédérale des États-Unis de 2018 sur la surveillance des données personnelles, notamment dans le Cloud. Elle modifie principalement le Stored Communications Act (en) (SCA) de 1986 en permettant aux forces de l'ordre (fédérales ou locales, y compris municipales ) de contraindre les fournisseurs de services américains, par mandat ou assignation, à fournir les données demandées stockées sur des serveurs, qu'ils soient situés aux États-Unis ou dans des pays étrangers. Critiquée par certaines associations de défense de la vie privée, cette loi permet notamment aux forces de l'ordre américaines d'obtenir les données personnelles d'un individu sans que celui-ci en soit informé, ni que son pays de résidence ne le soit, ni que le pays où sont stockées ces données ne le soit .
Code et autres lois du cyberespace est un livre de Lawrence Lessig paru en 1999. Il est connu en anglais sous le titre de Code: Version 2.0 (en) (version remaniée de Code and Other Laws of Cyberspace). Il traite de la différence entre les lois dans le monde physique et dans le monde virtuel, sujet de nombreux débats depuis l'apparition de l'Internet. Code: Version 2.0 (en) (en) existe en deux versions.
La Convention sur la cybercriminalité, aussi connue comme la Convention de Budapest sur la cybercriminalité ou Convention de Budapest, est le premier traité international qui tente d'aborder les crimes informatiques et les crimes dans Internet y compris la pornographie infantile, l'atteinte au droit d'auteur et la discours de haine en harmonisant certaines lois nationales, en améliorant les techniques d'enquêtes et en augmentant la coopération entre les nations et la protection adéquate des droits de l'homme et des libertés en application de la Convention de sauvegarde des droits de l'homme et des libertés, Pacte international relatif aux droits civils et politiques ou d'autre instruments internationaux relatifs aux droits de l'homme, et qui doit intégrer le principe de proportionnalité. Il a été rédigé par le Conseil de l'Europe avec la participation active d'observateurs délégués du Canada, du Japon et de la Chine. À la fin d'août 2011, plusieurs pays européens avaient signé le traité. Et aujourd'hui, en avril 2018, 57 pays, y compris Afrique du Sud, Canada, Japon, Philippines et États-Unis ont ratifié la convention. La loi no 2005-493 du 19 mai 2005 autorisant l'approbation de la convention sur la cybercriminalité et du protocole additionnel à cette convention relatif à l'incrimination d'actes de nature raciste et xénophobe commis par le biais de systèmes informatiques a été publiée au Journal Officiel le 20 mai 2005. En juillet 2017, 29 pays avaient ratifié le protocole y compris la France et l'Allemagne.
La DDL Intercettazioni est un projet de loi soumis au parlement italien en 2008. D'abord lié à l'écoute téléphonique, l'évolution du projet soulève la controverse dans le milieu médiatique italien depuis le début des années 2010.
La Digital Economy Act 2010 (c. 24), est une loi du Parlement du Royaume-Uni qui régule les médias numériques. Approuvée par sanction royale le 12 avril 2010, elle est entrée en vigueur le 12 juin de la même année.
La directive du 12 juillet 2002 sur la protection de la vie privée dans le secteur des communications électroniques (2002/58) (en anglais, Directive on Privacy and Electronic Communications) est une directive européenne qui vise à protéger de façon spécifique la vie privée sur Internet. Elle couvre les aspects laissés de côté par la directive de 1995 sur la protection des données personnelles (1995/46, dite Data Protection Directive, laquelle reprend la plupart des dispositions de la loi Informatique et libertés de 1978). Cette nouvelle directive ne couvre toutefois pas tout ce qui a trait à la sécurité nationale et au droit pénal.
La loi no 2011-267 du 14 mars 2011 d'orientation et de programmation pour la performance de la sécurité intérieure est une loi française qui concerne la gestion de la police et de la gendarmerie pour la période 2009-2013. Ce texte, appelé LOPPSI 2 en référence à la LOPSI de 2002 qui avait le même objet et porte presque le même nom mais sans « performance », a été élaboré par les ministres de l'Intérieur Michèle Alliot-Marie puis Brice Hortefeux (UMP). Le texte concerne en particulier la lutte contre la criminalité générale, la récidive, la délinquance routière, la « cyber-pédopornographie », l'instauration d'un couvre-feu pour les mineurs. Il donne également de nouveaux pouvoirs à la police et prévoit d'en déléguer aux polices municipales et aux entreprises de sécurité privée. Le Conseil constitutionnel a invalidé, par sa décision du 10 mars 2011, 13 des 142 dispositions du texte adopté par le Parlement, .
La loi d'orientation et de programmation pour la sécurité intérieure ou LOPSI est une loi française, promulguée le 29 août 2002, relative à la sécurité intérieure. Elle permet de recourir d'une manière générale à des procédures d'un partenariat public-privé (au sens du droit européen) allégées (la mise en concurrence n'est pas obligatoire) sans nécessité d'une qualité d'urgence ou de complexité, à l'opposé d'un contrat de partenariat.
La loi no 2006-64 du 23 janvier 2006 relative à la lutte contre le terrorisme a été votée en France sous l'impulsion de Nicolas Sarkozy, alors ministre de l'Intérieur du gouvernement Villepin. Cette loi est controversée, notamment en raison de l'article 6, qui impose aux opérateurs télécoms, aux fournisseurs d'accès (FAI), mais aussi à tout établissement public proposant un accès internet, comme les cybercafés, de conserver les données de connexion (logs) pendant un an. Outre cette disposition, la loi : a amolli les conditions de contrôle de mise en place de caméras de vidéosurveillance, ; a étendu la possibilité d'effectuer des contrôles d'identité dans les trains internationaux ; contraint par l'art. 7 les compagnies ferroviaires, aériennes, maritimes de transmettre les données APIS (pour Advance Passenger Information System (en)) concernant les passagers à la police et à la gendarmerie, données qui peuvent être comparées avec le fichier des personnes recherchées (FPR) et le système d'information Schengen (SIS). Un fichier des passagers aériens, pour lequel le droit d'opposition prévu par la loi informatique et libertés de 1978 a été dénié, a par la suite été créé,; a autorisé la lecture automatique de plaques minéralogiques en tout point du territoire, clichés qui peuvent être croisés avec le fichier des véhicules volés ainsi qu'avec le système d'information Schengen ; a augmenté la durée maximale de garde à vue de 4 à 6 jours en ce qui concerne les « suspects » d'actes terroristes (en sachant que tant qu'une personne n'a pas été condamnée, le principe de la présomption d'innocence s'applique en État de droit) ; a augmenté le délai durant lequel une dénaturalisation (« déchéance de nationalité », ou dénationalisation) est possible pour un condamné après sa naturalisation passe de 10 à 15 ans; modifié par son art. 13, l'article 30 de la loi Informatique et libertés, en limitant les informations fournies à la CNIL concernant « les traitements intéressant la sûreté de l’État, la défense ou la sécurité publique » ; a ajouté l'art. L34-1 dans le Code des postes et communications électroniques concernant la conservation des données informatiques.
La loi no 2009-669 du 12 juin 2009 favorisant la diffusion et la protection de la création sur internet, dite loi Hadopi 1 ou loi création et internet est une loi française qui vise principalement à mettre un terme aux partages de fichiers en pair à pair lorsque ces partages se font en infraction avec la législation sur les droits d'auteur. Cette loi comporte six chapitres et deux volets : le volet de riposte graduée et le volet d'amélioration de l'offre licite. La récidive est punie de manière croissante et le législateur parle de « riposte graduée ». Cette loi a créé la Haute autorité pour la diffusion des œuvres et la protection des droits sur Internet (Hadopi), organisme indépendant français de régulation, puis complétée par la loi Hadopi 2 du 31 décembre 2009.
Le projet de loi russe no 89417-6 (en russe : Законопроект № 89417-6) est une proposition de loi fédérale russe prévoyant l'établissement d'une liste noire de sites web contenant des informations interdites de diffusion en Russie.
La loi Godfrain du 5 janvier 1988, ou Loi no 88-19 du 5 janvier 1988 relative à la fraude informatique, est la première loi française réprimant les actes de criminalité informatique et de piratage. Nommée d'après le député RPR Jacques Godfrain, c'est l'une des lois pionnières concernant le droit des NTIC, après, notamment, la loi Informatique et libertés de 1978, qui introduit la notion de système de traitement automatisé de données (STAD) et prévoit plusieurs dispositions corrélatives de la loi Godfrain (notamment concernant les obligations du responsable du traitement quant à la garantie de la sécurité des données - art. 34 loi de 1978).
La loi n°78-17 du 6 janvier 1978 relative à l'informatique, aux fichiers et aux libertés, plus connue sous le nom de loi informatique et libertés, est une loi française qui réglemente la liberté de traitement des données personnelles, c'est-à-dire la liberté de ficher les personnes humaines. Cette liberté étant indissociable de l'activité informatique, cette loi réglemente donc les conséquences potentiellement antisociales de l'activité informatique.
La loi no 5651 aussi appelée loi sur la réglementation des émissions via Internet et la prévention des crimes commis par le biais de telles émissions est la loi turque régissant le droit de l'internet dans ce pays. Adoptée en 2007, elle a fait l'objet au cours de son existence de plusieurs amendements controversés en matière de droit à la liberté d'expression, notamment lorsque ceux-ci visent à exercer la censure sur des sites populaires. Au 20 mars 2016, 67 818 sites auraient été bloqués, dont YouTube, Vimeo, Twitter, Dailymotion, Blogger, WordPress.
La loi pour la confiance dans l'économie numérique, no 2004-575 du 21 juin 2004, abrégée sous le sigle LCEN, est une loi française sur le droit de l'Internet, transposant la directive européenne 2000/31/CE du 8 juin 2000 sur le commerce électronique et certaines dispositions de la directive du 12 juillet 2002 sur la protection de la vie privée dans le secteur des communications électroniques. La transposition de la directive 2000/31 aurait dû être effective le 17 janvier 2002 mais ne l'aura été que le 21 juin 2004. Le Journal officiel des communautés européennes indique la transposition de la « Directive relative à certains aspects juridiques des services de la société de l'information, et notamment du commerce électronique, dans le marché intérieur ». Elle visait à promouvoir le commerce électronique au sein de l'Union européenne, suivant en cela la logique des traités dont le crédo est « un espace sans frontière intérieure dans lequel la libre circulation des marchandises et des services ainsi que la liberté d'établissement sont assurées » tel que préconisé par l'article 14-2 du Traité instituant la Communauté européenne. Les rapporteurs de la LCEN furent Jean Dionis du Séjour (UDF) à l'Assemblée nationale et Pierre Hérisson (UMP) ainsi que Bruno Sido (UMP) au Sénat.
La loi pour une République numérique (abr. loi numérique) est une loi française initialement proposée par la secrétaire d'État au numérique Axelle Lemaire et promulguée le 7 octobre 2016. Par cette loi, l'objectif du gouvernement est double : « donner une longueur d'avance à la France dans le domaine du numérique en favorisant une politique d'ouverture des données et des connaissances » et « adopter une approche progressiste du numérique, qui s'appuie sur les individus, pour renforcer leur pouvoir d'agir et leurs droits dans le monde numérique ». Pour ce faire, la loi s'organise autour de trois axes : la circulation des données et du savoir, la protection des individus dans la société du numérique et l'accès au numérique pour tous. La discussion a démarré par une consultation publique en ligne, jusqu'au 18 octobre 2015, puis, enrichie de certaines propositions des internautes, la loi a été débattue puis votée à l'Assemblée nationale du 19 au 26 janvier 2016,,. Elle fait suite à la concertation « Ambition numérique » pilotée par Benoit Thieulin et Yann Bonnet dans le cadre des travaux du Conseil national du numérique,. Le projet de loi voté par les députés en première lecture introduit notamment l’ouverture par défaut des données publiques, la neutralité du net, une obligation de loyauté des plateformes en ligne, ainsi qu’une protection accrue pour les données personnelles des usagers du net. La loi pour une République numérique prévoit également les conditions d’un Internet accessible au plus grand nombre, au travers de l’accélération de la couverture du territoire en très haut débit et en téléphonie mobile, de mesures pour un meilleur accès des personnes handicapées aux services en ligne, et de la création d’un droit au maintien de la connexion internet en cas d’impayé pour les foyers en difficulté. C'est une loi majeure pour l'informatique. Elle succède à la Loi pour la confiance dans l'économie numérique de 2004.
La loi n° 2009-1311 relative à la protection pénale de la propriété littéraire et artistique sur internet dite loi HADOPI 2, est une loi française complémentaire à la loi favorisant la diffusion et la protection de la création sur internet, dite loi HADOPI. Elle a pour but de réintroduire le volet répressif de la première loi qui a été déclaré partiellement non conforme à la constitution par le Conseil constitutionnel.
La loi relative au droit d’auteur et aux droits voisins dans la société de l’information, dite loi DADVSI, est une loi française issue de la transposition en droit français de la directive européenne 2001/29/CE sur l'harmonisation de certains aspects du droit d'auteur et des droits voisins dans la société de l'information. Ce texte a été adopté par l'Assemblée nationale et le Sénat le 30 juin 2006, avant d'être examiné par le Conseil constitutionnel qui en a supprimé certaines dispositions. Le texte, publié au Journal officiel le 3 août 2006, prévoit des amendes d'un montant de 300 000 euros ainsi que 3 ans de prison pour toute personne éditant un logiciel manifestement destiné à la mise à disposition du public non autorisée d'œuvres ou d'objets protégés, et jusqu'à 6 mois de prison et 30 000 euros d'amende pour toute personne diffusant ou facilitant la diffusion d'un logiciel permettant de casser les mesures techniques de protection (DRM, pour Digital Rights Management) qui selon ses défenseurs visent à empêcher la contrefaçon. Le projet de « licence globale », prévu en décembre 2005, n'a pas été retenu (mais reste au programme de plusieurs partis politiques), et le droit à la copie privée limité par les dispositifs DRM. La loi est officiellement applicable en France, certaines dispositions devant être précisées par les décrets d'application. À cette loi ont fait suite sur le même sujet le rapport Olivennes et le projet de loi Hadopi. Ces différents textes régissent également le champ d'application de la copie privée, c'est-à-dire le droit à tout usager de procéder à la copie, l'enregistrement, la duplication et la sauvegarde pour strict usage personnel, des œuvres ou documents auquel il a légalement accès (à l'exclusion des supports, émissions ou fichiers contrefaits).
La loi relative au renseignement est une loi française, examinée par le Parlement à partir du 19 mars 2015 et promulguée le 24 juillet 2015. Visant à renforcer le cadre juridique national du renseignement en France, elle prévoit la mise en place de plusieurs mesures controversées sur le plan des atteintes à la vie privée, telles que l’installation chez les opérateurs de télécommunications de dispositifs, surnommés « boîtes noires », visant à détecter les comportements suspects à partir des données de connexion ; mais aussi des dispositions sur l’utilisation de mécanismes d’écoute, logiciels espions ou encore IMSI-catchers. Elle remplace la Commission nationale de contrôle des interceptions de sécurité (CNCIS) par une Commission nationale de contrôle des techniques de renseignement (CNCTR).
La loi no 2010-476 du 12 mai 2010 relative à l'ouverture à la concurrence et à la régulation du secteur des jeux d'argent et de hasard en ligne est une loi française qui ouvre le marché des jeux en ligne à la concurrence.
La loi sur la sécurité quotidienne (ou LSQ, ou loi Vaillant) est une loi française sur la sécurité civile votée le 15 novembre 2001 sur proposition du gouvernement Jospin (et de son ministre de l'intérieur Daniel Vaillant) deux mois après les attentats du 11 septembre. Il s'agit d'un « paquet législatif » regroupant des textes portant sur divers moyens de lutte contre le terrorisme, les trafics (notamment d'armes) et les nuisances sociales et incivilités. La LSQ comporte aussi un volet monétaire et financier portant sur la sécurité des moyens de paiement. En plusieurs endroits, la LSQ établit les outils de luttes contre les activités utilisant les nouvelles technologies. Elle crée par ailleurs l'Institut national de police scientifique (art. 58) et rend passible de prison le fait de refuser un prélèvement ADN, étendant par ailleurs l'étendue du FNAEG (art. 56).
PROTECT IP Act (abrégé PIPA, titre au long Preventing Real Online Threats to Economic Creativity and Theft of Intellectual Property Act of 2011, littéralement Loi de 2011 sur la prévention des menaces en ligne réelles sur la créativité économique et le vol de la propriété intellectuelle, aussi connue sous le nom de S. 968), est un projet de loi américain proposé au Sénat le 12 mai 2011 par le sénateur démocrate Patrick Leahy soutenu par onze co-sponsors.  
Le Stop Online Piracy Act (SOPA), aussi connu sous le nom de H.R.3261, est une proposition de loi déposée à la Chambre des représentants des États-Unis le 26 octobre 2011 par le représentant républicain Lamar S. Smith mais reportée sine die depuis le 20 janvier 2012. Ce projet de loi vise à élargir les capacités d'application du droit d'auteur et des ayants droit pour lutter contre sa violation en ligne et les contrefaçons. Examiné par la commission des affaires juridiques de la Chambre des représentants (en) à la mi-janvier 2012, il s'inscrit dans la lignée d'une disposition antérieure, le PRO-IP Act de 2008. Une proposition similaire a été déposée devant le Sénat, sous le nom de PROTECT IP Act. Le SOPA prévoit une série de mesures à l'encontre des sites contrevenants. Les pénalités prévues incluent notamment la suspension des revenus publicitaires et des transactions en provenance de services comme PayPal, l'interruption du référencement sur les moteurs de recherche, et le blocage de l'accès au site depuis les principaux opérateurs internet. Le SOPA rend également délictuel le streaming de contenu protégés. Les initiateurs du texte affirment qu'il protégerait les secteurs économiques américains liés au copyright et donc nombre d'emplois. Ainsi leur paraît-il nécessaire de renforcer la législation existante (Digital Millennium Copyright Act de 1998, etc.), notamment à l'encontre des sites étrangers. Ses détracteurs la qualifient de « censure numérique ». Elle malmènerait l'ensemble d'Internet et menacerait la liberté d'expression. La commission des affaires juridiques de la Chambre des représentants a tenu une audition à son propos le 16 novembre 2011. Elle devait être présentée devant la Chambre des représentants le 15 décembre 2011 mais le débat a finalement été repoussé à janvier 2012. Entre-temps, de nombreux changements et amendements sont intervenus. Des manifestations, pétitions et boycotts de compagnies qui encouragent la législation ont été engagés et plusieurs sites Internet très fréquentés ont été temporairement coupés en signe de protestation. Le 20 janvier 2012, Lamar S. Smith a annoncé la suspension des travaux de la commission sur ce texte, dans l'attente d'un accord .
L'expression système de traitement automatisé de données (ou STAD) est une expression utilisée en droit français. Cette notion a été introduite en droit français par la loi Informatique et libertés de 1978, puis reprise par la loi Godfrain du 5 janvier 1988.  
La « Taxe Google » est le nom donné à une série de projets de refonte de la fiscalité du numérique, formulés par différents acteurs au cours du temps : personnalités politiques, maisons de disques et éditeurs de presse notamment. Ils visent dans leur ensemble à augmenter le niveau de contribution de grands groupes Internet (en particulier Google) à la fiscalité nationale, au nom du financement de la création de contenus en ligne.
HyperMemory (en français, « hyper mémoire ») est une technologie et marque déposée créée par la société ATI Radeon, filiale graphique du fondeur américain AMD et deuxième plus importante entreprise de son secteur. La technologie HyperMemory permet à un ordinateur, essentiellement portable, d’embarquer une mémoire graphique moindre sans grande perte de performances dans le traitement de l’information. Cette technologie se base sur le standard PCI Express développé par Intel corporation et permet un partage de mémoire vive entre la carte mère et la carte graphique. En clair, lorsque la mémoire graphique HyperMemory arrive à saturation, la dite technologie permet de déborder sur la mémoire vive de l’ordinateur permettant à la carte graphique d’avoir l’espace nécessaire pour mener à bien ses différentes tâches.
Le centre de recherche IBM à Almaden ou IBM Almaden Research Center est situé à San José en Californie. Il est l'un des neuf centres de recherche d'IBM. Il est spécialisé dans la recherche fondamendale et appliquée en informatique, les services, les systèmes de stockage, les sciences physiques, sciences des matériaux et la technologie. Le centre a ouvert en 1986. Il emploie approximativement 500 chercheurs dans des postes techniques, et plus de la moitié sont titulaire d'un Ph. D. Le laboratoire est le site de dix IBM Fellows (en), de dix IBM Distinguished Engineers, neuf IBM Master Inventors (en) and de dix-sept membres de la IBM Academy of Technology (en). Almaden est situé dans les collines au-dessus de la Silicon Valley. Le site a été choisi pour sa proximité avec l'université Stanford, l'université de Californie à Berkeley aet d'autres institutions académiques. La recherche du laboratoire est organisée en quatre départements : science et technologie, informatique, systèmes de stockage, et services à la recherche. Les contributions scientifiques du IBM Almaden center comportent divers découvertes comme les développements sur la photorésistance et l'effet de mirage quantique (en). Le film d'animation en Stop-Motion intitulé A Boy and His Atom, représente un garçon jouant avec un atome pendant une minute. Le film est précédé de 30 secondes de mise en situation sur la méthode de production. Afin d'être visible, les images ont été agrandies par un facteur de 100 millions. Ce court-métrage a été produit au Almaden Research Center.
Measurement Studio est une suite de bibliothèques développées par National Instruments pour apporter des fonctionnalités de test, mesure et instrumentation à Visual Studio. Ainsi, les contrôles graphiques, les fonctions d'analyse et de traitement du signal, et les communications avec les instruments de mesure sont disponibles pour les langages C sharp, C++ et Visual Basic. Ces librairies reprennent les fonctionnalités disponibles sous LabVIEW et LabWindows/CVI.
Pentium est une marque déposée par Intel en 1993 au moment du développement des ordinateurs individuels avec écran couleurs, capables de traiter rapidement photos, vidéos, fichiers sons et jeux vidéo pour PC. La marque a aussi vu Intel entrer sur le marché des processeurs pour serveurs Windows NT puis Linux, nouvelle stratégie qui a contribué à la très forte hausse des sociétés de technologie de la seconde partie des années 1990. Elle a servi à désigner plusieurs évolutions majeures (Pentium, Pentium Pro, Pentium 4) et mineures (Pentium MMX, Pentium II, Pentium III, Pentium M, Pentium D) de l'architecture de processeur x86. Les autres marques utilisées parallèlement à la marque Pentium pour les x86 sont les marques Celeron pour l'entrée de gamme et Xeon pour les stations de travail et serveurs informatiques. En 2006, la marque Core a remplacé Pentium pour le segment principal du marché, les Pentium Dual-Core sortis en 2007 étant des Core 2 d'entrée de gamme.
PSION est une marque de logiciels et de matériel informatique britannique. Elle a été créée par David Potter en 1980. Le sigle signifie Potter Scientific Inc. Or Nothing, car le nom Potter Scientific Inc. avait déjà été déposé par quelqu'un d'autre, mais dans l'esprit têtu de David Potter, ce serait "ça ou rien".
TestStand est un séquenceur de tests grandement configurable et très modulaire distribué par National Instruments  depuis 1999.
TM-XML (Trade Mark Extensible Markup Language) est un standard XML ouvert pour le domaine des Marques et pour l'échange des données associées entre les Offices de Propriétés Industrielles ou Intellectuelles ainsi qu'avec les partenaires ou les usagers.
Wi-Fi est une marque détenue par le consortium Wi-Fi Alliance. Un constructeur informatique ou un fabricant de smartphones produisant un produit compatible avec une des normes IEEE 802.11 doit demander à la Wi-Fi Alliance le droit d'apposer le nom Wi-Fi et le logo correspondant. En France, la société Wi-Fi Alliance a déposé à l'INPI les marques Wi-Fi (le 4 août 2000) et WiFi (le 26 février 2002) dans la seule classe 9 (matériel informatique et périphériques, à savoir, produits sans fil pour réseaux locaux). D'autres sociétés ont également déposé des marques composées des lettres WiFi, voire la marque Wi-Fi dans d'autres classes comme le 16 juin 2010 par la société britannique The Cloud Networks Limited.
Un matériel informatique (en anglais : hardware) est une pièce détachée d'un appareil informatique. Il s'agit d'un domaine important de l’informatique qui va de pair avec le logiciel (software ou firmware). Il peut y avoir des pièces situées à l'intérieur de l'appareil et qui sont indispensables à son fonctionnement, comme secondaires ou disposées à l'extérieur (les périphériques). Les pièces intérieures sont, la plupart du temps, montées sur des circuits imprimés. Différentes pièces sont construites par différentes marques et connectées entre elles. Le respect des normes par les différentes marques permet le fonctionnement de l'ensemble. Les pièces servent soit à recevoir des informations, à les envoyer, les échanger, les stocker ou les traiter. Toutes les opérations sont effectuées conformément aux instructions contenues dans les logiciels et aux manipulations des périphériques de l'interface homme-machine.
L'accélération matérielle consiste à confier une fonction spécifique effectuée par le processeur à un circuit intégré dédié qui effectuera cette fonction de façon plus efficace.
L’Advanced Configuration and Power Interface (ACPI) (soit en français « interface avancée de configuration et de gestion de l’énergie ») est une norme très largement répandue dans les ordinateurs personnels et codéveloppée par Hewlett-Packard, Intel, Microsoft, Phoenix et Toshiba. Le but de cette norme est de réduire la consommation d’énergie d’un ordinateur en mettant hors tension certains éléments : lecteurs CD-ROM, disques durs, écran, etc. Pour cela, une interface a été spécifiée qui permet au système d’exploitation d’envoyer des signaux à ces différents périphériques matériels (il faut que ces périphériques prennent en charge l’ACPI également). Cette interface permet aussi au matériel d’envoyer des signaux au système d’exploitation, par exemple lorsque l’utilisateur appuie sur le bouton de mise en route sur le clavier ou que le modem reçoit un appel. Cette norme est utilisée aussi bien sur les ordinateurs portables que sur les ordinateurs de bureau postérieurs à 1996. La toute dernière spécification, publiée en Janvier 2016, porte le numéro de version 6.1.
L'alimentation électrique par câble Ethernet (Power over Ethernet ou PoE en anglais), permet de faire passer une tension de 48 V (environ) (jusqu'à 13 watts de puissance électrique, voire plus), en plus des données à 100 Mbit/s ou 1 Gbit/s. Cette technologie est définie par la norme IEEE 802.3af, appartenant au standard IEEE 802.3 (Ethernet) ratifiée le 11 juin 2003 et publiée le 11 juillet 2003,. Cette technologie alloue deux paires (ou plus) sur les quatre paires que contient un câble UTP ou STP afin d'alimenter certains appareils d'un réseau Ethernet tels que des téléphones IP, des disques durs réseaux, des imprimantes, des caméras IP ou des points d'accès Wi-Fi. La norme IEEE 802.3at, également appelée PoE+, en étend les caractéristiques techniques.
L'ALPM (de l'anglais, Aggressive Link Power Management) est une technique d'économie d'énergie appliquée aux disques durs des ordinateurs. L'économie se fait en paramétrant un lien SATA sur une faible puissance en l'absence de signaux en Entrées-Sorties (E/S), ce qui correspond à un temps d'inactivité. L'ALPM règle automatiquement le lien SATA sur un état actif une fois que les requêtes d'E/S sont en file d'attente sur ce lien. Les économies d'énergie offertes par l'ALPM se font aux dépens de la latence de disque. Ainsi, l'ALPM ne devrait être utilisé que dans le cas où le système connaîtra de longues périodes d'inactivité d'E/S. L'ALPM est uniquement disponible sur les contrôleurs SATA qui utilisent les interfaces AHCI (de l'anglais, Advanced Host Controller Interface). Lorsque disponible, l'ALPM est activé par défaut. L'ALPM possède trois niveaux : min_power Ce mode définit le lien sur son état le plus bas (SLUMBER) lorsqu'il n'y a pas d'E/S sur le disque. Ce mode est utile lorsque de longues périodes d'inactivité sont à prévoir. medium_power Ce mode définit le lien sur le second état le plus bas (PARTIAL) lorsqu'il n'y a pas d'E/S sur le disque. Ce mode est conçu afin de permettre des transitions entre états d'alimentation (par exemple lorsqu'il y a des alternances entre une utilisation intense d'E/S et une période d'inactivité d'E/S) tout en limitant au maximum l'impact sur la performance. Le mode medium_power permet au lien de faire la transition entre les états PARTIAL et alimentation complète (c'est-à-dire "ACTIVE"), et ce en fonction de la charge. Remarquez qu'il n'est pas possible d'établir une transition de lien directement de PARTIAL à SLUMBER (et vice-versa) ; dans ce cas, les états d'alimentation ne peuvent pas passer de l'un à l'autre sans transiter par l'état ACTIVE au préalable. max_performance L'ALPM est désactivé, le lien ne peut pas entrer dans un état de faible alimentation lorsqu'il n'y a pas d'E/S sur le disque. Pour vérifier si un adaptateur hôte SATA prend en charge l'ALPM, sous système GNU/Linux, on peut vérifier que le fichier /sys/class/scsi_host/host*/link_power_management_policy existe bien. Pour changer les paramètres, on écrit simplement les valeurs décrites dans cette section sur ces fichiers. On vérifie les paramètres en visualisant le contenu des fichiers.
AN/FSQ-32 était un ordinateur construit par IBM (International Business Machines) en 1960 et 1961 pour le Strategic Air Command (SAC). IBM l'appela « 4020 ». Un seul Q-32 a été construit.
Un anneau de protection (ring en anglais) est l’un des niveaux de privilèges imposés par l’architecture d’un processeur. De nombreuses architectures modernes de processeurs (architectures parmi lesquelles on trouve le populaire Intel x86) incluent une certaine forme de protection en anneau, bien que les logiciels d’exploitation ne l’exploitent pas toujours entièrement. Les rings étaient parmi les concepts les plus révolutionnaires mis en œuvre par le système d’exploitation Multics, un prédécesseur fortement sécurisé de la famille actuelle des systèmes d’exploitation UNIX.
L’asynchronisme désigne le caractère de ce qui ne se passe pas à la même vitesse, que ce soit dans le temps ou dans la vitesse proprement dite, par opposition à un phénomène synchrone.
L’AC'97, ou AC97, pour Audio Codec ’97 est un codec audio d'Intel. Il peut être implémenté par une puce sur la carte mère d'un ordinateur, sur un modem, ou encore sur une carte son. Depuis 2004, il est remplacé chez Intel par le High Definition Audio (en).
Une baie (en anglais rack, râtelier) est une armoire très souvent métallique parfois à tiroirs mais généralement à glissières (ou rails) destinée à recevoir les boîtiers d'appareils, généralement électroniques, réseau ou informatiques de taille normalisée. Les baies permettent d'optimiser l'encombrement, d'assurer une cohérence au niveau du câblage (ségrégation des tensions à risque par exemple), de mutualiser les systèmes d'alimentations et de refroidissement entre les équipements. Elles permettent également une maintenance facilitée du fait de leur modularité. Elles sont systématiquement utilisés dans les centres de données, pour les superordinateurs et les serveurs. Différents secteurs industriels (télécommunications, ferroviaire, marine, etc.) possèdent leurs propres dimensions standardisés. Mais le standard des racks 19 pouces est réputé être le plus répandu : (48,26 cm) de large pour 17 pouces (43,18 cm) de profondeur.  L'unité de rack (unité U) est la mesure communément employée dans tous les standards pour définir la hauteur des équipements (1 U = 1,75 pouce = 44,45 mm). Pour ce qui est de la largeur ou la profondeur de l'équipement, l'unité de mesure peut varier d'un standard à l'autre en fonction du secteur industriel. Par exemple, la largeur des équipements est comptée en multiples d'Horizontal Pitch (1HP = 5.08 mm) dans le standard des racks 19 pouces ou en multiples de T (1T = 4TE = 4 x 5.08 mm = 20.32 mm) dans le ferroviaire. Les baies sont en général fermées au moyen d'une porte pour restreindre l'accès aux équipements qu'elles contiennent, les protéger et contrôler le flux d'air. Ces portes sont de différents types : en verre ou en métal ; en métal grillagé, permettant une meilleure ventilation entre la baie, le couloir chaud et le couloir froid.
Le Banana Pi est un ordinateur à carte unique fabriqué en Chine. Il supporte Android, Ubuntu, ArchLinux, Fedora, Debian et Gentoo Linux. Raspbian peut être supporté mais le CPU est conforme au modèle Debian. Le processeur utilisé est le Allwinner A20 SoC. Le Banana Pi peut être utilisé comme plateforme de développement de logiciel open-source ainsi que beaucoup d'autres applications,,,,. Banana Pi n'est pas en relation directe avec la Raspberry Pi Foundation. Cependant, ce SBC est bien plus puissant comparé au Raspberry Pi 1, et la disposition de la carte se rapproche fortement de celle-ci. Les différences notables sont entre autres sa taille (environ 10 % plus grande), son prix, et le fait que le processeur et la RAM soient montés sur l'arrière de la carte.
Un barebone est la base constituante d'une unité centrale d'ordinateur : boîtier, alimentation électrique (dont le bloc peut être externe), carte mère et éventuellement lecteur(s). Il faut y ajouter les pièces principales : microprocesseur, mémoire vive, disque dur ou SSD et, éventuellement en fonction de l'utilisation et des performances requises, une carte graphique, une carte son. Les barebones permettent ainsi de construire une configuration « à la carte », en fonction des besoins de l'utilisateur. Il existe aussi des barebones au format PC portables[réf. souhaitée]. Ils sont composés du boîtier (taille d'un portable « standard »), de l'alimentation électrique, de l'écran, de la carte mère, de la carte graphique (généralement performante), de la carte son et du clavier. Il faut donc leur ajouter un disque dur, le microprocesseur, la mémoire vive et un lecteur ou graveur de disque optique. La plupart des barebones à la mode depuis les années 2010 sont des bases pour mini PC (qui ont notamment été popularisés par la marque Shuttle), ce qui a introduit une confusion entre barebones et petit ordinateur. Il est désormais courant d'utiliser le terme barebone pour désigner un mini PC. Il faut typiquement y ajouter de la mémoire vive, un disque (dur ou SSD), un clavier et une souris.
Un barebook est l'équivalent d'un barebone pour les portables. Il s'agit d'un bloc formé d'un moniteur, d'un boîtier ABS et de la carte mère du portable, permettant de fabriquer son portable avec sa propre configuration, sans être limité par les choix d'une marque.  Portail de l’informatique
Un baybus est un système composé d'un boîtier que l'on place en façade d'un ordinateur dans un emplacement 3.5" ou 5.25". Cet appareil comporte des interrupteurs, parfois des rhéostats permettant de commander plusieurs ventilateurs se trouvant dans le boîtier de l'ordinateur. Cet appareil peut contrôler d'autres éléments, comme des néons par exemple. Les modèles les plus sophistiqués permettent non seulement de commander plusieurs ventilateurs, mais aussi d'informer l'utilisateur de la température du processeur, disque dur, chipset, ram… Il peut aussi afficher l'heure et la date et comporter un a plusieurs ports USB ainsi qu'un lecteur de carte mémoire.  Portail de l’informatique
Better Together over Ethernet (ou BToE) permet de contrôler le poste de téléphonie IP à travers le client de communication unifiée installé sur la station de travail.
Le bloc d’alimentation (power supply unit en anglais, souvent abrégé PSU), ou simplement l’alimentation, d'un PC est le matériel informatique l'alimentant. L’alimentation est chargée de convertir la tension électrique du secteur en différentes tensions continues TBT, compatibles avec les circuits électroniques de l’ordinateur.
NovaScale est une famille de serveurs ouverts et standard, développée par Bull depuis 2003. Les serveurs NovaScale supportent les applications Windows, Linux (Red Hat, SuSe) et, pour les plus puissants d’entre eux, les applications mainframe GCOS (7 et 8). Les serveurs NovaScale peuvent s'organiser avec des serveurs en format tour ou rack, des serveurs lames et des grands serveurs multi processeurs (SMP), autonomes ou au sein d’architectures « clusters » ou « grilles informatiques ». Les serveurs NovaScale sont ainsi utilisés par TERA-10 (installé au commissariat à l'énergie atomique (CEA) en France).
Un calculateur stochastique est un concept déjà ancien (pour la jeune histoire de l'informatique) et contemporain de recherches et applications développées depuis le début des années 1960 et jusqu'au milieu de la décennie 1970. Sa définition dans le Grand Larousse encyclopédique est : « Calculateur dans lequel l'information est codée par une probabilité »
Une carte à coprocesseur est une carte électronique disposant d'un processeur spécialisé (par exemple, pour les opérations graphiques).  Portail de l’informatique
Les cartes ARM sont des nano-ordinateurs basés sur des processeurs d’architecture ARM.
La carte mère est le cœur de tout ordinateur compatible PC. Elle est essentiellement composée de circuits imprimés et de ports de connexion qui assurent la liaison de tous les composants et périphériques propres à un micro-ordinateur (disques durs, mémoire vive, microprocesseur, cartes filles, etc.) afin qu'ils puissent être reconnus et configurés par le microprocesseur grâce au programme contenu dans le BIOS, au travers de la carte, afin d'assurer la configuration et le démarrage correct de tous les équipements.
La plateforme Centrino d'Intel est une plateforme informatique pour les ordinateurs portables lancée en mars 2003 et disparue le 7 janvier 2010. Soutenues lors de son lancement par une vaste campagne marketing évaluée à 300 millions de dollars, les ventes d’ordinateurs portables ont rapidement rattrapé celles des ordinateurs de bureau.[réf. souhaitée] Intel a également lancé une architecture analogue pour les PC de bureau appelée Viiv.
On nomme chip select une entrée de contrôle de nombreux circuits intégrés, tels que les puces mémoires, permettant d'activer ou désactiver le circuit. Quand elle est active, le composant peut être adressé ; quand elle ne l'est pas, le composant est dans un mode dit standby (au repos). L'économie d'énergie qui en découle est appréciable, surtout quand le nombre de circuits désactivés est important (ce qui est le cas pour les puces formant la mémoire centrale d'un ordinateur par exemple).
En informatique, dans le domaine du matériel informatique, un clone est une marque de matériel (par exemple un processeur, un ordinateur, une imprimante, etc.) dont les fonctionnalités ont été copiées de la marque reconnue. Par exemple, pour les processeurs de PC, il existe plusieurs clones d'Intel ; pour les PC eux-mêmes, les compatibles PC sont des clones.  Portail de l’informatique
Codelet est une fonction qui peut être exécutée sur un accélérateur matériel.
En informatique, un commutateur écran-clavier-souris ou commutateur KVM (switch KVM ou keyboard-video-mouse switch en anglais) est un commutateur qui permet de partager clavier, écran et souris entre plusieurs ordinateurs.
La rétrocompatibilité, ou compatibilité descendante, est la compatibilité d'un produit vis-à-vis de ses anciennes versions ; la compatibilité ascendante est la compatibilité d'un produit vis-à-vis des versions plus récentes, voire encore en phase de conception. Pour appréhender ce vocabulaire, il faut imaginer ce produit sur une échelle de temps verticale, avec ses versions antérieures en dessous de lui et ses versions postérieures au-dessus de lui ; à partir du produit, l'accès aux versions plus anciennes se fait alors en descendant, et l'accès aux versions plus récentes en montant. Les problèmes de compatibilité, tant ascendante que descendante, sont fréquents en informatique en raison de la rapidité de l'évolution du matériel et des logiciels.
Les composants des ordinateurs modernes sont essentiellement le silicium (Si), qui a pris la place du germanium (Ge) dans les années 1970, et l’arséniure de gallium (GaAs). Ils sont nécessaires à la fabrication des circuits intégrés. Le silicium est un semi-conducteur utilisé dans la réalisation de transistors et de circuits intégrés. On arrive de nos jours à obtenir du silicium pur à 99,99999 %. Le germanium est encore utilisé pour la fabrication de diodes à faible chute. L’arséniure de gallium, aussi semi-conducteur, est utilisé dans les domaines des micro-ondes et de l’opto-électronique (ex : diodes luminescentes)  Portail de l’informatique
Une console système ou console (anciennement le pupitre de commande) est un périphérique informatique de télécommunications des entrées-sorties d'un système de traitement de l'information. C'est généralement un terminal dédié uniquement à l'envoi et au retour des commandes. Spécifiquement, il s’agit d'un dispositif de communication homme-machine qui permet d'administrer un système informatique de bas niveau, en particulier le BIOS, le chargeur d'amorçage, le noyau de système d'exploitation, le système d'initialisation d'un serveur (service ou démon comme le programme init sous Unix), le service d'enregistrement de journaux (Syslog) et toute interface de paramétrage d'un service ou d'un serveur dédié. Aujourd'hui, la communication avec une console système se fait souvent depuis un réseau informatique et de manière abstraite, via des flux standards (avec par exemple : stdin, stdout et stderr), mais il peut exister des interfaces spécifiques au système d'exploitation, par exemple celles utilisées par son noyau.
Un coprocesseur SQL est un dispositif d'accélération matérielle, au même titre que les coprocesseurs mathématiques au début de l'aire des ordinateurs (8087,80287,80387, FPU des processeurs Pentium), il reprend également les mêmes principes que les coprocesseurs graphiques GPU intégrés sur les cartes graphiques, Son rôle est donc d'accélérer matériellement le temps d'exécution des commandes SQL, l'accélération matérielle peut particulièrement être axée sur la commande SELECT pipelinisé, pour de meilleures performances du coprocesseur,
En informatique, et plus particulièrement en architecture, une couche d'abstraction matérielle (abrégé en HAL pour hardware abstraction layer) est un logiciel intermédiaire entre le système d'exploitation et le matériel informatique. Il offre des fonctions standardisées de manipulation du matériel informatique tout en cachant les détails techniques de la mise en œuvre. De nombreux producteurs de système d'exploitation incluent une couche d'abstraction matérielle dans leurs produits. C'est une pièce de logiciel importante dans les systèmes d'exploitation portables — susceptibles d'être utilisés sur différents types de matériel : en cas de portage seule la couche d'abstraction matérielle nécessite adaptation.
La courbe force course est une des caractéristiques physiques de la touche d'un clavier. Lors de l'appui sur une touche, une certaine force est exercée sur celle-ci, ce qui assure l'enfoncement (la course). La courbe représentant la force exercée en fonction de ce déplacement est en forme de S, caractéristique de la touche (en fonction de la forme de la membrane, du poids de la touche…). Lorsque la touche est relâche, la courbre est également en S, légèrement décalée.   Portail de l’informatique
La Cubox est un plug computer de petite taille conçu par l'entreprise SolidRun Ltd. Il permet l'auto-hébergement et peut servir de lecteur multimédia avec Kodi (anciennement dénommé XBMC Media Center). Il est construit autour d'un SoC Marvell Armada 510 (incluant notamment un processeur ARM v7, un processeur graphique Vivante GC600 et une unité de décodage vidéo Marvell vMeta).
Un découpe vinyle est une machine, contrôlée par ordinateur, qui découpe différentes formes et lettres dans de fines feuilles plastifiées autocollantes (les vinyles), selon une image vectorielle du résultat attendu. Les plus petits découpes vinyles rappellent les imprimantes de bureau.
PowerEdge est le nom de la gamme de serveurs du constructeur informatique Dell. Cette gamme représente 15 % du chiffre d'affaires global de Dell en 2007, l'année du lancement de plusieurs nouveaux modèles. Les serveurs PowerEdge existent au format tour, rack ou lame. La plupart des PowerEdge sont basés sur l'architecture x86, mais Dell a développé des systèmes à base d'Itanium (abandonné en 2005). En 2006 Dell annonce son intention de développer sa gamme en utilisant les architectures AMD Opteron.
Dreambox est le nom d'une gamme de démodulateurs pour la réception par satellite, TNT ou câble fonctionnant sous Linux, développés et commercialisés par la société allemande Dream Multimedia.
La technique Dynamic Device Mapping (DDM) est dédiée aux commutateur écran-clavier-souris, aussi appelé commutateur KVM, USB censés remplacer les standards d'émulation USB des claviers et souris. On connecte les principaux composants d'un ordinateur — le clavier, la souris et l'écran — au commutateur KVM, et puis on utilise le câble lié afin de connecter les différents ordinateurs sur lesquels on veut travailler. Chaque composant de l’ordinateur envoie les messages à la centrale en imitant les signaux. Mais, comme ce n’est qu’une émulation, il est impossible d'envoyer tous les signaux à la tour[pourquoi ?]. D’ailleurs, certains systèmes[Lesquels ?] n’identifient pas le dispositif USB lorsqu’on l'insère ou retire, c’est-à-dire la souris et le clavier ne sont pas identifiés quand on recommunique au commutateur KVM : l’utilisateur doit renouveler l’émulation logicielle afin d'adapter les compatibilités de systèmes différents.[pas clair] L’émulation logicielle est limitée initialement, c'est-à-dire qu'il est impossible de communiquer avec chaque composant de l’ordinateur. Donc, l'utilisation de KVM cause souvent des problèmes de comptabilité[pourquoi ?]. La technique Dynamic Device Mapping (DDM) assure la compatibilité à 100％[Comment ?], c’est-à-dire qu'il n’est plus nécessaire d'utiliser l’émulation logicielle pour identifier les descriptions de la souris et du clavier. Aussi, DDM diminue l’attente de connexion au dispositif. Même si l’on change la chaîne[Quoi ?] ou retire le composant de l’ordinateur[Quoi ?], les tours fonctionnent normalement.
Un écran d'ordinateur est un périphérique de sortie vidéo d'ordinateur. Il affiche les images générées par la carte graphique de l'ordinateur. Grâce au taux de rafraîchissement d'écran élevé, il permet de donner l’impression de mouvement. Il permet donc de travailler agréablement, de visionner de la vidéo, des films, de jouer à des jeux vidéo, etc. Un écran à cristaux liquides (LCD) se compose d'une dalle (qui est le support des images), des circuits vidéo dont un multiplexeur électronique et une alimentation stabilisée.
Un émetteur-récepteur est un équipement électronique combinant un récepteur et un émetteur qui partagent des circuits communs. Les Anglo-saxons parlent de « transceiver », contraction de « TRANSmitter » (« émetteur ») et de « reCEIVER » (« récepteur »). Ce mot est parfois francisé en « transcepteur ». Il est utilisé dans plusieurs contextes.
Dans un système à base d'un processeur, d'un microprocesseur, d'un microcontrôleur ou d'un automate, on appelle entrées-sorties les échanges d'informations entre le processeur et les périphériques qui lui sont associés. De la sorte, le système peut réagir à des modifications de son environnement, voire le contrôler. Elles sont parfois désignées par l'acronyme I/O, issu de l'anglais Input/Output ou encore E/S pour Entrées/Sorties. Dans un système d'exploitation : Les entrées sont les données envoyées par un périphérique (disque, réseau, clavier…) à destination de l'unité centrale ; Les sorties sont les données émises par l'unité centrale à destination d'un périphérique (disque, réseau, écran…). Exemple simplifié : taper sur les touches du clavier envoie une série de codes vers le processeur ; ces codes sont considérés comme des données d'entrée ; le processeur affiche les résultats du traitement des données sur un écran ; ce sont des données de sortie. Habituellement, l'écran est géré par un programme de gestion d'affichage.
Un équipement terminal de circuit de données (ETCD ; en anglais, Data Communication Equipment ou Data circuit-terminating equipment [DCE]) est un élément permettant la connexion des terminaux ETTD (équipement terminal de traitement de données) au canal de transmission de données. Ce raccordement nécessite généralement une adaptation de signal qui sera réalisée par un ETCD. Les ETCD permettent d'adapter le flux des données aux conditions de la ligne et de faire la transformation analogique numérique ou numérique analogique. Un exemple bien connu d'équipement ETCD est le modem. Ne pas confondre avec DTE (Data Terminating Equipment) qui lui marque la fin d'un circuit de données, par exemple, une imprimante.  Portail des télécommunications  Portail de l’informatique
Un expandeur (en anglais expander) est un processeur MIDI, matériel ou logiciel, qui stocke des sons échantillonnés sur une mémoire informatique. On peut le considérer comme un synthétiseur dépourvu de clavier. Cependant contrairement au synthétiseur, il n'est en théorie pas possible d'éditer des sons sur un expandeur bien qu'en pratique les expandeurs fassent souvent office de synthétiseurs. Le contrôleur MIDI utilisé pour commander l'expandeur sera par exemple un séquenceur ou un clavier maître. On peut également le relier à un ordinateur personnel, selon les modèles, soit par une prise MIDI, soit par une prise USB. Les différences entre les modèles d'expandeurs se font au niveau du nombre (jusqu'à plusieurs milliers), de la qualité des sons proposés, des possibilités d'extension de la banque de sons, et des interfaces proposées.  Portail de l’informatique  Portail de la musique électronique
Fab12 est une usine Intel fabriquant des wafers (substrat ou galette de silicium) de 300 mm gravés en 65 nm. Cette usine, qui se situe en Arizona, a été fermée en 2004 puis rouverte en 2005, dotée d'un investissement de 2 milliards de dollars afin de pouvoir produire des wafers de 300 mm. Elle est la cinquième usine estampillée du logo du fondeur de Santa Clara à pouvoir graver les galettes de silicium en 65 nm. Les processeurs Pentium 4E utilisaient une finesse de gravure de 90 nm. L'utilisation de la technologie 65 nm a permis à Intel de produire des processeurs plus complexes et plus rapides. Le Tejas a été le premier processeur Intel qui profita de la finesse de gravure 65 nm. Le Tejas devrait être le successeur du Prescott, ce nouveau processeur possédera un cache L1 de 24 Ko, un cache L2 de 1 Mo (pour la version 90 nm) ou de 2 Mo (pour la version gravée en 65 nm), une série de nouvelles instructions, une prédiction de branches et une technologie Hyper-Threading améliorées et éventuellement un support des instructions 64 bits. Sa fréquence d'introduction est comprise entre 4 et 4,2 GHz. Intel dispose d'autres sites de fabrication, appelés Fab22 (aussi en Arizona), et Fab18. Un autre site est capable de produire des wafers en 65 nm, appelé Fab24-2 et situé à Leixlip en Irlande.
Un fabricant d'équipement d'origine (FEO), en anglais Original Equipment Manufacturer (OEM), ou équipementier est une entreprise fabriquant des pièces détachées, principalement pour le compte d'une autre entreprise, l'intégrateur ou l'assembleur. Ce type d'entreprise se retrouve usuellement dans les industries automobile, aéronautique, informatique ou électronique. On parle également de sous-traitance pour définir le lien entre l'équipementier et l'assembleur ou bien même de co-traitance parfois lorsque l'équipementier est partenaire dans l'innovation. C'est majoritairement le cas dans l'automobile.
Un filtre de confidentialité pour écran d'ordinateur est une protection qui se place devant un écran informatique et qui restreint la vision des données affichées de part et d’autre de l’axe de vision.
Dans un système informatique, un firmware (ou micrologiciel, microcode, logiciel interne, logiciel embarqué ou encore microprogramme) est un programme intégré dans un matériel informatique (ordinateur, photocopieur, automate (API, APS), disque dur, routeur, appareil photo numérique, etc.) pour qu'il puisse fonctionner.
Un fond de panier (en anglais backplane) est une carte ou un bâti sur laquelle plusieurs connecteurs sont disponibles pour la connexion d'autres cartes à un appareil. Ces connecteurs peuvent être reliés à des bus d'alimentation, de contrôle ou de communication. En informatique, les emplacements de cartes avec leurs connecteurs permettant d'intégrer un ensemble d'éléments périphériques s'appelle couramment slots.
General MIDI est une norme visant à améliorer la compatibilité des instruments de musique électroniques utilisant le protocole MIDI. Elle établit entre autres une numérotation des timbres que peut jouer l'instrument et leur classification par familles d'instruments (les claviers/les guitares/les cuivres/les nappes/etc.). Il est donc possible d'échanger des fichiers General MIDI sans connaître le matériel dont dispose le destinataire, ce qui permet le respect des timbres choisis pour chaque piste MIDI par le fabricant du fichier. Dans un fichier MIDI qui ne respecterait pas la norme General MIDI, une piste programmée avec un timbre de guitare pourrait sonner par exemple avec un timbre de trompette sur un autre appareil MIDI.
Les ports GPIO (General Purpose Input/Output, littéralement Entrée/Sortie pour un Usage Général) sont des ports d'entrée/sortie très utilisés dans le monde des microcontrôleurs, en particulier dans le domaine de l'électronique embarquée, qui ont fait leur apparition aux débuts des années 1980.
GRAIL (acronyme pour GRAphical Input Language, littéralement « langage graphique d'acquisition ») est le premier système de reconnaissance de signes sur tablette graphique RAND, inventée en 1964, par l'ingénieur américain Tom Ellis. Le système est basé sur un langage de programmation appelé JOSS.
Hardware Compatibility List est une liste éditée par un éditeur de logiciel et indiquant les compatibilités entre les matériels du marché et ses programmes. Microsoft édite et maintient à jour une liste de matériels compatibles avec ses systèmes d'exploitation, tout comme les éditeurs de Linux ou VMware.
Les périphériques hot-plug sont ceux que l'on peut connecter ou déconnecter d'un ordinateur pendant que le système est en marche. Ils sont dits connectés ou déconnectés « à chaud ».
Un hub Ethernet ou concentrateur Ethernet est un appareil informatique permettant de concentrer les transmissions Ethernet de plusieurs équipements sur un même support dans un réseau informatique local.
L'électrocomposeuse IBM 4250 (nom de code Bregg ou HiRes) a constitué en 1983 une tentative par IBM de s'implanter dans les arts graphiques, sur le marché du prépresse. Son but était d'utiliser la puissance de l'ordinateur pour fournir des textes de qualité professionnelle en évitant au client les frais d'achat élevés d'une photocomposeuse.
L'Intel Compute Stick est un ordinateur à carte unique développée par Intel. L'ordinateur, selon Intel, est conçu pour être plus petit qu'un ordinateur de bureau classique ou que tout autre ordinateur de petite taille, tout en gardant des performances comparables. Son connecteur principal, un port HDMI 1.4, branché à moniteur compatible (ou à d'un écran de télévision) et muni d'un clavier et d'une souris Bluetooth, lui permet d'être utilisé pour des tâches informatiques générales comme un ordinateur régulier. Ce petit appareil a été lancé au début de 2015 avec un processeur économe en énergie Atom Z3735F de la famille Bay Trail (en) d'Intel, une famille de systèmes sur puce qui ont été principalement conçus pour une utilisation avec les tablettes et les ordinateurs 2-en-1. Le processeur offrait une fréquence d'horloge de 1,33 GHz et une mémoire vive maximale de 2 Go. Une telle configuration était suffisante pour un usage de divertissement à domicile, de bureautique, de client léger et d'affichage dynamique. En mi-2015, Intel a annoncé que la deuxième génération de l'Intel Compute Stick allait inclure les avancés de la famille de processeurs Bay Trail. Les nouveaux dispositifs (attendus au quatrième trimestre 2015) seront dotés de puissance de traitement supplémentaire ainsi que de 4 Go de mémoire vive pour supporter des programmes plus lourds et un multitâche plus rapide.
Une interruption matérielle (en anglais Interrupt ReQuest ou IRQ) est une interruption déclenchée par un périphérique d'entrée-sortie d'un microprocesseur ou d'un microcontrôleur. Les interruptions matérielles sont utilisées en informatique lorsqu’il est nécessaire de réagir en temps réel à un événement asynchrone, ou bien, de manière plus générale, afin d’économiser le temps d’exécution lié à une boucle de consultation (polling loop). L’autre sorte d’interruption est l’interruption logicielle (software interrupt ou soft IRQ en anglais), généralement déclenchée par une instruction spéciale du processeur. Les deux sortes d’interruptions déclenchent un basculement de contexte vers le gestionnaire d’interruption associé.
iSCSI est une abréviation de Internet Small Computer System Interface. C'est un protocole de stockage en réseau basé sur le protocole IP destiné à relier les installations de stockage de données. En transportant les commandes SCSI sur les réseaux IP, iSCSI est utilisé pour faciliter les transferts de données sur les intranets et gérer le stockage sur de longues distances. iSCSI peut être utilisé pour transmettre des données sur des réseaux locaux (LAN), réseaux étendus (WAN) ou Internet et peut permettre d'être indépendant sur l'emplacement physique du stockage ou de la récupération de données. Le protocole permet aux clients (appelés initiateurs) d'envoyer des commandes SCSI (CDB) à des périphériques de stockage SCSI (targets) sur des serveurs distants. Il s'agit d'un protocole de SAN (Storage Area Network), qui permet de rassembler les ressources de stockage dans un centre de données tout en donnant l'illusion que le stockage est local. Contrairement au fibre channel, qui nécessite une infrastructure matérielle dédiée, iSCSI peut s'utiliser en conservant une infrastructure existante. iSCSI a été standardisé par l'IETF en avril 2004.
Un lecteur de CD-ROM est un périphérique informatique d'entrée permettant de lire des CD-ROM. Les modèles de salon ou d'ordinateur permettent aussi de graver des CD vierge ou réinscriptibles.
Un lecteur de Laserdisc est un lecteur de disque optique utilisé pour exploiter les données numériques stockées sur des Laserdisc. Un lecteur de Laserdisc est également utilisé en tant que périphérique informatique d'entrée permettant de lire des Laserdisc. La plupart des grandes entreprises de l'âge d'or des jeux vidéo d'arcade ont utilisé ce support et créé des systèmes d'arcade munis de Laserdisc.
Le Lemur Input Device, plus simplement connu sous le nom Lemur, est un appareil multi-touch personnalisable fabriqué par la compagnie française JazzMutant. Il sert de contrôleur pour d'autres instruments comme les synthétiseurs ou les tables de mixage aussi bien que pour d'autres applications multimédia, notamment pour le VJing (Vidéo-Jockey). Comme outil audio, le Lemur est l'équivalent des contrôleurs MIDI, à l'exception du fait qu'il utilise le protocole Open Sound Control, un autre réseau à grande vitesse pouvant remplacer le MIDI. Le contrôleur est particulièrement adapté à l'utilisation des logiciels Reaktor et Max/MSP.
Une librairie de sauvegarde ou bibliothèque de sauvegarde (appelé parfois « silo de sauvegarde », « robot de sauvegarde », « jukebox » ou « autoloader »), ou encore TLD (tape library device) est un équipement utilisé dans les centres de traitement informatique pour automatiser la manipulation de bandes magnétiques lors des sauvegarde ou restauration de données. Une librairie est généralement composée d'un bras mécanique muni d'une pince, permettant de saisir une cartouche de sauvegarde et de l'insérer dans un lecteur/dérouleur. Suivant les modèles, la capacité varie de quelques centaines de gigaoctets jusqu'à plusieurs dizaines de téraoctets. Dans certains modèle de grande capacité, le bras mécanique peut atteindre une vitesse en déplacement horizontal/vertical d'environ 300 km/h. Certains modèles sont équipés de plusieurs bras mécanique, et sont donc accompagné de systèmes dit "passe-plats" pour échanger les bandes magnétiques entre les différentes zones. Une librairie est aussi équipée d'un panier de chargement, pour faire entrer ou sortir des bandes magnétiques. Ce panier de chargement peut être d'une capacité d'une cartouche jusqu’à beaucoup plus. Certaines librairies contiennent plusieurs paniers de chargement afin de faciliter les opérations d'entrées et de sorties de bandes. Une librairie peut être alimentée en bandes directement en utilisant les portes de maintenance mais ce n'est pas conseillé, car cette action verrouille les bras mécaniques de déplacement de bande et empêche tout nouveau chargement ou déchargement de bande magnétique dans un lecteur. Une librairie de sauvegarde, peut-être transformée en librairie d'archivage si elle contient des bandes magnétiques de type WORM. Une librairie peut-être partagée par plusieurs environnement (monde ouvert et mainframe) au moyen de couches logicielles comme l'ACSLS de chez Sun microsystem.
Librem est une gamme d'ordinateurs personnels (PC portable et smartphone) produit par Purism (en) et dont l'objectif est de préserver la liberté, la sécurité et la vie privée des utilisateurs,,. À cette fin, les appareils sont fournis avec PureOS (en), une distribution GNU/Linux entièrement libre (y compris le noyau), avec Coreboot (un BIOS libre) et avec des fonctionnalités de sécurité comme des "kill switches" pour le microphone, la webcam, le Wi-Fi et le Bluetooth,,.
Cet article contient la liste des ordinateurs à tubes à vide, par ordre de date d'entrée en service. Un ordinateur à tubes à vide est un ordinateur dont l'unité arithmétique et logique est construite avec des tubes à vide. Ces ordinateurs ont été précédés par des machines construites avec des relais électromécaniques. Ils ont été suivis par des ordinateurs construits de transistors. Les derniers ordinateurs de la liste contiennent parfois un mélange de tubes à vide et de transistors.
Un listing est un imprimé des lignes de codes d'un programme informatique ou d'un fichier de données dans un format lisible par un humain.  Dans les années 1960 et 1970, les listings étaient utilisés par les programmeurs pour vérifier et corriger leurs programmes. Aujourd'hui, les programmeurs vérifient et corrigent leurs programmes en les affichant sur un écran. Les écrans d'aujourd'hui se prêtent bien à cet usage parce qu'ils sont plus grands et ont une meilleure qualité d'affichage que les écrans des années 1960 et 1970. Les listings ont tout de même survécu à un endroit : on les retrouve encore dans les manuels de programmation. Dans les années 1960 et 1970, les listings étaient aussi utilisés pour conserver des données, surtout pour le stockage des données de façon permanente ou pour certaines copies de sauvegarde, par exemple la sauvegarde des programmes en développement. Avec la diminution du coût des disques durs, le stockage des données a migré vers ce medium et le papier n'est utilisé que très rarement pour conserver les données des systèmes informatiques. De plus, les fichiers d'aujourd'hui sont beaucoup trop gros pour être conservés sur papier. Aussi, les supports magnétiques permettent de copier et transmettre les données beaucoup plus facilement et permettre de faire diverses recherches et indexations des données. Dans des environnements nécessitant un haut niveau de sécurité, on conserve encore parfois des données sur papier.
La loi de Koomey décrit une tendance à long terme dans l'histoire des ordinateurs. Selon cette loi, le nombre de calculs par joule d'énergie dépensé double tous les 18 mois environ. Il s'avère que cette tendance a été remarquablement stable depuis les années 1950, le nombre de calculs par joule d'énergie dépensé ayant doublé environ tous les 1,57 ans. Cette loi énoncée par Jonathan Koomey aurait été énoncée comme suit: la quantité d'énergie dont une machine a besoin pour effectuer un nombre donné de calculs va diminuer d'un facteur deux chaque année et demi. « The idea is that at a fixed computing load, the amount of battery you need will fall by a factor of two every year and a half » Elle établit un parallèle avec la loi de Moore.
Les lois de Moore sont des lois empiriques qui ont trait à l'évolution de la puissance de calcul des ordinateurs et de la complexité du matériel informatique. Au sens strict, on ne devrait pas parler de « lois de Moore » mais de « conjectures de Moore » puisque les énoncés de Moore ne sont que des suppositions. Il existe trois « lois » de Moore, deux authentiques (au sens où elles furent émises par Gordon E. Moore), et une série de « lois » qui ont en commun de se prétendre « loi de Moore » mais qui n'en sont que des simplifications inexactes.
Le martèlement de mémoire, de l'anglais row hammer ou rowhammer, est un effet secondaire imprévu dans les mémoires dynamiques à accès aléatoire (DRAM) qui provoque une fuite de charge électrique dans des cellules de mémoire, et en conséquence provoque une interaction électrique entre ces cellules et d'autres cellules voisines. Le contenu mémorisé dans ces cellules voisines peut être ainsi modifié. Un programme informatique peut ainsi parvenir à modifier le contenu des cellules voisines sans avoir besoin d'accéder à ces cellules voisines, et donc sans avoir le droit d'y accéder,,,.
Un micro-casque est un équipement intégrant un microphone et un casque audio dans un même appareil, utilisé en informatique, en radiotéléphonie et studio d'enregistrement. Il peut se présenter sous différentes formes : une simple oreillette reliée à un micro une double oreillette sous forme de puce, le micro étant fixé sur le câble d'une de ces deux oreillettes un casque reliant les deux oreillettes (passant sur le dessus ou l'arrière du crâne) muni d'un micro au bout d'une tige En informatique, il est généralement doté de deux prises Jack de 3,5 mm : une pour le microphone, une autre pour le casque. Les ordinateurs de bureau et portables assez récents sont généralement dotés de prises en façade. Il permet la téléphonie par Internet via l'utilisation d'un ordinateur. En radiotéléphonie, console ou studios, il permet l'écoute de la communication ainsi que de la partie audio meme en ambiance bruyante (cockpit d'avion, navires, reportage, scène, etc.).   Portail de l’informatique  Portail des télécommunications
Le terme mini-ordinateur est à l'origine une catégorie d'ordinateurs de milieu de gamme, intermédiaires entre les ordinateurs centraux (mainframe) et les micro-ordinateurs, populaire dans les années 1970. Pendant une période cette catégorie avait un type de matériel et de systèmes d'exploitation spécifique (par exemple les systèmes Digital Equipment Corporation dans les années 1970-1990), mais à présent le terme correspond plutôt aux systèmes multi utilisateurs temps-réel tels que SPARC, POWER ou Itanium de Sun Microsystems, IBM ou Hewlett-Packard.
Un ordinateur multiprocesseur est doté de plusieurs processeurs, et est donc une forme d'architecture parallèle.
Le Musical Instrument Digital Interface ou MIDI est un protocole de communication et un format de fichier dédiés à la musique, et utilisés pour la communication entre instruments électroniques, contrôleurs, séquenceurs, et logiciels de musique. Apparu dans les années 1980 issu d'une volonté de l'industrie de normaliser les échanges entre les différents instruments, ce protocole est devenu aujourd'hui un standard très répandu dans le matériel électronique de musique.
La nano-informatique est un néologisme qui propose de nouveaux concepts et architectures des systèmes informatiques et de l'électronique numérique. Fondés sur la technologie du silicium ou sur des alternatives radicales, ils ont pour particularité d'exploiter (pour tout ou partie) les molécules elles-mêmes comme éléments de base des futurs ordinateurs. Pour des raisons autant économiques que techniques, on considère leur développement comme inéluctable.
Un ordinateur est un système de traitement de l'information programmable tel que défini par Turing et qui fonctionne par la lecture séquentielle d'un ensemble d'instructions, organisées en programmes, qui lui font exécuter des opérations logiques et arithmétiques. Sa structure physique actuelle fait que toutes les opérations reposent sur la logique binaire et sur des nombres formés à partir de chiffres binaires. Dès sa mise sous tension, un ordinateur exécute, l'une après l'autre, des instructions qui lui font lire, manipuler, puis réécrire un ensemble de données déterminées par une mémoire morte d'amorçage (Boot ROM, BIOS, UEFI, etc.). Des tests et des sauts conditionnels permettent de passer à l'instruction suivante et donc d'agir différemment en fonction des données ou des nécessités du moment ou de l'environnement. Les données à manipuler sont obtenues, soit par la lecture de mémoires, soit par la lecture d'information en provenance de périphériques internes ou externes (déplacement d'une souris, touche appuyée sur un clavier, température, vitesse, compression…). Une fois utilisés, ou manipulés, les résultats sont écrits, soit dans des mémoires, soit dans des composants qui peuvent transformer une valeur binaire en une action physique (écriture sur une imprimante ou sur un moniteur, accélération ou freinage d'un véhicule, changement de température d'un four…). L'ordinateur peut aussi répondre à des interruptions qui lui permettent d’exécuter des programmes de réponses spécifiques à chacune, puis de reprendre l’exécution séquentielle du programme interrompu. De 1834 à 1837, Charles Babbage concoit une machine à calculer programmable en associant un des descendants de la Pascaline (première machine à calculer mécanique inventée par Blaise Pascal) avec des instructions écrites sur le même type de cartes perforées que celles inventées par Jacquard pour ses métiers à tisser. C'est durant cette période qu'il imagine la plupart des caractéristiques de l'ordinateur moderne. Babbage passera le reste de sa vie à essayer de construire sa machine analytique, mais sans succès. Beaucoup de personnes s’y intéressèrent et essayèrent de développer cette machine, mais c'est cent ans plus tard, en 1937, qu'IBM inaugurera l'ère de l'informatique en commençant le développement de l'ASCC/Mark I, une machine basée sur l’architecture de Babbage qui, une fois réalisée, sera considérée comme l'achèvement de son rêve. La technique actuelle des ordinateurs date du milieu du xxe siècle. Les ordinateurs peuvent être classés selon plusieurs critères tel que le domaine d'application, la taille ou l'architecture.
Un ordinateur à programme enregistré (ou calculateur à programme enregistré; en anglais stored-program computer) est un ordinateur qui enregistre les instructions des programmes qu'il exécute dans sa mémoire vive. La définition précédente est souvent étendue pour exiger que le traitement des instructions et des données en mémoire doive être interchangeable et uniforme,,. Un ordinateur avec une architecture de von Neumann enregistre ses données et ses instructions dans la même mémoire; un ordinateur avec une architecture Harvard enregistre ses données et ses instructions dans des mémoires séparées,
Un ordinateur à carte unique ou ordinateur monocarte ou ordinateur mono-carte (abrégé parfois SBC, de l'anglais single-board computer) est un ordinateur complet construit sur un circuit imprimé, avec un ou plusieurs microprocesseur(s), de la mémoire, des lignes d'entrée/sortie (notée parfois I/O de l'anglais Input/Output) et d'autres éléments pour en faire un ordinateur fonctionnel. Contrairement à un ordinateur personnel classique, un ordinateur mono-carte ne possède généralement pas d'emplacements dans lesquels on enfiche des cartes périphériques (carte fille). Un ordinateur monocarte peut être construit sur quasiment n'importe quel microprocesseur existant, et peut être construit à l'aide de composants logiques discrets ou à l'aide de circuit logique programmable. Les modèles les plus simples, tels que ceux construits par des amateurs de l'informatique et l'électronique, utilisent souvent de la mémoire RAM statique et des processeurs à 8 ou 16 bits à faible coût.
Un ordinateur renforcé (Rugged computer en anglais), ou ordinateur durci, est un ordinateur destiné à être utilisé dans des conditions difficiles voire hostiles et à résister à différentes agressions extérieures telles que des chocs, des projections de liquides, des poussières ou des températures extrêmes. Ils sont particulièrement usités dans le monde industriel et de la recherche appliquée pour être embarqués dans des engins de transport (aéronefs, marine, etc.), notamment pour réaliser des phases de tests. Les ordinateurs renforcés concernent plutôt la gamme des ordinateurs portables, des tablettes PC ou des assistants personnels (PDA), mais existent également dans le monde PC en général, conçus pour être installés dans des baies, ou racks: on parle alors de 'PC rackable'.
En informatique le panneau frontal ou panneau avant était le dispositif principal des premières consoles systèmes. Il permettait d'afficher et de modifier les registres internes des premiers systèmes informatiques (calculateur analogique ou ordinateur) de type électronique. Le panneau avant est en général constitué de tableaux de voyants, d'interrupteurs à bascule et de boutons-poussoirs montés sur une plaque en tôle placé à l'avant de l'ordinateur. Dans les premières machines, des tubes cathodiques (CRT) peuvent également être présents (comme un oscilloscope ou, par exemple, pour refléter le contenu de la mémoire d'un tube de Williams). Avant le développement des consoles systèmes équipées d'un moniteur CRT), de nombreux ordinateurs tels que l'IBM 1620 possédaient des machines à écrire fixées sur leurs consoles. Habituellement, le contenu d'un ou plusieurs registres matériels (en) est représenté par une rangée de voyants, ce qui permet de lire directement le contenu lorsque la machine est arrêtée. Les commutateurs permettaient l'entrée directe des valeurs de données et d'adresses dans les registres ou dans la mémoire. Ils sont aujourd'hui utilisés pour commander et afficher les niveaux d'états basiques de certain ordinateur ou superordinateur comme la mise sous tension d'un élément, les températures, l'état du circuit de refroidissement ou encore les vitesses des ventilateurs.
Le PC Speaker est le haut-parleur interne du PC. Il est possible de jouer sur une seule voie. Seuls les signaux carrés sont disponibles. Il n'est en général pas possible de régler le volume.
Un PC-on-a-stick (signifiant PC-sur-une-clé en anglais) est un type d'ordinateur portatif de la taille d'une grosse clé USB. Ce type d'ordinateur est généralement composé d'un système sur une puce (SoC) à base d'architecture ARM, d'un connecteur HDMI, d'un lecteur de carte flash (généralement carte microSD) et d'au moins un port USB pour brancher un ou plusieurs périphériques de saisie, éventuellement via un hub USB et à alimenter l'ordinateur. Ils contiennent également pour la connexion réseau des interfaces réseau sans fil (le plus souvent Bluetooth et/ou Wi-Fi).
En informatique, les performances énoncent les indications chiffrées mesurant les possibilités maximales ou optimales d'un matériel, d'un logiciel, d'un système ou d'un procédé technique pour exécuter une tâche donnée. Selon le contexte, les performances incluent les mesures suivantes: Un faible temps de réponse pour effectuer une tâche donnée Un débit élevé (vitesse d'exécution d'une tâche) L'efficience : faible utilisation des ressources informatiques : processeur, mémoire, stockage, réseau, consommation électrique, etc. Une haute disponibilité du système ou de l'application Une bande passante élevée ou une faible latence Le passage à l'échelle (scalabilité) La capacité d'un canal de communication La performance du stockage informatique Les performances sont établies par la mesure lors d'une activité appelée benchmarking (en français, étalonnage), grâce à des tests de performance. Le test de performance consiste à définir le processus/système/matériel que l'on veut mesurer, à définir le cas d'utilisation à tester (incluant le mode d'interaction, les données et le scénario), et à exécuter le test tout en récoltant les mesures grâce à des capteurs ou des sondes. Les benchmarks et tests de performances sont souvent sujets à controverse. Ils sont souvent difficilement reproductibles, car très dépendants de détails non explicités : matériel utilisé, configuration de tous les éléments, charge de la machine, etc. L'optimisation est l'activité consistant à améliorer les performances de le cible. Il consiste d'abord à définir le test de performance à effectuer, quelles mesures doivent être optimisées, puis enfin à exécuter le test, mesurer, analyser, modifier la cible et recommencer pour mesurer s'il y a eu une amélioration. Ce processus est répété jusqu'à atteindre le niveau de performance souhaité.
Un périphérique informatique est un dispositif connecté à un système de traitement de l'information central (ordinateur, console de jeux, etc.) et qui ajoute à ce dernier des fonctionnalités.
Le Personal Internet Communicator (PIC) est un produit informatique mis en place par AMD en 2004 pour permettre aux populations des pays émergents comme l'Inde et le Mexique d'avoir accès à l'internet. Il est basé sur le processeur Geode et vendu 185$ avec le clavier, la souris et les logiciels, alors qu'il est vendu 249$ avec en plus l'écran.
En électronique et en technologie informatique, PHY est une abréviation couramment utilisée pour désigner la couche physique du modèle OSI (couche de plus bas niveau). Cette couche consiste en une cellule de conversion analogique-numérique. En effet, lors de l'émission de données, celles-ci doivent être modulées sous forme de signaux analogiques. Réciproquement, lors de la réception de signaux analogiques, ceux-ci doivent être démodulés afin d'en extraire un flux de données numériques. Le terme PHY désigne donc parfois le composant ou le bloc fonctionnel chargé d'effectuer cette conversion analogique-numérique.
Le PICA200 est un microprocesseur graphique (GPU en Anglais) pour systèmes embarqués conçu par la société nippone Digital Media Professionals Inc. (DMP), annoncée au SIGGRAPH 2005 et présentée au SIGGRAPH 2006. PICA est le nom donné par DMP à sa gamme de microprocesseurs graphiques pour systèmes embarqués. Cette gamme est utilisée tant pour les appareils portables que pour les bornes d'arcade hautes performances. Le nom PICA200 est un simple assemblage du nom de la gamme (PICA) à la fréquence du microprocesseur, en l’occurrence 200 MHz.
Une PirateBox est un dispositif électronique souvent composé d'un routeur et d'un dispositif de stockage d'information, créant un réseau sans fil qui permet aux utilisateurs qui y sont connectés d'échanger des fichiers anonymement et de manière locale. Par définition, ce dispositif qui est souvent portable, est déconnecté d'Internet. Les PirateBox sont à l'origine destinées à échanger librement des données libres du domaine public ou sous licence libre. Les logiciels utilisés pour la mise en place d'une PirateBox sont majoritairement open source (source ouverte), voire libres.
Le Pixar Image Computer est un ordinateur destiné à la conception graphique créé par Pixar en mai 1986, pour des secteurs tels que la médecine. La machine était vendue pour 135 000 $. La première version de cet ordinateur était en avance sur son temps, et a engendré beaucoup de ventes aux laboratoires de recherche. En 1987, Pixar sort une deuxième version de la machine, vendue à 30 000 $ cette fois. Pour s'implanter sur le marché médical, Pixar a fait don de dix machines à des hôpitaux. Toutefois, cela a eu peu d'effet sur les ventes, en dépit de l'aptitude de la machine à montrer des images du corps humain.  Portail sur Disney • section Pixar  Portail de l’animation
Le Plug and Play (l'abréviation PnP est également utilisée), qui signifie « connecter et jouer » ou « brancher et utiliser », est une procédure permettant aux périphériques récents d'être reconnus rapidement et automatiquement par le système d'exploitation dès le branchement du matériel, et sans redémarrage de l'ordinateur. Cette procédure permet l'installation en requérant un minimum d'intervention de la part de l'utilisateur, sans installation de logiciel dédié, et donc en minimisant les erreurs de manipulation et de paramétrage. Il existe plusieurs appellations ou acronymes qui recouvrent le même concept (tous sont d'origine anglaise) : PnP, et hot-swapping (littéralement « échange [de périphériques] à chaud »). Le terme Plug-and-Play est en général associé à Microsoft, qui l'utilisa en référence à son système d'exploitation Windows 95. D'autres systèmes d'exploitation supportaient déjà ce genre de périphériques, comme celui du Macintosh depuis ses débuts en 1984[réf. nécessaire], mais ils utilisèrent tous la même dénomination. L'expression française « prêt-à-tourner » a été proposée[réf. nécessaire] par l'office québécois de la langue française en 2006. Les périphériques véritablement PnP le sont à deux niveaux : celui du matériel et celui du logiciel. Tout d’abord, l’ordinateur doit être capable de supporter l’ajout de nouveaux périphériques sans créer de problèmes physique ou électrique sur ces derniers. Par exemple certaines interfaces ne supportent pas le fait d’être branchées ou débranchées durant la mise sous tension sans créer des dommages irréversibles aux périphériques. La plupart du temps cela se traduit par des décharges électrostatiques aux divers composants électroniques. L’ordinateur doit également être capable de détecter les changements de configurations au fur et à mesure que les périphériques sont ajoutés et enlevés. Les connectiques USB et FireWire ont été conçues dans ce but. Une forme plus restreinte de Plug-and-Play existe également pour des systèmes ne gérant pas ces changements dynamiques : le système « power down, plug, power up, and play » (« Éteindre, brancher, démarrer, et utiliser »). Les débuts du Plug and Play ont été assez difficiles (bien souvent les périphériques Plug and Play n'avaient de « Plug and Play » que le nom, ils étaient souvent plus complexes à configurer que les périphériques non estampillés « Plug and Play »), c'est pour cette raison que le Plug and Play était surnommé parfois de façon satirique « Plug and Pray », littéralement, « branchez et priez » (pour que ça marche). Cette technologie effectue plusieurs opérations à savoir dont Une reconnaissance automatique à chaque branchement de n’importe quel périphérique à l’unité centrale Elle effectue automatiquement les ressources nécessaires Elle enregistre les paramètres qui correspondent au matériel enregistré Sous windows, elle enregistre les paramètres dans la base de données du registre pour éviter les conflits éventuels
Un plug computer est un mini PC dont le facteur de forme (la taille) est particulièrement petit, et dont le coût est généralement réduit ; certains modèles ont la taille d'un boitier CPL, et leur usage est principalement domestique. Ils n'ont souvent pas de carte vidéo. Il s'agit la plupart du temps de petits serveurs dont les principaux usages sont : Partage de fichiers (SMB ou Samba, multimédia, etc.) ; Sauvegarde ; Serveur web ; Passerelle (serveur DLNA). service de sauvegarde serveur média Les plug computers consomment en général très peu d'électricité, ce qui les rend intéressants en terme d'économie d'énergie. Un fabricant par exemple, revendique que son plug computer à 119 $ consomme 1,2 watt et peut coûter 2 $ par an pour fonctionner.
Un port matériel est conçu pour brancher un certain type de périphérique, soit directement, soit au moyen d'un câble. Il est soumis à des normes aussi bien sur ses caractéristiques physiques (forme, considérations électriques ou optiques) que logiques (à quoi sert chaque quel fil/patte/connecteur, que signifie tel ou tel signal en entrée, en sortie). Les ports matériels se répartissent en : ports internes destinés soit à relier à une carte mère des périphériques internes au boîtier de l'ordinateur (disques, barrettes de mémoire ou même processeur en considérant un socket comme un port matériel) soit à insérer une carte d'extension enfichable sur un bus interne (on parle alors de connecteur d'extension). ports externes permettant de communiquer avec différents périphériques, souvent via un câble, quoiqu'on trouve aussi notamment dans le cas des portables des connecteurs d'extension reliés à un bus (par exemple PC-Card).
Le Port parallèle est un connecteur situé à l'arrière des ordinateurs compatibles PC reposant sur la communication parallèle. Il est associé à l'interface parallèle Centronics. La communication parallèle a été conçue pour une imprimante imprimant du texte, caractère par caractère. Les imprimantes graphiques (pouvant imprimer des images) ont ensuite continué à utiliser ce système pour profiter de l'interface parallèle normalisée. Le port parallèle est à l'origine unidirectionnel. Ce type d'interface a évolué vers le standard IEEE 1284, à la fois bidirectionnel et plus rapide.
Network-on-Chip ou Network-on-a-Chip (NoC or NOC) ou en français réseau sur une puce est une technique de conception du système de communication entre les cœurs sur les System on Chip (SoC). Les NoCs peuvent passer dans les domaines d'horloge synchrone ou asynchrone ou bien utiliser une logique de circuit asynchrone sans horloge. Le NoC applique les théories et méthodes de réseau aux communications à l'intérieur d'une puce et permet ainsi l'amélioration des performances par rapport aux interconnexions de bus et commutateur matriciel conventionnelles. Le NoC améliore la scalabilité des SoCs et l'efficacité énergétique des SoCs complexes relativement aux autres conceptions. Des recherches ont été menées sur des guides d'ondes lumineuse intégrées et des périphériques comprenant les Optical Network-on-Chip (ONoC, reseau optique sur une puce),.
La résolution écran large (ou format écran large) est une variété d'image utilisée dans les films et séries sur télévision et écran d'ordinateur. Le format de l'image est le rapport de sa largeur divisée par sa hauteur. Dans les domaines cinématographique et vidéo, un film au format écran large est un film dont le format d'image est supérieur, c'est-à-dire plus large, que le format académique (1,375).
En informatique, les ressources sont des composants, matériels ou logiciels, connectés à un ordinateur. Tout composant de système interne est une ressource. Les ressources d'un système virtuel incluent les fichiers, les connexions au réseau, et les zones de mémoire.
La rue Montgallet est une voie située dans le quartier de Picpus du 12e arrondissement de Paris.
Le serveur vidéo (video server en anglais) est un système de conversion, ou alors de stockage qui, dans une application vidéo, offre des ressources audiovisuelles (les images animées de la vidéo et de l'audio, sons) à un réseau d’ordinateurs clients. On fait appel aux serveurs vidéo pour les opérations de radiodiffusion ou broadcast et de postproduction.
Softmod (de l'anglais soft, « logiciel », et mod « modification ») est une méthode logicielle permettant de modifier le comportement habituel d'un dispositif comme les cartes mères, les cartes vidéo, les cartes son, ou les consoles de jeux... Elle est régulièrement appliquée dans l'overclocking des processeurs.  Portail de l’informatique
La Sun Ray est une station de type client léger créé pour le marché professionnel. Cette ligne de produits a été créée en 1999 par Sun Microsystems. Elle contient entre autres originalités un lecteur de type "smart card" permettant à l'utilisateur de s'authentifier via une carte à puce. Voici les différent modèles ayant existé ou existant: NeWT (NetWork Terminal) - Premier prototype, sans écran. Sun Ray 1 - supporte des résolutions jusqu'à 1280×1024 à 85Hz. Sun Ray 100 - Ray 1 intégré à un écran cathodique 15 pouces. Sun Ray 150 - Ray 1 intégré à un écran LCD 15 pouces. Sun Ray 1g - supporte des résolutions jusqu'à 1920x1200 à 75 Hz Sun Ray 170 - Ray 1 intégré à un écran LCD 17 pouces (l'électronique est située dans le pied). Sun Ray 2 - dessin plus fin que la version 1, consommation abaissée (4 watts), client VPN intégré, résolution maximale 1600x1200 à 60Hz. Sun Ray 2FS - permet d'utiliser deux écrans et supporte des résolutions jusqu'à 1920x1200 Sun Ray 270 - Ray 2 intégré à un écran LCD de 17 pouces (l'électronique est située dans l'écran). Sun Ray 3 - supporte des résolutions jusqu'à 1920x1200. Sun Ray 3 Plus - permet d'utiliser deux écrans et supporte des résolutions jusqu'à 2560x1600. Sun Ray 3i - Ray 3 intégré à un écran LCD 21,5 pouces.
Le synchronisme désigne le caractère de ce qui se passe en même temps, à la même vitesse. L'adjectif synchrone définit deux processus qui se déroulent de manière synchronisée. Il s'oppose à asynchrone. Il est particulièrement important dans le domaine technique, celui de l'électrotechnique et l'électronique. En particulier, on parle en électrotechnique de machines synchrones pour des machines tournantes dont la fréquence de rotation est égale à celle du réseau électrique. Il s'utilise aussi dans le domaine de l'audiovisuel pour la connectique. En informatique, les systèmes sont fréquemment fondés sur ce principe, qui permet des transferts d'information fiables, que ce soit au niveau des réseaux ou au niveau du processeur. Du point de vue du programmeur, une méthode est synchrone si elle attend une réponse avant de retourner la sienne. En histoire, le synchronisme retrace la simultanéité des événements dans un cadre déterminé (pays, continent, monde) sans que ces évènements soient obligatoirement dépendants directement les uns des autres. Dans le domaine du doublage, le synchronisme consiste à caler sur les mouvements des lèvres des acteurs originaux le texte doublé.
Tandem Computers fut un des premiers constructeurs d'ordinateurs à tolérance aux pannes, destinés aux clients désirant un système capable de ne jamais s'arrêter, même en cas de problème matériel ou de changement logiciel. Ses clients venaient surtout du domaine des finances (Bourses, banques, assurances, distributeurs automatiques), où l'informatique est considérée comme outil de production et non seulement de gestion. Les systèmes Tandem utilisent une architecture appelée NonStop, fondée sur la redondance des processeurs, des disques, et des cartes de réseau et des circuits internes, qui servent alors de système de basculement rapide en cas d'une panne de l'un des composants. Tandem Computers débuta par la conception de ses propres processeurs dans les années 1970 mais migra vers des processeurs plus communs dans les années 1990. La société a été rachetée en 1997 par Compaq, qui désirait une ligne de serveurs haut de gamme. Compaq a été racheté par Hewlett-Packard et la gamme NonStop a été rebaptisée Integrity NonStop. Le slogan de la société était : « redondance et symétrie », souvent illustré de photographies montrant l'usage courant de redondance associée à la symétrie dans la nature pour banaliser le concept.
En informatique, un testeur de carte mère (POST card en anglais) est une carte d'extension enfichable qui affiche l'avancement de l'auto-diagnostic au démarrage (power-on self-test ou POST) d'un ordinateur de type PC, ainsi que les codes d'erreurs éventuels. Il est utilisé pour dépanner les ordinateurs qui n'arrivent pas à démarrer.
La tête GMR (de l'anglais Giant MagnetoResistance) signifiant magnétorésistance géante en français, est l’un des trois types de têtes de lecture existant sur les disques durs.
La tête inductive est l'un des trois types de têtes de lecture existant sur les disques durs. Pour exécuter les fonctions de lecture et d'écriture, le système à tête inductive n'est constitué que d'un seul électroaimant.
La tête MR (pour MagnetoRésistive) est l'un des trois types de têtes de lecture existant sur les disques durs.
Le TiVo est un magnétoscope numérique à disque dur, qui permet d’enregistrer les programmes télévisés sur ce support, pour une lecture différée (appelé aussi timeshifting). Le TiVo enregistre également des programmes auxquels l’utilisateur pourrait s’intéresser. En outre, ceux regardés en direct peuvent être mis en pause ou lus à nouveau afin de répéter une séquence qui vient juste d’être vue. L’appareil a été créé par TiVo Inc. une société fondée par des anciens employés de Silicon Graphics et du système de vidéo numérique Full Service Network de Time Warner. TiVo peut aussi bien faire référence à la société qu’au service TiVo, qui est le réseau avec lequel l’enregistreur communique. Après plusieurs années de rumeurs de rachat de Tivo par Google, c'est finalement un accord commercial qui a été signé entre les parties en novembre 2009.
La tolérance aux pannes (aussi appelé « insensibilité aux pannes ») désigne une méthode de conception permettant à un système de continuer à fonctionner, éventuellement de manière réduite (on dit aussi en « mode dégradé »), au lieu de tomber complètement en panne, lorsque l'un de ses composants ne fonctionne plus correctement. L'expression est employée couramment pour les systèmes informatiques étudiés de façon à rester plus ou moins opérationnels en cas de panne partielle, c'est-à-dire éventuellement avec une réduction du débit ou une augmentation du temps de réponse. En d'autres termes, le système ne s'arrête pas de fonctionner, qu'il y ait défaillance matérielle ou défaillance logicielle. Un exemple en dehors de l'informatique est celui du véhicule à moteur conçu pour être toujours en état de rouler même si l'un de ses pneus est crevé.
La tour, ou le boîtier, d'un ordinateur contient les principaux composants de celui-ci : (carte mère, processeur, disque dur, etc.) Un boîtier est souvent composé d'acier, d'aluminium ou de plastique. Pour des raisons décoratives, il peut également contenir d'autres matériaux comme du bois ou du plexiglas.
L'Ultra-ATA est un protocole régissant les transferts de données entre les disques durs de type E-IDE et la carte mère.
L’unité centrale (ou couramment UC ou ordinateur) est la partie du complexe effectuant les calculs. Plus précisément, cela peut être deux choses : le boîtier d'un ordinateur et ses composants internes ; une partie du processeur de l'ordinateur.
Un dispositif électronique ou informatique est en veille si la plupart de ses fonctions ont été arrêtées temporairement dans le but de diminuer sa consommation électrique.
Un ventilateur d'ordinateur est un ventilateur installé à l'intérieur d'un ordinateur, ou fixé à celui-ci, et utilisé pour en refroidir activement le boîtier en permettant un flux d'air frais vers l'intérieur et d'air chaud vers l'extérieur ou en améliorant la circulation d'air sur le dissipateur thermique d'un composant particulier.
VLIW, initiales de very long instruction word en anglais, traduit littéralement par « mot d'instruction très long », dénote une famille d'ordinateurs dotés d'un processeur à mot d'instruction très long (couramment supérieur à 128 bits). VLIW est une technologie reportant une partie de la gestion du pipeline d'exécution d'un processeur dans les compilateurs. Cette technologie, semblable à l'EPIC proposée par l'Itanium d'Intel va donc fournir une instruction longue qui sera une agrégation d'instructions courtes indépendantes.
Video In Video Out, souvent vu sous l'acronyme VIVO, permet à une carte graphique de supporter les transferts vidéo bidirectionnels (entrant et sortant) au travers de seulement un connecteur, en utilisant un diviseur interne spécial. VIVO est à l'honneur sur plusieurs cartes graphiques, comme la collection nVIDIA GeForce et la collection ATI Radeon. Elle peut recevoir des données par la prise RCA ou la S-Vidéo. Par contre, en ce moment, l'utilisation de S-Vidéo est plus répandue (pour VIVO) autant par nVIDIA que par ATI. Certains câbles VIVO supportent aussi une sortie vidéo composite luminance, et 2x chrominance, ce qui permet le support de la télévision à haute définition. Même si cette connexion n'est pas numérique, elle ne supporte pas le HDCP qui serait requis pour le support HDTV officiel, conformément aux règles de l'EICTA.   Portail de l’informatique
Les vis pour boîtier de PC servent à maintenir ensemble les éléments d'un PC. Bien qu'il y ait un grand nombre de fabricants de boîtier pour PC, seulement trois types de filetage de vis sont d'usage courant. Les vis 6-32 sont souvent utilisées pour les disques durs et les éléments du boîtiers ; pour les couvercles et la fixation des cartes filles. Les vis M3 servent à fixer les lecteurs de disques optiques, les lecteurs de disquettes. Des vis à main 4-40 sont utilisées sur les connecteurs VGA, DVI, ports série, ports parallèle et les vieux ports de jeux. Toutefois certains constructeurs ont conçu des boîtiers dépourvus de vis et ne nécessitant aucun outil pour le montage et le démontage.
WHQL (sigle de Windows Hardware Quality Labs) est un laboratoire de Microsoft ayant la charge de certifier la qualité des pilotes pour le système d'exploitation Windows. C'est également une méthode de certification de Microsoft permettant de garantir qu'un ensemble matériel permet d'exécuter correctement Windows ou une de ses sous-parties (un gestionnaire de pilotes, par exemple).
Le XT3 (troisième génération du serveur XT) est un serveur vidéo professionnel de production développé par la firme liégeoise EVS. Apparu en avril 2011, il est le successeur des serveurs XT, XT2 et XT2+. Ce serveur vidéo permet aux sociétés de diffusion d’enregistrer, de contrôler, de lire et de diffuser les médias de façon simple, instantanée et intelligente. Conçu à l’origine pour répondre aux conditions extrêmes et à la cadence soutenue de la production sportive, le serveur XT3 est devenu partie intégrante de tous les environnements en direct ou quasi direct, en ce compris les diffusions en studio. Il offre un enregistrement en boucle ininterrompu et une numérisation multi-canal du contenu vidéo et audio provenant de n’importe quelle source. Grâce aux fonctionnalités réseau du XT3, tous les médias sont instantanément disponibles pour lecture ou montage en direct, se démarquant ainsi des systèmes traditionnels (avec cassette) qui nécessitent un temps d’attente, on parle de temps d'indisponibilité, plus ou moins long dû au manipulations des médias sur les cassettes (avance, copie, rembobinage, etc.). On parle, en anglais, de système tapeless (sans cassette). Les contenus audio-vidéo enregistrés par le serveur peuvent être également envoyés vers des équipements d’éditions tiers, par exemple des outils de montage spécialisés ou des systèmes d’automation (on parle alors de lecture en continu ou streaming en anglais), d’archivage et de stockage. L’architecture ouverte du serveur XT3 simplifie sa mise à niveau et offre des possibilités d’extension ultérieure. Il prend en charge de nombreux formats et codecs en mode natif, offrant ainsi une intégration transparente à n’importe quelle infrastructure de studio. Le XT3 assure la prise en charge native des codecs suivants : MJPEG, MPEG2, IMX, Avid DNxHD, Apple ProRes 422, Apple ProRes 422 HQ et AVC-Intra. Il permet ainsi d'éviter l'étape de transcodage qui peut parfois être longue et contre productive dans des environnements où l'instantanéité est importante.  Le serveur est entièrement conçu et créé avec des composants logiciels et matériels d’EVS à Liège. Il est piloté par les programmes Multicam[LSM] et/ou IPDirector Ce serveur a été utilisé à très large échelle au cours de tous les derniers grands évènements sportifs mondiaux : coupe du monde de football, rugby, MotoGP et Jeux olympiques,,, et est actuellement utilisé dans des grands studios du monde entier : BBC Football Programmes, NBC, France 2, Sky,, RTL-TVi, CCTV, ... Avec le Multicam(LSM), il permet de rejouer instantanément et à n'importe quelle vitesse n'importe quel angle de caméra, de réaliser des clips ou des listes de lecture. Dans cette combinaison, il est présent dans presque tous les cars du monde ,,,,,,,. Il a reçu un Emmy Award pour cette raison ainsi que d'autres prix d'innovation ,.
La cartographie d'information est une discipline qui trouve ses fondamentaux dans une pluralité de connaissances à la croisée de la gestion des connaissances, de la sémiologie graphique et des sciences cognitives. Elle fait appel à une grande variété de concepts issus de domaines et sous domaines parfois très spécialisés. Ce sont des outils privilégiés d'exploration et d'analyse de la complexité et des espaces de représentation (dimensions des données, réduction, projection/spatialisation),. Les cartes d'informations diffèrent des cartes géographiques, des cartes ou schémas heuristiques, des cartes éditoriales ou infographies, des cartes d'investigation et s'apparentent aux graphes, aux cartes sémantiques et dynamiques.
DEVS (de l'anglais Discrete Event System Specification) est un formalisme modulaire et hiérarchique pour la modélisation, la simulation et l'analyse de systèmes complexes qui peuvent être des systèmes à événements discrets décrits par des fonctions de transitions d'états et des systèmes continus décrits par des équations différentielles, par exemple et des systèmes hybrides (continus et discrets).
La fonction 91 de McCarthy est une fonction récursive définie par McCarthy dans son étude de propriétés de programmes récursifs, et notamment de leur vérification formelle. Avec la fonction de Takeuchi, elle est une exemple amplement repris dans les manuels de programmation.
En informatique, un logiciel est un ensemble de séquences d’instructions interprétables par une machine et d’un jeu de données nécessaires à ces opérations. Le logiciel détermine donc les tâches qui peuvent être effectuées par la machine, ordonne son fonctionnement et lui procure ainsi son utilité fonctionnelle. Les séquences d’instructions appelées programmes ainsi que les données du logiciel sont ordinairement structurées en fichiers. La mise en œuvre des instructions du logiciel est appelée exécution, et la machine est appelée ordinateur ou calculateur. Un logiciel peut être classé comme système, applicatif, standard, spécifique, ou libre, selon la manière dont il interagit avec le matériel, selon la stratégie commerciale et les droits sur le code source des programmes. Le terme logiciel propriétaire est aussi employé. Les logiciels sont créés et livrés à la demande d'un client, ou alors ils sont créés sur l'initiative du producteur, et mis sur le marché, parfois gratuitement. En 1980, 60 % de la production et 52 % de la consommation mondiale de logiciels est aux États-Unis. Les logiciels sont également distribués illégalement et la valeur marchande des produits ainsi distribués est parfois supérieure au chiffre d'affaires des producteurs. Les logiciels libres sont créés et distribués comme des commodités produites par coopération entre les utilisateurs et les auteurs. Créer un logiciel est un travail intellectuel qui prend du temps. La création de logiciels est souvent le fait d'une équipe, qui suit une démarche logique et planifiée en vue d'obtenir un produit de bonne qualité dans les meilleurs délais. Le code source et le code objet des logiciels sont protégés par la convention de Berne concernant les œuvres littéraires.
Un progiciel (mot-valise, contraction de produit, professionnel et logiciel), un logiciel professionnel standard ou parfois paquet logiciel (de l'anglais software package) est un terme commercial qui désigne un logiciel applicatif généraliste aux multiples fonctions, composé d'un ensemble de programmes paramétrables et destiné à être utilisé par une large clientèle. Un progiciel est une sorte de « couteau suisse » du logiciel, vendu en volume : plusieurs usages dans un même logiciel. Le mot a été lancé en 1973 et désigne à l'origine des logiciels produits de manière industrielle et distribués sur les marchés de masse (en). La production de ces logiciels est principalement réalisée par des multinationales de l'industrie du logiciel (en) tel que Microsoft, Oracle, IBM, SAP par exemple. Pour faire fonctionner son système d'information, une entreprise doit choisir entre des progiciels ou des logiciels spécifiques. Le progiciel est un logiciel standard, disponible sur le marché, tandis que le logiciel spécifique est construit sur demande, à l'attention et selon les vœux du client. Les mots progiciel, progiciel intégré, progiciel applicatif, progiciel de gestion intégré et ERP (le mot le plus usuel est ERP, acronyme anglophone) sont souvent utilisés pour désigner une catégorie de progiciels: des systèmes d'informations d'entreprise, qui comportent un ensemble de modules fonctionnels, couvrant typiquement les achats, les stocks, la production, les ventes, la distribution, ainsi que les salaires, la comptabilité, les finances et la trésorerie. Tous les modules sont généralement du même fournisseur, et travaillent sur une base de données unique.
Un système de communication d'urgence est un système (généralement assistée par ordinateur) qui est organisé dans le but principal de soutenir la communication unidirectionnelle et bidirectionnelle de messages d'urgence entre individus et groupes d'individus. Ces systèmes sont généralement conçus pour intégrer la coix-communication de messages entre une variété de technologies de la communication qui fortement un système de communication unifié qui peut être plus efficacement utilisée pour les situations d'urgence.[pas clair]
Un système logiciel est un ensemble de composants logiciels (programmes, documentations, fichiers de configuration). Un système logiciel est partie intégrante d'un système informatique, ce dernier étant composé de systèmes matériels et de systèmes logiciels.
L'accompagnateur du changement contribue à toutes les actions d'un projet (métier, informatique...) qui facilitent l'adoption et la maîtrise par les utilisateurs de nouvelle procédures, d'un nouveau logiciel... L'accompagnateur du changement est un chef de projet ou un assistant à maîtrise d'ouvrage qui organise les actions de : communication sur le projet informatique, sur son évolution formation sur le logiciel concerné documentation du logiciel formation métier organisation et modification des processus, procédures et modes opératoires. Dans ce cadre, il contribue aux actions de qualité dans l'entreprise.
L'administrateur de base de données (DBA : DataBase Administrator en anglais) est une personne responsable du bon fonctionnement de serveurs de bases de données, essentiellement relationnelles (OLTP) ou décisionnelles (OLAP), tant au niveau de la conception des bases, des tests de validation, de la coordination des intervenants, de l'exploitation, de la protection et du contrôle d'utilisation. Les compétences requises pour cette fonction sont multipolaires : système, développement, sécurité et fonctionnement des serveurs de bases de données. L'expérience y est prépondérante. Le DBA travaille souvent en relation étroite avec les administrateurs systèmes et les développeurs au sein d'une direction des systèmes d'information.
Un administrateur réseau est une personne chargée de la gestion du réseau, c'est-à-dire de gérer les comptes et les machines d'un réseau informatique d'une organisation (entreprise par exemple). Cela peut concerner notamment des concentrateurs, commutateurs, routeurs, modems, pare-feu, proxy, connectivité Internet, les réseaux privés virtuels (VPN). Il est souvent assisté d'un ingénieur architecte informatique qui conçoit une architecture réseau (voir aussi DMZ, DNS, LAN, NAT, SAN, WAN). L'administration de réseau est une discipline de l'informatique qui peut éventuellement s'étendre à la téléphonie. L'administrateur réseau est parfois également administrateur système, il gère alors également les postes de travail (PC, Macintosh), imprimantes et serveurs de l'entreprise.
Un administrateur sécurité est un informaticien responsable de la gestion des équipements de sécurité du système d'information d'une organisation.
Le rôle de l'analyste (parfois confondu avec celui de l'Ingénieur d'étude, l'Analyste-Programmeur) est d'étudier les différentes manières de mettre en œuvre un besoin informatique au sein d'un système d'information, sous la responsabilité du chef de projet. Un besoin informatique peut être : une demande d'un utilisateur du système informatique (SI), généralement dans le but d'améliorer l'ergonomie ou la performance du SI ; la mise en place d'une nouvelle fonctionnalité du SI ; l'intégration de contrainte extérieure au SI (réglementation) ; la correction d'une défaillance du SI.
L'analyste d'affaires est chargé d'évaluer les besoins d'une organisation ou d'une unité d'affaires et de documenter ses processus et systèmes et d'analyser son modèle d'affaires. Il agit généralement comme un agent de liaison entre les différentes parties prenantes de l'organisation.
Un architecte de données est un expert organisateur en informatique qui a la responsabilité de s'assurer que les objectifs stratégiques d'une organisation sont optimisés à travers l'utilisation de standards de données d'entreprise. Cela implique souvent la création et la mise à jour d'un registre de métadonnées centralisé. L'architecture de données concerne d'une façon générale la gestion des données. Elle inclut des domaines comme la gestion des métadonnées, la sémantique métier, la modélisation des données et la gestion de flux de métadonnées. Le travail de l'architecte de données inclut souvent la mise en œuvre d'un registre de métadonnées et autorise les parties prenantes spécifiques à chaque domaine à mettre à jour leurs propres données élémentaires. Un architecte de données a l'expérience des métiers et technologies suivants : Architecture de données Dictionnaire de données Registre de métadonnées Sémantique Entrepôt de données Bases de données Intégration d'applications d'entreprise Bases de données relationnelles Langage structuré de requêtes (SQL) XML Schéma XML XSLT (XML transformations)
En génie informatique, on appelle architecte informatique la personne chargée de l'analyse technique nécessaire à la conception du diagramme d'architecture, c'est-à-dire le plan de construction d'un logiciel, d'un réseau, d'une base de données, etc. Un architecte informatique peut travailler avec les développeurs du système actuel ou d'autres architectes informatique et produire par exemple les diagrammes suivants : diagramme d'architecture : un diagramme qui décrit dans les grandes lignes les acteurs d'un système informatique - humains ou machines, leur rôle, les flux d'informations entre les différents acteurs et les protocoles. diagramme d'architecture applicatif : diagramme qui décrit les flux d'informations entre un logiciel donné, et les autres acteurs du système d'information dans lequel il va être implanté. (usagers, autres logiciels: SGBD, annuaire,…) diagramme d'architecture technique : diagramme qui décrit les flux d'informations entre les différentes pièces d'un logiciel. diagramme d'architecture physique : diagramme qui décrit les flux d'informations entre les différents appareils d'un système d'informations. (serveurs, ordinateurs personnels, routeurs, …)  ...
Un assembleur d'ordinateur est la personne qui assemble les ordinateurs personnels, dits « PC » (Personal Computer). Le métier consiste à assembler les pièces qui composent l'ordinateur et à les installer dans un boîtier de protection pour qu'il fonctionne. Le boîtier des ordinateurs familiaux est appelé « tour » parce qu'il ressemble à une tour. Les composants assemblés sont, pour un PC d'entrée de gamme : le processeur plus son ventirad dit « de série », la carte mère, la barrette mémoire, aussi appelée mémoire vive ou RAM, une carte graphique si la carte mère ne possède pas de GPU embarqué, le disque dur le lecteur optique ou lecteur cd/dvd/blu-ray l'alimentation électrique, le boîtier. À ces composants peuvent être ajoutés : Un ventirad non fourni avec le processeur, très souvent plus performant, Des barrettes mémoires supplémentaires (dual-channel ou triple channel) Un ou plusieurs disques durs voire SSD dans le but de les utiliser en RAID ou indépendamment. Une ou plusieurs cartes graphiques peuvent aussi être ajoutées au GPU embarqué de la carte mère. Un système de refroidissement par eau (watercooling), très rare sur les PC montés en usine.  Portail de l’informatique
Les brigades de vérification des comptabilités informatisées (BVCI) sont une des structures du contrôle fiscal en France, appartenant à la Direction des Vérifications nationales et internationales (DVNI), elle-même appartenant à la Direction générale des Finances publiques (DGFiP) du ministère de l'Économie et des Finances. Les BVCI sont composées d'auditeurs informaticiens : des inspecteurs des finances publiques ayant en plus la qualification d'analyste. Les inspecteurs des BVCI interviennent sur demande et en appui des vérificateurs DGFiP, lorsque ceux-ci déterminent que l'état ou la complexité de l'infrastructure informatique de l'entreprise contrôlée le nécessite Et ceci : Soit pour lever un doute sur la réalité des chiffres issus des déclarations et de la comptabilité ; Soit pour s'assurer que la production des résultats par l'outil informatique a été réalisée dans le respect des dispositions relatives à la tenue de la comptabilité par un outil informatique (Article L13 - (Cf. Alinéa 2) du Livre des Procédures Fiscales) ; Soit pour utiliser simplement les données, voire les moyens techniques, de l'informatique de l'entreprise contrôlée aux fins des opérations de contrôle. Les modalités de contrôle possibles sont décrites à l'article Article L47 A du Livre des Procédures Fiscales.
Un bio-informaticien a des compétences en biologie, tout en maîtrisant l'informatique. Par ses travaux sur ordinateur, il aide et accompagne les biologistes dans la recherche de nouveaux traitements, l'analyse de molécules et de gènes ou la lutte contre les bactéries. Pour cela, il crée des logiciels sur mesure à la demande des biologistes[style à revoir]. De plus, il sait également utiliser les outils informatiques à des fins d'analyse, par exemple pour identifier la fonction de gènes. Un bio-informaticien a la possibilité de travailler dans des centres de recherche, dans les industries pharmaceutiques et alimentaires ou encore dans les sociétés de biotechnologies végétales[style à revoir].
Le Chief Data Officer (CDO, directeur des données) est un dirigeant de l'entreprise, responsable des données. Cette fonction découle directement des approches de type Big Data et sa nouveauté, ainsi que la définition encore peu précise du Big Data, en rendent les contours très variables. La responsabilité sur les données peut donner au CDO la responsabilité sur les analyses pour améliorer les décisions.
Un concepteur est une personne qui imagine et réalise quelque chose. Ce mot vient du verbe concevoir. En informatique, un concepteur est une personne qui est chargé d'identifier les besoins des utilisateurs et de les spécifier. Son rôle consiste en particulier à expliquer les concepts à des experts non informaticiens. Il invente et réalise un logiciel, un système d'exploitation ou un ordinateur. Les logiciels et les systèmes d'exploitation sont bien souvent fabriqués par des équipes de développeurs.
Le titre professionnel concepteur développeur informatique (CDI) est une certification de niveau II (équivalent bac + 4) du ministère chargé de l'Emploi (France). Ce titre peut être présenté à l'issue d'une formation de l'AFPA ou de tout autre organisme agréé par le Ministère chargé de l'Emploi comme l'AFPI. Il est également possible d'y accéder directement par la voie de la VAE. Ce titre a été modernisé par l'arrêté de révision] paru au Journal officiel en date du 23 octobre 2007, avec prise d'effet au 19 décembre 2008. Le titre est lié à la fiche ROME M1805, d'informaticien/informaticienne d’étude. Voici les activités et les compétences à mettre en œuvre dans l'emploi correspondant à ce titre, et qui font l'objet de la formation et de la certification CDI : Développer des composants d'interface (maquetter l'application ; programmer des formulaires et des états ; programmer des pages Web ; manipuler les données avec le langage de requête SQL ; développer les composants d'accès aux données ; installer les composants ; assister les utilisateurs ; organiser son temps ; communiquer dans un contexte professionnel ; utiliser l'anglais dans son activité professionnelle en informatique ; actualiser ses compétences techniques). Développer la persistance des données (modéliser les données ; mettre en place la base de données ; manipuler les données avec le langage de requête SQL ; Programmer dans le langage du SGBD : triggers et procédures stockées ; communiquer dans un contexte professionnel ; utiliser l'anglais dans son activité professionnelle en informatique ; actualiser ses compétences techniques). Développer une application x-tiers (définir l'architecture de l'application ; modéliser l'application à développer en utilisant UML ; appliquer une démarche qualité ; développer les composants métier ; manipuler les données réparties dans une architecture client/serveur x-tiers ; développer les composants de la couche de présentation (IHM) ; développer des composants intégrés à l'informatique nomade ; réaliser un test d'intégration ; déployer l'application ; animer l'équipe de développement ; communiquer dans un contexte professionnel ; utiliser l'anglais dans son activité professionnelle en informatique ; actualiser ses compétences techniques).
Un demomaker ou demoscener, est un mot du jargon informatique qui se traduit littéralement en « concepteur de démonstrations et d'autres productions de la scène démo ». Ce sont pour la plupart des étudiants ; assez jeunes (entre 15 et 30 ans) et passionnés d'informatique. Ils peuvent passer des journées voire des nuits au codage et à la programmation de leur demos. Ils prennent en général pour signer leur réalisation des pseudonymes mais peuvent aussi le faire en nom propre.[réf. nécessaire] Le demomaker peut être : musicien programmeur (codeur) graphiste 2D (graphiste bitmap) ou 3D swapper, celui qui s'occupe de la diffusion des productions. Les pays les plus connus et les plus actifs dans le domaine du demomaking sont la Scandinavie (Norvège, Suède et Danemark), la Finlande, l'Allemagne, les Pays-Bas, l'Espagne, la France, la Hongrie, la Pologne et la Suisse. Des pays de l'Est (Russie...) ainsi que la Grèce sont récemment arrivés avec d'assez bonnes réalisations.[réf. nécessaire] Comme une démo est assez longue à réaliser (le travail se compte en semaines voire en mois)[réf. nécessaire], et comme les personnes y travaillant sont plutôt spécialisées dans des disciplines spécifiques (un codeur est rarement un bon compositeur de musique par exemple)[réf. nécessaire], les demomakers s'unissent au sein de groupes (demogroups) auxquels ils donnent un nom. Un groupe de la scène démo est comme une firme, il a son nom (toutefois rarement déposé) et surtout une réputation. Les groupes de demomaking les plus connus[réf. nécessaire] sont : Future Crew, Impact Studios, Pulse, Red Sector, Kefren, Paradox, Razor 1911, Melon Dezign, Scoopex. Ils se rencontrent entre eux sur des canaux de discussion informatique comme IRC mais aussi à l'occasion de demoparties comme l'Assembly qui se tient généralement tous les ans en Finlande. Muttonheads a aussi commencé sa carrière par la scène démo vers 1990, sur Amiga.
En informatique, un développeur ou analyste programmeur est un informaticien qui réalise des logiciels et les met en œuvre à l'aide de langages de programmation.
Un développeur full stack est un informaticien capable de réaliser des tâches à n'importe quel niveau technique de la pile des différentes couches qui constituent une application informatique. De façon plus précise, il est capable de : configurer l'infrastructure matérielle et le système d'exploitation ainsi que les dépendances entre les différents composants logiciels à utiliser ; concevoir, manipuler et interroger des bases de données ; concevoir le code de 'back-end' et les API d'accès à ces données sous forme de webservices, en utilisant des langages tels que Java, Python, Ruby, PHP ; concevoir le code de 'front-end' qui est exécuté sur la plate-forme de l'utilisateur, le plus souvent un smartphone ou un navigateur Web, dans des langages tels que Javascript et ses bibliothèques, HTML, CSS ; être capable de gérer et planifier un projet avec des méthodes de développement agiles telles que Scrum ou Kanban, pouvoir interagir avec le client et ses besoins, et documenter l'ensemble des parties du projet. Un développeur full stack est ainsi familier avec chacune de ces couches, même s'il a souvent une affinité ou des compétences plus étendues dans l'une d'elles. Cette connaissance large lui permet de travailler sur un projet de A à Z sans avoir à recourir à d'autres développeurs pour les petits projets. Pour les gros projets, cela rend la vie plus facile aux autres développeurs du même projet, car il sait comment les différentes parties communiquent et s'articulent. Son caractère pluridisciplinaire permet également de faciliter les remplacements ponctuels ou durables.[réf. nécessaire]
Un développeur graphique est un développeur qui se charge de l'aspect esthétique qu'aura un logiciel. Il utilise, comme les autres développeurs sans but spécifiques, un langage informatique comme le Java. Contrairement au développeur qui se charge des scripts, il peut voir l'effet de son travail à tout moment en lançant la partie du logiciel qu'il a développé.  Portail du travail et des métiers
Dans le champ de l'informatique, on nomme développeurs les personnes qui conçoivent et mettent à jour les logiciels informatiques. Ce sont des professionnels salariés ou indépendants travaillant pour des entreprises informatiques ou des commanditaires particuliers, ou des bénévoles œuvrant le domaine du logiciel libre. Les développeurs travaillent souvent en communautés de développeurs.
Un développeur web est un programmeur spécialisé, ou expressément impliqué dans le développement des applications du World Wide Web, ou des applications qui sont exécutées à partir d'un serveur web sur un navigateur web et qui utilisent le protocole HTTP comme vecteur de transmission de l'information.
Le directeur des systèmes d'information (DSI), ou directeur informatique (DI), parfois directeur de l'organisation et des systèmes d'information (DOSI) ou encore plus récemment directeur des systèmes d'information et du numérique (DSIN) d'une organisation (entreprise, association, etc.) est responsable de l'ensemble des composants matériels (postes de travail, serveurs, équipements de réseau, systèmes de stockage, de sauvegarde et d'impression, etc.) et logiciels du système d'information, ainsi que du choix et de l'exploitation des services de télécommunications mis en œuvre. Dans le cadre de la transformation numérique, la DSIN va aussi prendre en charge l'innovation et les services numériques dans une logique d'orientation et de parcours numérique client (utilisateurs).
Le mot domaineur est dérivé de l'anglais Domainer. Un domaineur est un entrepreneur ou un investisseur spécialisé dans les noms de domaine ; il enregistre (crée) ou rachète des noms de domaine dans l'espoir d'en tirer profit tout en étant conscient qu'il peut tout aussi bien réaliser des pertes. Il est souvent confondu à tort avec le cybersquatteur. Contrairement à ces derniers, les domaineurs cherchent rarement à détourner la notoriété des marques à leur profit. Avant le boom du CPC, les domaineurs se focalisaient surtout sur les noms de domaine dits génériques tels que « acheter.com », « dormir.com », « sortir.net » etc. C'est encore le cas aujourd'hui, même si de plus en plus de domaineurs cherchent à rentabiliser leurs investissement en développant leurs noms de domaine. En leur donnant ainsi une valeur ajoutée, le nom de domaine peut parfois se revendre à meilleur prix sur des sites spécialisés dans la vente de sites web, comme Flippa ou Freemarket. Beaucoup de ventes de noms de domaine concernent les noms recevant du trafic (tels que les sites ayant existé et dont le domaine n'a pas été renouvelé) car ces derniers peuvent rapporter plusieurs dizaines ou même centaines d'euros par mois à leur propriétaire. L'exemple type est une page dite de parking qui propose au visiteur un assortiment de liens publicitaires ; lorsqu'un visiteur clique sur l'un de ces liens, une certaine somme allant de quelques centimes d'euro à plusieurs euros est reversée au propriétaire du nom de domaine. À ce niveau, la compétition est forte, et une dizaine de compagnies de parking se partagent le marché. L'activité du domaineur est nommé « domaining ». Cette industrie, peu développée du côté francophone, l'est toutefois beaucoup plus chez les anglophones. Plusieurs blogs, forums et journaux en ligne s'y consacrent entièrement. Le site DnJournal publie ainsi à chaque semaine un résumé de principales ventes de la semaine, avec des ventes dépassant souvent les 100 000 USD par nom de domaine L'industrie de l'achat-vente de noms de domaine, autrement dit le second marché, génère des dizaines de millions de dollars mensuellement en chiffre d'affaires. Les principales places de marché sont Sedo, Godaddy, Freemarket, Flippa et Afternic. Cette industrie connait des nouveaux développements avec l'arrivée des nouvelles extensions, comme le .paris, le .boutique, le .quebec. le .web, etc.
L'exploitation informatique est l'activité qui consiste à maintenir opérationnel de manière stable, sûre et sécurisée un outil informatique dans un environnement de développement, de qualification, de formation, ou de production, dans ses parties matérielles et logicielles.  Portail de l’informatique  Portail du travail et des métiers
On distingue dans la Maîtrise d'ouvrage (MOA) plusieurs rôles ou fonctions : le maître d’ouvrage stratégique (MOAS) ; l'architecte métier ; le maître d’ouvrage délégué (MOAD) ; le sponsor ; le maître d’ouvrage opérationnel (MOAO) ; l’assistant à maîtrise d’ouvrage (AMOA) ; l’expert métier (EM) ; l’utilisateur, au service duquel se trouvent toutes les autres fonctions ; la coordination des maîtrises d'ouvrage.
FUT est un sigle pour Friendly User Test, c'est-à-dire beta testeur. Un FUTeur est une personne acceptant de tester et de détecter les bugs d'une application ou logiciel dans une version en cours de déploiement ( non finalisée) et avant généralisation. L'usager volontaire du nouveau service accepte de collecter des informations précieuses aux développeurs . Les désagréments générés se transforment en participation active à la résolution des défauts rencontrés .  Portail du logiciel
L’hébergement de nom de domaine consiste à fournir les services des serveurs DNS requis pour assurer la publication d’un nom de domaine sur Internet. Il existe une confusion avec les deux notions suivantes : hébergeur Web, registrar. La plupart des registrars proposent la possibilité de fournir les serveurs DNS, mais pas tous. Au moment de l’enregistrement, il est possible qu’un registrar demande les adresses IP des serveurs DNS requis pour l’enregistrement (deux serveurs de noms au minimum), sans en proposer. Le propriétaire d’un nom de domaine va donc souvent se tourner vers son hébergeur web ou créateur de site web, qui propose très souvent de fournir les deux prestations (hébergement Web et nom de domaine). Mais il peut alors être plus difficile de changer son hébergeur web, une fois qu’il assure également la gestion DNS. Il existe pourtant une troisième alternative, surtout si vous êtes amené à devoir gérer plusieurs noms de domaines, l’utilisation de sociétés spécialisées dans l’hébergement de nom de domaine. Elles fournissent en général de base les avantages suivants : un portail web de gestion centralisé pour tous les noms de domaines, des services de rappels d’échéances et/ou d’auto-prolongation des noms de domaines, plus de deux serveurs DNS, situés chez de multiples opérateurs Internet, dispersés à travers le globe (au lieu de deux serveurs fragilisés car situés sur le même sous-réseau IP), des redirections de courriers électroniques, des redirections web, des fonctions DNS avancées pour gérer des CNAME, MX (MX record) ou autres éléments techniques propres aux serveurs DNS. Ces hébergeurs de noms de domaines, agissent systématiquement comme registrar ou plus exactement et généralement, comme “sous-registrar”. Le plus simple et le plus efficace est de les utiliser pour réserver un domaine.
L'infogérance (cas particulier d'externalisation) est un service défini comme le résultat d'une intégration d'un ensemble de services élémentaires, visant à confier à un prestataire informatique tout ou partie du système d'information (SI) d'un client, dans le cadre d'un contrat pluriannuel, à base forfaitaire, avec un niveau de services et une durée définie (définition de l'AFNOR). En d'autres termes, c'est l'externalisation de tout ou partie de la gestion et de l'exploitation du SI à un prestataire informatique tiers (SSII nouvellement appelé ESN). Cette prestation doit s'effectuer dans la durée et non de manière ponctuelle. L'externalisation informatique est la démarche qui conduit à l'infogérance. Son objectif est donc, historiquement, de réduire les coûts. Cependant les entreprises l'utilisent de plus en plus comme vecteur de transformation de leur système d'information et de leurs processus afin d'améliorer leurs performances, ainsi que pour pouvoir se recentrer sur leur métier de base. Actuellement, ce terme correspond aussi bien à la maintenance du parc, qu'à la gestion de projets, qu'à la sécurité informatique et même qu'aux formations.
Un informaticien ou une informaticienne est une personne qui exerce un métier dans l'étude, la conception, la production, la gestion ou la maintenance des systèmes de traitement de l'information. La définition populaire désigne le technicien ou l'ingénieur généraliste d'un système informatique. Comme pour le mathématicien, l'informaticien est plus strictement un scientifique diplômé d'un doctorat en informatique. Ses recherches se basent notamment sur la théorie de l'information.
Un informaticien de gestion est un informaticien spécialisé dans l'informatique de gestion.
Un ingénieur commercial en informatique est une personne chargée de vendre des logiciels et/ou des prestations de services en informatique.
En informatique, l'ingénieur système est un informaticien chargé de l'étude, des tests, et de la mise en place d'architectures basées sur un système d'exploitation destinées à un ou plusieurs serveurs informatiques. Contrairement à l'administrateur système qui exécute des procédures pré-définies, l'ingénieur système élabore des solutions qui découlent de l'analyse des besoins fonctionnels de l'entité pour laquelle il travaille. Ses capacités d'abstraction lui permettent aussi de modéliser et documenter des systèmes informatiques complexes dans son domaine d'expertise. Dans l'industrie de l'armement, l'aéronautique ou les transports terrestres, l'ingénieur système est un ingénieur pluridisciplinaire (génie informatique, génie civil, électronique, automatique, productique, etc.) chargé de spécifier ou de concevoir des systèmes complexes.   Portail du travail et des métiers
Un intégrateur web est une personne maitrisant différents métiers du web. Il développe des sites web professionnels respectant les dernières normes en matière de compatibilité, notamment les normes du World Wide Web Consortium (W3C). Les métiers du web sont nombreux. Le schéma ci-contre vous en donne un aperçu non exhaustif. L'intégrateur web joue aussi le rôle d'interface entre le client final et les experts informatiques. Il traduit ainsi en langage clair les explications et demandes émanant des prestataires de services web. L'intégrateur web assure également le suivi budgétaire du projet pour autant que le projet soit pertinent. La pertinence du projet est la première mission à laquelle doit s'attacher l'intégrateur web. C'est en effet à lui que revient la mission de valoriser de manière tant quantitative que qualitative l'opportunité pour une entreprise (principalement visées sont les TPE et PME de moins de 20 personnes) de développer son volet de communication numérique et aussi le niveau de cette communication qu'il convient de mettre en œuvre (site vitrine, publipostage électronique, site de commerce électronique, tunnel de commande, etc.). Enfin, la dernière mission mais non la moindre est l'accompagnement pas-à-pas du développement du projet dans ensemble.
La maîtrise d'œuvre (souvent abrégée MOE ou MŒ ou Moe ou moe) est la personne ou l'entité choisie par le maître d'ouvrage pour la réalisation, soit la mise en œuvre d'un projet dans les conditions de délais, de qualité ainsi que de coûts fixés par ledit projet, le tout conformément à un contrat ou un cahier des charges. Pour la partie relative au marché de travaux, la maîtrise d’œuvre désigne une personne physique ou morale qui, pour sa compétence, peut être chargée par le maître de l'ouvrage : de l'assister pour la consultation des entreprises et pour la conclusion du ou des marchés avec le ou les entrepreneurs ; de diriger l'exécution du ou des marchés de travaux ; d'assister le maître de l'ouvrage pour la réception des ouvrages et le règlement des comptes avec les entrepreneurs. Il est le plus souvent utilisé en conjonction avec le terme maître d'ouvrage qui désigne le propriétaire de l'ouvrage ou commanditaire des travaux (État, collectivités, entreprises publiques, maître d'ouvrage privé), et qui exécute la passation des marchés. Initialement employé dans le secteur économique du bâtiment et des travaux publics, le diptyque maître d'ouvrage/maître d'œuvre est très utilisé dans le domaine de l'informatique, aussi bien lors de la passation de marchés publics que dans le secteur privé.
La maîtrise d'ouvrage (MOA), aussi dénommée maître d'ouvrage est l'entité porteuse d'un besoin, définissant l'objectif d'un projet, son calendrier et le budget consacré à ce projet. Le résultat attendu du projet est la réalisation d'un produit, appelé ouvrage. La maîtrise d'ouvrage est à l'origine de l'idée de base du projet et représente, à ce titre, les utilisateurs finaux à qui l'ouvrage est destiné.
Médiamaticien est une profession du domaine de l'information et de la communication, exercée en Suisse par une personne formée à la médiamatique. Cela comprend différents domaines de compétence : La création et la retouche d'images ; L'informatique (matériel et logiciel) ; La photographie, la vidéo ; La création de support papier (affiches, magazines, prospectus) ; Le design et la création de sites web complets avec différents langages (HTML, CSS, JavaScript, PHP, MySQL) ; Les présentations interactives ; La maîtrise des outils de bureautique ; La gestion d'entreprise et la comptabilité, le droit et l'économie ; Le marketing.
Les métiers du web sont l'ensemble des métiers ayant pour but le développement et la maintenance de sites web. La classification des différents métiers du web est changeante selon les études, les pays, les différents auteurs. Ces métiers sont plus des fonctions regroupées en métiers selon l'entreprise, ou l'organisation. Pour plus de facilité on peut regrouper ces métiers en sept familles. Ces métiers sont en pleine évolution, tant au niveau des outils techniques, que des missions qui sont attribuées aux professionnels.
Le modeleur est la personne qui exécute des modèles d'après un objet ou un dessin de cet objet.
Un modérateur ou une modératrice (abrégé "Mod" ou "Modo") est un internaute, souvent bénévole, dont le rôle est d'animer et surtout de modérer le forum d'un site internet (généralement communautaire), d'un jeu vidéo, d'une communauté ou d'une discussion. Les modérateurs avertissent les joueurs, régulent leur langage s'il est incorrect, efface les messages qui n'ont pas leur place sur le forum, soit parce qu'ils contreviennent à la loi, soit parce qu'ils enfreignent les règles explicites ou implicites du forum. Le modérateur efface après discussion et justification les messages d'insulte ou de diffamation, ceux à caractère raciste ou incitant à la violence ou à la haine. Un modérateur se doit d'être impartial dans le débat, de tempérer son opinion personnelle afin qu'elle n'entache pas son jugement, et de rappeler à l'ordre les participants hors-sujet ou irrespectueux. La plupart du temps, les forums ont une « charte » que le modérateur doit faire respecter et qui l'aide à savoir ce qu'il doit sanctionner. Avant d'avoir le rôle de censurer, un modérateur a un rôle de présence afin de dissuader les membres de commettre des abus.
Un pilote opérationnel d'exploitation (POE) est, dans le domaine de l'informatique, une personne expérimentée techniquement et dans la gestion de projets et d'affaires. Grâce à ses compétences fortes en communication, son rôle est de coordonner l'activité des différents intervenants durant toute la vie d'une application. Il peut toutefois être titulaire de plusieurs applications et intervenir dans plusieurs contextes tous aussi variés les uns que les autres. Il est en contact permanent avec la maîtrise d'ouvrage (MOA), le maintien en condition opérationnelle (MCO), l'exploitation, le support technique et les services d'assistance. Le pilote opérationnel d'exploitation (POE) est garant de la disponibilité des applications.
Le rédacteur de documentation en informatique a pour fonction la production des textes et autres médias destinés à l'assistance des utilisateurs de logiciels. Les documentations peuvent être : la documentation métier. Elle décrira les fonctions du logiciel en fonction du métier de l'utilisateur. les procédures métier. Ces procédures décriront les actions de l'utilisateur sur le logiciel et hors logiciel. la documentation générale d'un logiciel. Elle décrira souvent toutes les fonctionnalités du logiciel. Ces documentations peuvent être : un manuel ou livre imprimé ; une aide en ligne accessible à partir du logiciel ou téléchargeable. une aide intégrée au logiciel ; De plus en plus, le métier de rédacteur de documentation rejoint le métier de formateur pour constituer le métier d'accompagnateur du changement.
Le rédacteur web est un des nouveaux métiers liés à l'internet. Ce métier consiste à produire des contenus rédactionnels adaptés au web. Les règles d'écriture diffèrent de celles de la presse écrite : l'écriture doit être concise, rapidement compréhensible et le style dynamique. De plus, l'avantage réside dans le fait de pouvoir modifier son contenu à n'importe quel moment grâce à un outil de gestion de contenu. Cette écriture doit également prendre en compte la dimension interactive du web avec les commentaires et les liens hypertextes par exemple. Elle doit également correspondre aux exigences des moteurs de recherche en prenant compte de la densité de mots-clés. C'est un métier où il faut allier technique et écriture. L'orthographe et la syntaxe se doivent d'être irréprochables. Une maîtrise de l'outil informatique est donc indispensable. Le rédacteur web peut exercer en externe, dans une agence Web ou en indépendant. Il peut occuper un poste en interne au sein d'une rédaction spécialisée à cet effet.
Le responsable de la sécurité des systèmes d'information (RSSI ; en anglais, Chief information security officer ou CISO) d'une organisation (entreprise, association ou institution) est l'expert qui garantit la sécurité, la disponibilité et l'intégrité du système d'information et des données.
Le monitoring ou monitorage est une activité de surveillance et de mesure d'une activité informatique. On parle aussi de supervision. Les raisons peuvent être variées : mesure de performance, en termes de temps de réponse par exemple mesure de disponibilité, indépendamment des performances mesure d'intégrité, l'état des processus sur une machine Unix par exemple, ou bien qu'une page web n'a pas été modifiée (sécurité informatique) mesure de changement, surveillance de sites de News avec Google Actualités
Technicien en maintenance informatique est un métier lié à l'entretien d'ordinateurs, de parcs informatiques ou d'éléments liés à l'univers informatique. Les domaines du technicien en maintenance informatique en entreprise sont : le matériel (monter un ordinateur pièce par pièce, installer des périphériques) le logiciel (installation des logiciels, de système d'exploitation ou des mises à jour) la maintenance (sauvegardes, débogage, les lags, les virus) la documentation et la formation aux logiciels étude et choix de logiciels à installer Virus, connexion impossible, erreur système, clavier défectueux, messagerie électronique mal configurée… Voilà ce à quoi le technicien de maintenance en informatique doit faire face au quotidien. En qualité d’expert, il est capable d’intervenir dans des domaines distincts. Son objectif est de résoudre rapidement et de manière efficace les problèmes relevant de ses compétences. Pour ce faire, il peut être amené à réparer ou changer des pièces défectueuses. Mais il doit avant tout réaliser un diagnostic pour proposer la solution la mieux adaptée. Ce bilan, il peut aussi l’effectuer par téléphone, et assister les utilisateurs en difficulté à distance. Téléassistant (ou hotliner, qui fait de l'assistance par téléphone), il accueille les demandes des utilisateurs confrontés à un dysfonctionnement. Il est particulièrement sollicité lors des phases d’installation et de mise à jour des matériels ou des logiciels. Pour pouvoir résoudre les problèmes, il pose des questions filtres (préétablies) afin d’identifier les données d’utilisation et de fonctionnement, ainsi que la configuration de la machine concernée. Cela lui permet d’émettre un diagnostic plus rapide. Il dispose généralement d’une base de données qui recense les pannes les plus courantes et les moyens d’y remédier. Selon la taille de l’entreprise, il effectue parfois l’installation de nouveaux matériels ou applications. Il forme et conseille les utilisateurs. La gestion du parc informatique de l’entreprise (composé de postes utilisateurs et de serveurs) est assurée par ses soins ou par le gestionnaire du parc micro-informatique. Il prend également en charge les achats, la planification et la réalisation des mises à jour. Garant de la sécurité, il veille au droit d’utilisation des logiciels et gère, en conséquence, les numéros de licence. Il procède enfin au suivi des incidents, si ces derniers se produisent de façon récurrente. Par système d'exploitation nous parlons bien sûr, surtout de : MS-DOS, Windows 95 / 98 / 98SE / ME / NT / 2000 / XP / Vista / 7 / 8 / 8.1 / 10, Linux, BeOS, OS/2. Le technicien en maintenance informatique est la ressource première d'une entreprise pour la formation à l'informatique, l'entretien et l'installation de nouveaux postes. Il y a technicien en informatique, technicien en réseautique, programmeur analyste, gestionnaire ou administrateur de réseau et bien d'autres. Bien que peu importe son titre, chacun peut être en mesure de faire plusieurs opérations techniques. Les sites web de recherche d'emploi regorgent d'ailleurs du titre "technicien informatique". Cependant, les tâches demandées sont souvent hors contextes. Un technicien en maintenance informatique en réalité ne fait pas de programmation, de gestion de base de données et d'administration réseau.
Un technicien supérieur de support en informatique est un technicien délivrant des services d'assistance en informatique. Ce titre est défini par la législation française.
La ou le Technicien(ne) supérieur(e) en réseaux informatiques et télécommunications a pour mission d’installer, mettre en service et assurer le bon fonctionnement des réseaux informatiques et télécommunications (voix, données, images) en veillant à leur disponibilité, leur sécurité et en participant à leur évolution dans le respect des procédures, de la politique de sécurité et des contrats de services.
Le technicien (ou la technicienne) supérieur(e) gestionnaire exploitant(e) de ressources informatiques met en exploitation, suit, sécurise, optimise et fait évoluer les ressources nécessaires à la production attendue du système d’information de l’entreprise : infrastructures réseaux, serveurs et systèmes d’exploitation, bases de données, applications transverses et applications métier, etc. Sa mission s’exerce dans le respect des méthodes, normes, standards du marché, des contrats passés avec les fournisseurs et des contrats de service conclus avec les utilisateurs.
Un testeur, en informatique, est une personne chargée de tester le logiciel produit par les programmeurs, et de livrer à intervalles réguliers un rapport décrivant les erreurs trouvées dans le produit. Les erreurs sont appelées « bugs » ou parfois « bogues » (forme francisée), et leur description doit inclure les étapes nécessaires pour reproduire le bug, afin qu'il soit corrigé. Il représente le client ou l'utilisateur final et, à ce titre, il est chargé de vérifier le bon fonctionnement du logiciel. On parle alors de tests de recette. Des tests dits de non-régression doivent ensuite être faits sur les versions subséquentes du logiciel, une fois que les programmeurs disent avoir réglé le bug, pour vérifier que les modifications ont bel et bien corrigé l'erreur et n'en ont pas introduit de nouvelles. On peut distinguer plusieurs types de tests : le test fonctionnel. Son objectif est de vérifier que le logiciel se comporte tel que prévu par le cahier des charges. le test linguistique. Dans le cas d'un logiciel distribué dans plusieurs langues ou dans plusieurs pays, le test linguistique a pour objectif de vérifier la qualité de l'internationalisation et de la régionalisation du logiciel.
Un ou une webmestre, webmaster ou administrateur ou administratrice de site est une personne responsable d'un site web, de sa conception à sa maintenance. Le ou la webmestre est une personne professionnelle des métiers d'internet.
1BASE5 (aussi appelé StarLan) est un standard de liaison Ethernet (aujourd'hui obsolète) créé en 1987 et mis au point par le groupe de travail IEEE 802.3e du sous-comité de standardisation IEEE 802.3. Celui-ci permet la transmission de données jusqu'à un débit de 1Mbits/s sur paires torsadées non blindée et sur une longueur pouvant atteindre 200 mètres.
10BASE-T est une norme Ethernet spécifiant une couche physique du modèle OSI utilisant une topologie réseau en étoile et des câbles à paires torsadées équipés de connecteurs RJ45.
10BASE2 (aussi appelé Ethernet Fin ou Thin Ethernet) est un standard Ethernet standardisant une couche physique dans le modèle OSI utilisant un câble coaxial fin. Celui-ci permet le transfert de données à des débits jusqu’à 10 Mbit/s. Plus simple et plus économique que le 10BASE5, cette solution s’est vite imposée pour un câblage simple.
10BASE5 est une norme Ethernet spécifiant une couche physique du modèle OSI utilisant une topologie réseau en bus, d'une longueur maximale de 500 mètres avec 100 connexions espacées au minimum de 2,50 m et une vitesse de 10 Mbit/s. Il était aussi appelé Ethernet épais ou Thick Ethernet. Son support est du câble coaxial épais relié aux cartes réseaux par des émetteur-récepteurs (transceivers).
10BROAD36 est un standard de liaison Ethernet (aujourd'hui obsolète) initialisé en 1985 et mis au point par le groupe de travail IEEE 802.3b du sous-comité de standardisation IEEE 802.3. Celui-ci permet la transmission de données jusqu'à un débit de 10Mbit/s sur du câble coaxial 75 ohms (Type CATV) et sur une longueur pouvant atteindre 3600 mètres.
10GBASE-T est une norme Ethernet initiée en 2002 et mise au point par le groupe de travail IEEE 802.3an en liaison étroite avec le comité ISO/IEC 11801 JTC 1/SC25/WG3. Cette norme ratifiée le 8 juin 2006 prévoit la transmission de données jusqu'à un débit de 10 Gbit/s sur du câble en cuivre à paires torsadées et à une distance jusqu'à 100 mètres.
80 PLUS est une initiative lancée en 2004 par Ecos Consulting pour promouvoir le rendement électrique des blocs d'alimentation des ordinateurs. Elle certifie qu'au moins 80 % de l'énergie reçue en entrée est effectivement transmise à la machine (c'est-à-dire moins de 20 % de pertes par effet Joule). De nombreuses entreprises ont adhéré à ce programme, comme Dell, HP et Lenovo. Depuis 2004, plusieurs évolutions du label ont vu le jour, augmentant à chaque fois le rendement exigé. La liste des certifications « 80 Plus » est disponible ci-dessous.
Le 100BASE-sx est une norme Ethernet qui supporte le full-duplex par l’utilisation de deux paires torsadées. Elle a été inventée par Lahcen Abidar. Le câblage 100BASE-SX peut être utilisé en topologie étoile ou sous forme de bus linéaire, d'une longueur maximale de 100 mètres entre deux équipements pour un débit de 100 Mbit/s. il vient corriger les inconvénients de la technologie 100BASE-TX. Son support est la paire torsadée (2 paires). Les deux des paires sont en câble catégorie 5 blindé ou câble catégorie 1 non blindé. La norme recommande l'utilisation du câble catégorie 5 et la limitation de la longueur du câble à 90 mètres entre prises murales réseau et l'équipement d'interconnexion auquel elles sont reliées, pour réserver 10 mètres au raccordement entre prise et matériel connecté.  Portail de l’informatique
Mis au point pour l'élaboration du Fast Ethernet en extrapolation de l'Ethernet (Norme IEEE). Pour une spécification telle que XBaseY : X désigne le débit binaire du réseau, exprimé en mégabits par seconde (Mbit/s). Y représente le type de connexion utilisée.  Ici on a Y = T : cela indique que c'est un réseau avec paires torsadées (Twisted Pair). Il peut y avoir un autre chiffre à côté du T qui représente le nombre de paires. Donc : 100BASE-T est un terme pour n'importe lequel des standards 100 Mbit/s sur paires torsadées. Cela comprend les standards 100BASE-TX, 100BASE-T4 et 100BASE-T2.
Le 100BASE-T4 est une norme de câblage réseau mise au point pour l'élaboration du Fast Ethernet 100BASE-T en améliorant la norme 10BASE-T afin d’augmenter la vitesse de transmission des réseaux informatiques et Ethernet.
Le 100BASE-TX est une norme de câblage réseau mise au point pour l'élaboration du Fast Ethernet 100BASE-T en extrapolation de l'Ethernet (norme IEEE).
La norme 1000BASE-T, aussi appelée Gigabit Ethernet, est une évolution de l’Ethernet classique. Celle-ci autorise des débits de 1 000 Mbit/s sur 4 paires de fils de cuivre de catégorie 5 ou supérieure (utilisation de connecteurs RJ45), sur une longueur maximale de 100 m. 1000BASE-T utilise chacune des 4 paires torsadées en mode full duplex, chaque paire transmettant 2 bits par baud, à l’aide d’un code à 5 moments. Soit un total de 1 octet par top d’horloge sur l’ensemble des 4 paires, dans chaque sens. Ce standard est compatible avec 100BASE-TX et 10BASE-T, il assure la détection automatique des taux d’envoi et de réception assurée. Celui-ci permet un fonctionnement sans switch, en mode « point à point ».
L'Academy Color Encoding System (ACES) est un système de gestion de couleurs développé pour l'industrie cinématographique par l'Academy of Motion Picture Arts and Sciences, qui organise également la cérémonie des Oscars du cinéma. L'ACES a été élaboré en étroite collaboration avec les fabricants de matériel. Il s'agit de proposer aux différents acteurs de la chaîne de traitement de l'image une référence colorimétrique stable dans le temps et indépendante du matériel. Pour cette raison, l'ACES est un système libre et ouvert, mis à la disposition des différents fabricants. Le film Chappie, de Neill Blomkamp, a notamment été tourné et finalisé par le chef-opérateur Trent Opaloch (de) à l'aide de l'ACES.
L’Advanced Configuration and Power Interface (ACPI) (soit en français « interface avancée de configuration et de gestion de l’énergie ») est une norme très largement répandue dans les ordinateurs personnels et codéveloppée par Hewlett-Packard, Intel, Microsoft, Phoenix et Toshiba. Le but de cette norme est de réduire la consommation d’énergie d’un ordinateur en mettant hors tension certains éléments : lecteurs CD-ROM, disques durs, écran, etc. Pour cela, une interface a été spécifiée qui permet au système d’exploitation d’envoyer des signaux à ces différents périphériques matériels (il faut que ces périphériques prennent en charge l’ACPI également). Cette interface permet aussi au matériel d’envoyer des signaux au système d’exploitation, par exemple lorsque l’utilisateur appuie sur le bouton de mise en route sur le clavier ou que le modem reçoit un appel. Cette norme est utilisée aussi bien sur les ordinateurs portables que sur les ordinateurs de bureau postérieurs à 1996. La toute dernière spécification, publiée en Janvier 2016, porte le numéro de version 6.1.
Apple Advanced Typography (AAT) est une technologie d’Apple de fonte numérique, principalement pour l’internationalisation et les fonctionnalités typographique avancées. Il succède à la technologie QuickDraw GX d’Apple des années 1990. C’est un ensemble d’extensions au format TrueType, similaire à la technologie OpenType développée par Microsoft et Adobe, la technologie Graphite de SIL International. Elle permet aussi de gérer des axes d’interpolation similaire à la technologie Multiple Master d’Adobe.
L'appellation Atom se réfère à deux standards liés. Le Format de Syndication Atom est un format ouvert de document basé sur XML conçu pour la syndication de contenu périodique, tel que les blogs ou les sites d'actualités. Pour l'utilisateur, la syndication est un procédé de souscription à des sources de contenu. Pour l'auteur, c'est un procédé de publication simultanée dans de multiples supports. Le Protocole de Publication Atom (APP) est un protocole simple basé sur HTTP pour la création et la mise à jour de ressources Web.
Le filtre BPF (Berkeley Packet Filter) est un langage permettant de filtrer les paquets échangés sur un réseau. Il est maintenu et a été conçu par les chercheurs du Laboratoire national Lawrence-Berkeley (États-Unis). Ce langage est utilisé entre autres par des logiciels spécialisés dans l'analyse d'échanges réseau (exemples : ngrep, tcpdump, snort, ntop) et sert à sélectionner des données transitant sur un réseau selon des critères précis, par exemple : Qui (quel est l'hôte qui) envoie les données ? Qui (quel est l'hôte qui) reçoit les données ? Quel est le protocole (Protocole réseau) utilisé ? Vers et depuis quel port sont envoyées les données ? etc.
BPDM (Business Process Definition Metamodel) est une proposition développée par l'Object Management Group (OMG).
Le Cadre commun d'interopérabilité (CCI) est un cadre qui définit les règles d'interopérabilité informatique entre les services gouvernementaux de l'administration, en France, dans le cadre du Référentiel général d'interopérabilité du programme ADELE. Ce cadre s'inspire du cadre européen : European Interoperability Framework, du programme IDABC qui travaille quant à lui à un plan européen d'interopérabilité. La vision européenne est également décrite par un ensemble de recommandations générales dont la première version a été publiée en novembre 2004.
CAN/CSA Z243.200-92, intitulé Claviers canadiens pour le français et l'anglais, ou clavier CSA est une norme canadienne de disposition des touches des claviers informatiques pour l’anglais et le français publiée en 1992 par l’Association canadienne de normalisation. Malgré sa disposition en QWERTY, il permet la saisie de tous les caractères accentués de la langue française. Il est conforme à la norme internationale ISO 9995 (publiée en 1994) et permet l’entrée des caractères de l’ISO 8859-1 (dit Latin-1) et de certains caractères de l’ISO 6937. Le clavier CSA, était jadis appelé clavier ACNOR, clavier canadien multilingue, clavier LaBonté, clavier canadien standard ou clavier québécois.  La norme a subi quelques évolutions, une version précédente comprenait des touches dites « mortes », comme celle qui permet de taper directement « ç » et qui a été remplacée par une touche cédille plus générale. Depuis 1999 ce clavier normalisé dispose du symbole de l'euro « € » « est exigé et utilisé entre autres par le gouvernement du Québec et le gouvernement fédéral. Les entreprises l’adoptent aussi pour sa facilité d’utilisation et pour l’uniformisation de leurs claviers,,,. » Le Commissariat aux langues officielles fédéral suggère également l'utilisation d'un clavier français, recommandant la norme CAN/CSA-Z243.200-92.
Les cartes topiques (ou cartes de thèmes, en anglais Topic Maps) sont un outil très général de représentation des connaissances, dont le but est d'agréger autour d'un point unique d'indexation (appelé topic) toutes les informations disponibles concernant un sujet donné, et de relier ces points par un réseau sémantique de relations appelées associations.
La CEI 61131-3, intitulée Automates programmables - Partie 3 : Langages de programmation, est une norme industrielle de la Commission électrotechnique internationale (CEI) définissant cinq langages de programmation à utiliser pour les automates programmables : Langage Ladder (schéma à relais) ; Sequential function chart (SFC), proche du langage Grafcet ; Boîtes fonctionnelles (FBD), sous forme de diagramme ; Texte structuré (ST) ; Liste d'instructions (IL), un pseudo-assembleur. Elle a été publiée la première fois en 1993, et une seconde édition a vu le jour en 2003.
Un code DTMF (dual-tone multi-frequency) ou FV (Fréquences Vocales) est une combinaison de fréquences utilisée pour la téléphonie fixe classique (sauf voix sur IP). Ces codes sont émis lors de la pression sur une touche du clavier téléphonique, et sont utilisés pour la composition des numéros de téléphones (en opposition aux anciens téléphones dits à impulsions, utilisant un cadran) ainsi que pour la communication avec les serveurs vocaux interactifs. Techniquement, chaque touche d'un téléphone correspond à un couple de deux fréquences audibles qui sont émises simultanément. De cette façon, huit fréquences bien distinctes permettent de coder seize touches. Ces fréquences peuvent être reconnues par des dispositifs électroniques et sont utilisées pour réaliser des serveurs vocaux. Dans le tableau suivant figurent des touches « A » à « D » : celles-ci étaient utilisées par l'armée américaine pour représenter la priorité d'une communication. Ainsi, le couple de fréquences correspondant à 1 est (1 209 Hz, 697 Hz), celui de 2 est (1 336 Hz, 697 Hz), et ainsi de suite, jusqu'à celui de D : (1 633 Hz, 941 Hz). Les huit fréquences utilisées restent dans la bande passante de la téléphonie fixe classique (qui se situe entre 300 et 3 400 Hz), et ont été choisies pour éviter les harmoniques.
Le Comité européen pour des systèmes interopérables (en anglais, European Committee for Interoperable Systems ou ECIS) est une organisation sans but lucratif créée en 1989 pour la promotion de systèmes interopérables.
CIF, abréviation de Common Intermediate Format, est une définition standardisée d'image numérique définie par l'Union internationale des télécommunications (ITU) : 352 × 288 pixels.  Portail de l’informatique  Portail de l’imagerie numérique
Le Common Warehouse Metamodel ou CWM est une spécification de l'OMG qui décrit un langage d'échange de métadonnées à travers un Entrepôt de données, un système décisionnel, un système d'ingénierie des connaissances (Gestion des connaissances), ou des technologies de portail. Cette spécification s'appuie sur les mêmes fondements (Core) que les spécifications du métamodèle UML, définies par le Meta-Object_Facility.
Le Compact Font Format (CFF), littéralement « format de fonte compact » en anglais, aussi appelé CFF/Type 2, est un format de compactage de fonte numérique développé par Adobe Systems. Il permet l’utilisation du Type 2 de charstring, une description de glyphes PostScript plus compact que celui des fontes de Type 1 ou des fontes CID, grâce à l’utilisation d’opérateurs à plusieurs paramètres, des valeurs prédéfinies par défaut, un codage plus efficace et des sous-routines réutilisables. Le CFF ou TrueType peuvent être utilisés dans le format de fonte numérique OpenType. Les fontes CFF peuvent être incorporées dans les fichiers PDF (depuis la version 1.2).
L'architecture Darwin Information Typing Architecture (DITA, littéralement Architecture Darwinienne d'information typée) est une norme de rédaction basée sur le XML, et dédiée au développement, à la conception, et à la publication de l'information technique. Le nom s'explique comme suit : Darwin : la norme inclut des principes d'héritage et de spécialisation. Information typing (typage d'information) : la sémantique des types des rubriques (concepts, tâches, références) est liée à la structure de leur contenu (phrase, liste d'étapes, tableau). Chaque rubrique a également un objectif défini. Architecture : DITA est flexible et permet de réutiliser et développer différents types de livrables (architecture modulaire). L'extensibilité de DITA permet aux organisations de définir des structures d'information spécifiques tout en continuant à utiliser une norme ET un format ouvert (c'est-à-dire à la fois documenté et non-propriétaire). La capacité à définir des architectures documentaires spécifiques pour une entreprise ou une industrie (voir les sous-comités du consortium OASIS) permet à DITA de réutiliser le contenu et de réduire les informations redondantes (en moyenne 50 %).
Embedded OpenType est un format de police créé par Microsoft à destination du World Wide Web, et supporté uniquement par Internet Explorer. Il reprend notamment le format OpenType en y ajoutant la compression MicroType Express. Le format Web Open Font Format 2 (WOFF2), regroupe les avantages de ce format et d'autres formats en les améliorants, notamment par une meilleure compression ou l'ajout de polices en couleur et pouvant être animées.  Portail d’Internet  Portail de l’écriture
Electronic Product Environmental Assessment Tool (EPEAT) est un écolabel permettant au consommateur d'évaluer l'effet d'un produit informatique sur l'environnement. Il distingue les produits certifiés en trois catégories répondant à différents critères de performance environnementale. EPEAT fut créé et est toujours géré par le GEC (Green Electronics Council), un programme de l'ISDF (International Sustainability Development Foudation), organisation à but non lucratif qui « imagine un monde où le commerce, les communautés et la nature se développent en harmonie ». Pour qualifier des produits électroniques à la norme IEEE 1680, famille de produits électroniques écoconçus, le GEC a signé un Mémorandum d'entente avec un groupe d'organismes d'évaluation technique et environnemental.
Evaluation and Report Language (EARL) est un vocabulaire, proposé par le W3C permettant d'exprimer des résultats de tests.
Extended Unix Coding (EUC) est un codage de caractères sur 8 bits utilisé premièrement par le japonais et le coréen. Au Japon, ce codage est intensivement utilisé par les systèmes d'exploitation de type Unix, mais est rarement utilisé ailleurs. EUC est cependant le moins utilisé des 3 principaux codage du japonais, derrière l'ISO-2022-JP (JIS) et le codage Shift-JIS.
XBRL (sigle de eXtensible Business Reporting Language) est un langage informatique basé sur XML, utilisé pour décrire les données financières. Le format XBRL est promu par ses créateurs depuis 1999 et se trouve actuellement à un stade de prototype avancé pour les taxonomies IFRS et intermédiaire pour les taxonomies COREP.
L'Extensible Markup Language (XML), « langage de balisage extensible » en français, est un métalangage informatique de balisage générique qui dérive du SGML. Sa syntaxe est dite « extensible » car elle permet de définir différents espaces de noms, c'est-à-dire des langages avec chacun leur vocabulaire et leur grammaire, comme XHTML, XSLT, RSS, SVG… Elle est reconnaissable par son usage des chevrons (<, >) encadrant les noms des balises. L'objectif initial de XML est de faciliter l'échange automatisé de contenus complexes (arbres, texte riche…) entre systèmes d'informations hétérogènes (interopérabilité). Avec ses outils et langages associés, une application XML respecte généralement certains principes : la structure d'un document XML est définie et validable par un schéma ; un document XML est entièrement transformable dans un autre document XML.
XSL (eXtensible Stylesheet Language) est le langage de description de feuilles de style du W3C associé à XML. Une feuille de style XSL est un fichier qui décrit comment doivent être transformés les documents XML basés sur une même DTD ou un même schéma. La spécification est divisée en trois parties : XSLT, le langage de transformation XPath, le langage de navigation dans un document XML XSL-FO, le vocabulaire XML de mise en forme
Filesystem Hierarchy Standard (« norme de la hiérarchie des systèmes de fichiers », abrégé en FHS) définit l'arborescence et le contenu des principaux répertoires des systèmes de fichiers des systèmes d'exploitation GNU/Linux et de la plupart des systèmes Unix. La version actuelle est la 3.0 et fut publiée le 3 juin 2015.
Une fonte Multiples master (fonte MM) est une fonte de caractères dont le format est une extension du format de fontes PostScript Type 1 d’Adobe Systems, contenant un ou plusieurs versions dites maitres (masters) — c'est-à-dire styles de fontes originaux — et permettent à l’utilisateur d’interpoler ces styles de fontes le long d’étendue continue d’axes. Ces axes (le gras, la largeur, la taille optique, etc.) peuvent être ajustés selon les besoins dans les applications capables d’utiliser ce format de fonte. Elles sont maintenant majoritairement remplacées par le format plus avancé OpenType.
Les fontes PostScript sont un ensemble de spécifications de fontes par contours développées par la société Adobe pour les polices de caractères digitales professionnelles, qui utilisent le format PostScript pour encoder l'information de la fonte.
Les fontes SVG sont un format de fontes vectorielles associées au format graphique vectoriel SVG, originellement créé pour le Web et la téléphonie mobile. Elles sont donc plutôt destinées à l'affichage qu'à l'impression.
Le G.711 est une norme de compression audio de l'UIT-T qui définit les codages PCM-U et PCM-A, s'appuyant sur les lois de quantification A (Europe, Afrique) ou µ (Amérique du Nord, Japon). Échantillonnage : 8 000 Hz pour une bande passante du téléphone de 300 — 3 400 Hz Bande passante sur le réseau : 64  ou  56 kbit/s Type de codage : MIC (Modulation d'impulsion codée, PCM en anglais) Son principe repose sur une grille de quantification non linéaire, permettant de diminuer le rapport bruit-sur-signal et l'erreur de quantification pour les sons de faible amplitude. Un codage sur 8 bits en G.711 correspond à une quantification sur 13  ou  14 bits en MIC (PCM en anglais). La norme G.711 a été révisée en 2000. Elle est la base du transport de la voix sur le réseau téléphonique commuté (RTC, PSTN en anglais) ou sur le RNIS (ISDN en anglais) et est également utilisée pour le transport de la voix avec peu de compression dans certains réseaux IP, comme sur les offres de téléphonie sur les « boxes » ou sur des réseaux locaux IP. Elle est en revanche assez peu utilisée pour faire de la téléphonie directement sur des réseaux étendus comme Internet à cause d'une utilisation importante de bande passante (64 kb/s).
La norme de codage mondiale G.722 normalisée par l'UIT-T en 1987 permet d'obtenir en voix sur IP une qualité de voix "haute définition" (dite téléphonie large-bande). Cette qualité est obtenue par doublement de la bande de fréquence codée (50-7 000 Hz) par rapport à la qualité téléphonique usuelle dite bande étroite (300-3 400 Hz) produite par le format de codage G.711 (MIC) utilisé en téléphonie "classique" sur les réseaux RTC. L'utilisateur bénéficie donc d'une sensation de présence de son interlocuteur, d'un confort d'écoute et d'une intelligibilité fortement améliorés. Cette qualité est obtenue au même débit de 64 kbit/s que le G.711 "bande étroite" grâce à la technologie MICDA de codage en sous bande : les parties basses fréquences [0, 4 kHz] et hautes fréquences [4, 8 kHz] du signal sont séparées (par filtrage QMF) puis codées et décodées séparément selon un algorithme de codage PCM différentiel adaptatif (dit codage MICDA). Cette technologie permet en outre au G.722 de fonctionner de manière très flexible (scalable) aux débits inférieurs de 56 kbit/s et 48 kbit/s. Les opérations de codage et de décodage se font échantillon par échantillon et n'induisent donc quasiment aucun retard de codage susceptible de nuire à l'interactivité des conversations et présentent en outre l'avantage d'être peu complexes (environ 10 MIPS) et sont donc peu coûteuses à mettre en œuvre. Afin d'optimiser l'usage de G.722 en VoIP, des systèmes de protection de la qualité (QoS) audio contre les pertes de paquets IP sont venus enrichir le standard (en Appendices III et IV de la norme). En outre aucun coût de licence n'est associé à ce codec. Le format de codage UIT-T G.722 a été choisi début 2007 par l'organisme de normalisation européen ETSI comme format obligatoire pour les terminaux DECT de nouvelle génération et haute qualité audio. Certains opérateurs (Orange, BT…) ont ainsi pu lancer avec succès des services de téléphonie IP en voix "haute définition" utilisant le format de codage G.722.
D'un point de vue strict, le G.723 faisait autrefois référence à une norme de codec audio de type ADPCM. Cette norme a été très peu mise en œuvre et est aujourd'hui obsolète. L'organisme normalisateur a eu la mauvaise idée de réutiliser une terminologie très similaire, G.723.1, pour désigner une autre norme de codec audio infiniment plus populaire, de type vocodeur. Aujourd'hui encore la confusion règne et la dénomination G.723 désigne dans le jargon courant ce qui devrait s'appeler G.723.1. La suite de cet article suit l'usage courant et le terme G.723 désigne la norme ITU-T G.723.1. Le codec G.723 est avant tout un codeur de voix. Son algorithme fait l'hypothèse que le signal traité correspond à une voix humaine occidentale. En particulier, il se révèle de qualité médiocre pour le codage de signaux non vocaux (musique, ...). Le G.723.1 est normalisé pour deux taux de compression, à savoir 5,3  et  6,4 kbit/s. Ces deux débits correspondent à deux types de codage différents, il ne s'agit pas d'un simple ajustement. La version à 6,4 kbit/s est d'une qualité légèrement supérieure à la version 5,3 kbit/s: la note MOS du 6,4 kbit/s atteint 3.9, contre 3.65 pour le 5,3 kbit/s. Ces deux codecs ont une latence algorithmique de 30 ms. La norme ITU-T G.723.1 est pourvue d'une annexe qui définit un comportement différent du codec lorsqu'un silence est détecté. Cet algorithme supplémentaire est dénommé VAD/CNG (voice activity detection/comfort noise generation, ou détection d'activité/bruit de confort). Dans ce cas seul le bruit de fond est modélisé et permet d'atteindre un débit extrêmement faible (1 octet toutes les 30 ms) tout en restituant l'ambiance sonore. En prenant en compte le pourcentage du temps où la voix est inactive dans une conversation typique, le G.723.1 avec VAD/CNG conduit à un débit moyen typique de l'ordre de 3,5 ~ 3,8 kbit/s. Le G.723.1 est surtout utilisé pour la visioconférence et la téléphonie sur IP (VoIP), que ce soit sur signalisation H.323, H.320 ou SIP. La compression audio G.723.1 est surpassée aujourd'hui, sur le plan de la qualité acoustique et du taux de compression. D'autres codecs ont bénéficié d'avancées en mathématiques et en puissance de calcul qui permettent d'améliorer le compromis qualité/débit. Une des raisons de la pérennité du G.723.1 est la contrainte de compatibilité et d'interopérabilité avec les équipements existants. Un autre aspect militant en sa faveur est sa relative simplicité algorithmique qui lui permet d'entrer dans des terminaux téléphoniques (IP-phone) à très bas coût où la puissance de calcul est limitée.
Le G.726 est une norme de compression audio de l'UIT-T, utilisée en téléphonie fixe. Elle utilise une "modulation par impulsions et codage différentiel adaptatif" (MICDA) à 40, 32, 24 ou 16 kbit/s.
La recommandation UIT-T G.729 définit un codage de la parole à 8 kbit/s par prédiction linéaire avec excitation par séquences codées à structure algébrique conjuguée. Le codec G.729 est moins consommateur en bande passante que G.711. Le codec G.729 est : Supporté par la plupart des PABX IP Utilisé pour le codage de la partie audio d'une visioconférence, Rencontré aussi pour transporter de la voix sur IP sur les WAN, Utilisé préférentiellement par les opérateurs de téléphonie. Depuis janvier 2017, G.729 est libre de redevance déclarée par SIPRO Lab Telecom. Par conséquent G.729 peut maintenant être utilisé sans payer de frais de licence.
La recommandation G.992.1 de l'UIT-T définit le mode de fonctionnement et d'interaction des équipements de communication ADSL de première génération, souvent dénommés aujourd'hui sous le nom "ADSL1" (par opposition avec l'ADSL2 et l'ADSL2+).
La recommandation UIT-T G.992.2 définit le mode de fonctionnement et d'interaction des équipements de communication ADSL de première génération qui peuvent être mis en œuvre sur une installation téléphonique d'abonné sans nécessiter la pose d'un filtre ADSL. En contrepartie, ces équipements présentent un niveau de performance inférieur à celui d'un équipement ADSL "classique".
La recommandation G.992.3 de l'UIT-T définit le mode de fonctionnement et d'interaction des équipements de communication ADSL2. La recommandation G.992.4, quant à elle, contient la même définition pour les équipements de communication ADSL2 susceptibles d'être mis en œuvre sur une installation d'abonné sans nécessiter de filtre ADSL.
La recommandation G.992.5 de l'UIT-T définit le mode de fonctionnement et d'interaction des équipements de communication ADSL2+.
Le Geography Markup Language (GML) est un langage dérivé du XML pour encoder, manipuler et échanger des données géographiques. C'est un standard développé par l'Open Geospatial Consortium pour garantir l'interopérabilité des données dans le domaine de l'information géographique et de la géomatique. Le GML consiste en un ensemble de schémas XML qui définissent un format ouvert pour l'échange de données géographiques et permettent de construire des modèles de données spécifiques pour des domaines spécialisés, comme l'urbanisme, l'hydrologie ou la géologie. Le GML est interopérable avec toutes les spécifications OpenGIS de l'OGC telles que Web Feature Service (WFS) ou le CityGML et un très grand nombre de systèmes d'information géographique. Le langage GML permet de décrire les objets géographiques les systèmes de projection, la géométrie, la topologie, le temps, les unités de mesures, et les attributs des objets géographiques. En janvier 2006, la version 3.1.1 du GML est proposée. Le GML fait partie des standards et spécifications OpenGIS élaborés par l'Open Geospatial Consortium (OGC). L'OGC et l'ISO/TC 211, le comité technique relatif à l'information géographique et la géodésie de l'Organisation internationale de normalisation (ISO), coopèrent au travers d'un groupe de consultation : l'ISO/TC 211 & OGC Joint Advisory Group (JAG). Ces travaux visent à rendre le GML conforme aux normes de la série 19100 de l'ISO/TC 211. Ils ont permis d'adopter le GML comme norme internationale : la norme ISO 19136 correspondant au standard OGC GML 3.2.1 a été publiée en 2007.
GeoSciML ou GeoScience Markup Language est un schéma d'application GML. C'est un langage de balisage respectant la syntaxe XML pour encoder, manipuler et échanger des données géographiques qui contiennent des informations géologiques. Il a été créé pour permettre l'interopérabilité des données cartographiques géologiques numériques et servir de langage d'échange dans le domaine des Sciences de la Terre. Le développement de GeoSciML, initié en 2003, est coordonné par l’Interoperability Working Group (IWG) de la Commission for the Management and Application of Geoscience Information (CGI), une commission de l’Union internationale des sciences géologiques (UISG) (International Union of Geological Sciences, IUGS). GeoSciML s’appuie sur le modèle OGC et est donc compatible avec les services OpenGIS Web Map Service (WMS) et Web Feature Service (WFS). GeoSciML est utilisé par le projet OneGeology afin d'agréger les données issues de services géologiques du monde entier pour reconstituer une Carte géologique numérique du Monde. version 1.1 publiée en 2006 version 2.0 publiée en décembre 2008
Health Level 7 (HL7) est une organisation qui définit un ensemble - auquel il donne son nom - de spécifications techniques pour les échanges informatisés de données cliniques, financières et administratives entre systèmes d'information hospitaliers (SIH). Ces spécifications sont diversement intégrées au corpus des normes formelles américaines (ANSI) et internationales (ISO) Initialement américaines, ces spécifications s'exportent et tendent à devenir un standard international pour ce type d'application. Entre autres, elles définissent structure et rôle des messages pour permettre une communication efficace des données liées au système de santé.
Les périphériques hot-plug sont ceux que l'on peut connecter ou déconnecter d'un ordinateur pendant que le système est en marche. Ils sont dits connectés ou déconnectés « à chaud ».
L'IAS-ECC (sigle de l'anglais Identification-Authentification-Signature European-Citizen-Card) est une norme française écrite par l' ANTS et le Gixel, groupe d'industriels spécialisés dans les cartes à puce, pour la mise en place d'une carte d'identité électronique (e-ID). Elle permet par ailleurs de fournir des services cryptographiques, tels que l'Identification, l'Authentification et la Signature (IAS). Elle provient de la mise aux normes européennes (ECC - European Citizen Card) de la norme française IAS écrite par la DGME, appelée aujourd'hui IAS Premium. La protection des données de la Carte Vitale 2 repose sur la première version de cette norme. La norme IAS-ECC permet l'interopérabilité des cartes e-Services au niveau européen.
ISIS (Image and Scanner Interface Specification) est un protocole informatique standard destiné au contrôle logiciel des scanners de document. ISIS a été développé par Pixel Translations en 1990 (aujourd'hui EMC captiva). ISIS est propriété de cette société privée et nécessite de payer des droits (runtime) pour pouvoir l'utiliser. De même, un droit par scanner (driver) doit également être payé en fonction du type de scanner.
L'Interface de gestion intelligente de matériel, (ou IPMI, Intelligent Platform Management Interface) est un ensemble de spécifications d'interfaces communes avec du matériel informatique (principalement des serveurs) permettant de surveiller certains composants (ventilateur, sonde de température, ...), mais également de contrôler l'ordinateur à distance, reboot, interrupteur, console à distance.
Le programme IDA (Interchange of Data between Administrations) est un programme d'échange de données entre administrations lancé en 1995 par l'Union européenne. IDA a été renommé en IDABC : Interoperable Delivery of European eGovernment Services to public Administrations, Businesses and Citizens. L'IDABC produit notamment les standards utilisables par le Cadre Commun d'Interopérabilité européen, qui est censé fournir les normes des Cadres Communs d'Interopérabilité de chaque État-membre de l'Union européenne, pour les services d'e-Gouvernement. La démarche de l'administration française sur le cadre national d'interopérabilité RGI est inspirée de la démarche européenne. Parmi les projets de l'IDABC, on trouve : MIReG : cadre commun sur les métadonnées, European Interoperability Framework. L'IDABC dispose également d'un observatoire du logiciel libre : Open Source Observatory
L'International Color Consortium (Comité International de la Couleur) fut constitué en 1993 d'une association de huit professionnels de l'industrie, dans le but de créer un système universel de gestion des couleurs qui fonctionnerait de manière transparente quels que soient la technologie du périphérique informatique, le système d'exploitation et le logiciel utilisés. La norme ICC, actuellement dans sa quatrième version, permet de garantir la fidélité de la perception colorée des images lors des transitions entre différents logiciels, de la création à l'impression photographique (ou l'utilisation finale) de l'image. Le principe utilisé par ICC pour transférer les couleurs d'un périphérique informatique à un autre, à perception colorée égale, repose en premier lieu sur la trivariance visuelle et la première loi de Grassmann qui déclare que toute couleur peut être obtenue à partir de trois couleurs primaires. On considère que chaque périphérique informatique possède son propre système de couleurs primaires qui définit son profil. En second lieu, pour améliorer la qualité du transfert des couleurs de l'image on applique le modèle d'apparence des couleurs CIECAM, proposé par la CIE, qui prend en compte les effets d’adaptation chromatique due aux contrastes provoqués par des couleurs différentes juxtaposées et les proportions et situations des différentes surfaces colorées sur l'image. Si l'application des profils ICC conduit à une bonne identité de perception des couleurs d'une même image produite par deux périphériques informatiques différents, c'est aux dépens d'une certaine distorsion des couleurs de cette image. Ce constat conduit à introduire une nouvelle notion de colorimétrie que l'on peut qualifier de colorimétrie d'apparence pour la différencier de la colorimétrie industrielle qui ne prend en compte que la couleur d'une surface indépendamment de la couleur des autres surfaces qui l'entourent.
La norme internationale ISO 8601 spécifie la représentation numérique de la date et de l'heure — respectivement basées sur le calendrier grégorien et le système horaire de 24 heures. Cette notation, créée en 1988, est particulièrement destinée à éviter tout risque de confusion dans les communications internationales due au grand nombre de notations nationales différentes. Elle a en outre de nombreux avantages pour une utilisation informatique par rapport aux autres notations. Il y a six niveaux de granularité dans ce format, selon les applications. Pour cette raison, il est possible d'omettre certains éléments. Exemples : 1977-04-22T01:00:00-05:00 correspond au 22 avril 1977, à 1 h du matin heure normale de l'est de l'Amérique du Nord (soit 5 heures de décalage). 1977-04-22T06:00:00Z correspond au même instant.
ISO/CEI 9995 Technologies de l’information — Disposition des claviers conçus pour la bureautique (en anglais ISO/IEC 9995 Information technology — Keyboard layouts for text and office systems) est une norme de l’ISO définissant des concepts destinés à faciliter la création de normes et standards nationaux et industriels. L’histoire de l’origine de cette norme fut étroitement liée à la situation particulière de l’informatique française dans les années 1970 et 1980, et aux efforts en vue de normaliser un clavier d’ordinateur performant pour la France – un processus qui pour être efficace, a dû rapidement s’inscrire dans une dimension internationale où la même absence de normes était à déplorer, avec toutes les conséquences négatives sur la productivité et l’ergonomie du poste de travail. ISO/CEI 9995 est une norme en plusieurs parties, dont l’une (partie 8) ne concerne pas le clavier d’ordinateur. Un clavier concret n’est pas tenu de les implémenter toutes. Après la fusion de la partie 6 avec la partie 5, et l’adjonction d’ISO/CEI 9995-11 en 2015, on compte 10  parties.
ITX est un format de carte mère développé par VIA pour ses plates-formes miniatures à faible dissipation thermique. En 2001, pendant qu'Intel et AMD en étaient encore à s'affronter à coup de gigahertz sans grand égard pour la dissipation, VIA présente ce format complètement à contre-courant. Ses produits remporteront un succès indiscutable mais limité par le manque de puissance globale du système.
L’acronyme JIS (Japanese Industrial Standard, « norme industrielle japonaise ») fait référence à l'ensemble des normes industrielles japonaises. En informatique, il est le plus souvent utilisé dans l’expression « codage JIS » pour désigner différents codages de caractères de la langue japonaise : Un jeu de caractères standards pour le japonais, notamment : JIS X 0201, la version japonaise de l’ISO 646 : 〄 . JIS X 0208, le jeu de caractères de type kanji le plus commun, contenant 6 879 kanji. JIS X 0212, extension de JIS X 0208 JIS X 0213, extension de JIS X 0208  JIS X 0202 (aussi connu sous le nom ISO-2022-JP), un mécanisme de codage de caractères utilisé pour envoyer des données JIS à travers un médium de 7-bit. En pratique, le « codage JIS » fait habituellement référence à des données de type JIS X 0208 encodées avec JIS X 0202.
La JSR 168 (ou Java Specification Requests : Portlet Specification) est la spécification des portlets définissant le contrat entre les conteneurs de portlets et les portlets. Cette requête de spécification Java (JSR), qui porte le n°168, prend place dans le cadre du JCP, le Java Community Process. L'idée de cette spécification est de rechercher l'interopérabilité entre les portlets et les portails. Cette spécification a pour but de permettre à n'importe quel portlet développé en Java de s'exécuter dans n'importe quelle architecture distribuée avec un serveur d'application compatible J2EE. La spécification définit un ensemble d'API pour les serveurs portails concernant les domaines de l'agrégation, la personnalisation, la présentation et la sécurité. Les buts principaux sont : Définir une API Java Portlet standard basée sur l'API Servlet ; Assurer l'interopérabilité et la portabilité ; Supporter des types de clients multiples (multi-terminal) ; Supporter la localisation et l'internationalisation ; Supporter l'exécution de portlet distant. Cette spécification a été soumise initialement par IBM et Sun. Depuis de nombreuses entités supportent cette spécification. La version 1.0 de la spécification JSR 168 a été adoptée en octobre 2003. Citons le cas du projet Apache Pluto qui a pour but de créer et de maintenir une implémentation de référence à la spécification Portlet Java JSR 168. Le projet doit fournir un conteneur indépendant de portlets, qui pourrait être embarqué dans différents portails, en particulier JetSpeed-2 (le portail open source de Jakarta). Il offre également aux développeurs une plate-forme exemple à partir de laquelle ils peuvent tester leurs portlets.
La JSR 286 (ou Java Specification Requests : Portlet Specification 2.0) est la spécification version 2 des portlets définissant le contrat entre les conteneurs de portlets et les portlets. Il s'agit d'une évolution de JSR 168. Les travaux JSR 286 ont démarré en novembre 2005 et se sont terminés en juin 2008.
Le JIS X 0208 est un codage de caractères sur 2 octets, défini comme standard industriel japonais (JIS : Japanese Industrial Standard), comptant 6879 caractères graphiques pour le traitement informatique du texte en langue japonaise. Il a été établi en 1978 comme JIS C 6226 et révisé en 1983. En 1990 il a été renommé JIS X 0208, et il a été révisé en 1997.
Le JTC1, créé en 1987 par convention entre l'ISO et la CEI est l'organe de référence pour la normalisation des Technologies de l'Information au niveau mondial. Comité technique commun à ses deux parents (JTC1 est un sigle dont le développement est Joint Technical Committee 1, Comité technique commun n° 1), il réunit les compétences de l'ancien TC97 de l'ISO relatives aux logiciels (langages de programmation, codage de l'information…) et les compétences de comités techniques de la CEI en matière de matériels : microprocesseurs, imprimantes, par exemple. Les normes publiées par le JTC1, quoique appelées souvent normes ISO, sont reconnaissables par leur numéro de nomenclature qui commence par les deux sigles ISO/CEI. Le total de normes publiées par le JTC1 depuis sa création était de 2489 en janvier 2012. Le comité comporte 37 pays membres et 54 observateurs.
Le standard de Kansas city ou encore le Kansas City standard (KCS) est un standard de stockage de données informatiques utilisant des cassettes audio ordinaires comme support. Il est aussi appelé le BYTE standard ou CUTS pour Computer Users’ Tape Standard, soit standard de cassette d'utilisateur informatique. Le standard est conçu en 1975, et utilise des données série asynchrones. La modulation est de type audio frequency-shift keying (AFSK) et encode un '0' par une représentation en 4 cycles de 1200 Hz d’une onde sinusoïdale. Le '1' étant représenté par 8 cycles de 2 400 Hz. Ceci aboutit à un débit de 300 bits par seconde. Le Kansas City standard fut utilisé par : Plusieurs systèmes basés sur le S-100, comme le MITS et l’Altair 8800 PT SOL-20 Ohio Scientific C1P/Superboard II Compukit UK101 Acorn Atom Nascom (Qui supporte aussi une variante à 1200 bit/s) Motorola MEK D1 6800 SWTPC basé sur le 6800  Portail de l’informatique
Le groupe Khronos est un consortium industriel fondé en 2000, et dont le but est de créer des API dont les spécifications sont rendues publiques et sont utilisables gratuitement, pour « créer et exécuter des applications multimédia sur un grand nombre de plateformes et appareils ». Toutes les entreprises membres peuvent participer au développement des spécifications des API gérées par Khronos. Elles ont également accès aux brouillons des spécifications ainsi qu'aux tests de conformité. Il y a cinq niveaux d'adhésion possibles, du simple utilisateur des marques déposées jusqu'aux compagnies qui dirigent le groupe. En 2006, ce sont : Apple, ARM, ATI, Creative, Dell, Ericsson, Freescale, Imagination Technologies, Intel, Motorola, Nokia, Nvidia, Samsung, SK Telecom, Sony, Sun, et Texas Instruments. Le deuxième groupe comporte environ 90 entreprises. Le 31 juillet 2006 lors de la conférence de la Siggraph, l'ARB (Architecture Review Board) a annoncé sa décision de transférer le contrôle de la spécification OpenGL au groupe Khronos. Les autres API contrôlées par le groupe Khronos ont été développées depuis le début par celui-ci.
Un Langage de balisage des émotions (EML ou EmotionML) est défini par le W3C Emotion Incubator Group (EmoXG) comme un Langage de balisage, d'annotation et de représentation des émotions, qui devrait être utilisable dans une grande variété de contextes technologiques dans lesquels il est nécessaire de représenter les émotions.
LLMNR (Link-local Multicast Name Resolution) est un protocole normalisé par l'IETF pour la résolution de noms sur un réseau local lorsqu'il n'y a pas d'administrateur système présent et qu'il est donc difficilement envisageable d'utiliser le DNS. Il fonctionne par diffusion restreinte (multicast) de la requête sur le réseau local. La machine qui se reconnaît répondra. LLMNR assure donc un service analogue à l'ancien NBP d'Apple. Et c'est un concurrent du mDNS / Bonjour d'Apple. Il est normalisé dans le RFC 4795.  Portail des télécommunications
La Linux Standard Base (abréviation : LSB) est un projet joint par nombre de distributions Linux sous la structure organisationnelle du Free Standards Group (en) afin de concevoir et standardiser la structure interne des systèmes d'exploitation basés sur GNU/Linux. La LSB est basée sur les spécifications POSIX, la « spécification unique d'UNIX », ainsi que sur d'autres nombreux standards ouverts, mais l'étend dans certains domaines. D'après eux :  « Le but de la LSB est de développer et promouvoir un ensemble de standards qui augmenteront la compatibilité entre les différentes distributions Linux et permettront aux applications de s'exécuter sur n'importe quel système conforme à la LSB. De plus, la LSB aidera à coordonner les efforts des vendeurs de logiciels pour porter et réaliser des produits pour Linux. »  La conformité d'un produit à la LSB doit être certifiée par une procédure dont la réalisation est de la responsabilité de l'Open Group en coopération avec le Free Standards Group. La LSB spécifie par exemple : un ensemble de bibliothèques standards, un nombre de commandes et d'utilitaires qui étendent le standard POSIX, la structure de la hiérarchie du système de fichiers, les différents run levels, plusieurs extensions à X Window System, et notamment, la commande lsb_release (-a) qui permet d'identifier facilement la distribution active sur une machine inconnue à laquelle vous seriez confrontés.
Un local technique désigne toute partie d'un bâtiment destiné à abriter des éléments techniques (ascenseur, chaudière, climatiseur, commutateur téléphonique, VMC, etc.), mais aussi toute partie destiné à abriter les fils et câbles électriques (fil de terre, électrique, téléphonique, de télévision, etc.) ainsi que les tuyaux (gaz, eau, etc.), les circuits et compteurs de gaz des fluides (gaz de ville, air comprimé, eau potable, eau chaude sanitaire, chauffage collectif, etc.) permettant le bon fonctionnement d'une maison ou d'un bâtiment. La gaine technique fait partie des locaux technique. Un local technique peut être une annexe d'un bâtiment (local de filtration d'une piscine par exemple).
LST 1582, intitulé Technologies de l’information — Clavier informatique lituanien — Agencement de caractères (en lituanien : Informacinės technologijos — Lietuviška kompiuterio klaviatūra — Ženklų išdėstymas, est une norme lituanienne de disposition des touches des claviers informatiques publiée en 2000 par Département de normes lituanien, et mis à jour en 2012. L’agencement de clavier définit suit la norme internationale ISO/IEC 9995 et remplace la norme LST 1205-92.
LUKS, pour Linux Unified Key Setup, est le standard associé au noyau Linux pour le chiffrement de disque créé par Clemens Fruhwirth.
LVS 23, intitulé Clavier informatique letton (en letton : Latviešu tastatūra datoriem), est une norme lettone de disposition des touches des claviers informatiques. Elle est publiée en 1993 par le Bureau de standardisation letton.
Dérivé du format de RSS dans sa version 2.0, le flux « media RSS » est un format de syndication de contenus multimédias : vidéos, images etc. Créé à l'origine par Yahoo (en 2004) au moment de la création de son moteur de recherches de vidéos. La spécification v1.0.0 est parue en 2005, et la v1.5.0 en 2009. L'abréviation MRSS désigne ce format et signifie : Media Real Simple Syndication
Un microformat (parfois abrégé sous μF ou uF) est une approche de formatage de données dans des pages WEB qui cherche à rationaliser et standardiser le contenu existant, comme les métadonnées, en utilisant des classes et attributs de balises XHTML et HTML. Cette approche est conçue pour permettre à l'information destinée aux utilisateurs finaux, telle que le carnet d'adresses, les coordonnées géographiques, les numéros de téléphone, les événements et autres données ayant une structure constante, d'être traitée automatiquement par les logiciels. Même si le contenu des pages web était déjà capable techniquement d'être « traité automatiquement » depuis la conception du web, il existait certaines limites. Les balises traditionnelles de marquage étaient en effet utilisées pour afficher l'information sur le Web et non pas pour décrire ce que voulait dire l'information. Les microformats sont destinés à combler ce fossé en attachant de la sémantique par la standardisation de la codification des balises HTML et XHTML, ce qui permet d'éviter d'autres méthodes plus compliquées de traitement automatisé, comme le traitement du langage naturel ou le screen scraping. L'utilisation, l'adoption et le traitement des microformats permet aux éléments de données d'être indexés, cherchés, sauvegardés ou référencés de manière que l'information puisse être réutilisée ou combinée. Les microformats actuels permettent l'encodage et l'extraction d'événements, d'information de contact, de relations sociales et ainsi de suite. Bon nombre d'autres formats sont en cours de développement.
Définition de la partie 3 du MPEG-4 : La troisième partie du MPEG-4 est une norme de compression pour le codage perceptuel et les signaux audio ; elle spécifie notamment le format audio AAC. Le format de compression audio préconisé par le standard ISO/IEC 14496-3 (MPEG-4) est le codec AAC-LC qui assure le meilleur rapport qualité/débit des codec audio à perte.
Le MPRII est le premier ensemble de normes concernant l'ergonomie et les économies d'énergie sur les moniteurs. Pour respecter le standard MPRII, le moniteur d'ordinateur doit disposer d'un mode veille qui réduit au minimum sa consommation électrique quand il ne reçoit pas de signal vidéo et être compatible avec la norme DCC (Plug and Play). Après MPRII qui est le strict minimum, il y a eu le TCO 95, puis le TCO 99.  Portail de l’informatique
Multimodal Architecture and Interfaces (« architecture multimodale et interfaces » en anglais) ou MMI-Arch est un standard ouvert en développement par le W3C depuis 2005. Depuis 2010 est une recommandation d'architecture à composants conteneurs du W3C. Le document est le rapport technique de spécification d'une architecture informatique générique et de ses interfaces pour faciliter l’intégration et la gestion des interactions multimodales dans un environnement informatique. Il a été produit par le groupe de travail en Interaction multimodale du W3C.
La norme NF Z 42-013 énonce un ensemble de spécifications de l’AFNOR concernant les mesures techniques et organisationnelles à mettre en œuvre pour le versement, l'archivage, la consultation, l'élimination et la restitution de documents électroniques, dans des conditions qui en garantissent l’intégrité. Le titre de la norme révisée en mars 2009 a évolué : « Spécifications relatives à la conception et à l’exploitation de systèmes informatiques en vue d’assurer la conservation et l’intégrité des documents stockés dans ces systèmes ». La norme révisée est destinée : aux organismes ou entreprises qui souhaitent mettre en œuvre des systèmes d'archivage électronique dans lesquels ils pourront archiver des documents électroniques de telle façon que leur intégrité soit préservés ; aux éditeurs de systèmes d’archivages électroniques (SAE) ; aux tiers archiveurs assurant un service d'exploitation d'un SAE externalisé ou mutualisé.  
La norme NF Z 65-130 énonce un ensemble de spécifications de l'AFNOR concernant la gestion de projet et la qualité d'un logiciel. Elle a été créée en avril 1987.
Les normes de métadonnées sont des normes qui décrivent les données sur les données, employées pour la structuration des ressources informatiques en général (pas seulement les documents électroniques) et l'interopérabilité informatique. Étant donné les multiples utilisations des métadonnées, à la fois dans les ressources informatiques et les systèmes, il est nécessaire d'employer des normes. La plupart de ces normes ne sont disponibles qu'en anglais. Il n'existe que deux normes disponibles en français : celle sur le référentiel Dublin Core, et celle sur le patrimoine culturel immatériel (ISO 21127). Les normes de métadonnées portant sur des domaines particuliers découlent de la définition de communautés d'intérêt (voir DoDAF).
En informatique la Notation 3, généralement appelé N3, est une norme relative à la sérialisation non-XML des modèles RDF, développée pour être lisible par les humains : la notation N3 est bien plus compacte et lisible que la notation RDF/XML. N3 a plusieurs fonctionnalités qui vont plus loin que la sérialisation des modèles RDF, comme le support des règles basées sur RDF.
Le système de numérotation ISO des semaines est un système de calendrier faisant partie de la norme d’horodatage ISO 8601. Le système est principalement utilisé par les gouvernements et entreprises pour baser également les années comptables et fiscales et la planification de projets à cycles hebdomadaires de travail, ainsi que pour le paiement des salaires ou des loyers (quand ceux-ci sont versés hebdomadairement). Ce système a pour but d’identifier une semaine au cours d’une année, ou même de servir de système de date alternatif remplaçant le système plus complexe de date basée sur les mois de l’année. Le système utilise le même cycle de sept jours par semaine que le calendrier grégorien. Les semaines commencent le lundi. Les années ISO sont numérotées approximativement comme les années grégoriennes, mais pas exactement (voir dessous). Une semaine ISO appartient tout entière à la même année ISO, c'est-à-dire qu'une année ISO a 52 ou 53 semaines entières (364 ou 371 jours). L’éventuelle semaine additionnelle est appelée « semaine intercalaire ». Une année ISO qui termine avec une semaine intercalaire est parfois appelée « année intercalaire ». Ce système a un cycle de 400 années, soit 146 097 jours (20 871 semaines), avec une année de longueur moyenne d’exactement 365,2425 jours (exactement comme le calendrier grégorien). Puisque les années non intercalaires ont exactement 52 semaines, il y a 71 années intercalaires tous les 400 ans (soit une année intercalaire tous les 5 à 6 ans en moyenne).
oEmbed est un protocole ouvert qui permet d’insérer le contenu d’une page web dans une autre page. Le contenu inséré peut être de plusieurs types : photo, vidéo, URL ou extrait HTML. L’échange d’information a lieu entre un site client et un site serveur. Par exemple, le site client peut afficher une représentation d’une page web telle qu’une image ou une vidéo. Le serveur doit disposer d’un service utilisant l’API oEmbed pour permettre aux clients de récupérer les informations de la représentation à afficher.
L'Open Data Protocol (OData) est un protocole permettant le partage de données, basé sur Atom et AtomPub. Attention à ne pas confondre avec la notion d’Open Data. Les spécifications d'OData sont publiées sous Microsoft Open Specification Promise (OSP), garantissant le format ouvert par l'absence de poursuites basées sur les brevets logiciels. Microsoft a publié un SDK comprenant des bibliothèques PHP, Java, JavaScript, webOS, .Net et pour iPhone.
ODX (Open Diagnostic Data Exchange, ODX-Standard (ASAM MCD-2D v2.0)) est une norme de description formelle des informations de diagnostic des calculateurs véhicule. Les véhicules actuels contiennent des calculateurs dont la taille du logiciel atteint facilement plusieurs kilo - voire mégaoctets. Le diagnostic s'effectue par connexion de l'outil à une prise de diagnostic véhicule permettant de communiquer avec les calculateurs véhicule. ODX permet de décrire l'ensemble des services disponibles en externe et est basé sur le XML (du W3C un consortium pour la description des données de diagnostic format normalisé pour des informations structurées). Il a été développé par le groupe de travail ASAM (Association for Standardisation of Automation and Measuring Systems)   Portail de l’informatique
The Open Group Architecture Framework, également connu sous l'acronyme TOGAF, est un ensemble de concepts et un standard industriel couvrant le domaine des architectures informatiques d'entreprise.
OpenCL (Open Computing Language) est la combinaison d'une API et d'un langage de programmation dérivé du C, proposé comme un standard ouvert par le Khronos Group. OpenCL est conçu pour programmer des systèmes parallèles hétérogènes comprenant par exemple à la fois un CPU multi-cœur et un GPU. OpenCL propose donc un modèle de programmation se situant à l'intersection naissante entre le monde des CPU et des GPU, les premiers étant de plus en plus parallèles, les seconds étant de plus en plus programmables.
OpenGL (Open Graphics Library) est un ensemble normalisé de fonctions de calcul d'images 2D ou 3D lancé par Silicon Graphics en 1992. Cette interface de programmation est disponible sur de nombreuses plateformes où elle est utilisée pour des applications qui vont du jeu vidéo jusqu'à la CAO en passant par la modélisation. OpenGL permet à un programme de déclarer la géométrie d'objets sous forme de points, de vecteurs, de polygones, de bitmaps et de textures. OpenGL effectue ensuite des calculs de projection en vue de déterminer l'image à l'écran, en tenant compte de la distance, de l'orientation, des ombres, de la transparence et du cadrage,. L'interface regroupe environ 250 fonctions différentes qui peuvent être utilisées pour afficher des scènes tridimensionnelles complexes à partir de simples primitives géométriques. Du fait de son ouverture, de sa souplesse d'utilisation et de sa disponibilité sur toutes les plates-formes, elle est utilisée par la majorité des applications scientifiques, industrielles ou artistiques 3D et certaines applications 2D vectorielles. Cette bibliothèque est également utilisée dans l'industrie du jeu vidéo où elle est souvent en rivalité avec la bibliothèque de Microsoft : Direct3D. Une version nommée OpenGL ES a été conçue spécifiquement pour les applications embarquées (téléphones portables, agenda de poche, consoles de jeux…).
OpenKODE est une spécification informatique de plateforme créée par le Khronos Group, dans le but d'unifier différentes api d'accélération matérielle et de gestion du système. Il regroupe : compatibilité POSIX réduite OpenGL/ES 2.0 (et 1.1) pour la 3D OpenVG pour l'accélération vectorielle 2D OpenWF pour le compositing du gestionnaire de fenêtre. OpenMAX pour l'accélération multimedia (décodage/encodage d'images et de flux audio et vidéo). OpenSL/ES pour l'accélération audio OpenCL pour l'accélération du calcul intensif. Les SoC Tegra2 de Nvidia, à base d'architecture ARM sont en 2010, les premières puces complètement compatibles OpenKODE[réf. nécessaire].
OpenMAX est une Interface de programmation ouverte et libre de droit qui fournit un accès aux codecs multimédias. L'interface est spécifiée et normée par le Khronos Group. Elle fait partie de la plateforme OpenKODE. Elle est divisée en plusieurs couches : OpenMAX AL (Application Layer, c'est-à-dire, couche d'application) est une interface permettant aux applications de s'abstraire de la gestion des middlewares multimédia qui exécuteront les tâches dévolues (lecture, enregistrement, etc.). OpenMAX IL (Integration Layer, c'est-à-dire, couche d'intégration) est une interface permettant aux applications ou frameworks de communiquer avec les différents codecs (logiciels ou matériels) audio, video, et d'imagerie de façon transparente et standard. OpenMAX DL (Development Layer, c'est-à-dire, couche de développement) est une interface contenant un ensemble de fonctions d'encodage et décodage pouvant être implémentées et optimisées par les concepteurs des processeurs matériels.
OpenType (OT) est un format de fonte numérique, correspondant à la norme ISO de Open Font Format (OFF). Il a été développé à l'origine par Microsoft, en ajoutant à la structure de base de TrueType de nombreuses structures complexes enrichissant les possibilités typographiques. La spécification débuta au sein de Microsoft, Adobe contribuant également au moment de l'annonce publique en 1996. La spécification continue à être développée activement, gagnant les caractéristiques d'un format ouvert. Cependant, le nom OpenType reste une marque déposée de Microsoft. Étant maintenant un format répandu, offrant une grande richesse typographique, y compris des dispositions pour représenter la majorité des systèmes d'écriture, les fontes OpenType sont utilisées couramment aujourd'hui sous tous les systèmes d'exploitation.
OpenVG est une API conçue par le groupe Khronos. OpenVG est conçue pour le dessin vectoriel 2D permettant une accélération matérielle. Cette API est plus particulièrement destinée aux téléphones portables, consoles de jeux portatives, PDA, et autres appareils portables. Cela devrait faciliter la création d'interfaces utilisateur efficaces moins dépendantes du CPU et devrait aider à réduire la consommation électrique de ces appareils. OpenVG est bien adaptée aux applications utilisant les technologies SVG ou Flash. Le groupe de travail chargé du contrôle de cette API a été créé au sein du groupe Khronos le 6 juillet 2004 par des entreprises internationales telle que 3Dlabs, Bitboys, Imagination technologies, Ericsson, Hybrid Graphics, Motorola, Nokia, PalmSource, Symbian, et Sun Microsystems. Le premier brouillon des spécifications a été publié fin 2004, et la version 1.0 est sortie le 1er août 2005. En 2010, OpenVG est théoriquement supporté par Mesa 3D, le système de rendu 2D/3D utilisé par défaut sous GNU/Linux mais n'y est plus régulièrement développé, ainsi que par le système Android (basé sur le noyau Linux), ainsi les pilotes pour différents systèmes des processeurs graphiques inclus dans les SoC à architecture ARM. OpenVG est également défini dans la norme de la plateforme OpenKODE du Khronos Group.
Peer to Peer Session Initiation Protocol (P2PSIP) est un Draft de l'organisme IETF ayant pour objet la spécification d'un protocole ouvert reposant sur un réseau peer to peer et le protocole SIP
Le système PANOSE est une méthode pour classer les polices de caractères en fonction de leurs caractéristiques visuelles. Il est utilisé pour identifier une police inconnue d'après sa représentation graphique, ou pour substituer une police à une autre. Le nom PANOSE est constitué des six lettres représentatives des six groupes de lettres définis à l’origine : P (lettre comportant une partie droite et une arrondie), A (avec diagonale), N (« carrées »), O (« rondes »), S (« semi-rondes »), E (« semi-carrées »). Il existe des définitions PANOSE pour les polices Latin Text, Latin Script, Latin Decorative, Iconographic, Japanese Text, Cyrillic Text et Hebrew. Le premier système PANOSE a été développé en 1985 par Benjamin Bauermeister ; il consistait en 7 chiffres hexadécimaux (étendus plus tard à 10 digits) pour chaque police. Chaque chiffre était calculé d'après une métrique visuelle, comme le poids de la police ou la présence ou l'absence d'empattements. Par exemple, les nombres de PANOSE pour le Times New Roman sont: PANOSE a été incorporé dans de nombreux font metadata tags en 1992 par ElseWare Corporation. Le système de classification, ses algorithmes de comparaison, ses paramètres de classification etc. ainsi que la marque ont été achetés par Hewlett Packard en 1995. Un moteur de synthèse de polices appelé Infinifont a également été acheté par Hewlett Packard. En 1996, lors de l'élaboration des brouillons du W3C sur les CSS1, HP a proposé l'extension de syntaxe PANOSE pour la substitution des polices. Elle n'a pas été retenue, en partie à cause de problème de licences.
PictBridge est une norme informatique qui a été créée afin de permettre l’impression directe d’image depuis un appareil photo, un téléphone mobile, un scanner ou tout autre appareil permettant de la capture d’image, vers une imprimante, sans passer par un ordinateur. Elle a été mise au point par l’association CIPA qui regroupe de nombreux fabricants d’appareils photos et de périphériques d’impression. L'un des buts est de permettre la compatibilité entre périphériques de marques différentes.
P3P (Platform for Privacy Preferences Project) est un projet à l'initiative du consortium W3C (qui énonce les standards du Web et d'Internet), datant de 2002. Il vise à standardiser le moyen par lequel un site web peut informer l'internaute de sa politique en matière de protection des données personnelles. Internet Explorer est le seul navigateur web de quelque importance à supporter P3P. La technologie est considérée aujourd’hui comme obsolète.
Le Plug and Play (l'abréviation PnP est également utilisée), qui signifie « connecter et jouer » ou « brancher et utiliser », est une procédure permettant aux périphériques récents d'être reconnus rapidement et automatiquement par le système d'exploitation dès le branchement du matériel, et sans redémarrage de l'ordinateur. Cette procédure permet l'installation en requérant un minimum d'intervention de la part de l'utilisateur, sans installation de logiciel dédié, et donc en minimisant les erreurs de manipulation et de paramétrage. Il existe plusieurs appellations ou acronymes qui recouvrent le même concept (tous sont d'origine anglaise) : PnP, et hot-swapping (littéralement « échange [de périphériques] à chaud »). Le terme Plug-and-Play est en général associé à Microsoft, qui l'utilisa en référence à son système d'exploitation Windows 95. D'autres systèmes d'exploitation supportaient déjà ce genre de périphériques, comme celui du Macintosh depuis ses débuts en 1984[réf. nécessaire], mais ils utilisèrent tous la même dénomination. L'expression française « prêt-à-tourner » a été proposée[réf. nécessaire] par l'office québécois de la langue française en 2006. Les périphériques véritablement PnP le sont à deux niveaux : celui du matériel et celui du logiciel. Tout d’abord, l’ordinateur doit être capable de supporter l’ajout de nouveaux périphériques sans créer de problèmes physique ou électrique sur ces derniers. Par exemple certaines interfaces ne supportent pas le fait d’être branchées ou débranchées durant la mise sous tension sans créer des dommages irréversibles aux périphériques. La plupart du temps cela se traduit par des décharges électrostatiques aux divers composants électroniques. L’ordinateur doit également être capable de détecter les changements de configurations au fur et à mesure que les périphériques sont ajoutés et enlevés. Les connectiques USB et FireWire ont été conçues dans ce but. Une forme plus restreinte de Plug-and-Play existe également pour des systèmes ne gérant pas ces changements dynamiques : le système « power down, plug, power up, and play » (« Éteindre, brancher, démarrer, et utiliser »). Les débuts du Plug and Play ont été assez difficiles (bien souvent les périphériques Plug and Play n'avaient de « Plug and Play » que le nom, ils étaient souvent plus complexes à configurer que les périphériques non estampillés « Plug and Play »), c'est pour cette raison que le Plug and Play était surnommé parfois de façon satirique « Plug and Pray », littéralement, « branchez et priez » (pour que ça marche). Cette technologie effectue plusieurs opérations à savoir dont Une reconnaissance automatique à chaque branchement de n’importe quel périphérique à l’unité centrale Elle effectue automatiquement les ressources nécessaires Elle enregistre les paramètres qui correspondent au matériel enregistré Sous windows, elle enregistre les paramètres dans la base de données du registre pour éviter les conflits éventuels
Le Port parallèle est un connecteur situé à l'arrière des ordinateurs compatibles PC reposant sur la communication parallèle. Il est associé à l'interface parallèle Centronics. La communication parallèle a été conçue pour une imprimante imprimant du texte, caractère par caractère. Les imprimantes graphiques (pouvant imprimer des images) ont ensuite continué à utiliser ce système pour profiter de l'interface parallèle normalisée. Le port parallèle est à l'origine unidirectionnel. Ce type d'interface a évolué vers le standard IEEE 1284, à la fois bidirectionnel et plus rapide.
POSIX est une famille de normes techniques définie depuis 1988 par l'Institute of Electrical and Electronics Engineers (IEEE), et formellement désignée par IEEE 1003. Ces normes ont émergé d'un projet de standardisation des interfaces de programmation des logiciels destinés à fonctionner sur les variantes du système d'exploitation UNIX. Le terme POSIX a été suggéré par Richard Stallman, qui faisait partie du comité qui écrivit la première version de la norme. L'IEEE choisit de le retenir car il était facilement mémorisable,. Les quatre premières lettres forment l’acronyme de Portable Operating System Interface (interface portable de système d'exploitation), et le X exprime l'héritage UNIX.
Predictive Model Markup Language ou PMML est un langage de marquage basé sur XML conçu pour définir des modèles de données et visant à rendre interopérables les systèmes de datamining. La version 4.0 est sortie le 16 juin 2009.
RapidIO ou RapidIO Interconnect Architecture est un système de communication (bus) à commutation de paquets et à haute performance, permettant d'interconnecter des microprocesseurs sur un circuit imprimé, ainsi que des cartes électroniques indépendantes via des connecteurs. Cette technologie est conçue spécialement pour les systèmes embarqués, pour les marchés du traitement du signal, des réseaux et des télécommunications. RapidIO est un standard ouvert, normalisé au début des années 2000 par la « RapidIO Trade Association » ; il a pour objectif d'être un moyen d'interconnexion optimisé lorsque des processeurs embarqués, DSP, FPGA et ASIC doivent être reliés par des liaisons point-à-point ou par un réseau en étoile ou en arbre. RapidIO s'est rapidement diffusé dans les domaine application suivantes : Stations de base de réseau mobile; Dans le domaine militaire ; ordinateurs sur une carte et systèmes de traitement de radar, d'acoustique et d'image; Traitements Vidéo; Serveurs; Imagerie médicale Applications industrielles La principale technologie concurrente est le PCI Express, mais celui-ci vise davantage le marché des PC, des postes de travail informatiques et le domaine des serveurs hautes performances et moins les systèmes embarqués. PCIe n'est pas conçu pour des systèmes de grande taille, contrairement à RapidIO. En effet PCIe utilise un pontage non transparent pour la répartition de la mémoire qui n'est pas aussi simple d'utilisation que celui de RapidIO. Ce dernier utilise un système d'architecture symétrique où chaque point terminal peut posséder sa propre mémoire privée. Le standard RapidIO a évolué au milieu des années 2000 pour proposer une version « série » (serial RIO ou sRIO) de cette interface, avec des débits série bi-directionnels variants de 1 à 5 Gbit/s (par paire de transmission). Cette version série (sRIO) supporte à la fois des échanges entre points terminaux, en mode mémoire partagée et un mode message apte à transporter des flux de données et des protocoles (comme IP).
La redondance N+1 est un terme utilisé en informatique pour décrire un système impliquant un nombre d'équipements (N) plus un équipement supplémentaire (+1) et dont la mission est de permettre à l'ensemble de continuer à fournir un service quasi-nominal dans le cas de la perte de l'un des N équipements d'origine. On utilise aussi ce terme en électricité pour qualifier un système dont la multiplication des éléments fiabilise le système (exemple : Onduleurs parallèles).
Renderscript est une API pour des graphismes haute performance sur les tablettes et téléphones Android. Il s'agit d'une API de bas niveau dont le but est de hautes performances de rendu 3D et d'opération de calcul. Il permet aux développeurs de maximiser les performances de leurs applications tout en nécessitant d'écrire une plus importante quantité de code et de plus grande complexité. Il fournit au développeurs trois outils de base : Une simple API de rendu 3D au-dessus de la couche d'accélération matérielle, une API de calcul plus simple pour les développeurs similaire à CUDA ou GLSL et un langage de programmation familier en C99.
Resource Description Framework (RDF) est un modèle de graphe destiné à décrire de façon formelle les ressources Web et leurs métadonnées, de façon à permettre le traitement automatique de telles descriptions. Développé par le W3C, RDF est le langage de base du Web sémantique. L'une des syntaxes (ou sérialisations) de ce langage est RDF/XML. D'autres syntaxes de RDF sont apparues ensuite, cherchant à rendre la lecture plus compréhensible ; c'est le cas par exemple de Notation3 (ou N3). En annotant des documents non structurés et en servant d'interface pour des applications et des documents structurés (par exemple bases de données et GED) RDF permet une certaine interopérabilité entre des applications échangeant de l'information non formalisée et non structurée sur le Web.
RSS (sigle venant de l'anglais « Rich Site Summary » ou encore « Really Simple Syndication ») est une famille de formats de données utilisés pour la syndication de contenu Web. Un produit RSS est une ressource du World Wide Web dont le contenu est produit automatiquement (sauf cas exceptionnels) en fonction des mises à jour d’un site Web. Les flux RSS sont des fichiers XML qui sont souvent utilisés par les sites d'actualité et les blogs pour présenter les titres des dernières informations consultables. On emploie parfois à tort le terme RSS pour désigner le format concurrent Atom. Trois formats de données peuvent être désignés par ces initiales : Rich Site Summary (RSS 0.91) sorti en 1999 ; RDF Site Summary (RSS 0.90 et 1.0) sorti en 2000 ; Really Simple Syndication (RSS 2.0) sorti en 2002.
Un Safety Integrity Level , (SIL ou niveau d'intégrité de sécurité) est défini comme un niveau relatif de réduction de risques inhérents à une fonction de sécurité, ou comme spécification d'une cible de réduction de risque. Plus simplement, c'est une mesure de la performance attendue pour une fonction de sécurité (ou SIF). Les exigences pour un niveau donné de SIL ne sont pas toujours cohérentes entre les différentes normes traitant de sécurité. Dans les 'European Functional Safety standards', on trouve la définition de quatre SIL, allant de SIL 1 pour le moins sûr à SIL 4 pour le plus sûr (grande fiabilité). Un SIL est déterminé à partir d'un certain nombre de facteurs quantifiés dans la gestion du cycle de développement et/ou du cycle de vie.
Le service d'observation des capteurs (Sensors Observation Service - SOS) est un service Web pour interroger les données instantanées et temporelles des capteurs et fait partie du projet Sensor Web. Les données du capteur proposées comprennent les descriptions des capteurs eux-mêmes, codées au format SensorML et les valeurs mesurées dans le format O&M (Observations et Mesures). Le service Web ainsi que les deux formats de fichiers sont des standards ouverts et des spécifications du même nom défini par le Open Geospatial Consortium (OGC). Si le SOS prend en charge un fonctionnement par transactions (SOS-T), les nouveaux capteurs peuvent être enregistrés sur l'interface du service et les mesures insérées. Une implémentation de SOS peut être utilisée à la fois pour les données provenant de capteurs in situ et à distance. En outre, les capteurs peuvent être mobiles ou fixes. Depuis 2007, le SOS est une norme OGC officielle. L'avantage de SOS est que les données du capteur - de toute sorte - sont disponibles dans un format standardisé en utilisant des opérations normalisées. Ainsi, l'accès aux données du capteur via le Web est simplifié. Il permet également une intégration facile dans les infrastructures de données spatiales existantes ou les systèmes d'information géographique. En 2016, OGC a accepté la spécification standard de l'API SensorThings, une nouvelle norme RESTful et JSON fournit des fonctions similaires à SOS. Comme l'API SensorThings et SOS sont basées sur l'OGC/ISO19156:2011, il a été démontré dans un projet pilote OGC IoT que les deux spécifications peuvent interopérer l'une avec l'autre.
Simple Query Interface est l'un des standards émergeant pour l'interopérabilité entre les plates-formes de e-formation.  Portail de l’informatique
Single UNIX Specification (SUS) est un nom désignant un ensemble de spécifications permettant de certifier un système d'exploitation comme étant un Unix. Il est basé sur la norme POSIX, à laquelle il ajoute quelques éléments. Le SUS est développé et maintenu par l'Austin Group (en), il est basé sur des travaux plus anciens de l'IEEE et de l'Open Group.
La Society of Motion Picture and Television Engineers ou SMPTE, fondée en 1916, est une association internationale, située aux É.-U., et composée d'ingénieurs. Elle développe des standards vidéos (elle en a déjà plus de 400 à son actif), qui sont utilisés par exemple par la télévision ou le cinéma numérique. Les documents présentant les standards SMPTE peuvent être achetés sur le site de la SMPTE. Les standards les plus significatifs à l'actif de la SMPTE incluent : tous les formats de transmission de cinéma et de télévision, y compris numérique ; les interfaces physiques pour la transmission de signaux de télévision et les données associées (comme le timecode SMPTE) ; le Material Exchange Format, ou MXF.
OpenType SVG est un format de police de caractères vectorielles dérivé de SVG Font, devenu sous-ensemble du format OpenType.
SyncML est un langage de synchronisation de données (calendrier, agendas...) entre appareils portables et postes fixes, basé sur XML ou WBXML. C'est aussi le nom du consortium (Ericsson, IBM, Lotus, Motorola, Nokia, Psion, et quelques autres) à l'origine de cette norme. On parle aujourd'hui d'OMA-DS (Open Mobile Alliance-Data Synchronization), Open Mobile Alliance étant le consortium qui porte différents "enabler" pour les mobiles.
Le Systems Biology Markup Language (SBML) est un format de représentation, basé sur XML, de communication et de stockage de modèles de processus biologiques. C'est un standard libre et ouvert avec une prise en charge par de nombreux logiciels et développeurs. Un modèle SBML peut représenter différentes classes de phénomènes biologiques, y compris les réseaux métaboliques, les chemins de signalisation métaboliques, les réseaux de régulation génétiques, les maladies infectieuses et bien d'autres processus biologiques,,. c'est le standard de facto pour la représentation des modèles informatiques en biologie des systèmes aujourd'hui.
TCO 92 est une norme dont le nom vient d’un syndicat suédois, l'Organisation centrale des employés. Cette norme vise à réduire le reflet, le scintillement et les rayonnements électromagnétiques d’un moniteur d'ordinateur. Cette recommandation est l’une des plus sévères du monde.
Évolution de la norme TCO 92 encore plus sévère.
Évolution de la norme TCO 95, encore plus sévère, la norme TCO 99 préconise d’avoir une couleur claire pour l’habillage de l’écran. Ainsi un habillage noir n’est pas conforme à TCO 99, par conséquent les constructeurs proposant des modèles noirs doivent se restreindre à les estampiller TCO 95.
Les threads POSIX, souvent appelés pthreads, sont un sous-standard de la norme POSIX décrivant une interface de programmation permettant de gérer des threads. Il s'agit du standard IEEE Std 1003.1c-1995 (POSIX.1c, Threads extensions). Cette interface est disponible sur la plupart des systèmes Unix modernes, par exemple Linux, les différentes variantes modernes de BSD, Mac OS X et Solaris,. Elle n'est pas disponible nativement sous Microsoft Windows mais il existe plusieurs implémentations dont une de Microsoft.
Le TM Forum (anciennement TeleManagement Forum et avant OSI/Network management forum) est une association internationale à but non lucratif d'entreprises du secteur des télécommunication et du numérique. Elle vise par des programmes de coopération à conduire des recherches, développer de bonnes pratiques et des normes, à définir des API ouvertes, et à faciliter la transition au numérique par l'organisation d'événements. L'organisation compte 850 membres dans 66 pays, et coopère avec l'ISO et avec l'Union Internationale des Télécommunications. en contribuant aux travaux de normalisation.
TrueType est un format de fonte numérique créé par Apple vers la fin des années 1980, pour concurrencer le format PostScript Type 1, standard développé par Adobe. Comme pour ce dernier, la forme de chaque glyphe d'une police TrueType est définie par des courbes mathématiques, les courbes de Bézier quadratiques, ainsi que par des algorithmes d'optimisation (« hinting ») sophistiqués. Ceci constituait une avancée importante par rapport au rendu d'images matricielles (ou « bitmap »), car il était possible de synthétiser une police à plusieurs tailles différentes, en atténuant de surcroît le problème du crénelage. Depuis le milieu des années 1990, ces polices sont gérées par une couche logicielle intégrée au système : FreeType pour les systèmes libres comme GNU ; intégré à GDI pour Windows ; Suitcase pour macOS (auparavant Mac OS X). Ce format a servi de base pour la conception du format OpenType, développé conjointement par Adobe et Microsoft, vers la fin 2002, et reste encore très largement utilisé.
En informatique, Turtle (Terse RDF Triple Language) est une syntaxe d'un langage qui permet une sérialisation non-XML des modèles RDF. C'est un sous-ensemble de la syntaxe Notation3. La spécification de Turtle est actuellement un brouillon créé par le RDF Working Group du W3C le 9 août 2011.
TWAIN est un protocole informatique standard destiné principalement à relier un scanner d'image à un ordinateur, permettant à tout logiciel de traitement d'image compatible TWAIN de piloter le scanner pour contrôler et lancer l'acquisition de l'image, quels que soient le modèle et le constructeur du scanner.
L'USB Mass Storage, UMS ou USB MSC, est un protocole utilisé pour permettre à un ordinateur de communiquer avec une grande variété d'appareils électroniques comme des appareils photos numériques, des clefs USB, ou des baladeurs, via un bus USB. Il permet à l'ordinateur et au périphérique connecté d'échanger des données. L'appareil apparait en général à l'utilisateur comme un disque dur externe, permettant l'échange de fichiers par glisser-déposer.
V.21 est une norme de modulation pour modem à 300 bauds full-duplex pour l'utilisation sur le réseau téléphonique public commuté (RTC).
Le Vidéotex est un service de télécommunications permettant l'envoi de pages composées de textes et de graphismes simples à un utilisateur en réponse à une requête de ce dernier (interactivité). Ces pages sont destinées à être visualisées sur un écran cathodique, par exemple sur une télévision ou tout autre écran au format de la télévision. Le Minitel français est le terminal adapté à ce service. Le nom du terminal a fini par être utilisé, dans le langage courant, comme nom générique du service et est donc en français devenu synonyme de Vidéotex. Le service utilise une norme de communication basée sur une syntaxe de description des pages. Le service Télétel (ou Minitel) utilise la norme Antiope (Option 2 de la norme internationale T.100) Le service est généralement rendu par un système comportant des terminaux de type écran-clavier connectés par le réseau téléphonique commuté à un point d'accès spécifique assurant la connexion à un serveur au travers d'un réseau de transmission de données (en France le réseau Transpac de commutation de données par paquets) vers des serveurs, voire vers des terminaux pairs. Seul le service Télétel (aussi appelé Minitel, du nom du terminal), basé sur une option de consommation à la demande permise par l'utilisation de Points d'Accès Videotex intégrés comme autocommutateurs dans le réseau téléphonique commuté, a connu une exploitation commerciale d'une durée significative. Les services britannique (Prestel), allemand (Bildschirmtext), italien (Videotel), canadien (Télidon) et japonais (Captain), tous basés sur un système d'abonnement, n'ont pas connu de succès commercial et ont été arrêtés peu après la fin des phases d'expérimentation. Néanmoins, le système Viewdata (dont Prestel était la marque commerciale) continue d'être utilisé par les agences de voyage au Royaume-Uni.
Vulkan d'abord annoncé sous l'appellation OpenGL Next est une interface de programmation graphique proposée par le consortium Khronos Group et qui a pour but de remplacer à terme OpenGL et ses dérivés en exploitant plus efficacement les architectures informatiques modernes. 25 ans après la création d'OpenGL, il vise à unifier les versions mobile (OpenGL ES) et bureau (OpenGL), fonctionne nativement sur Microsoft Windows, GNU/Linux, et Android ainsi que sous MacOS et iOS via la portability initiative en se basant sur Metal . Une première version est sortie en février 2016, supportée entre autres par AMD et Nvidia, et est compatible avec tout système supportant OpenGL ES 3.1.
Web Map Service ou WMS est un protocole de communication standard qui permet d'obtenir des cartes de données géoréférencées à partir de différents serveurs de données. Cela permet de mettre en place un réseau de serveurs cartographiques à partir desquels des clients peuvent construire des cartes interactives. Le WMS est décrit dans des spécifications maintenues par l'Open Geospatial Consortium.
Web Tile Map Service (ou WMTS) est un service web standard défini par l'OGC qui permet d'obtenir des cartes géoréférencées tuilées à partir d'un serveur de données sur le réseau. Ce service est comparable au Web Map Service mais tandis que le WMS permet de faire des requêtes complexes (dont la reprojection ou la symbolisation de données vecteur) nécessitant une certaine puissance de calcul côté serveur, le WMTS met l'accent sur la performance et ne permet de requêter que des images précalculées (tuiles) appartenant à des dallages prédéfinis. Cela permet aux utilisateurs de construire des cartes interactives en ligne avec bonne réactivité de l'IHM. Ce standard étant récent et orienté web, encore peu de SIG sont capables d'exploiter les flux WMTS.
Web Ontology Language (OWL) est un langage de représentation des connaissances construit sur le modèle de données de RDF. Il fournit les moyens pour définir des ontologies web structurées. Sa deuxième version est devenue une recommandation du W3C fin 2012,. Le langage OWL est basé sur les recherches effectuées dans le domaine de la logique de description. Il peut être vu en quelque sorte comme un standard informatique qui met en œuvre certaines logiques de description, et permet à des outils qui comprennent OWL de travailler avec ces données, de vérifier que les données sont cohérentes, de déduire des connaissances nouvelles ou d'extraire certaines informations de cette base de données. Il permet notamment de décrire des ontologies, c'est-à-dire qu'il permet de définir des terminologies pour décrire des domaines concrets. Une terminologie se constitue de concepts et de propriétés (aussi appelés « rôles » en logiques de description). Un domaine se compose d'instance de concepts.
Web Ontology Language for Web Services (OWL-S) est une ontologie basée sur le dialecte XML OWL, permettant la description des propriétés et des capacités d'un service web.
Le Web Open Font Format (WOFF) est un format de police de caractère comprimée pour un usage sur les sites web. Le format développé en 2009 est un fichier de police de caractères SFNT (TrueType, OpenType) qui a été comprimé à l’aide d’un outil de codage WOFF. Le format utilise la compression zlib, ce qui permet d’avoir une réduction de taille de fichier de plus de 40 %. Le format est soutenu par plusieurs entreprises de typographie, et pris en charge par la version 3.6 du navigateur Firefox de Mozilla. Le 12 avril 2010, le WOFF 1.0, présenté par Microsoft Corporation, Mozilla Foundation et Opera Software est accepté par le W3C. En 2012, commence une nouvelle version du format. Celle-ci est publiée en 2013 sous la norme WOFF 2.0 et peut être utilisé dans les navigateurs Chrome et Chromium de Google, à partir de la version 36, ainsi qu'Opera à partir de la version 23.
Web Open Font Format 2, également appelé WOFF2 ou WOFF 2.0 est une norme de format de fichier de police vectorielle initialement proposé par Google, en 2013, et proposant d'améliorer le format WOFF original. Ces principales avancées sont : Il remplace l'algorithme de compression gzip par brotli, proche des principes de LZMA. Il compresse un peu moins que LZMA, mais tout de même 30 à 50 % plus que gzip utilisé par WOFF. Sa vitesse de décompression s'approche de celle de deflate, soit environ 10 fois plus rapide que LZMA. Il comporte également, en s'inspirant du format MicroType Express (MTX), la possibilité de mémoriser le corps d'un caractère sans accent, et de le réutiliser pour le représenter avec accent, plutôt que de mémoriser le même caractère pour chaque accent. Ce format permet officiellement d'accepter la compatibilité avec les fontes SVG OpenType, multicolores et animées acceptées précédemment dans les standard du format OpenType. Il permet aussi de réunir dans un même fichier des polices à la graisse différente, italique et non-italique.
WBEM (Web-Based Enterprise Management), qui pourrait se traduire par « Gestion de l'entreprise s'appuyant sur le Web ». est un ensemble de techniques et de standards Internet de gestion servant à unifier la gestion des environnements d'informatique distribuée. WBEM s'appuie sur des standards Internet et sur les standards ouverts publiés par l'organisme DMTF (Distributed Management Task Force). Il s'agit de l'infrastructure et du schéma Common Information Model (CIM), de CIM-XML, du fonctionnement de CIM par-dessus HTTP et de WS-Management. Bien que le nom de WBEM comporte Web-Based (s'appuyant sur le Web), il n'est pas nécessairement lié à une quelconque interface utilisateur. Les alternatives à WBEM sont les interpréteurs de commandes distants, certaines solutions propriétaires, ainsi que l'architecture de gestion réseau SNMP.
eXtensible Access Method (XAM) est une norme utilisée en enregistrement de données informatiques ( standard ANSI), développée et utilisée par la SNIA (Storage Networking Industry Association).  Portail de l’informatique
XForms est un dialecte XML servant à créer des formulaires en ligne destinés à être utilisés avec HTML, XHTML, WML ou SVG. C'est une spécification du W3C, cependant elle semble à ce jour être mise de côté, le W3C soutenant plus activement le développement de HTML5, sans toutefois que XForms ne soit totalement abandonné.
XFrames est un dérivé d'XML permettant d'afficher plusieurs pages à l'intérieur d'une fenêtre de navigateur. Ce procédé est destiné à être utilisé avec XHTML. C'est une spécification du W3C.
XFT (eXchange For Travel) est une norme XML. Le XFT a été élaboré pour faire office d’outil de communication entre les acteurs du tourisme (tours opérateurs, distributeurs ou autres prestataires). Ce langage permet d'élaborer des transactions de vente de voyage, de gestion post-réservation ou de back-office(Exchange For Travel).
XKMS (XML Key Management Specification) est une spécification de validation et d'enregistrement de clé publique utilisable de façon conjointe au signature XML et proposée notamment par Microsoft et Verisign au W3C.
XPDL (XML Process Definition Language, c'est-à-dire langage de définition de processus en XML ) est une norme de format ouvert et extensible du consortium Workflow Management Coalition qui permet de définir des processus d'affaires à l'aide du langage XML et de les mettre en œuvre avec un moteur de workflow. XPDL est compatible avec la norme BPMN qui permet de représenter graphiquement les workflows , et dont XPDL est un format d'échange . La version actuelle est la 2.2. La définition d'un processus comporte les principaux éléments (balises) suivant(e)s notables : les marques de début et de fin du ou des processus, les activités, leurs inter-relations (les transitions), les attributs qualifiant certains comportements de l'activité, les participants, rôles, groupes, et les interactions, relations entre les acteurs et les activités. La définition ne comporte pas de façon native des attributs de positionnement (exemple : attributs (X, Y) d'une activité du diagramme représentant le processus) mais inclus la notion d'attributs étendus (la balise Extended Attribute) pour la plupart des composants. Certains éditeurs XPDL s'en servent pour mettre des attributs de positionnement. XPDL 1.0 est annoncé en décembre 2002. XPDL 2.0 est annoncé le 3 octobre 2005.
XML-RPC est un protocole RPC (Remote procedure call), une spécification simple et un ensemble de codes qui permettent à des processus s'exécutant dans des environnements différents de faire des appels de méthodes à travers un réseau. XML-RPC permet d'appeler une fonction sur un serveur distant à partir de n'importe quel système (Windows, Mac OS X, GNU/Linux) et avec n'importe quel langage de programmation. Le serveur est lui-même sur n'importe quel système et est programmé dans n'importe quel langage. Cela permet de fournir un Service web utilisable par tout le monde sans restriction de système ou de langage. Les processus d'invocation à distance utilisent le protocole HTTP pour le transfert des données et la norme XML pour la structuration des données. XML-RPC est conçu pour permettre à des structures de données complexes d'être transmises, exécutées et renvoyées très facilement. XML-RPC est une alternative aux Services Web WS-*, dont SOAP.
ZIF est un acronyme de l’anglais zero insertion force qui signifie force d’insertion nulle. Habituellement un support de circuit intégré (CI) ou de processeur exige que le CI soit poussé dans des contacts à ressorts qui s’accrochent alors par frottement. Pour un CI avec des centaines de broches, la force d’insertion totale peut être très grande (des dizaines de newtons), entraînant le danger d’abîmer le dispositif, si toutes les broches ne sont pas correctement alignées, ce qui a été le problème causé par les sockets low insertion force (LIF).
Le basculement (en anglais, fail-over qui se traduit par passer outre à la panne) est la capacité d'un équipement à basculer automatiquement vers un réseau ou un système alternatif ou en veille.
Le Databending (ou torsion de données) est un processus de manipulation de données (ex. fichiers) dans un certain format à partir d'éditeur logiciel adapté à un format différent. Une torsion du contenu intervient comme résultat. Ce processus s'inscrit dans un mouvement plus large : le Glitch art.
Un dépannage informatique est une opération technique, généralement faite par un professionnel, visant à rétablir le fonctionnement normal de tout équipement informatique, à la suite d'une panne matérielle ou à un usage dommageable d'un des logiciels utilisés. En cas de panne matérielle et sous certaines conditions, notamment de validité de garantie, un ordinateur équivalent peut être prêté au client le temps de procéder à la remise en état. À moins de s'engager dans des frais très considérables, le dépannage se limite à des remplacements de pièce ou à des opérations de maintenance simples : la récupération massive de données sur un disque dur défectueux est notamment hors de son champ d'action.
L’écran bleu de la mort (abrégé en anglais en BSoD pour Blue Screen of Death) est le surnom du message d'erreur affiché sur un écran d’ordinateur par l'OS Microsoft Windows lorsque celui-ci ne peut plus récupérer d'une erreur système ou lorsqu'il arrive à un point critique d’erreur fatale. Il évolue à chaque nouvelle version de Windows. Windows a deux types d’écrans d’erreur : un « arrêt d’urgence » (comme décrit dans les manuels de Windows XP) et le véritable « écran bleu de la mort », qui a une signification d’erreur plus sérieuse que la première. Depuis la version 2.0 de Windows, les écrans bleus de la mort ont toujours été présents — sous une forme ou sous une autre — dans les versions des systèmes d’exploitation de Microsoft. Ils sont devenus au fil des années un sujet quasi folklorique de ce système d’exploitation et une source de raillerie de la part des détracteurs de ce système, mais surtout une source d’énervement pour les utilisateurs. Le 20 avril 1998, lors de la présentation à la presse de Windows 98 pour la conférence COMDEX, le PDG de Microsoft, Bill Gates, souhaitait souligner la facilité d'usage du système d'exploitation et le support amélioré du Plug-and-Play (PnP), une des avancées majeures de Windows. Toutefois, lorsque son assistant Chris Capossela a branché un scanner et a essayé de l'installer, le système d'exploitation a planté, affichant un écran bleu de la mort. Après les applaudissements et les acclamations de l'auditoire, Bill Gates déclare : « Ce doit être la raison pour laquelle nous ne commercialisons pas encore Windows 98 ». La vidéo de cet événement est devenu un phénomène Internet populaire. En août 2014, les mises à jour de Windows 7 et Windows 8.1 ont provoqué des bugs, dont un pouvant conduire à un écran bleu de la mort, amenant Microsoft à les retirer et à recommander à ses utilisateurs de les désinstaller. Le 10 avril 2018, alors que la première mise à jour majeure de l'année 2018 pour Windows 10, baptisée April Update (anciennement Spring Creators Update), commençait à être déployée, Microsoft a détecté un bug de grande ampleur touchant certains utilisateurs ayant installé la nouvelle mise à jour bisannuelle, qui conduit à cet écran bleu de la mort après l'installation. Cela a obligé la firme américaine à repousser la sortie de la mise à jour à une date ultérieure. Le bug a été corrigé dans la build 17134, qui succède à la 17133, qui était censé être la RTM, c'est-à-dire la version finale de la mise à jour.
L'écran rouge de la mort (Red Screen of Death en anglais, en abrégé RSoD, parfois appelé Red Screen of Doom est le surnom du message d'erreur qui a existé dans certaines versions bêta du système d'exploitation Microsoft Longhorn (devenu plus tard Windows Vista), mais aussi dans les premières bêtas de Microsoft Memphis (devenu plus tard Windows 98). Il a été abandonné dans la Bêta 1 (Build 5112) de Windows Vista pour toutes les erreurs, sauf celles liées au bootloader. L'emploi du terme « Screen of Death » fait penser à un autre, l'écran bleu de la mort sur les autres systèmes d'exploitation Microsoft Windows, mais aussi au Panique du noyau (Kernel Panic) des systèmes Unix et Linux et du Guru Meditation des ordinateurs personnels sous AmigaOS. Dans les versions récentes du logiciel Lotus Notes, l'écran rouge de la mort renvoie parfois à des erreurs fatales, mais celles-ci ne sont pas en plein écran comme le Microsoft rouge ou des écrans bleus de la mort, mais sont plutôt des boîtes de couleur rouge vif avec des bordures noires. Un outil est disponible pour modifier la couleur d'un écran bleu de la mort par d'autres couleurs sous Windows 9x dans system.ini.
Une erreur système est un évènement qui se produit au niveau du système d'exploitation d'un ordinateur. Selon le degré de gravité, l'erreur peut être ou non récupérée et traitée par le système d'exploitation. Dans le pire des cas, cela conduit à un crash de la machine sur laquelle elle se produit. Les causes d'une erreur système peuvent être très diverses : soit le système d'exploitation comporte un bug ; soit le logiciel en cours d'exécution a enfreint les règles fondamentales du système ; soit il s'est produit quelque chose au niveau du matériel (panne inopinée, conflit matériel, erreur humaine, etc.).
La gestion des incidents est un processus de gestion du cycle de vie de tous les incidents. Elle s’assure que l'exploitation normale des services soit rétablie le plus rapidement possible et que l’impact sur le business soit réduit au minimum. L’exploitation normale des services est définie dans l’accord sur les niveaux de service (SLA). Il décrit le service informatique, documente les cibles de niveau de service et spécifie les responsabilités du fournisseur de service informatique et du client. La gestion des incidents est un processus inclus dans la démarche ITIL et ISO 20000.
Guru Meditation (littéralement, « méditation du gourou ») est l'erreur qui se produit lors d'un plantage général sur les premiers modèles de la gamme d'ordinateurs personnels Amiga. À l'origine, c'est en partie une plaisanterie interne à l'entreprise. Le Guru Meditation est analogue au Blue Screen of Death que l'on trouve dans le système d'exploitation Microsoft Windows, ou de la panique du noyau que l'on trouve dans les systèmes d'exploitation basés sur Unix et Linux.
Un Mac triste est une icône utilisée par une génération ancienne de l'ordinateur Macintosh (matériel utilisant la Old World ROM) pour indiquer un problème qui a empêché le système d'exploitation de démarrer avec succès. L'icône Mac triste est présentée avec deux lignes de codes hexadécimaux qui indiquent le type d'erreur à la place de celle Mac heureux lorsque la procédure se déroule normalement. Les premiers modèles l'accompagnaient avec un jingle puis ce fut le son numérisé d'un crash automobile. Le Mac triste est lui-même inspiré du Blue Screen of Death du système d'exploitation Microsoft Windows, mais aussi de la panique du noyau des systèmes basés sur Unix et Linux et du Guru Meditation du système AmigaOS. Sur l'iPod, lorsqu'un blocage intervient un iPod triste apparaît, d'une manière très similaire au Mac triste.
En informatique, un message d'erreur est un message affiché de manière autonome par un ordinateur, informant l'utilisateur qu'une erreur prévue par les programmeurs s'est produite dans l'exécution d'un logiciel. Il sert aux informaticiens à identifier la source du bug empêchant le logiciel de fonctionner. Quelquefois, il indique à l'utilisateur la cause du non-fonctionnement. Sous Windows, il est généralement rédigé de manière incompréhensible pour le néophyte, qui ne le répercute pas au programmeur. Jusqu'à Windows 2000, l'apparition de l'écran bleu empêchait souvent l'affichage de message d'erreur. Sous Mac OS X comme sous Windows XP ou Windows Vista, le plantage d'une application provoque la génération d'un rapport d'erreur, qui peut être envoyé à Apple ou à Microsoft si l'utilisateur a autorisé la communication de ces informations.
La panique du noyau (kernel panic) est un mécanisme de signalement d'erreur système du noyau d'un système d'exploitation, en particulier UNIX, GNU/Linux ou macOS. Ce terme est lui-même inspiré de l'écran bleu de la mort des systèmes d'exploitation Microsoft Windows et du Guru Meditation des ordinateurs personnels sous AmigaOS. [réf. nécessaire]
En informatique, on appelle panne byzantine ou comportement byzantin tout comportement d'un système ne respectant pas ses spécifications, en donnant des résultats non-conformes. On distingue couramment les pannes byzantines naturelles des pannes byzantines volontaires. Les pannes byzantines naturelles proviennent généralement d'erreurs physiques non détectées (mémoire, transmissions réseaux, etc.). Les pannes byzantines volontaires proviennent principalement d'attaques visant à faire échouer le système (sabotage, virus, etc.). L'authentification et la signature par des moyens de cryptographie permettent de limiter les erreurs byzantines.
Dans le jargon informatique, un plantage (parfois appelé, par anglicisme, « crash ») est une interruption anormale, souvent inattendue, d’un logiciel (lequel peut aussi bien être une application qu’un système d'exploitation) due à une panne, un incident ou un bug. Un plantage peut avoir plusieurs conséquences : le programme s’interrompt abruptement (parfois avec un message d’erreur produit par le programme lui-même ou par le système d’exploitation), le programme ne répond plus (fenêtre aréactive), l'image à l'écran se fige (la machine ne réagit plus aux sollicitations des périphériques d'entrée, et aucune évolution de son état n’est apparente), ou l'ordinateur peut redémarrer plus ou moins subitement (après en avoir averti l’utilisateur ou non). Un plantage a souvent pour origine soit une division par 0, soit une instruction non attendue (ou utilisant une valeur non attendue). Le plantage peut se produire à 3 niveaux différents : Logiciels : la conséquence est une aréaction. De nos jours, la plupart des systèmes d'exploitation permettent de reprendre la main (automatiquement ou manuellement) et d'arrêter le fonctionnement du logiciel, en perdant les données non sauvegardées dans le logiciel en question, mais sans avoir besoin de redémarrer l'ordinateur. Système d'exploitation : Techniquement, c'est un logiciel, sauf qu'en cas de plantage, l'utilisateur n'a en général plus de moyen d'action sur son ordinateur (redémarrage forcé avec perte de toutes les données non sauvegardées). Matériel : plantage d'un processeur (surchauffe, rayon cosmique, fréquence trop haute, erreur de conception, etc.) Le terme peut être utilisé comme verbe : « le système s'est planté ». Il est recommandé par l’office québécois de la langue française et usuellement utilisé dans d'autres pays francophones comme la France. On utilise parfois les termes plus généraux d’« incident » et de « panne ».
Le Red Ring of Death (abrégé en RRoD ; littéralement « Anneau rouge de la mort » en français) est une panne affectant les consoles de jeux vidéo Xbox et Xbox 360 de Microsoft, et se soldant par l'allumage en rouge du voyant de mise sous tension. Le terme a été créé pour trouver un équivalent au Yellow Light of Death de la PlayStation 3, eux-mêmes inspirés du Blue Screen of Death que l'on connaît sur les systèmes d'exploitation Windows de la firme américaine.
Schiaparelli ou ExoMars EDM (EDM pour ExoMars Entry, Descent and Landing Demonstrator Module), est un atterrisseur expérimental développé par l'Agence spatiale européenne (ESA), qui s'est écrasé à l'atterrissage sur la planète Mars le 19 octobre 2016, en raison de l'échec de la procédure de freinage. La mission de Schiaparelli devait permettre de valider les techniques de rentrée atmosphérique et d'atterrissage qui seront mises en œuvre par de futures missions martiennes européennes. L'engin spatial est développé dans le cadre du programme ExoMars de l'ESA, avec la participation de l'agence spatiale russe Roscosmos. Schiaparelli est lancé le 14 mars 2016 par une fusée russe Proton avec l'orbiteur martien ExoMars Trace Gas Orbiter, qui assure son transport jusqu'à proximité de Mars. Le 19 octobre 2016, il entame sa descente vers le sol martien mais tout contact est perdu peu après le largage du bouclier thermique et une trentaine de secondes avant l'atterrissage. L'orbiteur de la NASA MRO parvient à photographier les traces de l'impact de Schiaparelli qui s'est écrasé à quelques kilomètres du centre de la zone d'atterrissage prévue. L'engin spatial d'une masse totale de 577 kg utilisait un véhicule de descente équipé d'un bouclier thermique le protégeant de la chaleur générée par la rentrée atmosphérique à grande vitesse, d'un parachute déployé une fois la vitesse tombée à Mach 2, et enfin, de moteurs-fusées à ergols liquides qui devaient lui permettre de se poser en douceur. Il emportait également une petite charge utile scientifique, mais sa durée de vie prévue sur le sol martien était limitée par la capacité de ses batteries qui n'étaient pas rechargeables.
La tolérance aux pannes (aussi appelé « insensibilité aux pannes ») désigne une méthode de conception permettant à un système de continuer à fonctionner, éventuellement de manière réduite (on dit aussi en « mode dégradé »), au lieu de tomber complètement en panne, lorsque l'un de ses composants ne fonctionne plus correctement. L'expression est employée couramment pour les systèmes informatiques étudiés de façon à rester plus ou moins opérationnels en cas de panne partielle, c'est-à-dire éventuellement avec une réduction du débit ou une augmentation du temps de réponse. En d'autres termes, le système ne s'arrête pas de fonctionner, qu'il y ait défaillance matérielle ou défaillance logicielle. Un exemple en dehors de l'informatique est celui du véhicule à moteur conçu pour être toujours en état de rouler même si l'un de ses pneus est crevé.
Le vol 501 est le vol inaugural du lanceur européen Ariane 5, qui a eu lieu le 4 juin 1996. Il s'est soldé par un échec, causé par un dysfonctionnement informatique (appelé aussi bug), qui vit la fusée se briser et exploser en vol seulement 36,7 secondes après le décollage.
Le Yellow Light of Death (abrégé en YLoD, littéralement « Lumière jaune de la mort ») est une expression familière désignant une panne de la console de jeu PlayStation 3. Le terme a été créé pour trouver un équivalent au Red Ring of Death de la Xbox 360, qui est lui-même inspiré du Blue Screen of Death des systèmes d'exploitation Windows. Le problème se caractérise par une lumière jaune dès l'allumage, suivie par un clignotement de la lumière rouge, indiquant que la console ne peut plus démarrer.
Fravia, ou Fravia+ de son vrai nom Francesco Vianello (30 août 1952, 3 mai 2009 à Bruxelles), était un hacker italien connu dans le milieu de l'ingénierie inverse. Fravia était aussi un expert en linguistique et parlait six langues : allemand, italien, français, anglais, finnois, espagnol.
Dans le domaine de l'informatique, la programmation est l'ensemble des activités qui permettent l'écriture des programmes informatiques. C'est une étape importante du développement de logiciels (voire de matériel). Pour écrire un programme, on utilise un langage de programmation. Un logiciel est un ensemble de programmes (qui peuvent être écrits dans des langages de programmation différents) dédié à la réalisation de certaines tâches par un (ou plusieurs) utilisateurs du logiciel. La programmation représente donc ici la rédaction du (ou des) code source d'un logiciel. On utilise plutôt le terme développement pour dénoter l'ensemble des activités liées à la création d'un logiciel et des programmes qui le composent (cela inclut la spécification du logiciel, sa conception, puis son implémentation proprement dite au sens de l'écriture des programmes dans un langage de programmation bien défini et aussi la vérification de sa correction)...
En informatique, le concept d'abstraction identifie et regroupe des caractéristiques et traitements communs applicables à des entités ou concepts variés ; une représentation abstraite commune de tels objets permet d'en simplifier et d'en unifier la manipulation.
ActiveX désigne l'une des technologies du Component Object Model de Microsoft avec COM+ et Distributed COM utilisées en programmation pour permettre le dialogue entre programmes.
ADD est une instruction utilisée par certaines calculatrices programmables ainsi que dans certains langages informatiques, par exemple, dans le langage cobol ou certains Assembleurs. Elle consiste à additionner deux ou plusieurs valeurs, deux registres en assembleur, ou deux variables ou plus en Cobol, par exemple: ADD X, Y, Z GIVING TOTAL ADD MONTANT TO RESULTAT
L’adressage mémoire est, en électronique et informatique, la façon dont sont accédées des données stockées en mémoire. Une adresse mémoire est un nombre entier naturel (rarement une autre sorte d'identifiant) qui désigne une zone particulière de la mémoire, ou juste le début d'une zone. Le plus souvent, une donnée peut être lue ou écrite. La mémoire peut être temporaire (mémoire vive) pour le travail ou au contraire durable (mémoire non volatile) pour le stockage.
En algorithmique et en programmation informatique, une affectation, aussi appelée assignation par anglicisme, est une structure qui permet d'attribuer une valeur à une variable. Il s'agit d'une structure particulièrement courante en programmation impérative, et dispose souvent pour cette raison d'une notation courte et infixée, comme x = expr​ ou x := expr​. Dans certains langages, le symbole est considéré comme un opérateur d'affectation, et la structure entière peut alors être utilisée comme une expression. D'autres langages considèrent une affectation comme une instruction et ne permettent pas cet usage.
En programmation C++, un allocateur (anglais : allocator) est un composant de la bibliothèque standard de C++ (Standard Template Library ou STL) qui gère les demandes d'allocation et de désallocation de la mémoire. La bibliothèque standard fournit plusieurs structures de données, telles que les listes et les ensembles, communément appelées conteneurs. Un point commun parmi ces conteneurs est leur capacité à changer de taille pendant l'exécution du programme. Pour ce faire, des techniques d'allocation dynamique de la mémoire sont généralement employées. Les allocateurs gèrent toutes les demandes d'allocation et de désallocation de la mémoire pour un conteneur donné. La bibliothèque standard de C++ fournit des allocateurs génériques utilisés par défaut, cependant, des allocateurs personnalisés peuvent également être écrits par le développeur.
L'analyse dynamique de programmes est une analyse réalisée sur un programme informatique en l'exécutant sur un vrai processeur ou un processeur virtuel. Pour que l'analyse dynamique de programmes produise des résultats intéressants, le programme cible doit être exécuté avec des entrées suffisamment variées. L'utilisation de techniques de test logiciel telles que la couverture de code aide à s'assurer qu'un ensemble adéquat des comportements possibles du programme a été observé. Il faut également s'assurer de limiter autant que possible les effets que l'instrumentation a sur l'exécution (y compris sur les propriétés temporelles) du programme cible.
Une antisèche (en informatique, on utilise généralement le terme anglais cheatsheet) est un document rédigé à l'avance afin d'éviter de « sécher » sur un sujet. Dans le domaine éducatif, il s'agit d'une fraude permettant par exemple de répondre correctement à une question lors d'un examen. Dans ce cas, l'antisèche, encore appelée pompe en argot scolaire, doit être de petite taille afin d'être aisément dissimulée. La cheatsheet reprend ce concept en présentant, de la façon la plus concise possible, les grandes lignes d'un logiciel ou d'un langage informatique, comme les raccourcis, termes ou données spécifiques à celui-ci. En Belgique, le terme copion est utilisé pour désigner une antisèche tandis qu'en Suisse, on utilise le terme mascogne. Dans le Midi de la France, on utilise couramment le substantif tust(e) qui vient de l'occitan tustar, et fafiot (billet) dans l'Est.
En informatique, l'API management (ou gestion d'API) est une discipline qui consiste à exploiter au mieux les API sans mettre en péril le système d’information et sans affecter l’expérience utilisateur. En d'autres termes, l'API management est un outil de management, comme son nom l'indique, qui permet de manager la publication, la promotion et la supervision des échanges de données entre un service fournisseur et un service client, au sein d'un environnement sécurisé et évolutif.
En informatique, une Application Binary Interface (ABI, interface binaire-programme), décrit une interface de bas niveau entre les applications et le système d'exploitation, entre une application et une bibliothèque ou bien entre différentes parties d’une application. Une ABI diffère d’une API, puisqu'une API définit une interface entre du code source et une bibliothèque, de façon à assurer que le code source fonctionnera (compilera, si applicable) sur tout système supportant cette API. Une ABI définit notamment des conventions d'appel des fonctions pour une architecture donnée.
En informatique, les arguments sont les données traitées par une fonction. Ils sont remplacés par les entrées lors de l'exécution du programme qui utilise cette fonction.
l'art évolutionnaire (anglais : evolutionary art) est une branche de l'art génératif (ou procédural) utilisant l'algorithmie évolutionniste. Dans l'art génératif l'artiste ne construit pas l'œuvre, mais la laisse se construire via un système qu'il aura créé. Dans l'art évolutionnaire une œuvre initiale est insérée dans le système, et un processus itératif de sélection et modification permet d'arriver à l'œuvre finale, où l'artiste sera l'agent de sélection.
L'art génératif est une création artistique généralement numérique se basant sur des algorithmes pour concevoir des œuvres se générant d'elles-mêmes et/ou non déterminées à l'avance.
The Art of Computer Programming (TAOCP) est une série de livres en plusieurs volumes sur la programmation informatique, écrits par Donald Knuth. Seuls les trois premiers ont été publiés en entier, le premier tome du quatrième volume étant paru début 2011 : Volume 1, Fundamental Algorithms (troisième édition 1997) ; Volume 2, Seminumerical Algorithms (troisième édition 1997) ; Volume 3, Sorting and Searching (seconde édition, 1998) ; Volume 4A, Enumeration and Backtracking (2011). Les deux autres tomes prévus pour le quatrième volume, Combinatorial Algorithms, sont en cours de rédaction. Certaines parties sont d'ailleurs disponibles sur la page de TAOCP. Au total, sept volumes sont prévus.
L'auto-hébergement désigne l'utilisation d'un programme informatique, partie d'une chaîne d'outils ou d'un système d'exploitation pour produire de nouvelles versions dudit programme. Par exemple, un compilateur qui peut compiler son propre code source. Le logiciel auto-hébergé est commun sur les ordinateurs personnels et les systèmes plus larges. D'autres programmes qui sont typiquement auto-hébergés sont les noyaux, les assembleurs et les shells. Si un système est si neuf qu'aucun logiciel n'a été écrit pour lui, alors on développe le logiciel sur un autre système auto-hébergé et placé dans une mémoire informatique que le nouveau système peut lire. Le développement continue de cette manière jusqu'à ce que le nouveau système puisse héberger son propre développement. Par exemple, le développement du système d'exploitation Linux a été initialement hébergé sur le système d'exploitation Minix. Écrire de nouveaux outils de développement logiciel sans partir d'un système existant ne se fait plus aujourd'hui. Plusieurs langages de programmation sont auto-hébergés, dans le sens qu'un compilateur pour ce langage, écrit dans le même langage, est disponible. Pour un nouveau système, le premier compilateur d'un nouveau langage de programmation doit être écrit dans un autre langage (dans de rares cas, le langage machine). Les langages auto-hébergés incluent le Lisp, Forth, C, Pascal, Modula-2, Oberon, Smalltalk, OCaml, ooc et FreeBASIC. La confiance aveugle en des outils de programmation auto-hébergés est un risque de sécurité comme démontré par le hack de Thompson.
En informatique, une bibliothèque logicielle est une collection de routines, qui peuvent être déjà compilées et prêtes à être utilisées par des programmes,. Les bibliothèques sont enregistrées dans des fichiers semblables, voire identiques aux fichiers de programmes, sous la forme d'une collection de fichiers de code objet rassemblés accompagnée d'un index permettant de retrouver facilement chaque routine. Le mot « librairie » est souvent utilisé à tort pour désigner une bibliothèque logicielle. Il s'agit d'un anglicisme fautif dû à un faux-ami (library). Les bibliothèques sont apparues dans les années 1950, et sont devenues un sujet incontournable de programmation. Elles sont utilisées pour réaliser des interfaces de programmation, des framework, des plugins ainsi que des langages de programmation. Les routines contenues dans les bibliothèques sont typiquement en rapport avec des opérations fréquentes en programmation : manipulation des interfaces utilisateur, manipulation des bases de données ou les calculs mathématiques. Les bibliothèques sont manipulées par l'éditeur de lien et le système d'exploitation. Les manipulations sont différentes suivant que la bibliothèque est statique ou partagée. Les emplacements et les noms des bibliothèques varient selon les systèmes d'exploitation.
Une bibliothèque standard pour un langage de programmation est une bibliothèque logicielle qui est utilisée dans toute implémentation de ce langage. Une bibliothèque standard peut inclure : Fonctions Macros Variables globales Classes Gabarits (templates) La plupart des bibliothèques standard incluent : des algorithmes (par exemple pour le tri) ; des structures de données (telles que les listes, les arbres et les tables de hachage) ; des routines d'entrées-sorties et d'appel système.
Un binding ou liaison (qui est un terme anglais désignant l'action de lier des éléments entre eux) peut avoir plusieurs significations en informatique : binding de langage, qui permet l'utilisation d'une bibliothèque logicielle dans un autre langage de programmation que celui avec lequel elle a été écrite. On parle alors binding de langage. XML data binding, qui permet la lecture d'un document XML en générant un objet représentant ces données. Data binding, qui permet de lier des objets entre eux pour les faire communiquer.
Un booléen, en logique et en programmation informatique, est un type de variable à deux états. Les variables de ce type sont ainsi soit à l'état vrai soit à l'état faux (en anglais, true et false). Le booléen est nommé ainsi d'après le mathématicien, logicien et philosophe britannique George Boole, fondateur de l'algèbre de Boole. Généralement, les conditions sont de type booléen, car elles nécessitent une réponse binaire du type oui ou non.  Certains langages utilisent les nombres pour représenter des booléens : par exemple, 0 est faux en C et toute autre valeur est vraie. La convention n’est pas la même d’un langage à l’autre ; ainsi, 0 est le seul entier vrai en shell, tous les autres étant faux. D’autres langages utilisent des mots-clés, souvent les mots anglais true et false. Il est possible d’utiliser un unique bit en mémoire pour stocker une valeur booléenne, mais à moins d’une contrainte en mémoire, l’utilisation de mots est plus simple.
En informatique, un bouchon (stub en anglais) est un code qui n'effectue aucun traitement et retourne toujours le même résultat. Un bouchon sert d'alternative temporaire à un code qui n'est pas utilisable parce qu'il n'est pas encore codé ou qu'il est en cours d'évolution.  Portail de la programmation informatique
Une boucle de consultation en informatique est le fait de consulter activement de manière répétitive le statut d'un périphérique externe depuis un programme client en tant qu'activité synchrone. Une telle technique est utilisée majoritairement pour des entrées/sorties
Une boucle infinie est, en programmation informatique, une boucle dont la condition de sortie n'a pas été définie ou ne peut pas être satisfaite. En conséquence, la boucle ne peut se terminer qu'à l'interruption du programme qui l'utilise.
En informatique, le calcul séquentiel consiste en l'exécution d'un traitement étape par étape, où chaque opération se déclenche que lorsque l'opération précédente est terminée, y compris lorsque les deux opérations sont indépendantes. Le calcul séquentiel s'oppose au calcul parallèle, où plusieurs opérations peuvent être simultanées. Il s'agit du mode de traitement le plus fréquemment utilisé, car compatible avec n'importe quel type d'opération. Il est cependant limité par la puissance des calculateurs, là où le calcul parallèle n'est limité que par le nombre de calculateurs.
Un chien de garde, en anglais watchdog, est un circuit électronique ou un logiciel utilisé en électronique numérique pour s'assurer qu'un automate ou un ordinateur ne reste pas bloqué à une étape particulière du traitement qu'il effectue. C'est une protection destinée généralement à redémarrer le système, si une action définie n'est pas exécutée dans un délai imparti. En informatique industrielle, le chien de garde est souvent réalisé par un dispositif électronique, en général une bascule monostable. Il repose sur le principe que chaque étape du traitement doit s'exécuter en un temps maximal. À chaque étape, le système arme une temporisation avant son exécution. Si la bascule retourne à son état stable avant que la tâche ne soit achevée, le chien de garde se déclenche. Il met en œuvre un système de secours qui peut soit déclencher une alarme, soit faire redémarrer l'automate, soit mettre en marche un système redondant... Les chiens de garde sont souvent intégrés aux microcontrôleurs et aux cartes mères dédiées au temps réel. Quand il est réalisé par logiciel, il s'agit en général d'un compteur qui est régulièrement remis à zéro. Si le compteur dépasse une valeur donnée (timeout) alors on procède à un reset (redémarrage) du système. Le chien de garde consiste souvent en un registre qui est mis à jour via une interruption régulière. Il peut également consister en une routine d'interruption qui doit effectuer certaines tâches de maintenance avant de redonner la main au programme principal. Si une routine entre dans une boucle infinie, le compteur du chien de garde ne sera plus remis à zéro et un reset est ordonné. Le chien de garde permet aussi d'effectuer un redémarrage si aucune instruction n'est prévue à cet effet. Il suffit alors d'écrire une valeur dépassant la capacité du compteur directement dans le registre : le chien de garde lancera le reset. D'une façon plus générale, il peut également s'agir d'un logiciel destiné à l'observation de certaines conditions. Par exemple, en matière de sécurité, à s'assurer régulièrement qu'aucune action malveillante n'est effectuée (dans la mémoire vive en particulier) ou que certaines conditions de fonctionnement sont bien remplies. Il s'agit généralement de processus ou de service exécuté en tâche de fond et donc, invisible à l'utilisateur.  Portail de l’informatique
Un code automodifiable est, en programmation informatique, un programme qui peut se modifier lui-même, c’est-à-dire appeler des routines, fonctions ou méthodes qui seront créées par le programme lui-même.
Code Club est une initiative volontaire, fondée en 2012, dont le but est de donner à des enfants de 9 à 11 ans l'opportunité de développer des talents de programmation dans des clubs scolaires gratuits. En novembre 2015, plus de 3 800 écoles et autres établissements publics avaient créé des Code Clubs, auxquels on estime que participaient régulièrement 44 000 jeunes du Royaume-Uni. L'organisation s'est développée dans le monde entier, et il existe actuellement hors Grande-Bretagne plus de 2 000 Code Clubs, particulièrement en Australie et au Brésil. Des volontaires programmeurs et développeurs de logiciels donnent de leur temps pour animer des sessions de Code Club, transmettant leurs compétences et supervisant les jeunes étudiants,. Les enfants créent leurs propres jeux vidéo, leurs animations et leurs sites web, apprenant à utiliser la technologie de manière créative.
Codecademy est une plateforme interactive en ligne qui propose d'apprendre gratuitement six langages de programmation, tels que Python, PHP, JavaScript, ou des langages de balisage, comme HTML et CSS, et des tutoriels pour construire des sites web ou améliorer des programmes[réf. nécessaire]. En janvier 2014, le site passe la barre des 100 000 millions d’exercices réussis, avec 24 millions d'utilisateurs,,. La plateforme propose un profil personnel, un glossaire pour chaque tutoriel, un espace de discussion, et un outil pour créer son propre cours. L'utilisateur est récompensé par des badges lorsqu'il achève des exercices. Le site est particulièrement bien accueilli par la presse, dont le New York Times et TechCrunch. Le site a été traduit en français par l'association Bibliothèques sans frontières.
Le coding dojo est une rencontre entre plusieurs personnes qui souhaitent travailler sur un défi de programmation de façon collective. Le défi peut être un problème algorithmique à résoudre ou un besoin à implémenter. Chaque coding dojo se concentre sur un sujet particulier, et représente l'objectif de la séance. Ce sujet doit permettre d'apprendre de façon collective sur le plan technique et sur la manière de réussir le défi. L'exercice peut être effectué entre personnes d'une même entreprise, d'une école ou encore venant d'horizons différents. Les coding dojo mettent en œuvre des techniques de programmation lié aux méthodes agile et à l'extreme programming : Pair programming Test driven development C'est un exercice qui tend à se développer depuis 2009,, bien que certains clubs existent depuis plusieurs années.
Coinhive est une bibliothèque Javascript, lancée en 2017. Elle permet à un site Web d’utiliser l’ordinateur client pour miner de la crypto-monnaie Monero. Ses auteurs souhaitent proposer aux propriétaires de sites, une alternative à la publicité en ligne. En septembre 2017, le site The Pirate Bay intègre Coinhive, en justifiant auprès des utilisateurs : « Nous voulons vraiment nous débarrasser de toutes les publicités. Mais nous avons également besoin d’argent pour maintenir le site en cours d’exécution. ». Toutefois, il l'abandonne suite a des retours négatifs de ses utilisateurs . Malgré son caractère innovant, l’utilisation abusive voire frauduleuse de Coinhive ternit sa réputation auprès des utilisateurs, des logiciels antipub et des logiciels antimalware,. Depuis octobre 2017, les auteurs de Coinhive propose donc AuthedMine, une alternative exigeant l’accord du client pour réaliser le minage.
La coloration syntaxique est une fonctionnalité informatique proposée par certains éditeurs de texte, qui consiste à formater automatiquement chacun des éléments du texte affiché en utilisant une couleur et une fonte caractéristique de son type. Le but est d’améliorer la lisibilité d'un code source en mettant en évidence les structures syntaxiques de son langage de programmation ou de description.
Le terme concaténation (substantif féminin), du latin cum (« avec ») et catena (« chaîne, liaison »), désigne l'action de mettre bout à bout au moins deux chaînes de caractères ou de péricopes.
En informatique, la continuation d'un système désigne son futur, c'est-à-dire la suite des instructions qu'il lui reste à exécuter à un moment précis. C'est un point de vue pour décrire l'état de la machine.
En programmation fonctionnelle, la curryfication désigne la transformation d'une fonction à plusieurs arguments en une fonction à un argument qui retourne une fonction sur le reste des arguments. La curryfication permet de créer des fonctions pures. L'opération inverse est possible et s'appelle la décurryfication. Le terme vient du nom du mathématicien américain Haskell Curry, bien que cette opération ait été introduite pour la première fois par Moses Schönfinkel.
En informatique, et notamment dans la conception et l'analyse des langages de programmation, le problème du dangling else (anglais que l'on pourrait traduire par le problème du « sinon pendant ») est un problème de programmation informatique qui résulte de l'ambiguïté de l'interprétation de la clause sinon dans l'imbrication de deux instructions conditionnelles de la forme si-alors-sinon. Formellement, la grammaire non contextuelle du langage est ambiguë, ce qui signifie qu'il peut y avoir plusieurs arbres d'analyse corrects pour une même instruction.
Dangling pointer (traduction littérale : « pointeur pendouillant » ou « pointeur sautillant ») est un terme anglais de programmation informatique désignant un pointeur qui ne pointe pas vers un type d'objet approprié. Les Dangling pointers sont créés lors de la destruction de l'objet.
En programmation informatique, la déclaration permet d'indiquer au compilateur l'existence d'une entité informatique (variable, routine, etc.) , en spécifiant: son identifiant; son type de données (dans le cas d'un langage de programmation typé); les paramètres (identifiant et type) (dans le cas d'une routine). Avec certains langages de programmation, notamment le langage C/C++, il est nécessaire de déclarer les entités informatiques avant de pouvoir les utiliser.
En programmation informatique, une déclaration avancée est une déclaration d'un identificateur représentant une entité (par exemple un type, une variable, une fonction) pour laquelle la définition n'est fournie qu'ultérieurement dans le code.
En programmation procédurale, un déclencheur (trigger en anglais) est un dispositif logiciel qui provoque un traitement particulier en fonction d'événements prédéfinis. Par extension, c'est l'événement lui-même qui est qualifié de déclencheur. En programmation objet, tout message à un objet est lui-même un déclencheur. Dans les interfaces graphiques, ces déclencheurs sont nommés en général callbacks. En programmation système, la modification d'un fichier peut constituer un déclencheur, soit pour maintenir à jour les informations affichées (contenu d'un répertoire, par exemple), soit pour lancer des opérations de sécurité. En Linux, c'est Gamin (anciennement : FAM, File Access Monitoring) qui est utilisé à cette fin.
La définition d'opérateur est une fonctionnalité offerte par certains langages de programmation qui permet d'utiliser des opérateurs (comme +, = ou ==) comme des fonctions ou des méthodes en les définissant pour de nouveaux types de données. Les opérateurs ne sont pas nécessairement des symboles. Parfois, la définition de nouveaux opérateurs est autorisée. Il s'agit généralement de sucre syntaxique, et peut facilement être émulé par des appels de fonction ou de méthode : avec définition d'opérateurs : a + b * c ; sans définition d'opérateurs : somme (a, produit (b, c)). Lorsque les opérateurs sont des fonctions, on parle en général de surcharge d'opérateur, car l'implémentation est choisie en fonction du type des opérandes (on parle également de polymorphisme ad hoc). C'est notamment le cas en C++. Tous les langages permettant la définition d'opérateur ne permettent pas la surcharge. Python, par exemple, ne supporte pas la surcharge, mais permet de définir des opérateurs via des méthodes spéciales. Dans le cas où les opérateurs peuvent être appelés implicitement, ils deviennent plus utiles qu'esthétiques. C'est le cas avec l'opérateur to_s de Ruby, qui retourne une représentation chaîne d'un objet et avec les opérateurs de PostgreSQL, sur lesquels peuvent être définies des transformations mathématiques. PostgreSQL peut aussi employer de nombreuses optimisations sur les expressions qui utilisent ces opérateurs.
La dégradation logicielle, nommée également érosion logicielle, est le déséquilibre entre l'architecture logicielle et son implémentation. Le vieillissement logiciel est également utilisé comme terme pour faire allusion aux défaillances rencontrées dans un logiciel au fil du temps. Il semble impossible d'empêcher ce dit vieillissement mais il existe des moyens pour le ralentir, d'où l'intérêt des architectures logicielles. L'architecture logicielle permet de décrire comment doit être conçu le logiciel pour répondre aux spécifications de celui-ci. L'implémentation logicielle doit correspondre au modèle d'architecture produit lors de la phase de conception. Dans la pratique, il n'est pas toujours évident de respecter cette règle. Les origines des décalages sont multiples, les principaux sont : l'évolution logicielle, les erreurs d'implémentations et les contradictions dans l'architecture envisagée qui ne pouvaient être prévues avant le développement. Il est possible de faire face à cette problématique en appliquant des concepts du génie logiciel.
En informatique, un dépôt ou référentiel (de l'anglais repository) est un stockage centralisé et organisé de données. Ce peut être une ou plusieurs bases de données où les fichiers sont localisés en vue de leur distribution sur le réseau ou bien un endroit directement accessible aux utilisateurs. En programmation informatique, le concept de dépôt s'applique aux logiciels de gestion de versions.
Le désassemblage est l'action inverse de l'assemblage. Cette opération va, grâce à un désassembleur, essayer de convertir un fichier binaire en code assembleur. Mais cette tâche n'est pas évidente. En effet, le désassembleur a parfois du mal à distinguer le code des données dans un fichier binaire. Le désassemblage peut aider par exemple à comprendre le fonctionnement de tel ou tel programme informatique.  Portail de la programmation informatique
Un désassembleur est un programme informatique qui traduit du langage machine (un fichier exécutable) en langage assembleur (aussi désigné sous le nom de langage « bas niveau »). Cette opération, le désassemblage, est l'inverse de celle effectuée par un programme assembleur, l'assemblage. Il est possible de désassembler le code machine généré par un compilateur de langage haut-niveau (C/C++, Delphi, etc.), mais on n'obtiendra pas le code source du langage d'origine, juste la correspondance en assembleur du code machine. Pour réaliser cette opération, il faudrait utiliser un décompilateur. La sortie d'un désassembleur est plus souvent destinée à lire le code de manière plus simple, plus humaine que de lire des suites de bits du code machine (même en hexadécimal), plutôt que de servir d'entrée à un programme assembleur.[réf. nécessaire] En effet, les symboles (nom des variables, étiquettes, noms des procédures) ne sont pas restitués ni même les commentaires de l'auteur (utiles pour comprendre, si l'auteur en avait écrit).
Discipulus est un langage de programmation informatique servant à l’exploitation de bases de données relationnelles. Il a été introduit par le groupe Μῆτις de l’Université de Sherbrooke et a été présenté dans le mémoire de maîtrise de Zouhir Abouaddaoui en juillet 2012. Il a été conçu de sorte à : Permettre la représentation d’éléments complexes de la vie réelle par l’intégration de la programmation orientée objet au modèle relationnel Assurer l’évolutivité avec les mécanismes d’héritage et d’héritage multiple Garantir la fiabilité par l’incorporation des principes de programmation par contrat et en corrigeant certaines lacunes qu’on retrouve dans les systèmes de gestion de base de données (abr. SGBD) relationnels existants. Il possède un système de typage statique et fort inspiré du langage de programmation Eiffel. Il inclut également un langage algorithmique et un langage relationnel qui sont tous deux basés sur ce système de typage.
En informatique, un drapeau ou fanion (en anglais : flag) est un ensemble de bits fournissant une information contextuelle. Une technique couramment utilisée consiste à interroger un groupe de drapeaux en une seule instruction (du type ET, OU exclusif, etc.) par recours à un masque binaire.
En informatique DWIM est un sigle pour Do What I Mean. Cela signifie « fais ce que je veux dire ». Il est utilisé en informatique pour dénoter la qualité des langages qui permettent d'éviter de spécifier « en long, en large et en travers » l'action à effectuer. En d'autres termes : on peut spécifier le « quoi » sans spécifier le « comment ». Mais souvent, comme dans l'exemple développé ci-dessous, c'est le « quoi » lui-même qui est omis et qui doit donc être inféré. Le DWIM est aussi la qualité des programmes ou parties de programmes qui exploitent le DWIM du langage utilisé. Comme en anglais, où le rôle grammatical des noms est assez souple, DWIM peut aussi utilisé comme verbe : « Les langages de bas niveau ne DWIMent pas ». En effet, lorsqu'on écrit un programme en assembleur, on doit spécifier des détails de la mise en œuvre comme les registres par lesquels transitent les valeurs calculées. Pire, ces détails enferment le programme dans l'architecture matérielle choisie et empêchent sa portabilité. La conséquence souhaitable du DWIM est d'obtenir des programmes concis mais lisibles. Le DWIM est la qualité de certains langages puissants mais complexes. Cette qualité est très subjective et son appréciation dépend beaucoup des goûts du programmeur. Typiquement un langage qui utilise des informations contextuelles DWIMe. La complexité du langage augmente aussi la charge cognitive du programmeur qui doit bien en connaître les mécanismes pour comprendre un programme en apparence ambigu. Les langages de haut niveau n'ont pas nécessairement la qualité DWIM. L'importance du DWIM et les contextes de son usage raisonnable sont controversés. Le DWIM de l'un est souvent l'assombrissement de l'autre.
Lors du développement d'un programme informatique, l’édition des liens est un processus qui permet de créer des fichiers exécutables ou des bibliothèques dynamiques ou statiques, à partir de fichiers objets et de bibliothèques statiques. La compilation d'un fichier source vers un fichier objet laisse l'identification de certains symboles à plus tard. Avant de pouvoir exécuter ces fichiers objets, il faut résoudre les symboles et les lier à des bibliothèques ou d'autres fichiers objets. Le lien avec une bibliothèque peut être : statique : le fichier objet et la bibliothèque sont liés dans le même fichier exécutable ; dynamique : le fichier objet est lié avec la bibliothèque, mais pas dans le même fichier exécutable ; les liens dynamiques sont établis pendant l'exécution du fichier exécutable, c'est-à-dire du programme exécutable. Les assembleurs et les compilateurs sont généralement livrés avec un lieur (linker) ou éditeur de liens, un programme chargé de faire l'édition des liens. Certains langages modernes (Java, langages .NET) n'ont pas besoin de cette étape d'édition des liens et résolvent les adresses dynamiquement (au prix d'un temps de calcul plus important).
En informatique, une fonction est dite à effet de bord (traduction mot à mot de l'anglais side effect, dont le sens est plus proche d'effet secondaire) si elle modifie un état autre que sa valeur de retour. Par exemple, une fonction pourrait modifier une variable statique ou globale, modifier un ou plusieurs de ses arguments, écrire des données vers un écran ou un fichier ou lire des données provenant d'autres fonctions à effet de bord. Souvent, ces effets compliquent la lisibilité du comportement des programmes et/ou nuisent à la réutilisabilité des fonctions et procédures. Un langage comme Haskell les exile délibérément vers des composants nommés les monades. Plus communément un effet de bord apparaît la plupart du temps lorsqu'une modification d'un programme cohérent (valeurs et états pris conformes aux spécifications) aboutit à des valeurs ou des comportements non prévus, à cause de la non prise en compte de la portée, de l'ensemble de définition de variables, ou du contrat des fonctions. Note : un « semi » effet de bord consiste simplement pour une fonction à utiliser pour ses calculs, outre ses arguments qu'elle ne modifie pas (s'ils ne sont pas passés par référence), des variables globales, mais utilisées cependant juste en lecture. Ce n'est pas à proprement parler un effet de bord car aucune variable globale n'est modifiée, et l'utilisation de variables globales non passées en argument permet la réduction lors de l'appel de fonctions des arguments aux seules variables changeantes entre les divers appels, donc souvent[réf. nécessaire] à une réduction importante du nombre des arguments. Cependant, par l'usage de bonnes sous-structures de structures de données globales complexes passées en arguments par référence, ou en faisant de ces fonctions des méthodes de classes, les variables globales non passées en argument devenant les attributs de classe, ce type de « semi » effet de bord peut être évité, restaurant par là l'idéal du comportement purement fonctionnel, qui maximise la réutilisabilité des fonctions.
Le terme espace de noms (namespace) désigne en informatique un lieu abstrait conçu pour accueillir des ensembles de termes appartenant à un même répertoire, comme dans l'exemple suivant où les espaces de noms sont nommés « Jean-Paul » et « Jean-Pierre » :
En programmation, les espaces de noms aident à la construction de programmes modulaires. Par exemple, le symbole de fonction sin pourrait renvoyer au calcul d'une sinusoïde dans un espace de noms regroupant des fonctions mathématiques et au péché (le mot anglais sin en est une traduction) dans un autre espace de nom traitant de problèmes religieux. Cela préserve des inévitables conflits entre symboles homonymes.
Eval est une fonction utilisée en programmation. Elle est présente dans de nombreux langages interprétés et permet d'exécuter une commande à partir d'une chaîne de caractères (ou String) générée par le programme lui-même en cours d'exécution.
En informatique, execute in place (XIP) est une méthode d'exécution d'un programme directement depuis une mémoire ROM plutôt que de copier le programme sur une RAM. Les mémoires flash de type NOR permettent ce type de fonctionnement.
En informatique, l'exécution est le processus par lequel un ordinateur ou une machine virtuelle met en œuvre les instructions d'un programme. Les instructions du programme entraînent des séquences d'actions élémentaires sur la machine d'exécution. Les effets qu'entraînent ces actions sont conformes à la sémantique des instructions du programme. Un programme en cours d'exécution est appelé un processus. L’exécution symbolique permet d'explorer les chemins d'exécution possibles d'un programme informatique à partir des symboles contenus dans son code source. Elle diffère de l’exécution concrète qui ne suit qu'un seul des chemins possibles. Alors que l’exécution concrète met directement à jour les variables en mémoire, l’exécution symbolique enregistre les formules logiques liant les variables entre elles. Le but est d'analyser statiquement un programme pour trouver des bugs ou prouver certaines propriétés du programme. Il s'agit d'une interprétation abstraite d'un programme.
EXEL est un langage de description algorithmique conçu en 1973 par l'informaticien français Jacques Arsac qui a baptisé ce langage à partir du mot excellent. Un des buts de ce langage est de décrire un algorithme de façon indépendante du langage dans lequel il sera ensuite implémenté qui, au demeurant, peut être EXEL lui-même... Le langage EXEL permet de préciser aussi bien les caractéristiques des données (matrice quelconque, matrice triangularisée, etc.) que les algorithmes eux-mêmes (par exemple produit matriciel). Un algorithme d'inversion de matrice que l'on compose avec les caractéristiques d'une matrice triangulaire donne automatiquement par simplifications algébriques un algorithme simplifié d'inversion optimisé pour les matrices triangulaires, par exemple. L'auteur définit le schéma des étapes de la méthode informatique comme suit: Situation concrète → Problème → Algorithme → Programme → Résultats Au début, il y a un état des choses qui génère un problème: c'est ce que l'auteur désigne par une situation concrète décrite en langue naturelle. Par abstraction, elle lui est associée une représentation formelle. Celle-ci définit le problème.Pour résoudre ce problème, il faudra dans la mesure du possible (problèmes NP-complet...) construire un algorithme. La traduction de cet algorithme dans un langage de programmation constitue le Programme qui sera exécuté sur ordinateur et fournira des résultats. L'interprétation des résultats est laissée aux soins de l'Homme. L'idée, implicite de la conception du langage EXEL, c'est, dans la mesure du possible, de construire un algorithme performant, de façon systématique: toutes les transformations et simplifications seront pris en charge par la machine, dans ce cas l'ordinateur, permettant d'aboutir aux résultats... EXEL est à l'algorithmique ce que la géométrie analytique est à la géométrie: ou bien d'emblée une solution géométrique est trouvée, sinon en peut toujours rapporter les hypothèses du problème à la géométrie analytique (i.e. coordonnées cartésiennes...). Avec le langage EXEL, ou bien d'emblée un algorithme performant est trouvé, ou bien le langage EXEL offre des possibilités pour la manipulation formelle des programmes qui permettent de construire un algorithme performant...Il n' y à pas de méthode générale pour construire un algorithme pas plus qu'il y a une méthode générale pour résoudre les vieux problèmes de construction géométrique, ou pour trouver une démonstration d'un théorème. C'est pourquoi la présentation du langage EXEL se fera à l'aide d'exemples. L'écriture fréquente de longs programmes avec des langages tels que Fortran, Algol ,Pascal... est fastidieuse. Pour permettre une écriture condensée des programmes, à l'instar des mathématiques où il existe des abréviations comme                         ∀                 {\displaystyle \forall }   , pour désigner tout élément,                         ∃                 {\displaystyle \exists }   , pour il existe... les instructions du langage EXEL ont été définies comme suit :
En informatique, un exemple minimal fonctionnel (en abrégé MWE pour Minimal Working example), est un ensemble de code source et d'autres fichiers de données qui permettent de reproduire un bug ou de démontrer un problème. Un exemple minimal fonctionnel doit être : le plus petit et le plus simple possible ; suffisant pour démontrer le problème ; exempt de complexité inutile ou de dépendances qui vont rendre la résolution plus difficile.
Exhibit est un framework de publication de données qui permet d'ajouter aux pages Web les fonctionnalités de tri, filtrage, visualisations riches. Il permet notamment la navigation à facettes dans les données. Cette technologie fonctionne en utilisant du code HTML, du CSS et du JavaScript. Le visiteur n'a aucun téléchargement à effectuer lorsqu'il visite une page utilisant Exhibit et son créateur profite gratuitement de cet outil, car cette technologie est Open source. Exhibit est développé par SIMILE Project, initiative provenant du Massachusetts Institute of Technology libraries et du Massachusetts Institute of Technology Computer Science and Artificial Intelligence Laboratory (CSAIL).
Une expression booléenne est une expression qui donne une valeur booléenne (vrai ou faux). Par exemple, la valeur pour 5 > 3 est vrai et la valeur pour 5 < 4 est faux. Les expressions booléennes sont utilisées pour le contrôle de flot et pour la sélection parmi un ensemble de valeurs selon un prédicat.
En informatique, une expression régulière ou expression normale ou expression rationnelle ou motif, est une chaîne de caractères, qui décrit, selon une syntaxe précise, un ensemble de chaînes de caractères possibles. Les expressions régulières sont également appelées regex. Les expressions rationnelles sont issues des théories mathématiques des langages formels des années 1940. Leur capacité à décrire avec concision des ensembles réguliers explique qu’elles se retrouvent dans plusieurs domaines scientifiques dans les années d’après-guerre et justifie leur adoption en informatique. Les expressions régulières sont aujourd’hui utilisées pour programmer des logiciels avec des fonctionnalités de lecture, de contrôle, de modification, et d'analyse de textes ainsi que dans la manipulation des langues formelles que sont les langages informatiques. Ces expressions régulières ont la qualité de pouvoir être décrites par des formules ou motifs, (en anglais patterns) bien plus simples que les autres moyens.
En programmation informatique, la factorisation de code consiste à rassembler les suites d'instructions identiques dispersées dans un programme en une fonction, pour améliorer la lisibilité du code et en faciliter la correction et les modifications ultérieures.
Un feature freeze est un point temporel dans un cycle de développement logiciel où l'on décide d'interdire tout ajout de nouvelles fonctionnalités. Ce choix délibéré a généralement pour but de permettre de finaliser un état stable d'un programme pour aboutir à une livraison.  Portail de la programmation informatique
Dans un langage de programmation, une fermeture ou clôture (en anglais : closure) est une fonction accompagnée de son environnement lexical. L'environnement lexical d'une fonction est l'ensemble des variables non locales qu'elle a capturé, soit par valeur (c'est-à-dire par copie des valeurs des variables), soit par référence (c'est-à-dire par copie des adresses mémoires des variables). Une fermeture est donc créée, entre autres, lorsqu'une fonction est définie dans le corps d'une autre fonction et utilise des paramètres ou des variables locales de cette dernière. Une fermeture peut être passée en argument d'une fonction dans l'environnement où elle a été créée (passée vers le bas) ou renvoyée comme valeur de retour (passée vers le haut). Dans ce cas, le problème posé alors par la fermeture est qu'elle fait référence à des données qui auraient typiquement été allouées sur la pile d'exécution et libérées à la sortie de l'environnement. Hors optimisations par le compilateur, le problème est généralement résolu par une allocation sur le tas de l'environnement.
En informatique (développement), un fichier objet est un fichier intermédiaire intervenant dans le processus de compilation. Ce fichier contient du code machine, ainsi que d'autres informations : nécessaires à l'édition de liens (symboles) ; nécessaires lors de la phase de déboguage. La plupart des compilateurs donnent à ces fichiers le suffixe '.o' (comme GCC) et d'autres '.obj' (comme le compilateur de Microsoft vendu dans le produit Visual C++).   Portail de la programmation informatique
En informatique, le flot de contrôle (ou flot d'exécution) est l'ordre dans lequel les instructions d'un programme impératif sont exécutées. L'emphase sur la manipulation explicite par le programmeur du flot de contrôle, grâce aux structures de contrôle, est la principale caractéristique des langages de programmation impératifs vis-à-vis des autres paradigmes de programmation. L'ensemble des flots de contrôle possibles d'un programme forme son graphe de flot de contrôle.  Portail de la programmation informatique
Flowgorithm est un outil de création et modification graphiques de programmes informatiques sous forme d'algorigramme. Il permet ensuite aux programmeurs d'exécuter ces programmes tout en pouvant en suivre graphiquement le déroulement . Cette approche vise à mettre en avant les algorithmes plutôt que la syntaxe d'un langage de programmation spécifique. L'algorigramme peut être traduit en différents langages de programmation. Flowgorithm est un langage et un environnement de programmation conçu pour être uniquement un instrument d'apprentissage.
En informatique, un flux (souvent remplacé par le mot anglais stream) est une suite infinie d'éléments gérés de façon temporelle. Un flux présente ainsi une analogie avec une bande transporteuse où les éléments sont traités séquentiellement, plutôt que globalement. Les flux ne sont pas traités comme les lots de données - en effet les fonctions usuelles n'y fonctionnent pas de façon globale - parce qu'ils sont des données potentiellement illimitées et non pas des données classiques (par définition finies). Les fonctions qui opèrent sur des flux, produisant des autres flux, fonctionnent plutôt comme des filtres, et peuvent être comparées à des chaînes de traitement (ou pipelines), car elles agissent de manière analogue à la composition des fonctions. Comme les filtres, ces fonctions sur les flux calculent sur un constituant à la fois ou parfois peuvent produire un élément du flux de sortie à partir de plusieurs constituants de l'entrée, à la manière d'un moyenneur glissant .
La fonction 91 de McCarthy est une fonction récursive définie par McCarthy dans son étude de propriétés de programmes récursifs, et notamment de leur vérification formelle. Avec la fonction de Takeuchi, elle est une exemple amplement repris dans les manuels de programmation.
En informatique, une fonction de rappel (callback en anglais) ou fonction de post-traitement est une fonction qui est passée en argument à une autre fonction. Cette dernière peut alors faire usage de cette fonction de rappel comme de n'importe quelle autre fonction, alors qu'elle ne la connaît pas par avance.
Une fonction imbriquée ou fonction interne est une fonction dont la définition est encapsulée dans une autre fonction. Elle ne peut être appelée que par la fonction englobante ou par des fonctions imbriquées directement ou non dans la même fonction englobante. En d'autres termes, la portée de la fonction imbriquée est limitée par la fonction englobante; elle offre un contrôle très strict de leur visibilité (scope) par le reste du programme.
En informatique, une fonction primitive peut désigner une fonction de base fournie par une couche logicielle, juste au-dessus de l'architecture matérielle d'un ordinateur. Les fonctions primitives sont souvent plus rapides et efficaces que leurs versions équivalentes programmées à haut niveau, car elles sont optimisées pour le matériel piloté, en gérant les entrées-sorties.
En programmation informatique, une fonction variadique est une fonction d'arité indéfinie, c'est-à-dire qui accepte un nombre variable de paramètres. De nombreuses opérations mathématiques et logiques peuvent se représenter sous forme de fonctions variadiques. Par exemple, l'addition de nombres ou la concaténation de chaînes de caractères peuvent s'appliquer à un nombre arbitraire d'opérandes.
En programmation informatique, une fonction wrapper (de l'anglais « wrapper function ») est un programme dont la fonction principale est d'appeler une autre fonction.
FTBFS, sigle pour Fails To Build From Source (« Erreur de construction à partir des [fichiers] sources ») est un sigle appartenant au jargon des informaticiens. Il n'est employé que dans le domaine de la programmation informatique. Il décrit la situation où les sources d'un programme ne peuvent être compilées sur un système.
En informatique, un générateur est une routine non transparente référentiellement, généralement sans argument. Comme son nom l'indique, elle sert à créer de nouveaux objets. Parmi les générateurs les plus classiques, on trouve les générateurs de nombres aléatoires. Certains générateurs parcourent virtuellement une liste infinie, définie algorithmiquement. De tels générateurs sont appelés compréhensions de listes. Une fonction renvoyant tour à tour les éléments de la suite de Fibonacci ou tous les nombres premiers serait un tel générateur.
En programmation, la généricité (ou programmation générique), consiste à définir des algorithmes identiques opérant sur des données de types différents. On définit de cette façon des procédures ou des types entiers génériques. On pourrait ainsi programmer une pile, ou une procédure qui prend l'élément supérieur de la pile, indépendamment du type de données contenues. C'est donc une forme de polymorphisme, le polymorphisme de type dit aussi paramétrage de type : en effet, le type de donnée général (abstrait) apparaît comme un paramètre des algorithmes définis, avec la particularité que ce paramètre-là est un type. C'est un concept important pour un langage de haut niveau car il permet d'écrire des algorithmes généraux opérant sur toute une série de types : la généricité augmente donc le niveau d'abstraction des programmes écrits dans un langage qui possède cette fonctionnalité. Divers mécanismes ont été conçus pour permettre la programmation générique.
Gerrit est une application Web libre et gratuite de revue de code pour le travail en équipe. Chacun peut y lire, approuver ou rejeter les modifications d'un code source via un navigateur web. Il s'utilise avec Git qui s'occupe de poster ces changements de code.
Un gestionnaire de dépôts d'objets binaires (binary repository manager en anglais) est un outil logiciel conçu pour optimiser le téléchargement et le stockage des objets binaires utilisés et produits dans le cadre du développement de logiciels (généralement des fichiers). Il centralise la gestion de tous les artéfacts générés et utilisés par les logiciels de l'organisation (éditeur logiciel). Il permet de répondre à la problématique de complexité découlant de la diversité des types d'objets binaires, de leur position dans l'ensemble du flux de travail ainsi que les dépendances entre eux. Un dépôt d'objets binaires est un dépôt de logiciels pour les paquets, artefacts et leurs métadonnées. Il peut être utilisé pour stocker les fichiers binaires produits par l'organisation elle-même (releases et builds intermédiaires des produits) ou pour stocker les binaires tiers (dépendances) qui doivent être traités différemment pour des raisons techniques et juridiques.
Le Google Code Jam est un concours annuel international de programmation informatique parrainé et administré par Google. La compétition a vu le jour en 2003 et était originellement utilisée par Google pour repérer des candidats prometteurs pour ses groupes de développement technique. La compétition consiste en un ensemble de problèmes qui mesure l’habilité des participants à concevoir un algorithme pour résoudre un problème, puis programmer et tester cet algorithme en un temps prédéfini. Les compétiteurs peuvent utiliser le langage de programmation et l’environnement de développement de leur choix. Les langages les plus utilisés sont C++ et Java.
En programmation informatique, une grande boule de boue est un terme utilisé pour décrire un système ou logiciel informatique n'ayant pas d'architecture évidente.
Le mot hackathon désigne un événement où un groupe de développeurs volontaires se réunissent pour faire de la programmation informatique collaborative, sur plusieurs jours. C'est un processus créatif fréquemment utilisé dans le domaine de l'innovation numérique.
Hello world (traduit littéralement en français par « bonjour le monde ») sont les mots traditionnellement écrits par un programme informatique simple dont le but est de faire la démonstration rapide d’un langage de programmation (par exemple à but pédagogique) ou de tester un compilateur. Certains des programmes imprimant ces mots sont étonnamment complexes, particulièrement dans un contexte d’interface graphique. D’autres sont très simples, particulièrement ceux qui utilisent un interpréteur de ligne de commande pour afficher le résultat. Dans plusieurs systèmes embarqués, le texte peut être envoyé sur une ou deux lignes d’un afficheur LCD (ou dans d’autres systèmes, une simple DEL peut se substituer à un hello world).
Un Here Document aussi appelé heredoc, document en ligne ou encore document « ici-même » est une manière de déclarer une chaîne de caractères dans les interpréteurs de commandes comme les shells Unix (sh, ksh, csh, bash, zsh) et dans les langages de programmation ou de script comme Perl, PHP, Python, Ruby et JCL. Il conserve tous les sauts de lignes et espaces dans le texte, indentation comprise. Certains langages permettent l'interprétation de variable dans la chaîne. La syntaxe la plus courante consiste en une paire de chevrons << suivie d'un délimiteur, suivi sur la ligne suivante par la première ligne du heredoc, clos à la fin par le délimiteur sur sa propre ligne. Si la paire de chevrons est immédiatement suivie d'un tiret <<- toutes les tabulations en début de ligne du heredoc seront éliminées lors de l'interprétation de la commande. Ceci permet d'indenter le contenu du heredoc par rapport au reste du script et assurer une meilleure lisibilité.
HEX (Intel) est un format de fichier pour prendre en charge de l'information binaire pour des applications comme programmer des microcontrôleurs, des EEPROM et d'autres composants programmables. C'est un des formats les plus anciens pour cette utilisation, étant utilisé depuis les années 1970. [citation nécessaire]
En programmation informatique, l'homoiconicité (de homo- : semblable, et icône : image) est une propriété de certains langages dans lesquels la principale représentation des programmes est aussi une structure de données d'un type primitif du langage. Les langages homoiconiques rassemblent la plupart des membres de la famille Lisp mais aussi APL, Prolog, REBOL, SNOBOL, XSLT, TRAC, Tcl, Io, Julia et Red. Cette propriété s'apparente à la réflexion dans les langages informatiques.
Un hook (littéralement « crochet » ou « hameçon ») permet à l'utilisateur d'un logiciel de personnaliser le fonctionnement de ce dernier, en lui faisant réaliser des actions supplémentaires à des moments déterminés. Le concepteur du logiciel prévoit des hooks au long du fonctionnement de son programme, qui sont des points d'entrée vers des listes d'actions. Par défaut, le hook est généralement vide et seules les fonctionnalités de base de l'application sont exécutées. Cependant, l'utilisateur peut « accrocher » des morceaux de programme à ces hooks pour personnaliser le logiciel. Techniquement, un hook peut se présenter sous la forme d'un fichier, généralement dans un langage de script, localisé à un endroit particulier. Le programme principal impose généralement les caractéristiques du hook : son type, sa localisation, voire son nom, et le moment auquel il sera exécuté. Exemples de hooks : Dans TortoiseSVN, il existe plusieurs hooks permettant l'exécution de batch. Il existe notamment un hook pre-commit et un hook post-commit. Il faut copier les .bat dans le répertoire dédié de Tortoise. Ils seront alors automatiquement exécutés par Tortoise. Dans ce cas, la localisation, le nom et le type de fichier est imposé. Cependant, il est possible, depuis le batch, d'appeler d'autres programmes. Dans UltraBackup, l'interface graphique permet de pointer sur des programmes qui seront lancés avant ou après la copie. Dans ce cas, l'utilisateur a une liberté totale sur le type de fichier à appeler, son nom et sa localisation. Dans Drupal, le système dans son ensemble repose sur un système de "hook". Le "hook_form_alter" permet par exemple de modifier un formulaire. La documentation officielle sur les "hooks" dans Drupal : http://api.drupal.org/api/drupal/includes!module.inc/group/hooks/7  Portail de l’informatique  Portail de la programmation informatique
En programmation informatique, un identificateur ou identifiant est un mot choisi par le programmeur et qui, tel une étiquette, désigne une donnée du programme : variable, constante, procédure, type, etc. Un identifiant et sa valeur forment une sorte de symbole, comparables à ceux des mathématiques, à la différence qu'en programmation courante la valeur peut changer au cours du temps. On peut distinguer les langages de programmation suivant les règles d'écriture des identifiants, et notamment les jeux de caractères autorisés : ASCII uniquement, jeux de caractères locaux, ou Unicode.
Idle est un terme le plus souvent utilisé en informatique venant de la langue anglaise et qui peut avoir plusieurs significations voisines, selon le contexte dans lequel il est utilisé. idle (anglais) ['aidl], inaction (d'une personne) inoccupé, oisif. To run idle (d'une machine) tourner à vide. To idle (over), tourner au ralenti. Lorsqu'une voiture est en marche sans conducteur, on dit qu'elle est sur idle. Dans le jargon des programmes informatiques, désigne un processus inactif. Dans le processus IMAP, IDLE (en) permet d'être informé immédiatement de l'arrivée d'un message. IDLE est également le nom d'un IDE pour le langage de programmation Python. "Idle no more" est un slogan repris par les peuples natifs pour inviter à une plus grande prise en compte de leurs besoins et de leur lien indéfectible avec la Terre-Mère  Portail de l’informatique  Portail de la programmation informatique
En informatique, l'incrémentation est l'opération qui consiste à ajouter 1 (et par extension une valeur entière fixée) à un compteur. L'opération inverse, la décrémentation, consiste à retirer 1 (ou une valeur entière) au compteur. Cette opération est très courante dans les programmes informatiques, notamment dans les boucles d'itération, si bien que la plupart[réf. souhaitée] des langages de programmation implémentent des opérateurs d'incrémentation et de décrémentation. Ainsi, dans un grand nombre de langages, la syntaxe suivante permet d'ajouter ou de retirer une unité à la variable x : x++ (resp. x--) en forme préfixée, qui équivaut à x = x + 1 (resp. x = x - 1); ++x (resp. --x) en forme suffixée , qui équivaut à x = x + 1 (resp. x = x - 1). Si la valeur de x, après l'évaluation de l'instruction x++ ou ++x est toujours le successeur de x, il y a néanmoins une subtilité concernant la valeur de retour de l'instruction, selon la forme utilisée ; L'une est la valeur avant incrémentation, l'autre est la valeur après incrémentation : y = x++ équivaut à y = x ; x = x + 1 y = ++x équivaut à x = 1 + x ; y = x D'autres opérateurs permettent d'incrémenter ou de décrémenter par un nombre différent de 1. Par exemple, x += 3.5 ajoute 3.5 à la valeur de x.
Un langage de programmation utilise l'indentation comme syntaxe si une zone de code indentée délimite un bloc. Un bloc est une entité programmatique qui délimite une portée. Peter J. Landin a inventé le concept de l'indentation comme syntaxe et créé le terme anglais offside rule qui le désigne en 1966.
En informatique, l'initialisation est une opérations préliminaires à la mise en fonction d'un ordinateur ou d'un programme. L'initialisation d'un ordinateur est souvent appelée l'amorce de l'ordinateur.
L'injection de dépendances (dependency injection en anglais) est un mécanisme qui permet d'implémenter le principe de l'inversion de contrôle. Il consiste à créer dynamiquement (injecter) les dépendances entre les différents objets en s'appuyant sur une description (fichier de configuration ou métadonnées) ou de manière programmatique. Ainsi les dépendances entre composants logiciels ne sont plus exprimées dans le code de manière statique mais déterminées dynamiquement à l'exécution.
En informatique, une instruction conditionnelle, (aussi appelé expression conditionnelle), est une fonction d'un langage de programmation, qui effectue différents calculs ou actions, en fonction de l'évaluation d'une condition booléenne, à savoir vraie ou fausse. Dans les langages de programmation impératives, le terme « instruction conditionnelle » est souvent utilisé, alors qu'en programmation fonctionnelle, le termes « expression conditionnelle » est préféré, parce que ces termes ont tous une signification distincte.
En programmation informatique, une instruction nulle (ou instruction vide) est une instruction particulière, qui dicte à l'ordinateur de n'effectuer aucune action.
En informatique, l'instrumentation du code source est une opération consistant à ajouter des instructions machine supplémentaires à un programme informatique sans nécessiter la modification du code source original. Elle est apparentée à l'instrumentation dans les sciences, d'où son nom.
Une interface de prestation de service ou SPI (Service Provider Interface) est une technique de programmation permettant la substitution de composants sans changer d'interface. Les interfaces de prestation de service sont souvent utilisées en programmation orientée objet sous Java pour des composants permettant la liaison aux systèmes de gestion de base de données, la gestion des documents XML, la gestion des différents formats d'images, etc.
En informatique, une interface de programmation applicative (souvent désignée par le terme API pour application programming interface) est un ensemble normalisé de classes, de méthodes ou de fonctions qui sert de façade par laquelle un logiciel offre des services à d'autres logiciels. Elle est offerte par une bibliothèque logicielle ou un service web, le plus souvent accompagnée d'une description qui spécifie comment des programmes consommateurs peuvent se servir des fonctionnalités du programme fournisseur. Dans l'industrie contemporaine du logiciel, les applications informatiques se servent de nombreuses interfaces de programmation, la programmation se fait en réutilisant des briques de fonctionnalités fournies par des logiciels tiers. Cette construction par assemblage nécessite pour le programmeur de connaître la manière d’interagir avec les autres logiciels, qui dépend de leur interface de programmation. Le programmeur n'a pas besoin de connaître les détails de la logique interne du logiciel tiers, et celle-ci n'est généralement pas documentée par le fournisseur. Des logiciels tels que les systèmes d'exploitation, les systèmes de gestion de base de données, les langages de programmation, ou les serveurs d'applications comportent une interface de programmation.
Le International Obfuscated C Code Contest (en français : « Concours international de code C obscur ») est un concours de programmation organisé chaque année depuis 1984 (à l'exception de 1997, 1999, 2002 et 2003). Il y a plusieurs entrées gagnantes chaque année, et chaque année rentre dans une catégorie du genre : « Plus grand abus du préprocesseur C » ou « Comportement le plus incohérent ».
En informatique, un interprète, ou interpréteur (voir infra), est un outil ayant pour tâche d'analyser, de traduire et d'exécuter les programmes écrits dans un langage informatique. On qualifie parfois, et abusivement, les langages dont les programmes sont généralement exécutés par un interpréteur de langages interprétés. Un interpréteur se distingue d’un compilateur par le fait que, pour exécuter un programme, les opérations d’analyse et de traductions sont réalisées à chaque exécution du programme (par un interpréteur) plutôt qu’une fois pour toutes (par un compilateur).
L’inversion de contrôle (inversion of control, IoC) est un patron d'architecture commun à tous les frameworks (ou cadre de développement et d'exécution). Il fonctionne selon le principe que le flot d'exécution d'un logiciel n'est plus sous le contrôle direct de l'application elle-même mais du framework ou de la couche logicielle sous-jacente. L’inversion de contrôle est un terme générique. Selon le problème, il existe différentes formes, ou représentation d'IoC, le plus connu étant l'injection de dépendances (dependency injection) qui est un patron de conception permettant, en programmation orientée objet, de découpler les dépendances entre objets.
ISWIM est un langage de programmation abstrait (ou plus précisément une famille de langages de programmation) conçu par Peter J. Landin et décrit dans un article célèbre intitulé Les 700 prochains langages de programmation et publié dans les Communications of the ACM en 1966. Son acronyme ISWIM signifie « If you See What I Mean » (« Si vous voyez ce que je veux dire »). ISWIM a fortement influencé la conception des langages de programmation qui l'ont suivi, en particulier dans le domaine de la programmation fonctionnelle, de SASL (en) à Haskell, en passant par ML et Miranda (en) et leurs successeurs, et dans le domaine de la programmation dataflow comme Lucid (en) et Lustre. En fait, dans son article, Landin définit les grands principes de ce que devaient être, en 1965, les langages de programmation du futur. À cette époque, ALGOL 60, Lisp et PL/1 venaient de naître. Fortran et Cobol étaient les seuls langages de programmation largement utilisés.
Le JavaScript discret (unobtrusive JavaScript) est une pratique dans la programmation informatique en JavaScript. Cette pratique n'est pas décrite par une norme. Cette pratique consiste en la séparation du code HTML et du JavaScript.
Dans l'argot d'informaticien, un kludge est une solution jugée maladroite, grossière, rudimentaire, et irrespectueuse des règles de l'art, typiquement créée dans le but de résoudre rapidement et temporairement un problème ardu. Telle solution est également qualifiée de hack, de bidouille, de bricolage ou de verrue. Il peut également s’agir d’un morceau de code correspondant aux mêmes critères de non-élégance, créé dans le but de corriger rapidement et efficacement un bug. On parle alors également de verrue. Sur les BBS, les kludges étaient des en-têtes de messages sur Echomail.
En informatique théorique, un L-système ou système de Lindenmayer est un système de réécriture ou grammaire formelle, inventé en 1968 par le biologiste hongrois Aristid Lindenmayer. Un L-système modélise le processus de développement et de prolifération de plantes ou de bactéries. C'est une forme de grammaire générative. Ces grammaires ont été mises en œuvre graphiquement par de nombreux auteurs, menant à de spectaculaires images. Une étude systématique d'une certaine formulation a été entreprise par Przemyslaw Prusinkiewicz (de) dans les années 1980. Une étude mathématique, débutée dès l'introduction des systèmes par Lindenmayer, a débouché sur une théorie élaborée dont une première synthèse a été faite, en 1980 déjà, par Rozenberg et Salomaa. Au départ, Lindenmayer conçoit sa formalisation comme un modèle de langages formels qui permet de décrire le développement d'organismes multicellulaires simples. À cette époque il travaille sur les levures, les champignons et des algues. Mais sous l'influence des théoriciens et des praticiens de l'informatique, ce système a conduit à des familles de langages formels et aussi à des méthodes pour générer graphiquement des plantes idéalisées très complexes. Formellement, un L-système est un ensemble de règles et de symboles qui modélisent un processus de croissance d'êtres vivants comme des plantes ou des cellules. Le concept central des L-systèmes est la notion de réécriture. La réécriture est une technique pour construire des objets complexes en remplaçant des parties d'un objet initial simple en utilisant des règles de réécriture. Dans l'interprétation biologique, les cellules sont modélisées à l'aide de symboles. À chaque génération, les cellules se divisent, ce qui est modélisé par l'opération consistant à remplacer un symbole par un ou plusieurs autres symboles consécutifs.
En programmation informatique, un langage de haut niveau est un langage de programmation orienté autour du problème à résoudre, qui permet d'écrire des programmes en utilisant des mots usuels des langues naturelles (très souvent de l'anglais) et des symboles mathématiques familiers. Un langage de haut niveau fait abstraction des caractéristiques techniques du matériel utilisé pour exécuter le programme, tels que les registres et les drapeaux du processeur,. Les langages de haut niveau sont plus proches des langues naturelles, ce qui facilite et vulgarise l'écriture des programmes. Ils sont généralement indépendants de la machine : le même programme pourra être utilisé tel quel sur plusieurs types d'ordinateurs — quoique les programmes puissent également être conçus pour un système d'exploitation en particulier. Les langages de haut niveau sont apparus dans la seconde moitié des années 50 (Fortran en 1954, Lisp et Algol en 1958, COBOL en 1959). Ils ont permis d'écrire des programmes d'une manière plus familière, proche de l'anglais, et qui ne dépend pas du processeur qui sera utilisé. En 2010, il existe plus de 200 langages de programmation de haut niveau.
Un langage de programmation de bas niveau ne fournit que peu d'abstraction par rapport au jeu d'instructions du processeur de la machine. Les langages de bas niveau sont à opposer aux langages de haut niveau, qui permettent de créer un programme sans tenir compte des caractéristiques particulières (registres, etc) de l'ordinateur censé exécuter le programme. Le langage machine et le langage d'assemblage sont les archétypes de langages de bas niveau, puis qu'ils permettent de manipuler explicitement des registres, des adresses mémoires, des instructions machines. Hors ces cas extrêmes, la distinction entre bas niveau et haut niveau n'est pas binaire. Les discussions stériles pour savoir si des langages comme C et C++, qui ont à la fois des aspects de bas niveau (possibilités d'accès à la mémoire en utilisant des adresses explicites, "bitfields" dans les structures) et de haut niveau, méritent ou pas le nom de langage de haut ou bas niveau ont toujours cours. Une citation attribuée à Alan Perlis dit : « A programming language is low level when its programs require attention to the irrelevant. » ; ce qui pourrait être traduit par : « Un langage de programmation est bas niveau lorsqu'il nécessite de faire attention aux choses qui ne sont pas pertinentes ». Néanmoins, cette citation doit être nuancée : dans cette citation, ce qui est considéré non-pertinent est la gestion du matériel et ses contraintes. Dans certains cas, la gestion du matériel est un impératif, et est loin d'être non pertinente. Il est donc souvent nécessaire d'utiliser un langage de bas niveau, un langage de haut niveau ne pouvant permettre de coder le programme désiré.
Cet article traite d'une classe des langages de programmation. Pour la méthode consistant en la réduction du temps d'exécution d'un algorithme, voir programmation dynamique On utilise le terme langage de programmation dynamique en informatique pour décrire une classe de langages de haut niveau qui exécutent au moment de l'exécution des actions que d'autres langages ne peuvent exécuter que durant la compilation. Ces actions peuvent inclure des extensions du programme, en ajoutant du code nouveau, en étendant des structures de données et en modifiant le système de types, cela pendant l'exécution du programme. Ces comportements peuvent être émulés dans pratiquement tous les langages de complexité suffisante, mais les langages dynamiques ne comportent pas de barrière, tel que le typage statique, empêchant d'obtenir directement ces comportements. Les concepts de langages dynamiques et de typage dynamique ne sont pas des concepts identiques [pas clair], et un langage dynamique n'est pas nécessairement typé dynamiquement bien que la plupart des langages dynamiques le soient.
La ligne de code, ou ligne de code source (SLOC en anglais) est une métrique logicielle servant à mesurer la taille d’un programme informatique en dénombrant le nombre de lignes de son code source. Les lignes de code sont habituellement employées pour quantifier l'effort qui sera exigé pour développer un programme informatique, ainsi que pour estimer la valeur d'un logiciel produit.
La lisibilité est une caractéristique d'un programme informatique le rendant plus lisible et compréhensible au niveau de sa logique. Par exemple, un programme sans effets de bord et sans commentaire superflu est beaucoup plus lisible[réf. nécessaire] qu'un programme trop commenté, comportant de nombreux sauts de branchements, ou manipulant des structures de données mutables.  Portail de la programmation informatique
Voici une liste non-exhaustive de codes pour effectuer le programme Hello world.
En programmation informatique, la syntaxe de certains langages de programmation permet de définir des listes en compréhension, c'est-à-dire des listes dont le contenu est défini par filtrage du contenu d'une autre liste selon un principe analogue à celui de la définition en compréhension de la théorie des ensembles. Cette construction syntaxique se distingue de la construction la plus courante dans les langages de programmation qui est de définir les listes par énumération de ses éléments. Cette construction syntaxique offre des avantages de lisibilité et de concision et se rapproche de la notation utilisée en mathématiques :                         S         =         {         x                    |                  x         ∈                    N                  ,                    x                        2                             >         3         }                 {\displaystyle S=\{x|x\in \mathbb {N} ,x^{2}>3\}}    En langage de programmation Haskell, la syntaxe est la suivante : S = [ x | x←[0..], x^2>3 ] La liste [0..] représente la liste des entiers naturels et x^2>3 représente la propriété caractéristique de la liste. Cela peut être lu comme suit : « S est la liste de tous les x où x est un élément de la liste des nombres naturels et x a son carré plus grand que 3. »
Le listener, en français écouteur ou encore auditeur, est un terme anglais utilisé de façon générale en informatique pour qualifier un élément logiciel qui est à l'écoute d'évènements afin d'effectuer des traitements ou des tâches. On distingue notamment : l'écoute sur des serveurs informatiques ; l'écoute en programmation événementielle
LiveCode est un outil informatique de prototypage et de programmation. C'est un descendant d'HyperCard, outil de développement rapide mythique sur Macintosh dans les années 1990.
En programmation, une macro-commande est une suite de commandes,,.
En programmation informatique, une macro-définition ou simplement macro est l'association d'un texte de remplacement à un identificateur, tel que l'identificateur est remplacé par le texte dans tout usage ultérieur. Le plus souvent, on permet également le passage de paramètres syntaxiques. L'usage d'une macro comme instruction est souvent appelée macro-instruction et l'opération de remplacement d'une macro-instruction par sa définition la macro-expansion. Les macros sont donc un moyen de faire de la métaprogrammation.
Une marque substitutive (ou placeholder en anglais « remplisseur ») est un terme, signe ou autre marque qui tient la place d'un contenu inconnu ou non identifié. Dans les disciplines impliquant des langages formels, particulièrement les mathématiques et l'informatique, les variables libres et les variables liées sont des marques substitutives dont les variables métasyntaxiques sont un exemple humoristique. Note : outre marque substitutive, la traduction en français du mot placeholder peut avoir plusieurs significations sensiblement différentes selon le contexte : emplacement réservé ; terme générique ; paramètre fictif ; substitut ; remplacement ; structure générique ; espace vide (destiné à être utilisé ultérieurement).
Un marqueur (en anglais : tag) est un mot-clé remarquable dans un fichier texte, que le rédacteur ou le lecteur est susceptible de vouloir rechercher. La caractéristique lui conférant cette qualité dépend de la nature du texte qui le contient.
Le masquage est une opération de logique utilisée en informatique et en électronique pour sélectionner dans un groupe de bits un sous-ensemble de bits à conserver, ou au contraire à écraser. L'opération de masquage est réalisée à l'aide d'opérations bit à bit pour agir sur la donnée initiale sans avoir à traiter indépendamment chacun de ses bits.
En programmation, une métaclasse est une classe dont les instances sont des classes. Autrement dit, une métaclasse est la classe d'une classe.
Un métaobjet est la réification des entités d'un objet, comme son interface, sa classe, ses méthodes, etc.
La métaprogrammation, nommée par analogie avec les métadonnées et les métaclasses[réf. souhaitée], désigne l'écriture de programmes qui manipulent des données décrivant elles-mêmes des programmes. Dans le cas particulier où le programme manipule ses propres instructions pendant son exécution, on parle de programme auto-modifiant. Elle peut être employée pour générer du code interprété par un compilateur et donner un résultat constant, afin d'éviter un calcul manuel. Il permet également de réduire le temps d'exécution du programme si le résultat constant avait été classiquement calculé par le programme comme pour les résultats variables. Cette méthode ne s'applique pas uniquement aux calculs mais aussi au remplissage de données constantes telles que des tableaux ou des structures plus complexes. Cependant cette technique ne fonctionne que pour des valeurs constantes. En effet, si une donnée manipulée par le métaprogramme est une entrée du programme, par exemple une saisie de l’utilisateur, elle ne peut pas être connue avant l'exécution du programme. Il est donc impossible qu'un tel métaprogramme soit interprété par un compilateur. L'optimisation par métaprogrammation est alors totalement perdue. La métaprogrammation ne se limite pas seulement à l'écriture de données contenant un programme destiné au compilateur. Elle peut simplement être la manipulation d'un programme en fonction d'entrées variables. Par exemple, un programme peut, selon ses entrées, muter le code d'un métaprogramme. Ce métaprogramme peut alors être destiné à une exécution ultérieure ou une génération de code.
La métaprogrammation avec des patrons est une technique de programmation dans laquelle les patrons sont utilisés de sorte que le compilateur, lors de la compilation du code, exécute un programme. Ces programmes peuvent générer des constantes ou des structures de données. Cette technique est utilisée principalement dans le langage de programmation C++.
La microprogrammation est une technique de réalisation du séquenceur d'un processeur, utilisé dans la technologie CISC, dans laquelle le comportement du séquenceur est décrit par le contenu d'une mémoire. Ce contenu est appelé microprogramme ou microcode. Il peut être vu comme un programme destiné à une machine très simple appelée micro-séquenceur ; le processeur final est alors considéré comme une machine virtuelle simulée par le micro-séquenceur. Par opposition la technologie RISC ne contient pas de microcode. La réalisation microprogrammée des séquenceurs s'oppose à leur réalisation câblée. A priori plus onéreuse au départ, sa souplesse facilite l'évolution des modèles.
En programmation orientée objet, un mixin ou une classe mixin est une classe destinée à être composée par héritage multiple avec une autre classe pour lui apporter des fonctionnalités. C'est un cas de réutilisation d'implémentation. Chaque mixin représente un service qu'il est possible de greffer aux classes héritières.
Un module désigne originellement un fichier de code de programmation ou un fichier de bibliothèque statique ou dynamique. Pour reprendre l'image de la programmation objet, un module est une instance unique qui n'utilise pas d'héritage et ne contient aucun module fils. Chaque module peut exporter ou importer certains symboles comme des variables, des fonctions ou des classes. Les modules peuvent se regrouper en package (espace de noms) éventuellement hiérarchique. Dans l'histoire de la programmation, la programmation modulaire succède à la programmation procédurale et donnera naissance à la programmation orientée objet. Typiquement quand on parle de programmation modulaire on parle du C qui a utilisé, en premier lieu, ce principe pour scinder la compilation de projet. Dans ce cas, un module est identifié par un fichier de déclaration (en-têtes ou prototypes) et un fichier de description (définition ou code). On n'est plus obligé de recompiler l'ensemble d'un programme lorsque seuls quelques modules sont modifiés. Les fonctions et variables déclarées dans le fichier de déclaration sont publiques et peuvent être utilisées par n'importe quel autre module, il s'agit de l'interface du module. Les fonctions et variables déclarées dans le fichier de description sont privées et ne peuvent être utilisées que par ce module particulier. En Java, on n'utilise généralement pas le terme de module, mais certains compilateurs permettent de déclarer plusieurs classes successives (sans imbrication) dans un même fichier. On peut alors considérer un fichier de code Java comme un module contenant un ensemble de classes. Les langages Modula et Ada sont typiquement modulaires avec des instructions pour la gestion des modules. Le langage C/C++ est modulaire mais les instructions pour la gestion des modules se retrouvent dans un langage de prétraitement (dédié au préprocesseur). Le langage Objective Caml, qui repose essentiellement sur la programmation fonctionnelle et modulaire, propose un système de modules récursifs et paramétrables très puissant qui permet de fournir une grande généricité au code.  Portail de l’informatique
En mathématiques et en programmation informatique, on désigne par modulo l’opération de calcul du reste de la division euclidienne. Dans certains langages informatiques, le modulo est représenté par un signe pourcent « % ». On écrira a mod n pour représenter le reste de la division de a par n. Un modulo équivaut donc à la différence entre a et la multiplication de la valeur tronquée du quotient de a par n. En mathématiques, on note parfois ceci :                         x         =         ⌊         x         ⌋         +         {         x         }                 {\displaystyle x=\lfloor x\rfloor +\{x\}}   , donc                         a                    mod                        n                             =         a         −         (         ⌊         a                    /                  n         ⌋         ×         n         )                 {\displaystyle a{\bmod {n}}=a-(\lfloor a/n\rfloor \times n)}   . Par exemple,                         9                    mod                        4                             =         9         −         (         ⌊         9                    /                  4         ⌋         ∗         4         )         =         9         −         2         ×         4         =         1                 {\displaystyle 9{\bmod {4}}=9-(\lfloor 9/4\rfloor *4)=9-2\times 4=1}   . Le reste de la division de 9 par 4 est bien 1 puisque 2 × 4 + 1 = 9 ; voir l'article Partie entière et partie fractionnaire pour plus de précisions sur la notation. Cette fonction consiste en fait à déduire la partie décimale du quotient exact a/b, puis de le multiplier par b pour obtenir le quotient euclidien de a/b. La différence entre a et ce quotient euclidien est donc le reste de la division euclidienne a/b. Toutefois les langages de programmation divergent souvent de cette définition quand le diviseur n ou le dividende a est négatif, à cause de l'ambiguïté sur le mot "valeur tronquée" dans la définition informelle.
Un Monkey-Patch (aussi écrit Monkey Patch, MonkeyPatch) traduit par modification-singe consiste en la modification ou l'extension sans toucher au code source original. Cela est possible dans les langages de programmation dynamiques. Cela peut être appliqué à la modification des logiciels de base d'un système si ce système autorise la gestion de paquets. La notion se nomme aussi : Guerilla patch ; Extension des classes précédemment déclarées ; Réouverture des classes ; Hijacking.
Nana est une bibliothèque logicielle C++ multiplate-forme pour créer des interfaces graphiques. Elle utilise une Interface de programmation indépendante de la plateforme et fonctionne actuellement avec Windows et Linux (X11). Nana est un logiciel gratuit à code source ouvert utilisable selon les termes de la licence logicielle Boost.
Ne vous répétez pas (don’t repeat yourself en anglais, aussi désigné par l’acronyme DRY) est une philosophie en programmation informatique consistant à éviter la redondance de code au travers de l’ensemble d’une application afin de faciliter la maintenance, le test, le débogage et les évolutions de cette dernière. La philosophie DRY est explicitée par la phrase suivante, formulée par Andy Hunt et Dave Thomas dans leur livre The Pragmatic Programmer : « Dans un système, toute connaissance doit avoir une représentation unique, non-ambiguë, faisant autorité ». Les auteurs appliquent ce principe pour inclure les bases de données, les plans de tests, le système de construction logiciel et même la documentation logicielle. Lorsque le principe DRY est bien appliqué, la modification d'un élément d'un système ne change pas les autres éléments non liés logiquement. De plus, tous les éléments liés logiquement changent uniformément, de manière prévisible et restent synchronisés. En utilisant les méthodes et les sous-routines dans leur code, Thomas et Hunt se reposent sur les générateurs de code source, les systèmes de construction automatique, et les langages de scripts pour respecter le principe DRY à travers les diverses étapes de construction d'un logiciel.
En programmation informatique, le terme magic number (en français « nombre magique ») peut désigner : une constante numérique ou un ensemble de caractères utilisé pour désigner un format de fichier ou un protocole ; une constante numérique non nommée ou mal documentée ; un ensemble de valeurs ayant un sens particulier (par exemple, les GUID).
En programmation informatique, un notebook, aussi fréquemment appelé calepin électronique, voire simplement calepin,, est une interface de programmation interactive (en) permettant de combiner des sections en langage naturel et des sections en langage informatique. Les notebooks sont notamment utilisés en science des données pour explorer et analyser des données. Les notebooks peuvent ensuite être enregistrés sous forme de document. C'est un exemple de programmation lettrée. L'usage des notebooks en science permettant de faciliter le partage et la reproductibilité des résultats.
En informatique, un objet de première classe (ou valeur de première classe, ou entité de première classe) dans le contexte d'un langage de programmation particulier est une entité qui peut être utilisée sans restriction. Selon le langage, cela peut impliquer : être expressible comme une valeur anonyme littérale ; être affecté à des variables ou des structures de données ; avoir une identité intrinsèque ; être comparable pour l'égalité ou l'identité avec d'autres entités ; pouvoir être passé comme paramètre à une procédure ou une fonction ; pouvoir être retourné par une procédure ou une fonction ; pouvoir être constructible lors de l'exécution. Par exemple, en C, il n'est pas possible de créer de nouvelles fonctions à l'exécution alors que d'autres sortes d'entités peuvent l'être. Donc, en C, les fonctions ne sont pas des objets de première classe. Elles sont quelquefois appelées objets de seconde classe car elles peuvent être manipulées de la plupart des manières décrites ci-dessus. Elles peuvent l'être grâce à des pointeurs de fonctions. De manière similaire, les chaînes de caractères ne sont pas des entités de première classe en FORTRAN 66 car elles ne peuvent pas être assignées à des variables alors que les nombres peuvent l'être.
En informatique, l’Offset est un terme anglais qui désigne une adresse de manière relative. C'est une valeur entière représentant le déplacement en mémoire nécessaire, par rapport à une adresse de référence, pour atteindre une autre adresse. Autrement dit, l'offset est la distance séparant deux emplacements mémoire.
En programmation informatique, un opérateur est une fonction spéciale dont l'identificateur s'écrit généralement avec des caractères non autorisés pour l'identificateur des fonctions ordinaires. Il s'agit souvent des équivalents aux opérateurs mathématiques pour un langage de programmation. Les opérateurs peuvent effectuer des opérations arithmétiques, booléennes ou agir sur des chaînes de caractères. Contrairement aux fonctions, les opérateurs fournissent souvent les opérations primitives du langage. Leur nom est constitué de caractères symboles ou de ponctuation. La terminologie varie néanmoins de langage en langage. En informatique, l'utilisation du mot opérateur va au-delà des opérateurs arithmétiques communs. Le langage C, par exemple, supporte aussi des opérateurs comme &, ++ et sizeof. Des opérateurs comme sizeof sont alphanumériques plutôt que des caractères symboles ou de ponctuation ; on les appelle quelquefois opérateurs nommés. Les opérateurs en C sont des opérations primitives du langage que le compilateur peut transposer relativement directement en instructions du microprocesseur. Au contraire, dans certains langages de programmation, tels C++ ou Haskell, les opérateurs peuvent être définis ou surchargés afin de faciliter l'écriture et la relecture du code. Dans des langages comme Haskell et Prolog, les opérateurs sont purement du sucre syntaxique. On peut utiliser toute combinaison de symboles et de caractères de ponctuation comme opérateur, et on peut définir la précédence et l'associativité d'un tel opérateur. Haskell permet seulement de définir de nouveaux opérateurs binaires alors que Prolog permet de définir des opérateurs qui sont soit binaires soit unaires et qui peuvent être préfixés, infixés ou postfixés. En Haskell, on peut définir et appliquer un opérateur comme une fonction et réciproquement par l'utilisation de parenthèses et d'accents graves. Dans certains langages de programmation tels que PostScript, l'utilisation du mot opérateur a une signification plus précise. Un opérateur est un élément exécutable sur la pile. Comme les opérateurs sont toujours écrits de manière postfixée comme dans tout langage qui utilise la notation polonaise inverse, l'utilisation de parenthèses est facultative.  
En logique, une opération bit à bit est un calcul manipulant les données directement au niveau des bits, selon une arithmétique booléenne. Elles sont utiles dès qu'il s'agit de manipuler les données à bas niveau : codages, couches basses du réseau (par exemple TCP/IP), cryptographie, où elles permettent également les opérations sur les corps finis de caractéristique 2. Les opérations bit à bit courantes comprennent des opérations logiques bit par bit et des opérations de décalage des bits, vers la droite ou vers la gauche.
En programmation informatique, l'optimisation de code est la pratique consistant à améliorer l'efficacité du code informatique d'un programme ou d'une librairie logicielle. Ces améliorations permettent généralement au programme résultant de s'exécuter plus rapidement, de prendre moins de place en mémoire, de limiter sa consommation de ressources (par exemple les fichiers), ou de consommer moins d'énergie électrique.
En programmation informatique, l'optimisation dirigée par les profils (profile-guided optimization ou PGO en anglais) est la pratique qui consiste à optimiser la compilation statique d'un logiciel pour une utilisation type.
En mathématiques, la priorité des opérations ou ordre des opérations précise l'ordre dans lequel les calculs doivent être effectués dans une expression complexe. Les règles de priorité sont : les calculs contenus entre parenthèses (ou crochets) sont prioritaires sur les calculs situés en dehors de ces parenthèses. La barre d'une fraction ou d'une racine carrée joue le rôle d'une parenthèse ; les exposants sont prioritaires sur les multiplications, divisions, additions et soustractions ; les multiplications et divisions sont prioritaires sur les additions et soustractions; Lorsque les parenthèses sont effectuées, lire les multiplications et divisions de gauche à droite. Ensuite, même chose pour les additions et soustractions.
Un organigramme est une représentation d'une programmation sous forme d'un schéma. Il existe différents types d'organigrammes, désignés par des néologismes variés.
Un organigramme de programmation (parfois appelé algorigramme, logigramme ou plus rarement ordinogramme) est une représentation graphique normalisée de l'enchaînement des opérations et des décisions effectuées par un programme d'ordinateur.
Un paramètre est au sens large un élément d'information à prendre en compte pour prendre une décision ou pour effectuer un calcul. On parle aussi d'argument.
En programmation, le partage de structures ou simplement le partage (en anglais sharing) est un mécanisme qui évite de copier des objets dupliqués. En effet, la programmation conduit souvent à la duplication de sous-structures, mais cette duplication peut avoir un effet déplorable sur la croissance de l'espace mémoire si des précautions ne sont pas prises pour utiliser la mémoire avec parcimonie.
En informatique, les performances énoncent les indications chiffrées mesurant les possibilités maximales ou optimales d'un matériel, d'un logiciel, d'un système ou d'un procédé technique pour exécuter une tâche donnée. Selon le contexte, les performances incluent les mesures suivantes: Un faible temps de réponse pour effectuer une tâche donnée Un débit élevé (vitesse d'exécution d'une tâche) L'efficience : faible utilisation des ressources informatiques : processeur, mémoire, stockage, réseau, consommation électrique, etc. Une haute disponibilité du système ou de l'application Une bande passante élevée ou une faible latence Le passage à l'échelle (scalabilité) La capacité d'un canal de communication La performance du stockage informatique Les performances sont établies par la mesure lors d'une activité appelée benchmarking (en français, étalonnage), grâce à des tests de performance. Le test de performance consiste à définir le processus/système/matériel que l'on veut mesurer, à définir le cas d'utilisation à tester (incluant le mode d'interaction, les données et le scénario), et à exécuter le test tout en récoltant les mesures grâce à des capteurs ou des sondes. Les benchmarks et tests de performances sont souvent sujets à controverse. Ils sont souvent difficilement reproductibles, car très dépendants de détails non explicités : matériel utilisé, configuration de tous les éléments, charge de la machine, etc. L'optimisation est l'activité consistant à améliorer les performances de le cible. Il consiste d'abord à définir le test de performance à effectuer, quelles mesures doivent être optimisées, puis enfin à exécuter le test, mesurer, analyser, modifier la cible et recommencer pour mesurer s'il y a eu une amélioration. Ce processus est répété jusqu'à atteindre le niveau de performance souhaité.
En programmation informatique, une permutation consiste à intervertir les valeurs de deux variables. Il s'agit d'une opération courante, mais rarement intégrée aux langages de programmation et jeu d'instructions des processeurs. De nombreux algorithmes, en particulier des algorithmes de tri, utilisent des permutations.
En programmation, la gestion de la persistance des données (en anglais : persistence) et parfois des états d'un programme réfère au mécanisme responsable de la sauvegarde et de la restauration des données. Ces mécanismes font en sorte qu'un programme puisse se terminer sans que ses données et son état d'exécution ne soient perdus. Ces informations de reprise peuvent être enregistrées sur disque, éventuellement sur un serveur distant (un serveur de bases de données relationnelles, par exemple). Du fait de la différence de modèles entre les bases de données et les langages de programmation (notamment les langages objet) la notion de correspondance entre modèles (en anglais : mapping) est centrale.
Un phidget (parfois orthographié phydget) est un composant physique d'interface utilisateur. Le terme a été choisi par analogie avec les composants d'interface graphique, appelés widgets en anglais. De la même façon que les outils de construction d'interface graphiques permettent de composer des interfaces par simple assemblage graphique de widgets, l'objectif des phidgets est de rendre aussi facile la création d'interfaces tangibles. Les phidgets sont un jeu de blocs de construction pour la détection bas prix et le contrôle depuis un ordinateur individuel. Tous les phidgets se connectent sur port USB ; la complexité est masquée par une interface de programmation (API) unifiée. Les applications peuvent être développées sous les systèmes d'exploitation Mac OS X, Linux et Microsoft Windows. Les phidgets peuvent être programmés en utilisant une variété de logiciels allant de Java à Microsoft Office (via VBA). Les phidgets sont issus d'un projet de recherche de 2001 dirigé par Saul Greenberg du département de sciences informatique de l'Université de Calgary. La diffusion est prise en charge par la société Phidgets, Inc, créée à la suite de ce projet.
Un pointeur est en programmation une variable contenant une adresse mémoire.
En informatique, un pointeur faible est une référence qui n'exclut pas que la valeur référencée soit récupérée par le ramasse-miettes.  Portail de la programmation informatique
En informatique, un pointeur intelligent (en anglais smart pointer) est un type abstrait de données qui simule le comportement d'un pointeur en y adjoignant des fonctionnalités telles que la libération automatique de la mémoire allouée ou la vérification des bornes. La gestion manuelle de la mémoire dans les langages utilisant les pointeurs est une source courante de bugs, en particulier de fuites de mémoire ou de plantages. Les pointeurs intelligents diminuent ce risque en rendant automatique la libération des ressources : quand le dernier pointeur vers un objet est détruit, par exemple parce qu'il sort de portée, l'objet pointé est détruit simultanément. Cela peut être implémenté par exemple avec le décompte de références. L'utilisation d'un ramasse-miette permet de se passer de pointeurs intelligents.
La portabilité d'un programme informatique est sa capacité à pouvoir être adapté plus ou moins facilement en vue de fonctionner dans différents environnements d'exécution. Les différences peuvent porter sur l'environnement matériel (processeur) comme sur l'environnement logiciel (système d'exploitation). La différence d'environnement peut également porter sur une combinaison des deux éléments. C'est le cas par exemple dans les domaines de l'informatique embarquée, des super calculateurs et des machines virtuelles. L'action de modifier un programme pour qu'il puisse s'exécuter sur un autre environnement est le portage.
Le format PE (Portable Executable, exécutable portable) est le format des fichiers exécutables et des bibliothèques sur les systèmes d'exploitation Windows 32 bits et 64 bits : .exe (programmes), .ocx (OLE et ActiveX), .dll et .cpl (élément du panneau de configuration Windows). C'est un format dérivé du COFF.
En informatique, la portée (scope en anglais) d'un identifiant est l'étendue au sein de laquelle cet identifiant est lié. Cette portée peut être lexicale ou dynamique.
En informatique, la portée lexicale (dite également portée statique), est une méthode pour déterminer la portée d'une variable en fonction de sa position dans le code. Une variable est dite lexicale si sa portée est définie par le texte du programme. Par exemple, une variable nommée balance : (defun solde (balance)         ...         (lambda (foo bar)              (+ (* foo balance) bar))         ...)  Cette variable est définie dans la portée de la fonction solde, à partir de la liste des paramètres, et jusqu'à la fin de la définition de la fonction. La fermeture lexicale (dénotée par l'expression lambda) utilise la définition lexicale de balance de la fonction solde : c'est une capture de variable lexicale ; la variable ainsi capturée a la durée de vie de la fonction qui la capture, qui peut excéder la durée d'un appel à solde ; mais la portée de cette capture est strictement limitée par le texte de la fermeture. Elle est garantie ne pas interférer avec une autre variable de même nom qui serait définie dans un fragment de code du programme hors de la fonction solde, ou encore avec la variable nommée balance dans d'autres appels de la même fonction. Cela assure au programmeur que les variables privées ne sont pas accidentellement accédées ou modifiées par d'autres fonctions. Cela est considéré comme une amélioration considérable par rapport à la combinaison de portée globale et portée dynamique plus anciennes. La portée lexicale fut d'abord introduite par le langage Algol, puis utilisée par d'autres langages depuis. Certains langages ont introduit la portée lexicale dans leurs incarnations les plus modernes, comme Perl. D'autres offrent les variables dynamiques et lexicales, par choix (Common Lisp) ou par le fait d'une évolution progressive (Emacs Lisp, avec lexical-let).
Le principe de localité est un terme générique en informatique, qui correspond à une observation des programmes actuels et regroupe différents types de localités.
Le processus itératif est une séquence d'instructions destinée à être exécutée plusieurs fois et autant de fois qu'on peut en avoir besoin. C'est aussi une exécution de la séquence.
En informatique, le profilage de code (ou code profiling en anglais) consiste à analyser l'exécution d'un logiciel afin de connaitre son comportement à l'exécution.
La programmation en binôme (de l'anglais pair programming), parfois appelée programmation par paires ou binômage, est une méthode de travail dans laquelle deux développeurs travaillent ensemble sur un même poste de travail. La personne qui rédige le code est appelée conducteur (driver). La seconde personne, appelée observateur (observer), assiste le conducteur en décelant les imperfections, en vérifiant que le code implémente correctement le design et en suggérant des alternatives de développement. Les rôles s'échangent régulièrement pendant la séance de programmation. La programmation en binôme fait partie des bonnes pratiques de l'extreme programming.
En informatique, la programmation événementielle est un paradigme de programmation fondé sur les événements. Elle s'oppose à la programmation séquentielle. Le programme sera principalement défini par ses réactions aux différents événements qui peuvent se produire, c'est-à-dire des changements d'état de variable, par exemple l'incrémentation d'une liste, un mouvement de souris ou de clavier. La programmation événementielle peut également être définie comme une technique d'architecture logicielle où l'application a une boucle principale divisée en deux sections : la première section détecte les événements, la seconde les gère. Elle est particulièrement mise en œuvre dans le domaine des interfaces graphiques. À noter qu'il n'est pas ici question d'interruptions logicielles : le traitement d'un événement ne peut pas être interrompu par un autre, à part en des points précis explicitement prédéterminés du code logiciel (points qui, en fait, créent une seconde boucle événementielle au sein de la première). La programmation événementielle peut être réalisée dans n'importe quel langage de programmation, bien que la tâche soit plus aisée dans les langages de haut niveau (comme Java). Certains environnements de développement intégrés (par exemple Qt Software) permettent de générer automatiquement le code des tâches récurrentes dans la gestion des événements.
La programmation hors-ligne, communément appelée PHL en abrégé, ou CFAO robotique est une méthode de programmation des robots industriels. Historiquement, les robots industriels étaient uniquement programmés par la méthode par apprentissage, qui consiste à créer des trajectoires en faisant mémoriser aux robots des points correspondants à des coordonnées cartésiennes. Même encore aujourd'hui, cette méthode est la plus utilisée. Utile pour des applications de manipulation ou ne nécessitant pas la création d'un grand nombre de points, la programmation par apprentissage présente cependant de nombreux désavantages : Nécessité d'arrêter le robot pour lui faire apprendre le programme Temps de programmation pouvant s'avérer très long si le programme a beaucoup de points Essais/erreurs pour arriver à générer de bonnes trajectoires dus à des problématiques propres à la programmation robotique (Singularités, limites en termes de portée, limites de joints...) La programmation hors-ligne permet de s’affranchir de ses contraintes en séparant le processus de programmation du robot du processus de production. Ainsi, un robot peut effectuer une tâche de travail alors que son programmeur pourra en même temps, sur un ordinateur dédié, programmer la prochaine tâche via un logiciel de programmation hors-ligne. Cette méthode de programmation nécessite cependant d'avoir accès aux modèles CAO des pièces qui seront utilisées par le robot car le programmeur va créer les trajectoires robotiques en se basant sur la géométrie du modèle CAO de la pièce qu'il programme. Il pourra par la suite visualiser le résultat de sa programmation grâce à un simulateur intégré qui est une représentation virtuelle de l'environnement de travail du robot avec toutes ses composantes. Certains logiciels permettent la programmation de trajectoires de manière automatique grâce à des algorithmes de calcul poussés, le gain ainsi obtenu par rapport à la programmation par apprentissage autorise l’utilisation des robots pour de la production unitaire.
La programmation lettrée (ou programmation littéraire) est une approche de la programmation préconisée par Donald Knuth qui se veut différente du paradigme de programmation structurée des années 1970.  « Je crois que le temps est venu pour une amélioration significative de la documentation des programmes, et que le meilleur moyen d'y arriver est de considérer les programmes comme des œuvres littéraires. D'où mon titre, « programmation lettrée ». Nous devons changer notre attitude traditionnelle envers la construction des programmes : au lieu de considérer que notre tâche principale est de dire à un ordinateur ce qu'il doit faire, appliquons-nous plutôt à expliquer à des êtres humains ce que nous voulons que l'ordinateur fasse. Le praticien de programmation lettrée peut être vu comme un essayiste, qui s'attache principalement à l'exposition du sujet et à l'excellence du style. Un tel auteur, le dictionnaire à la main, choisit avec soin les noms de ses variables et explique la signification de chacune. Il cherche à obtenir un programme qui est compréhensible parce que les concepts ont été présentés dans le meilleur ordre pour la compréhension humaine, en utilisant un mélange de méthodes formelles et informelles qui se complètent l'une l'autre. »  — Donald Knuth, Literate Programming Le paradigme de programmation lettrée, tel que conçu par Knuth, s'éloigne de la contrainte d'écrire des programmes dans l'ordre imposé par l'ordinateur, et à la place permet aux développeurs de concevoir leurs programmes dans l'ordre requis par la logique et le fil de leur pensée. Les programmes lettrés sont écrits comme une exposition ininterrompue de la logique dans un langage naturel, à la manière d'un essai, dans lequel des macros qui masquent les abstractions et la complexité sont incluses. Des outils de programmation lettrée sont utilisés pour obtenir deux représentations à partir d'un fichier source lettré : l'une utilisable par un compilateur ou exécutable, le code « emmêlé », et une autre pour la lecture comme une documentation formatée, qui est dite « tissée » à partir de la source lettrée. Alors que les premiers outils de programmation lettrée étaient spécifiques à un langage de programmation, les suivants sont devenus indépendants du langage et existent au-dessus de celui-ci.
La programmation logique est une forme de programmation qui définit les applications à l'aide d'un ensemble de faits élémentaires les concernant et de règles de logique leur associant des conséquences plus ou moins directes. Ces faits et ces règles sont exploités par un démonstrateur de théorème ou moteur d'inférence, en réaction à une question ou requête. Cette approche se révèle beaucoup plus souple que la définition d'une succession d'instructions que l'ordinateur exécuterait. La programmation logique est considérée comme une programmation déclarative plutôt qu’impérative, car elle s'attache davantage au quoi qu'au comment, le moteur assumant une large part des enchaînements. Elle est particulièrement adaptée aux besoins de l’intelligence artificielle, dont elle est un des principaux outils.
La programmation orientée prototype est une forme de programmation orientée objet sans classe, fondée sur la notion de prototype. Un prototype est un objet à partir duquel on crée de nouveaux objets. Dans le langage de programmation orientée prototype Self, les propriétés d'un objet, qu'elles renvoient à des attributs ou à des méthodes, sont appelés slots ; il n'y a pas la même distinction entre les slots de données et les slots de code qu'on a avec les classes. La grande différence avec la programmation objet à classe est qu'on peut remplacer le contenu des slots, en ajouter d'autres ou changer la hiérarchie d'héritage que cela soit prévu dans l'objet original ou pas. Self fut le premier langage à prototypes. Il a été conçu dans les laboratoires de Sun dans les années 1990. Le plus connu actuellement est JavaScript.
La programmation par contrat est un paradigme de programmation dans lequel le déroulement des traitements est régi par des règles. Ces règles, appelées des assertions, forment un contrat qui précise les responsabilités entre le client et le fournisseur d'un morceau de code logiciel. C'est une méthode de programmation semi-formelle dont le but principal est de réduire le nombre de bugs dans les programmes. Historiquement, la programmation par contrat a été introduite par Bertrand Meyer dans son langage Eiffel datant de 1985, qui était inspiré de la notation Z créée par Jean-Raymond Abrial.
La programmation sans ego (Egoless programming) est un style de programmation des ordinateurs dans lequel les facteurs personnels sont minimisés, de façon à améliorer la qualité. Les méthodes de coopération suggérées sont semblables à celles utilisés dans d'autres projets collectifs, comme Wikipédia.
La programmation sécurisée consiste à prendre en compte la sécurité informatique à tous les moments de la conception, de la réalisation et de l'utilisation d'un programme informatique. Cela permet d'éviter au maximum les trous de sécurité et autres bugs.
En informatique, la programmation séquentielle est un paradigme de programmation dans laquelle le déroulement des instructions du programme est toujours le même (les instructions elles-mêmes peuvent être différentes en fonctions des embranchements,...). La programmation séquentielle s'oppose à la programmation événementielle dans laquelle la séquence d'instructions exécutée est déterminée ou modifiée en permanence par les différents événements extérieurs ayant une incidence sur le traitement durant son exécution. Exemples Le calcul de la paye relève de la programmation séquentielle : une fois lancé le traitement exécute toutes ses instructions dans un certain ordre sans avoir à prendre en compte d'événement externe. Par opposition le traitement qui prend en charge l'interface homme-machine (l'affichage sur l'écran) sur un micro-ordinateur (par exemple Linux) relève de la programmation événementielle : le traitement prend en compte en permanence les actions de l'utilisateur qui viennent interagir avec la séquence de ses instructions.
En programmation informatique, la programmation spaghetti est un style d'écriture de code source qui favorise l'apparition du syndrome du plat de spaghettis : un code peu clair et qui fait un usage excessif de sauts inconditionnels (voir goto), d'exceptions en tous sens, de gestion des événements complexes et de threads divers. En fait, la programmation spaghetti qualifie tout ce qui ne permet pas de déterminer le qui, le quoi et le comment d'une prise de contrôle par une portion de programme (incompréhension du flux de contrôle). Le code est donc plus long à mettre à jour car cela nécessite de remonter le fil des renvois. Cette notion s'applique aussi au niveau du flux de données, c'est-à-dire à tout ce qui ne permet pas de déterminer le qui, le quoi et le comment d’une modification de données. Cette situation est causée par un usage excessif de couplage fort. La programmation spaghetti est un exemple d'anti-patron.
Un programme informatique est un ensemble d'opérations destinées à être exécutées par un ordinateur. Un programme source est un code écrit par un informaticien dans un langage de programmation. Il peut être compilé vers une forme binaire, ou directement interprété. Un programme binaire décrit les instructions à exécuter par un microprocesseur sous forme numérique. Ces instructions définissent un langage machine,. Un programme fait généralement partie d'un logiciel : un ensemble de composants numériques destiné à fournir un service informatique ; un logiciel peut comporter plusieurs programmes. On en retrouve ainsi dans les appareils informatiques (ordinateur, console de jeu, guichet automatique bancaire…), dans des pièces de matériel informatique, ainsi que dans de nombreux dispositifs électroniques (imprimante, modem, GPS, téléphone mobile, machine à laver, appareil photo numérique, décodeur TV numérique, injection électronique, pilote automatique…). Les programmes informatiques sont concernés par le droit d'auteur, et font l'objet d'une législation proche des œuvres artistiques.
PyPI (de l'anglais « Python Package Index ») est le dépôt tiers officiel du langage de programmation Python. Son objectif est de doter la communauté des développeurs Python d'un catalogue complet recensant tous les paquets Python libres. Il est analogue au dépôt CPAN pour Perl. La plupart des gestionnaires de paquets Python utilisent ce dépôt, le plus connu est pip.
En informatique, le raffinement consiste à détailler la conception pour arriver par itérations à l'implémentation finale. À chaque itération correspond un niveau de granularité de plus en plus fin. Quand cette technique est appliquée au code source, la conception est alors matérialisée par du pseudo-code. Cette technique peut aussi être appliquée au modèle de données. Cette technique est utilisée par différentes méthodes : Approche descendante (stepwise refinement) ; FermaT Transformation System (en) ; Méthode B (1996) ; Méthode de la machine à états abstraits (ASM) ; Prototypage logiciel vertical ; Sous-typage comportemental (behavioral subtyping), voir le principe de substitution de Liskov Microsoft Solutions Framework Process Model. L'opposée du raffinement est la programmation modulaire.
Le raisonnement à partir de cas (RàPC) (nommé en anglais case-based_reasoning (CBR)) est un type de raisonnement qui copie le comportement humain qui consiste à faire naturellement appel à l'expérience pour résoudre les problèmes de la vie quotidienne, en se remémorant les situations semblables déjà rencontrées et en les comparant à la situation actuelle pour construire une nouvelle solution qui, à son tour, s’ajoutera à l'expérience. Ce type de raisonnement résout les problèmes en retrouvant des cas analogues dans sa base de connaissances et en les adaptant au cas considéré. Cette technologie est apparue il y a une quinzaine d’années[Quand ?] mais les travaux initiaux sur le sujet remontent cependant aux expériences de Schank et Abelson en 1977 à l'Université Yale. Elle reste pourtant encore assez méconnue par rapport à d’autres technologies appartenant au domaine des sciences cognitives comme le data mining. Elle diffère de cette dernière par son approche. En effet, ici, on n’utilise qu’indirectement les données pour retrouver les cas proches, à partir desquels on va générer une solution.
Dans les méthodes agiles, un récit utilisateur ou user story est une phrase simple dans le langage de tous les jours permettant de décrire avec suffisamment de précision le contenu d'une fonctionnalité à développer. La phrase contient généralement trois éléments descriptifs de la fonctionnalité : Qui ? Quoi ? Pourquoi ?.  En tant que <qui>, je veux <quoi> afin de <pourquoi>  Le pourquoi est optionnel. Il permet cependant d'identifier l'intérêt de la fonctionnalité.
En informatique, la réentrance est la propriété pour une fonction d'être utilisable simultanément par plusieurs tâches utilisatrices. La réentrance permet d'éviter la duplication en mémoire vive d'un programme utilisé simultanément par plusieurs utilisateurs. Si une fonction de plusieurs threads doit accéder à une variable globale, il suffit de l'encadrer par des Mutex. L'écriture de code réentrant était autrefois[Quand ?] une tâche très ardue. Elle s'est simplifiée avec les langages actuels qui autorisent : l'allocation dynamique de mémoire, la séparation, dans des segments distincts, du code et des données, la gestion de piles. Le noyau d'un système d'exploitation comporte souvent des parties non réentrantes afin d'éviter des complications fâcheuses (incohérence de données critiques, perte de performances). L'écriture de modules destinés à être exécutés dans l'espace noyau reste, pour cette raison, délicate.
La notion de référence croisée désigne l’allusion à un élément ou à des données apparaissant ailleurs dans un document. Dans un index une référence croisée est notée Voir aussi. En programmation, le cross-referencing ou référencement croisé est le listage des noms de fichier et de leurs numéros de ligne où un identificateur apparaît dans le code source d'un programme. Une référence croisée aide à renforcer la structure d'un document.
En programmation informatique, la réflexion est la capacité d'un programme à examiner, et éventuellement à modifier, ses propres structures internes de haut niveau lors de son exécution. On appelle réflexivité le fait pour un langage de programmation de permettre l'écriture de tels programmes. Un tel langage de programmation est dit réflexif.  
Dans la pensée, la réification (du latin res, chose) consiste à considérer une idée abstraite comme une chose concrète. Dans l'usage ordinaire du langage, il est normal de réifier des idées et la réification sera comprise comme telle. Par exemple dans ce vert bleuté est beau ou j'aime ce vert bleuté : seule la chose qui est de cette couleur existe réellement, sa couleur est une idée dans l'esprit qui est ici réifiée, mais cela ne pose pas de problème. Par contre, l'usage de réifications dans le discours abstrait est souvent trompeur et est considéré comme un sophisme : une erreur de pensée ou une manipulation intellectuelle.
Le réusinage de code est l'opération consistant à retravailler le code source d'un programme informatique – sans toutefois y ajouter des fonctionnalités ni en corriger les bogues – de façon à en améliorer la lisibilité et par voie de conséquence la maintenance, ou à le rendre plus générique (afin par exemple de faciliter le passage de simple en multiple précision) ; on parle aussi de « remaniement ». Cette technique utilise quelques méthodes propres à l'optimisation de code, avec des objectifs différents. Le terme réusinage est originaire du Québec. L'équivalent en anglais est code refactoring, parfois rendu par refactorisation, terme qui, selon l'Office québécois de la langue française (OQLF), est à éviter.
La réutilisation de code désigne l'utilisation de logiciel existant, de connaissances sur ce logiciel, de composants logiciels ou du code source, pour créer de nouveaux logiciels. La réutilisation s'appuie fréquemment sur le concept de modularité. Par extension, ce terme désigne également l'ensemble des techniques informatiques proposées ou mises en œuvre pour faciliter cette réutilisation.
RGSS est une implémentation de l'interpréteur de Ruby en C++ développé par l'éditeur de jeu vidéo Enterbrain. Il est distribué sous licence propriétaire dans la série RPG Maker dans ses deux dernières versions XP et VX. La dernière version stable (RGSS 3) est sortie en même temps que l'extension de Rpg Maker VX : Rpg Maker VX Ace.  Portail de la programmation informatique
En informatique, une routine est une entité informatique qui encapsule une portion de code (une séquence d'instructions) effectuant un traitement spécifique bien identifié (asservissement, tâche, calcul, etc.) relativement indépendant du reste du programme, et qui peut être réutilisé dans le même programme, ou dans un autre. Dans ce cas, on range souvent la routine dans une bibliothèque pour la rendre disponible à d'autres projets de programmation, tout en préservant l'intégrité de son implémentation. Les routines permettent de diviser un problème en décomposant le programme à réaliser en portions de code plus faciles à produire, à utiliser, à gérer et à entretenir. Les instructions réalisant la routine sont encapsulées à l'intérieur de celle-ci et le programmeur peut faire appel à la routine sans se préoccuper des détails internes à celle-ci ; la routine joue le rôle d'une boite noire dans les routines qui l'emploient.
Une S-expression (ou expression symbolique) est une convention pour la représentation de données ou d'expressions d'un programme sous forme textuelle. Les S-expressions sont utilisées dans la famille de langages Lisp, incluant Scheme et DSSSL (en), ainsi que comme métalangage dans des protocoles de communication tels IMAP ou le langage CBCL (Common Business Communication Language) de John McCarthy. Les détails de la syntaxe et les types de données supportés diffèrent en fonction du langage, mais la propriété la plus commune est l'utilisation de la notation préfixée parenthésée (affectueusement connue sous le nom de Notation polonaise de Cambridge).
En programmation informatique, la signature de type définit les types de données acceptables pour une fonction ou une méthode. Une signature inclut au moins le nom de la fonction et le nombre de paramètres. Dans certains langages, elle peut aussi spécifier le type de la valeur de retour et les types de ses paramètres.
Le signe égal (=) est un symbole mathématique utilisé pour indiquer l’égalité.
Le Software craftsmanship (ou l'« artisanat du logiciel ») est une approche de développement de logiciels qui met l'accent sur les compétences de codage des développeurs. Il se propose comme une réponse aux maux récurrents de l'industrie du logiciel et à la tendance à l'externalisation. Il inclut la priorisation des préoccupations financières vis-à-vis de la responsabilité du développeur. Ce mouvement prône le côté artisanal du développement logiciel, autrement dit, d'après le manifeste de l'artisanat du logiciel, il ne suffit pas qu'un logiciel soit fonctionnel, mais il faut qu'il soit bien conçu. L'idée principale est de garantir la fiabilité et la maintenabilité des applications d'où l'importance de professionnels aptes à concevoir des logiciels dans le respect d'indicateurs de qualité logicielle. Le software craftsmanship et l'agilité sont alors complémentaires, car là où l'agilité se limite à la souplesse des cycles de développement, le software craftsmanship s'étend sur la façon même dont est conçu et écrit le code.
Les standards Gnits, également appelés Gnits, désignent une collection de « bonnes pratiques » et leur description, écrites et maintenues jusqu'en mai 2007 par un petit groupe de mainteneurs du projet GNU pour la construction du système GNU. Ces recommandations n'étaient pas officiellement reconnues comme des standards du projet, mais représentaient plutôt des avis pouvant servir d'introduction aux normes de codage GNU. Plus généralement, ces recommandations ont cependant été adoptées par la communauté des programmeurs de logiciels libres dans les domaines de la programmation, la maintenance et la distribution logicielles. D'après la documentation des « Gnits », aujourd'hui disponible à des fins historiques, la plupart de ces « bonnes pratiques » a été intégrée dans la documentation des Autotools.
Sublime Text est un éditeur de texte générique codé en C++ et Python, disponible sur Windows, Mac et Linux. Le logiciel a été conçu tout d'abord comme une extension pour Vim, riche en fonctionnalités. Depuis la version 2.0, sortie le 26 juin 2012, l'éditeur prend en charge 44 langages de programmation majeurs, tandis que des plugins sont souvent disponibles pour les langages plus rares.
La substitutiabilité est un principe de la programmation informatique. Il affirme que si S est un sous-type de T, alors on peut substituer des objets de type S à des objets de type T sans altérer les propriétés désirables de ce programme (exactitude, tâche bien exécutée...). Le principe est souvent utilisé en programmation orientée objet.
Summly est une application facilitant l'agrégation d'informations et la lecture créée par un adolescent Britannique, Nick D'Aloisio. L'application a reçu l'Apple's Best Apps of 2012 award. Le succès des téléchargements sur les smartphones (plus d'un million en quelques mois) en a fait un choix évident pour le groupe Yahoo! qui a annoncé l'avoir acheté pour un montant estimé à 30 millions de dollars (25 millions d'euros) par Yahoo! le 26 mars 2013,.
La surcharge (également connue sous le nom de surdéfinition, polymorphisme ad hoc ou overloading en anglais) est une possibilité offerte par certains langages de programmation qui permet de choisir entre différentes versions d'une même fonction ou méthode selon le nombre et le type des arguments fournis. Le polymorphisme ad hoc ne doit pas être confondu avec le polymorphisme d'inclusion des langages à objets, permis par l'héritage de classe et la redéfinition de méthode (overriding en anglais). La surcharge peut être statique (le choix de la version est alors fait en fonction du nombre d'arguments et de leur type statique déclaré à la compilation) ou dynamique (le choix de la version est alors fait en fonction du type dynamique des arguments constaté à l'exécution). La surcharge dynamique est également appelée « dispatch multiple » et une méthode surchargée dynamiquement « multiméthode ».
En programmation informatique, un symbole est une étiquette apposée sur certains éléments du code objet, du bytecode ou d'un arbre syntaxique abstrait, permettant de les identifier sous cette forme transformée du code source. Un symbole est très proche d'un identificateur du langage source, mais le concept d'identificateur ne recouvre pas totalement le concept de symbole. Le meilleur exemple en est le name mangling (en) effectué par les compilateurs C++ qui consiste à trouver un symbole unique pour un identificateur dont le nom est surchargé. Le code objet, le bytecode et les arbres syntaxiques abstraits proviennent du processus de compilation, qui consiste à convertir le code source (écrit dans un langage de programmation) en un code exécutable ou bien en une structure plus facilement utilisable par l'ordinateur. L’exécution symbolique permet d'explorer les chemins d'exécution possibles d'un programme informatique à partir des symboles contenus dans son code source. Elle diffère de l’exécution concrète qui ne suit qu'un seul des chemins possibles. Les symboles enregistrés dépendent du processus de compilation, de l'étape dans le processus de compilation et du langage de programmation utilisé. Par exemple, en C, les symboles exportés dans du code objet sont les informations sur les structures (nom, nom et type des champs) (déclarées avec le mot clé struct), les fonctions globales (nom, nombre et type des paramètres, type de retour) et nom et type des variables globales (non marqués avec le mot clé static, ou bien marqués avec le mot clé extern) afin qu'ils puissent être utilisés lors de l'étape d'édition des liens. De même, compiler du code C pour le débogage génère les symboles de débogage qui permettent d'identifier la portion du code source correspondant au code binaire exécuté. En Java, tous les symboles sont exportés : nom des classes, nom et type des attributs, méthodes (nombre et type des paramètres, type de retour, nom). Ils servent aussi bien à la résolution des noms lors de l'exécution qu'au débogage. Un échec lors de la résolution des noms va déclencher des exceptions telles que ClassNotFoundException, NoSuchMethodError ou bien NoSuchFieldError.
En informatique, la syntaxe abstraite d'une structure de données représente les types qui définissent ces données sans forcément leur assigner une représentation ou un codage précis. Elle sert en particulier à la représentation du code source des langages de programmation, et est généralement stockée dans un arbre syntaxique abstrait. Par opposition, la syntaxe concrète inclut l'information indiquant comment sont représentées les données.
Le typage statique est une technique utilisée dans certains langages de programmation impératifs (C++, Java, Pascal, ou même Visual Basic avec l'Option Explicit) pour associer à un symbole dénotant une variable le type de la valeur dénotée par la variable ; et dans certains langages de programmation fonctionnels (ML, OCaml, Haskell, etc.) pour associer à une fonction (un calcul) le type de son paramètre et le type de la valeur calculée. Une telle association présente les bénéfices potentiels suivants : un compilateur de langage à typage statique détecte les erreurs de types avant que le programme ne soit exécuté (on obtient ainsi la sûreté du typage) ; le même compilateur peut tirer parti de l'information sur les types pour réaliser certaines optimisations du code objet ; enfin, puisque les types des objets manipulés sont connus, le compilateur peut éliminer cette information du code objet produit, avec pour principal avantage un gain de mémoire par rapport aux systèmes à typage dynamique.
En programmation informatique, un type énuméré (appelé souvent 'énumération' ou parfois 'type énumératif' et liste énumérative) est un type de données qui consiste en un ensemble de constantes appelées énumérateurs. Lorsque l'on crée un type énuméré on définit ainsi une énumération. Lorsqu'un identificateur tel qu'une variable est déclaré comme étant de type énuméré, cette variable peut recevoir n'importe quel énumérateur (lié à ce type énuméré) comme valeur. Si le langage de programmation autorise de donner un nom à un type énuméré, le nom est généralement choisi pour décrire collectivement les énumérateurs de l'ensemble. Dans certains langages, le type booléen est un type énuméré prédéfini qui possède deux énumérateurs (true et false). Toutefois, les énumérateurs sont souvent écrits en majuscule afin d'indiquer que ce sont des constantes.
En programmation informatique et théorie des types, un type récursif est un type de données dont la définition fait appel au type lui‐même, de façon récursive. Cela permet entre autres des structures de données qui contiennent des sous‐structures du même type. Cette notion s'applique naturellement dans l'étude des listes et des arbres.
Un uniligne (one liner pour les anglophones) est généralement un programme informatique jetable, mais peu banal, écrit pour une tâche ponctuelle en un langage de script tel que Perl ou Ruby, et tenant sur une seule ligne. Des langages qui imposent plusieurs phases avant l'exécution, la déclaration explicite de variables, de fonctions ou, pire, de classes, ne se prêtent pas aux unilignes. Les unilignes bénéficient des qualités DWIM (Do What I Mean) et DRY (Don't Repeat Yourself) propres à certains langages. Parce qu'un uniligne est rarement destiné à être réutilisé, les considérations de lisibilité par un tiers sont secondaires. Certains unilignes sont des assombrissements délibérés. En revanche, ils peuvent être pédagogiques en démontrant de manière concise certains idiotismes du langage. Il faut néanmoins savoir que certains idiotismes deviennent inapplicables ou dangereux dans le cadre de programmes avec beaucoup de lignes de code.
En informatique, dans une structure de données de taille variable, une des techniques pour dénoter la fin des données est l'utilisation d'une valeur sentinelle. Celle-ci est stockée dans la structure de donnée de la même manière que ses données, ce qui impose que cette valeur ne puisse pas être confondue avec des données valides. L'utilisation d'une valeur sentinelle implique a priori un parcours séquentiel de la structure de données (même si celle-ci est à accès aléatoire).
En informatique, les variables sont des symboles qui associent un nom (l'identifiant) à une valeur. La valeur peut être de quelque type de donnée que ce soit. Le nom doit être un identifiant unique (et si le langage en possède, différents des mots-réservés ). Dans la plupart des langages et notamment les plus courants, les variables peuvent changer de valeur au cours du temps (dynamique). Dans les langages de certains paradigmes, notamment la programmation fonctionnelle, leur valeur est au contraire figée dans le temps (statique).
En programmation informatique, une variable globale est une variable déclarée à l'extérieur du corps de toute fonction ou classe, et pouvant donc être utilisée n'importe où dans le programme. On parle également de variable de portée globale.
En programmation informatique, une variable locale est une variable qui ne peut être utilisée que dans la fonction ou le bloc où elle est définie. La variable locale s'oppose à la variable globale qui peut être utilisée dans tout le programme. Selon le langage utilisé, une variable locale à une fonction sera accessible ou non aux fonctions que celle-ci appelle (notion de portée d'une variable ; voir aussi la notion de « fief » en Algol 68).
En programmation informatique, une variable métasyntaxique est une variable générique, qui aurait la valeur grammaticale d'un pronom (en ce qu'ils sont la représentation générique de tout une classe d'objet spécifique: il, elle, on... ici "var" pour variable serait l'exemple type). Ces variables sont utilisées dans les exemples pour se concentrer sur le fond plutôt que sur la forme, et dont le nom est choisi pour être tacitement reconnu comme tel par les administrateurs et les programmeurs. Le mot toto est l'exemple le plus parlant. L'utilisation des variables métasyntaxiques permet de libérer le programmeur de la recherche d'un nom de variable logique adéquat au sujet étudié. Les variables métasyntaxiques sont appelées ainsi car : ce sont des variables dans le métalangage employé pour parler des programmes, etc. (voir également le pseudo-code) ; ce sont des variables dont les valeurs sont souvent des variables (comme dans des utilisations telles que « la valeur de f( toto, tata ) est la somme de toto et tata »). Ce terme fait partie du jargon informatique.
Une variable temporaire est, dans le domaine de la programmation informatique, une variable dont la durée d'existence est courte (en général limitée à la procédure ou la fonction qui l'utilise). Du fait de sa courte durée de vie, sa portée est souvent limitée. Un exemple typique d'utilisation est l'échange de deux variables : il faut stocker la valeur de la première donnée dans une zone temporaire avant de déplacer la valeur de la deuxième donnée. temp := a a := b b := temp
En programmation informatique, une variable volatile est une variable sur laquelle aucune optimisation de compilation n'est appliquée. Le mot-clé existe en C, C++, C# et Java. Le préfixe volatile est notamment utilisé quand la variable d'un programme peut être modifiée par un autre programme (cas des entrées/sorties, ou de threads).
Dorothy Johnson Vaughan (20 septembre 1910-10 novembre 2008) est une mathématicienne afro-américaine ayant travaillé pour le National Advisory Committee for Aeronautics (NACA), l'ancienne NASA. Avant d'intégrer le centre de recherche Langley du  NACA en 1943, Vaughan a travaillé comme professeure de mathématiques au R. R. Moton High School à Farmville, en Virginie. En 1949, Vaughan a pris la tête des West Area Computers, un groupe de travail composé entièrement de mathématiciennes afro-américaines. La mathématicienne Katherine Johnson a travaillé avec le groupe de Vaughan, avant d'être transférée à Langley à la division de la recherche. Vaughan a continué à Langley après que la NACA soit devenue la NASA. Elle s'est spécialisée pour le reste de sa carrière dans l'électronique, l'informatique et la programmation Fortran. Elle a travaillé au département d'analyse et de calcul du Centre de recherche Langley et a également participé au projet Scout.
La vectorisation (dans le cadre du calcul parallèle), est un cas particulier de la parallélisation, dans lequel des logiciels qui effectuent par défaut une seule opération à la fois sur un seul thread sont modifiés pour effectuer plusieurs opérations simultanément. La vectorisation est le processus de conversion d'un programme informatique à partir d'une implémentation scalaire, qui traite une seule paire d'opérandes à la fois, à une implémentation vectorielle qui traite une opération sur plusieurs paires d'opérandes à la fois. Le terme vient de la convention de mettre les opérandes dans des vecteurs ou des matrices. Le calcul vectoriel est une caractéristique majeure concernant à la fois les ordinateurs classiques et les superordinateurs modernes, pouvant réaliser des opérations vectorielles qui effectuent simultanément des opérations telles que par exemple les quatre additions suivantes :                                                                                                    c                                        1                                                                                                    =                                    a                                        1                                                     +                                    b                                        1                                                                                                                                c                                        2                                                                                                    =                                    a                                        2                                                     +                                    b                                        2                                                                                                                                c                                        3                                                                                                    =                                    a                                        3                                                     +                                    b                                        3                                                                                                                                c                                        4                                                                                                    =                                    a                                        4                                                     +                                    b                                        4                                                                                                     {\displaystyle {\begin{aligned}c_{1}&=a_{1}+b_{1}\\c_{2}&=a_{2}+b_{2}\\c_{3}&=a_{3}+b_{3}\\c_{4}&=a_{4}+b_{4}\end{aligned}}}    Cependant, dans la plupart des langages de programmation on écrit généralement des boucles qui effectuent séquentiellement des additions de grands nombres. Voici un exemple d'une telle boucle, en C :  La vectorisation automatique est un sujet de recherche majeur en informatique ; cela consiste à rechercher des méthodes qui permettent à un compilateur de convertir (sans assistance humaine) des programmes scalaires en programmes vectorisés.
W3.js est une librairie JavaScript légère et gratuite proposée par W3Schools.  Elle offre différentes fonctions permettant, par exemple, de manipuler dynamiquement le DOM, créer un slideshow, une requête AJAX. Cette librairie présente une alternative à Jquery.
Le yield est une instruction de programmation qui sert à imposer la constructivité d'une fonction. Elle est plus souvent présente dans les langages à haut niveau comme Python, Ruby ou le C#. Son utilisation dans une fonction permet de retourner un générateur. Dans le cas du python, appeler la méthode next() de ce générateur exécutera la fonction et retournera une valeur. Le yield procède comme un return au détail près que cette fonction est liée à un générateur et que le prochain appel de la méthode next() reprendra l'exécution là où elle en était.
L’expression « alphabétisme informatique » désigne la capacité à se servir des ordinateurs et des nouvelles technologies de façon efficace. Cette expression peut aussi désigner en particulier le niveau d’aisance d’une personne face à des programmes informatiques et des techniques qui y sont habituellement associés. La connaissance du fonctionnement des ordinateurs constitue également une autre part importante de l’alphabétisme informatique. Dans les pays développés, les compétences de base en informatique sont un atout important. La définition précise de l’alphabétisme informatique peut varier selon les personnes. En dehors du contexte informatique, le mot alphabétisé qualifie habituellement une personne capable de lire n’importe quel livre dans sa langue maternelle et qui, lorsqu’elle rencontre un mot inconnu, peut éventuellement faire une recherche sur ce dernier. De la même manière, pour un professionnel de l’informatique, la capacité à s’auto-former (c.-à-d. apprendre à utiliser de nouveaux programmes ou réaliser de nouvelles tâches) est une partie importante de l’alphabétisation informatique. Ainsi, dans le langage courant, un « alphabétisé informatique » est souvent une personne qui est capable de faire plus que de simplement utiliser plusieurs logiciels précis (il s’agit souvent des produits Microsoft Word, Internet Explorer et Outlook [citation nécessaire]) pour des usages précis et dont la technique d’apprentissage se résume en grande partie à du par cœur. On pourrait comparer cette situation à celle d’un enfant affirmant savoir lire, alors qu’il se serait simplement contenté d’apprendre par cœur quelques livres simples pour enfant. Pour ces personnes, les vraies difficultés apparaîtront lorsqu’elles seront amenées à découvrir un nouveau programme. L’une des qualités d’une personne dite « alphabétisée informatiquement » est donc l’adaptation et la capacité à faire face à de nouveaux outils. On parle plus couramment de maîtrise de l'informatique.
Le programme One to One était un service payant de formation privée dans les magasins Apple Store. Apple a annoncé l'arrêt du service en septembre 2015.
Le passeport de compétences informatique européen, abrégé en PCIE est un certificat d'aptitude qui atteste que le titulaire a les connaissances de base pour utiliser un ordinateur ainsi que les principaux outils bureautiques. Initié par le CEPIS (Council of European Professional Informatics Societies), le Conseil Européen des Associations de Professionnels des Technologies de l'Information, il est soutenu par la Commission européenne et géré depuis 1997 par la Fondation ECDL. Le passeport de compétences informatique européen (PCIE) est devenu le standard mondial avec 150 pays et plus de 14 millions de candidats. Le nom donné au passeport en dehors de l'Europe est l'ICDL (International Computer Driving Licence).
En informatique, un nœud est synonyme de sommet en théorie des graphes. Mais le plus souvent il sera synonyme d’unité de calcul, par exemple un ordinateur parallèle sera composé de plusieurs nœuds (de calcul). Contrairement à l'architecture réseau client-serveur, l'architecture réseau composée de nœuds est symétrique. Chaque nœud a la même capacité d’émettre, de recevoir et de calculer que les autres nœuds de son réseau. Nœuds est le nom donné à la machine et au logiciel dans les réseaux pair à pair.
Un réseau informatique est un ensemble d'équipements reliés entre eux pour échanger des informations. Par analogie avec un filet (un réseau est un « petit rets », c'est-à-dire un petit filet), on appelle nœud l'extrémité d'une connexion, qui peut être une intersection de plusieurs connexions ou équipements (un ordinateur, un routeur, un concentrateur, un commutateur). Indépendamment de la technologie sous-jacente, on porte généralement une vue matricielle sur ce qu'est un réseau. De façon horizontale, un réseau est une strate de trois couches : les infrastructures, les fonctions de contrôle et de commande, les services rendus à l'utilisateur. De façon verticale, on utilise souvent un découpage géographique : réseau local, réseau d'accès et réseau d'interconnexion.
Le réseau 6bone était un réseau de test IPv6. Il s'agit d'une émanation du groupe de travail IPng de l'IETF qui a conçu la nouvelle mouture du Protocole Internet. Le 6bone a démarré en 1996, en marge des réunions IETF, et devient un projet collaboratif mondial, sous la supervision du groupe de travail NGtrans de l'IETF. La mission originale du 6bone était de créer un réseau pour permettre le développement, le test et le déploiement d'IPv6 sur le modèle du Mbone, le réseau multicast expérimental qui l'a précédé et dont il s'est inspiré du nom. Le 6bone est créé comme un réseau virtuel, utilisant des tunnels IPv6 sur IPv4 à travers Internet, et ajoute peu à peu des connexions IPv6 natives. Outre les tests d'implémentations IPv6, le 6bone permet aussi de tester des procédures opérationnelles et des mécanismes de transition, et aussi d'acquérir une expérience pratique avec IPv6. Le 6bone utilisait des adresses unicast globales dédiées sous le préfixe 3ffe::/16 (RFC 2471). En 2003, à son pic d'utilisation, le 6bone comptait 150 préfixes routés qui représentaient plus de 1000 sites dans 50 pays. Quand les offres de connectivité IPv6 natives furent disponibles, il fut mis fin au réseau 6bone (RFC 3701) à la date du 6 juin 2006 (6/6/06).
6LoWPAN est l'acronyme de IPv6 Low power Wireless Personal Area Networks ou IPv6 LoW Power wireless Area Networks. C'est également le nom d'un groupe de travail de l'IETF. Le groupe 6LoWPAN a défini les mécanismes d'encapsulation et de compression d'entêtes permettant aux paquets IPv6 d'être envoyés ou reçus via le protocole de communication IEEE 802.15.4. IPv4 et IPv6 sont efficaces pour la délivrance de données pour les réseaux locaux, les réseaux métropolitains et les réseaux étendus comme l'internet. Cependant, ils sont difficiles à mettre en œuvre dans les capteurs en réseaux et autres systèmes contraints en raison, notamment, de la taille importante des en-têtes. 6LoWPAN devrait permettre à IPv6 d'intégrer ces matériels informatiques contraints et les réseaux qui les interconnectent. La spécification de base développée par le groupe 6LoWPAN est connue sous la forme de deux principales RFC : Le principal document de cette problématique est la RFC 4919. La spécification en elle-même est l'objet de la RFC 4944.
A2DP qui signifie Advanced Audio Distribution Profile est un profil Bluetooth. Il spécifie les protocoles pour la transmission de grande qualité sur les canaux ACL. Un exemple type d'utilisation est le streaming audio à l'aide d'une connexion Bluetooth. On envoie un contenu audio en « direct » d'un lecteur de musique à un casque audio ou à des enceintes Bluetooth. Pour le transport de flux audio, A2DP utilise nativement un codec appelé SBC mais il peut optionnellement gérer des codecs tels que MP3 ou Vorbis. Depuis les années 2000, il peut utiliser le codec aptX qui permet une bien meilleure transmission, d'une qualité équivalente a celle d'un CD.
Antares est l'acronyme de « Adaptation nationale des transmissions aux risques et aux secours » et qui est le réseau numérique des services publics qui concourent aux missions de sécurité civile.
Un contrôleur de livraison d'applications (application delivery controller en anglais, ADC) est un dispositif de réseau informatique dans un centre de données, souvent faisant partie d'un réseau de distribution d'applications (ADN), qui aide à effectuer des tâches courantes telles que celles effectuées par les sites web pour éliminer la charge des serveurs web eux-mêmes. Beaucoup assurent aussi de la répartition de charge. Les ADC sont généralement placés dans la DMZ, entre le pare-feu ou un routeur et d'une ferme Web.
L'architecture de réseau est l'organisation d'équipements de transmission, de logiciels, de protocoles de communication et d'infrastructure filaire ou radioélectrique permettant la transmission des données entre les différents composants.
ARTE Concert (anciennement ARTE Live Web) est une plateforme numérique dédiée à la diffusion de concerts et de spectacles vivant. Créée en mai 2009 par la chaîne de télévision franco-allemande ARTE, elle diffuse des spectacles de la scène européenne contemporaine en livestream et en replay.
En sécurité informatique, l’attaque Sybil est une attaque au sein d’un système de réputation (en) qui est renversé par la création de fausses identités dans un réseau informatique pair à pair.
Un Autonomous System (abrégé AS), ou système autonome, est un ensemble de réseaux informatiques IP intégrés à Internet et dont la politique de routage interne (routes à choisir en priorité, filtrage des annonces) est cohérente. Un AS est généralement sous le contrôle d'une entité ou organisation unique, typiquement un fournisseur d'accès à Internet. Chaque AS est identifié par un numéro de 16 bits (ou 32 depuis 2007, selon la RFC 4893) , appelé « Autonomous System Number » (ASN). Il est affecté par les organisations qui allouent les adresses IP, les Registres Internet régionaux (RIR). En Europe, c'est le RIPE-NCC qui assume cette charge, par délégation de l'IANA. Les numéros d'AS (les ASN) sont utilisés par le protocole de routage Border Gateway Protocol (BGP) entre les systèmes autonomes (les AS). Ils furent historiquement conçus en 1982 pour le prédécesseur de BGP (le protocole EGP). Au sein d'un AS, le protocole de routage « interne » de type IGP (notamment OSPF ou IS-IS) permet aux routeurs de cet AS de communiquer entre eux, et plus particulièrement en vue de monter des sessions BGP internes (iBGP). Entre systèmes autonomes, le routage est « externe », en BGP externe (eBGP). En général, l'ASN n'apparait pas dans les protocoles de routage internes (les IGP) puisque, par définition, ils sont limités à un seul AS. Cependant, certains protocoles de routage internes, tels que Enhanced Interior Gateway Routing Protocol (EIGRP), sont configurés pour n'établir d'adjacence qu'avec les routeurs qui annoncent le même système autonome. Le nombre d'AS composant Internet dépassait 5 000 en 1999, 30 000 fin 2008, 35 000 mi-2010, plus de 45 000 en mai 2013.
Belnet est le réseau national de la recherche et de l'enseignement en Belgique. Il fournit l'accès internet à haut débit aux universités, aux hautes écoles, aux centres de recherches et aux services publics en Belgique. Belnet a été fondé en 1993 au sein des Services fédéraux des affaires scientifiques, techniques et culturelles (SSTC) et est financé publiquement à hauteur de 8 millions d'euros annuels, il ne propose pas ses services aux particuliers ni aux sociétés commerciales. Sa dorsale est constituée de fibre noire qui permet par le multiplexage en longueur d'onde d'atteindre des débits de 10 Gbit/s et plus. Belnet est connecté au réseau GÉANT2 du consortium DANTE ainsi qu'à quatre points d'interconnexion: le SFINX (Paris), l'AMS-IX (Amsterdam), le LINX (Londres) et le BNIX (Bruxelles), dont il a la gestion, en plus de la connexion multi-gigabit vers l'Internet commercial. Outre la connectivité internet, les services offerts incluent l'IPv6 et le multicast depuis 2001. Bien que souvent écrit en capitales, Belnet n'est pas un acronyme.
Le bit-time' est une notion informatique qui désigne le temps mis pour transmettre un bit depuis une carte réseau sur un câble. Pour calculer le bit-time, on procède comme suit :         bit time = 1 / Débit de la carte réseau  Avec une carte réseau ayant un débit de 10Mbit/s, le calcul du bit-time sera :         bit time = 1 / (10 * 10^6)                  = 10^-7                  = 100 * 10^-9                  = 100 nanosecondes  Un bit sera transféré par la carte réseau toutes les 100 nanosecondes  Portail des télécommunications  Portail de l’informatique
BITNET était un réseau informatique coopératif d'universités américaines, fondé en 1981 sous l'égide d'Ira Fuchs et de Greydon Freeman de la « City University of New York » (CUNY).
Le câblage structuré est l'ensemble des techniques, méthodes et normes permettant de réaliser l'interconnexion physique des différents locaux d'une entreprise, d'un centre de données ou d'une zone plus large (campus ou ville). On peut lister un ensemble de sujets concernant le câblage structuré : Connexions entrantes : là où un bâtiment est connecté au reste du monde ("arrivée télécom"). Salles d'équipements réseaux : là où sont connectés les équipements servant aux utilisateurs (serveurs par exemple). Salles de télécommunication (ou Sous-répartiteur, Local Technique d'Etage, etc.) : là où l'interconnexion se fait entre le câblage vertical et le câblage horizontal. Câblage vertical : tous les liens reliant les salles de télécommunications entre elles, ainsi que les bâtiments et les réseaux extérieurs. Câblage horizontal : Lien entre les salles de télécommunications et chaque connecteur individuel dans les bureaux. Composants utilisateur : câble de raccordement des équipements qui connectent l'utilisateur au câblage horizontal.
CANARIE est un organisme canadien, créé en 1993, à but non lucratif soutenu par ses membres, par ses partenaires de projet et par le gouvernement fédéral, qui déploie, exploite et fait évoluer un réseau de télécommunications à haut débit qui fédère les réseaux scientifiques provinciaux et, par leur entremise, les universités, les instituts de recherche, les laboratoires publics, les écoles ainsi que d'autres sites admissibles. Il les relie également à des réseaux internationaux similaires. Le réseau est appelé CA*Net ou CAnet.
Le Croatian Academic and Research Network ou CARNet est l'organisme responsable du réseau national de la recherche et de l'enseignement de Croatie. Ses principaux partenaires sont le centre de calcul de l'université de Zagreb et la Faculté de génie électrique et d'informatique. L'organisme, fondé en 1991 par le ministère de la Science et de la Technologie, a mis en place la première connexion Internet en Croatie, le 17 novembre 1992. La mission de l'organisme a été confirmée le 1er mars 1995 par un décret du gouvernement croate. La mission du CARNet est de fournir l'infrastructure, les ressources et le personnel nécessaire au développement d'une société de l'information dans le pays. Ses activités comprennent le développement et l'entretien du réseau universitaire national et des interconnexions internationaules ainsi que le développement de l'infrastructure réseautique de la Croatie. CARNet dispose d'une interconnexion internationale de 1,2 Gb/s par le biais du réseau interuniversitaire européen GÉANT2. De plus, il agit en tant que registraire national du domaine de premier niveau .hr, réservé à la Croatie.
Une clé Miracast ou parfois dongle Miracast (en anglais, « Miracast key », « Miracast dongle » ou « WiFi Display Dongle »), est un type de passerelle multimédia, constitué d'un périphérique se branchant sur un port HDMI et permettant de lui transmettre un flux vidéo via la technologie Miracast, utilisant du Wi-Fi Direct pour la diffusion sans fil. De nombreuses versions de ces clés supportent également le standard DLNA et le protocole Air Play d'Apple. Ces clés contiennent, comme les PC-on-a-stick, un SoC d'architecture ARM, et en font donc un ordinateur complet. Si leur but originel se limite à la diffusion d'un flux vidéo et quelques autres fonctions accessoires, plusieurs de ces clés ont été rootées par des hackers (bidouilleurs) pour y installer des systèmes complets. Il existe sur le marché de nombreuses clés depuis 2012, mais elles n'ont pas eu de notoriété avant la sortie de la clé Chromecast de Google, en juillet 2013.
La congestion d'un réseau informatique est la condition dans laquelle une augmentation du trafic (flux) provoque un ralentissement global de celui-ci. Les trames entrantes dans les buffers des commutateurs sont rejetées dans ce cas. La congestion est liée à la politique du multiplexage établie sur le réseau considéré.
Une connexion réseau désigne le fait qu'un équipement informatique (au sens ETTD, c’est-à-dire un ordinateur, une imprimante, etc.) soit rattaché à un réseau informatique. Le terme connexion réseau peut être interprété à différents niveaux. En partant des couches basses du Modèle O.S.I. : Il peut s'agir d'une connexion physique : l'équipement informatique considéré est alors en interaction électrique (sur un câble réseau coaxial ou à paires torsadées) ou électro-magnétique (ondes radio, infrarouge, etc.) avec les autres équipements du réseau. Il peut s'agir d'une connexion logique au sens des protocoles réseaux, c’est-à-dire que l'équipement informatique considéré dispose de la pile de protocoles réseaux nécessaires. En activant ces protocoles, on lui permet de dialoguer avec d'autres équipements en exploitant la connexion physique. Par exemple, l'équipement a alors accès au réseau local, puis en activant des protocoles supérieurs, à un réseau plus étendu, par exemple jusqu'à Internet.
Le couplage téléphonie informatique ou CTI (Computer telephony integration) regroupe l'ensemble des concepts, des standards, des techniques et des fonctionnalités permettant ou facilitant le dialogue entre un appareil informatique et un appareil téléphonique. Ce terme décrit principalement les interactions menées à partir d'un poste de travail dans le but d'améliorer la productivité de l'utilisateur, bien qu'il puisse également décrire des fonctionnalités sur serveur comme le routage automatique des appels. Il s'inscrit dans la logique de convergence numérique. Ce dispositif reliant un centre d'appels interne ou externe au système informatique d'une entreprise, apporte aux sociétés la possibilité d'utiliser les ressources du système d'informations et d'Internet, afin d'améliorer le service rendu aux clients et la productivité d'un centre d'appels. Ceci est rendu possible par l'automatisation de certaines tâches et la possibilité d'instaurer une personnalisation poussée de la relation avec le client grâce aux informations contenues dans les applications métiers CRM & ERP.
Cyclades était un projet expérimental français ayant pour but de créer un réseau global de télécommunication utilisant la commutation de paquets. Créé en 1971, conçu par Louis Pouzin, il fut abandonné en 1978. Ses concepts ont influencé les travaux de développement de l'Internet en inspirant sa suite de protocoles.
Le Data Communication Service ou DCS est le nom commercial en Belgique du réseau de transmission de données par paquets utilisant le protocole X.25 (en France, son homologue porte le nom de Transpac). Initialement développé par l'ex-RTT (Régie des télégraphes et des téléphones) en 1980, ce service continue d'être commercialisé par Belgacom en 2009 et ce pour quelque temps encore. Bien que vétuste et lente, cette technologie se caractérise par son extrême fiabilité ce qui la rend encore très utile pour les transmissions ne nécessitant pas de grandes performances de vitesse (terminaux de paiement, loterie nationale, ligne louées inter-bancaires, balises d'alerte nucléaires, , etc.) tant au niveau national qu'international (X.25 étant un protocole mondialement utilisé). Son exploitation devrait toutefois s'arrêter à l'horizon 2011 pour le trafic international.  Portail de l’informatique  Portail des télécommunications
Le datagramme est un paquet de données dans un réseau informatique ou un réseau de télécommunications. Il est utilisé par des protocoles orientés « non connectés » tels que : IPX ou UDP. Le datagramme est le terme généralement utilisé pour désigner la transmission d'un paquet via un service non « fiable » : le protocole utilisé ne garantit pas que le paquet est arrivé à sa destination (un peu comme une lettre sans accusé de réception). L'inventeur du datagramme est l'ingénieur Louis Pouzin. En informatique, ce concept est également utilisé de manière plus générale pour décrire des blocs de données. Il est notamment utilisé pour décrire un bloc de données transféré à une fonction, dans l'esprit des réseaux, mais appliqué entre processus internes d'un ordinateur. On le retrouve également dans le formatage des fichiers afin de décrire, soit des blocs, soit des mots identifiables. Par exemple, dans les formats d'archivage (tel le TAR), de compression (tel le ZIP) ou d'encapsulation (pour l'audio-vidéo entre autres).
Decnet est une architecture réseau en couches, sur un protocole défini par Digital Equipment Corporation. Les plus malicieux disent "Do Expect Cuts" (attendez-vous à des coupures). DECnet est aussi un groupe de produits de communications de données comportant une suite de protocoles développée et soutenue par Digital Equipment Corporation (Digital). La première version de DECnet, 1975, a permis à deux mini-ordinateurs PDP-11 de communiquer conjointement. Ces dernières années[Quand ?], Digital a ajouté la prise en charge de protocoles non propriétaires, mais DECnet reste la plus importante des offres du réseau de Digital. Ses deux grands concurrents étaient l'IBM Systems Network Architecture et la DSA, de CII-Honeywell-Bull, présentée en 1976 mais intensément développée dès 1971 sous forme de New Network Architecture, avec des liaisons du Réseau cyclades dès 1972.
En informatique et en télécommunications, le deep packet inspection (DPI), en français inspection des paquets en profondeur, est l'activité pour un équipement d'infrastructure de réseau consistant à analyser le contenu (au-delà de l'en-tête) d'un paquet réseau (paquet IP le plus souvent) de façon à en tirer des statistiques, à filtrer ceux-ci, à les prioriser ou à détecter des intrusions, du spam ou tout autre contenu prédéfini. Le DPI peut servir notamment à la censure sur Internet ou dans le cadre de dispositifs de protection de la propriété intellectuelle. Il s'oppose au Stateful Packet Inspection, qui ne concerne que l'analyse de l'en-tête des paquets. Le DPI peut provoquer un ralentissement sensible du trafic là où il est déployé.
On désigne par default-free zone (DFZ) l'ensemble des routeurs et des Autonomous Systems (AS) d'Internet qui n'utilisent pas de route par défaut. Les routeurs de la DFZ disposent d'une table de routage Internet complète, que l'on appelle aussi table de routage globale ou table BGP globale.
Le délai de commutation est le temps nécessaire à un commutateur pour recevoir complètement un paquet et le retransmettre sur un système à commutation de paquets en mode différé (ou mode Store and Forward).  Portail de l’informatique  Portail des télécommunications
La distributed system architecture (DSA) est une architecture de réseau en couches définie par la CII-Honeywell-Bull en 1971, puis rebaptisée DSA en 1976, avant de donner naissance en 1978 au modèle OSI, appelé aussi "OSI-DSA", puis aux protocoles TCP-IP.
La distribution numérique ou distribution en ligne, décrit la fourniture de contenus multimédia, tels la musique, les films, les logiciels ou encore les jeux vidéo, sans l'utilisation d'un support physique conventionnel. La distribution numérique contourne les méthodes classiques de distribution physique, que sont le papier ou le disque optique. Le terme est généralement appliqué aux produits autonomes ; les extensions téléchargeables pour les autres produits sont plus communément connu sous la dénomination contenu téléchargeable. Un service en ligne qui se charge de distribuer des applications est généralement appelé un store ou marché d'applications. Avec l'avancement des capacités de bande passante, la distribution numérique a pris de l'importance dans les années 2000. Le contenu distribué en ligne peut être lu en continu (streaming) ou téléchargé. Le streaming permet la lecture d'un flux en continu à mesure qu'il est diffusé, tandis que la diffusion par téléchargement nécessite de récupérer l'ensemble des données sur le disque dur avant de pouvoir les lire. Des réseaux spécialisés, Content delivery network (CDN), favorisent la distribution numérique sur Internet.
Un domaine de collision est une zone logique d'un réseau informatique où les paquets de données peuvent entrer en collision entre eux, en particulier avec le protocole de communication Ethernet. Un domaine de collision peut être un seul segment de câble Ethernet, un seul concentrateur ou même un réseau complet de concentrateurs et de répéteurs.
En général, un système (ordinateur, réseau) et par extension son utilisation ou ce qu'il contient, est dit en ligne s'il est connecté à un autre réseau ou système (au moyen d'une « ligne » de communication, bien que ce ne soit pas toujours le moyen ; c'est comme « être au téléphone »). Diverses significations plus spécifiques existent : En langage courant, le réseau est en général Internet si bien que « en ligne » décrit un service, une information accessible par Internet ou l'état d'un ordinateur et le fait que son utilisateur y soit connecté, etc. ; L'utilisation de services internet (tels que la lecture et rédaction de courriels) est dite « en ligne » si elle nécessite une connexion (cas de la messagerie web) ou par opposition « hors ligne » si, entre d'éventuels brefs échanges de données en ligne (un courriel par exemple), un programme (client de messagerie) est capable de réaliser ce service de manière locale et autonome sans être constamment connecté à un serveur externe.
Un environnement pervasif (du latin "pervadere" pour "s'insinuer, se propager, s'étendre, envahir,...", également mentionné sous le vocable environnement ubiquitaire) correspond à un fonctionnement global de la communication où une informatique diffuse permet à des objets communicants de se reconnaitre entre eux et de se localiser automatiquement.
Les équipements d’interconnexion d'un réseau informatique sont les briques constitutives des réseaux informatiques physiques. L’interconnexion des réseaux c’est la possibilité de faire dialoguer plusieurs sous réseaux initialement isolés, par l’intermédiaire de périphériques spécifiques (récepteur, concentrateur, pont, routeur, modem), ils servent aussi à interconnecter les ordinateurs d’une organisation, d’un campus, d’un établissement scolaire, d’une entreprise. Il est parfois indispensable de les relier. Dans ce cas, des équipements spécifiques sont nécessaires. Lorsqu’il s’agit de deux réseaux de même type, il suffit de faire passer les trames de l’un vers l’autre. Dans le cas de deux réseaux qui utilisent des protocoles différents, il est nécessaire de procéder à une conversion de protocole avant de transporter les trames (paquet des données).
Euronet est un réseau privé X.25 similaire à celui appelé Transpac, utilisé entre banques de données scientifiques.[réf. nécessaire]   Portail des télécommunications
EARN (European Academic and Research Network) est un réseau informatique fondé en 1984 et basé à Paris, qui avait pour fonction de service de passerelle entre les centres de calcul académiques entre l'Europe et les États-Unis. Il partageait le même protocole et l'espace de nommage de BITNET. En 1995, EARN fusionne avec RARE pour former TERENA.
Fibre to the office ou Fiber to the office (FTTO) est un concept de câblage LAN en environnement bureautique. Cette solution combine des éléments passifs (câblage fibre optique, panneaux de brassage, boîtes d'épissures, connecteurs et cordons cuivre standards 8P8C) à des mini-commutateurs actifs (aussi appelés FTTO switches) pour fournir du Gigabit Ethernet aux terminaux. Le FTTO fait appel aux techniques de câblage centralisé en Fibre Optique pour créer un canal mi-vertical, mi-horizontal; ce canal court des postes de travail au répartiteur central et permet l'emploi de câbles pull-through ou à épissure dans le local technique.  Ce concept est décrit dans ISO/IEC 11801, et TIA/EIA-568-B3,.
Dans un réseau commuté Ethernet, la méthode Fragment free est une méthode de transmission des trames prise en charge par les commutateurs. Cette méthode bloque la trame reçue jusqu'à ce que les 64 premiers octets aient été lus depuis la source afin de détecter une collision avant de transmettre la trame sur les autres ports. Cela n'a un sens que s'il est possible d'avoir une collision sur le port source. Fragment free n'a donc pas d'intérêt sur un commutateur travaillant en full duplex et le procédé cut through sera plus avantageux en termes de latence. Dans la méthode Fragment free, les trames commencent à être retransmises avant qu'une somme de contrôle n'ait pu être calculée. Cette technique peut être vue comme un compromis entre le mode différé (store and forward), qui présente une latence élevée et une forte intégrité, et le mode direct (cut through), caractérisé par une latence minimale et une faible intégrité.
Frognet est un Réseau social francophone développé sur l'Internet dans les années 1991-1995. Il est sans doute un des premiers réseaux de ce type lancé sur l'Internet.
GÉANT est une association issue de la fusion de DANTE et de TERENA. DANTE (Delivery of Advanced Network Technology to Europe) était une organisation à but non lucratif établie à Cambridge en Angleterre en 1993 et dont la mission est de définir, bâtir et gérer des réseaux de recherche et d’éducation pour le compte des réseaux nationaux pour la recherche et l’éducation (National Research and Education Networks, NREN) en Europe. En octobre 2014, DANTE et TERENA fusionnent pour former l'association GÉANT.
GÉANT2 (Gigabit European Advanced Network Technology 2) est un réseau à très haut débit destiné à la recherche et à l'éducation en Europe. Il est le successeur du réseau pan-européen GÉANT qui en est aujourd'hui (octobre 2006) à sa septième génération. Avec plus de 30 millions d’utilisateurs issus de 34 pays européens, GÉANT2 propose une couverture géographique inégalée, des services de bande passante haut débit, une technologie réseau hybride et des services utilisateurs. Sa couverture étendue interconnecte plusieurs continents. Le projet est mené par un consortium constitué de 32 NREN (National Research and Education Network, réseaux nationaux pour l'enseignement et la recherche ; exemple : le réseau RENATER pour la France) et de l'association TERENA (Trans-European Research and Education Networking Association), la coordination du projet étant assurée par DANTE. DANTE (Delivery of Advanced Network Technology to Europe) est l’organisation qui vise à la mise en place de réseaux de recherche partout dans le monde. Le projet GÉANT2 (nom de code : GN2) est cofinancé par la Commission européenne selon le sixième Programme cadre pour la recherche et le développement. Il a commencé officiellement le 1er septembre 2004 et doit durer quatre ans.
Le Global Information Grid (GIG) est un réseau de transmission et de traitement de l'information maintenu par le département de la Défense des États-Unis (DoD). De façon plus descriptive, le GIG est un réseau mondial de transmission d'information, des procédés associés et du personnel servant à recueillir, à traiter, à sauvegarder, à transmettre et à gérer cette information. Le GIG la rend immédiatement disponible aux personnels militaires, aux responsables des politiques militaires et aux personnels de soutien. Il inclut toutes les infrastructures, achetées ou louées, de communications, électroniques, informatiques (incluant les logiciels et les bases de données) et de sécurité. Il est la manifestation la plus visible du network centric warfare (NCW). Les spécifications du GIG ont été publiées par DoD le 22 septembre 1999 et le secrétaire adjoint à la Défense l'a officiellement mandaté le 19 septembre 2002 pour réaliser ce projet. Bien qu'en 2008, les objectifs ambitieux du programme n'aient pas été réalisés, des communications informatiques entre les soldats et leurs commandants sur le champ de bataille ont été réalisés avec succès, les exemples les plus connus étant survenus pendant l'invasion de l'Irak de 2003.
L'HOST ID est la partie restante de l'adresse IP, située à droite du NET ID, qui désigne une machine sur le réseau. Si tous les bits de l'HOST ID sont à zéro, l'adresse désigne le réseau lui-même. Si tous les bits sont à 1, il s'agit de l'adresse de broadcast de ce réseau (diffusion). Ces deux adresses ne peuvent être utilisées pour désigner une machine.
L'hyper-convergence (ou hyperconvergence) est un type d'architecture informatique matérielle qui intègre de façon étroitement liée les composants de traitement, de stockage, de réseau et de virtualisation. L'hyper-convergence permet une consolidation importante au niveau des centres informatiques.
Systems network architecture (SNA) est une architecture réseau en couches définie par IBM en 1974. SNA est désormais largement remplacé par le modèle OSI, mis en place en 1978.
Le concept d’Industrie 4.0 ou industrie du futur correspond à une nouvelle façon d’organiser les moyens de production. Cette nouvelle industrie s'affirme comme la convergence du monde virtuel, de la conception numérique, de la gestion (finance et marketing) avec les produits et objets du monde réel. Les grandes promesses de cette quatrième révolution industrielle sont de séduire les consommateurs avec des produits uniques et personnalisés, et malgré de faibles volumes de fabrication, de maintenir des gains. Ces mêmes consommateurs peuvent ainsi communiquer avec les machines durant les phases de réalisation, ce type de production s'appelle la « Smart Product ». Selon ce principe, dans le contexte de l’automatisation industrielle, cela se caractérise par la mise en œuvre de capteurs qui sont les éléments de base des système d'acquisition et de contrôle de données (SCADA). Ils permettent de transformer des grandeurs physiques (température, pression, position, concentration, autres…) en signaux, le plus souvent électriques, qui renseignent sur ces grandeurs. Ces capteurs permettent aux robots d'une chaîne de production de dialoguer et d'adapter l'outil de production aux différents besoins, de manière non exhaustive, les maintenances, les besoins des marchés ou les modifications des clients. Outre les aspects technologiques, cette quatrième révolution industrielle influe sur différents aspects de nos sociétés modernes. De nouveaux enjeux apparaissent au travers de cette nouvelle manière de produire. L'industrie 4.0 touche évidemment l'aspect économique mais a également des impacts sociaux, politiques ou environnementaux. Il pose la question de l'emploi de millions de salariés à travers le monde. En effet, l'accompagnement des salariés actuels et la formation des futurs salariés sont à prendre en compte. Il semble difficile d'envisager que des millions de travailleurs se retrouvent sans emploi. Plus généralement, il est nécessaire de réfléchir à la place de l'humain dans cette industrie 4.0.
Infrastructure convergée , ou Infrastructure convergente , traductions du terme Converged infrastructure en anglais, regroupe plusieurs composants informatiques en une seule solution optimisée. Les composants d’une infrastructure convergée peuvent être des serveurs, des dispositifs de stockage de données, des équipements réseau et des logiciels pour la gestion des infrastructures, pour l'automatisation et pour l’orchestration. Les infrastructures convergées sont utilisées par les départements informatiques pour centraliser la gestion des ressources informatiques, consolider les systèmes, accroître les taux d’utilisation des ressources, et réduire les coûts d’exploitation. Ces objectifs sont rendus possibles par la création de pools d'ordinateurs, d’équipements réseaux et de stockage qui peuvent être partagés par plusieurs applications et gérés de manière collective, en utilisant des procédés respectant des politiques données. Les fournisseurs et analystes de l'industrie informatique utilisent différents termes pour décrire le concept d’infrastructure convergente. Ceux-ci incluent système convergé, fabric computing, unified computing, fabric-based computing, et infrastructure dynamique. Le marché des infrastructures convergées est vaste et en pleine croissance. Il implique les grands acteurs comme Cisco, Dell, HP, IBM, Intel, Microsoft, NetApp, Oracle, et VMware. La valeur du marché exploitable total (TAM) en 2011 est de 323 milliards de dollars US, et sa prévision pour 2017, selon le cabinet Wikibon en août 2012 est de 408 milliards de dollars US.
En informatique, la latence (ou délai de transit, ou retard) est le délai de transmission dans les communications informatiques (on trouve parfois l’anglicisme lag). Il désigne le temps nécessaire à un paquet de données pour passer de la source à la destination à travers un réseau. À n'importe quel paquet transmis par réseau correspond donc une valeur de latence. Le terme est néanmoins utilisé pour désigner les délais plus longs, perceptibles par les utilisateurs. On parle aussi de latence pour le temps d'accès à une information sur une mémoire ou un système de stockage (disque ou bande magnétique).
LibreCMC est un système GNU/Linux embarqué pour du matériel informatique exigeant un minimum de ressources, comme les routeurs wifi, les ordiphones ou le mini-ordinateur Ben Nanonote (en). Il est basé sur OpenWrt et constitué exclusivement de logiciels libres. Un projet analogue appelé LibreWRT et soutenu en son temps par la Free Software Foundation a été listé par le site prism-break.org comme l'une des alternatives aux firmware propriétaires. Les deux projets LibreWRT & libreCMC ont fusionné en mai 2015 sous le nom libreCMC.
Le listener, en français écouteur ou encore auditeur, est un terme anglais utilisé de façon générale en informatique pour qualifier un élément logiciel qui est à l'écoute d'évènements afin d'effectuer des traitements ou des tâches. On distingue notamment : l'écoute sur des serveurs informatiques ; l'écoute en programmation événementielle
En informatique, on travaille souvent en mode client-serveur : une ou plusieurs machines envoient des requêtes à un serveur central qui envoie les réponses appropriées. C'est par exemple le cas d’un serveur web ou d’un serveur de bases de données. Lors de la phase de développement d’un programme informatique impliquant des échanges sur un réseau, il n’est pas pour autant nécessaire de disposer de plusieurs machines physiques ni même virtuelles : la même machine physique peut parfaitement héberger le serveur et un ou plusieurs clients, exactement dans les mêmes conditions : en communiquant par des ports. Dans le domaine des réseaux informatiques, localhost (l’hôte local en français) est le nom habituel qui désigne une interface logique de l’ordinateur local. Le ou les clients hébergés sur une machine utilisent le protocole IP pour communiquer. Il importe peu de savoir où se trouvent physiquement les programmes, les couches basses du protocole se chargeant justement d’en masquer les détails. Le nom localhost est associé à l’adresse IPv6 ::1 et à la plage d’adresses IPv4 127.0.0.0/8 (toutes les adresses IPv4 comprises entre 127.0.0.1 et 127.255.255.255 dont la plus utilisée est 127.0.0.1). L’interface réseau virtuelle utilisée dans cette situation se nomme l’interface de loopback (abrégée par lo sous Unix) ou boucle locale.
CFT (Cross File Transfer) est un logiciel de transfert de fichiers développé par la société Axway, créé par Crédintrans puis cédé à Axway. On parle également de "moniteur" de transfert de fichier. CFT et Inter.Pel(Pelican), sont désormais connus sous les noms de Axway Transfer CFT, Axway Transfer InterPEL, anciennement appelé XFB (Axway File Broker). CFT est utilisé dans les systèmes informatiques complexes et multi plate-formes qui souhaitent transmettre des fichiers en appliquant des contrôles (sécurité, accusé de réception, reprise après interruption, traçage des erreurs). Historiquement, CFT utilisait le protocole X.25 notamment sur le réseau Transpac. Depuis 1992, Transfer CFT fonctionne sur TCP/IP et est donc utilisé sur n'importe quel réseau IP (par exemple Internet) et ce de manière sécurisée. CFT utilise des protocoles d'échanges comme PESIT, ETEBAC3 (protocoles français utilisés pour les échanges bancaires), mais aussi OFTP (protocole du secteur automobile) ce qui le rend compatible avec d’autres produits comme Inter.Pel, Connect:Express, Pelican, etc.
La loi de Metcalfe est une loi théorique et empirique de l'effet de réseau énoncée par Robert Metcalfe (fondateur de la société 3Com et à l'origine du protocole Ethernet).  L’utilité d’un réseau est proportionnelle au carré du nombre de ses utilisateurs.
La loi de Sarnoff postule que la valeur d'un réseau de diffusion (radio, TV) est proportionnelle au nombre de ses utilisateurs (auditeurs ou téléspectateurs). Elle a été attribuée à David Sarnoff.
Un rebouclage, loopback ou loop-back (de l'anglais signifiant "boucle arrière") est un système matériel ou logiciel en informatique, réseaux ou télécommunications, destiné à renvoyer un signal reçu vers son envoyeur sans modification ni traitement, et qui peut par exemple être utilisé à des fins de tests.
Le réseau Mbone (pour Multicast Backbone) est un réseau virtuel expérimental pour la transmission du trafic multicast IP développé au début des années 1990. Il a été inventé par Van Jacobson, Steve Deering et Stephen Casner en 1992. Le but du Mbone est de transmettre de façon efficace, en temps réel et de manière symétrique, des flux de nombreux utilisateurs répartis. Il utilise généralement des routeurs dédiés. La plupart des routeurs Internet n'étant pas configurés pour le support du multicast, le Mbone a évolué en un réseau permettant les échanges de trafic multicast entre les participants. La commercialisation du service multicast s'est avérée difficile en raison du manque de contrôle des accès au contenu, conséquence directe de la symétrie du protocole. En 1994, un concert des Rolling Stones est retransmis via le Mbone. Un an plus tard le Mbone est utilisé, cette fois de manière symétrique (émission et réception simultanés sans hiérarchie entre participants), pour une première expérience d'interaction graphique temps réel sans l'intermédiaire d'aucun centre (Générateur Poïétique). Le réseau s'étend à l'Antarctique et à la Russie dès 1995. Une grande variété d'applications collaboratives ont été développées spécialement pour le Mbone, vidéoconférence, chat, tableaux partagés. Actuellement, le Mbone est surtout utilisé par les réseaux de recherche. Les protocoles de routage utilisés sont DVMRP à l'origine, puis MOSPF.
Dans un protocole de routage, la métrique est une mesure de la « distance » qui sépare un routeur d'un réseau de destination. Elle peut correspondre : au nombre de sauts IP nécessaires pour atteindre le réseau destination, comme dans RIP ; à un coût numérique qui dépend de la bande passante des liens franchis, comme dans OSPF ; ou au résultat d'un calcul plus complexe, qui tient compte de la charge, du délai, du MTU, etc. NB : Le Protocole EIGRP (propriétaire Cisco) n'utilise pas MTU pour le calcul des métriques.. Quand plusieurs chemins vers une même destination sont possibles, le protocole préférera celui dont la métrique est la plus faible.
Mikrotīkls Ltd., connu sous l’appellation internationale MikroTik, est un fabricant de matériel de réseau informatique basé en Lettonie. Il vend des composantes de réseau sans fil et des routeurs. La compagnie fut fondée en 1996, se concentrant à la vente de produits de réseautique sans fil sur les marchés émergents. En 2016, elle comptait 140 employés.
Un miroir de port, appelé également port d'écoute ou port mirroring et port monitoring en anglais, est une fonction supplémentaire de certains commutateurs réseau. Cette fonction de surveillance permet de copier des paquets transitant par le commutateur réseau, configuré pour cet usage, vers un port de destination choisi.
Mnet est un logiciel de réseau informatique décentralisé dans lequel on peut stocker des fichiers (système de fichiers distribué). Les fichiers restent disponibles une fois que la personne les ayant déposé est déconnectée du réseau. Mnet est également un réseau émergent. C’est-à-dire qu'il permet de relier différents nœuds n'ayant aucune relation entre eux et de les faire travailler de concert. La première application ayant vu le jour sur la base de Mnet logiciel d'échange de fichiers.
Le mode connecté est l'établissement d'une session de communication entre deux parties qui veulent échanger des données. Cette session comporte un début, une fin et une validation (vérification des erreurs), classiquement un identifiant un mot de passe. Elle correspond à la couche numéro 5 dite "session" du modèle OSI, entre les couches "présentation" et "transport". Le protocole TCP se base sur le mode connecté.
En mode différé (ou mode Store and Forward), un commutateur attend d'avoir complètement reçu et analysé une trame de données sur l'une de ses interfaces d'entrée pour commencer sa retransmission sur une ou plusieurs de ses interfaces de sortie. Ce mode impose la mise en mémoire tampon de la totalité de la trame et retarde sa transmission. En contrepartie, cela garantit l'intégrité de la trame et évite les erreurs. Cela implique aussi un délai de commutation plus important.
Myrinet (norme ANSI/VITA 26-1998) est un protocole réseau haute-vitesse conçu par Myricom, afin d'être utilisé comme système d'interconnexion de plusieurs machines formant une grappe (ou cluster). Myrinet cause beaucoup moins de charge réseau consacré à son propre protocole de communication que des protocoles plus utilisés tels qu'Ethernet, et dès lors propose un débit binaire plus important, moins d'interférences et moins de latence quand il utilise le processeur système. Bien qu'il puisse être utilisé comme un protocole réseau traditionnel, Myrinet est souvent utilisé de manière directe par des programmes qui savent l'utiliser directement, annulant ainsi le besoin d'appels systèmes. Physiquement, Myrinet fonctionne avec deux câbles en fibre optique, une pour l'envoi de donnée, et l'autre pour la réception, chacun connectés à une machine via un unique connecteur. Les machines en question sont reliées les unes aux autres via des routeurs et des commutateurs réseau à faible latence (les machines ne sont donc pas directement connectées les unes aux autres). Myrinet propose en outre quelques fonctionnalités améliorant la tolérance aux erreurs, principalement gérées par les commutateurs réseau. Ces fonctionnalités incluent le contrôle de flux, le contrôle d'erreur, et la surveillance d'état de chaque connexion physique. La quatrième et dernière version de Myrinet, également appelée Myri-10G, supporte un débit de 10 Gb/s et est interopérable du point de vue physique avec la norme Ethernet 10 Gb/s (câbles, connecteurs, distance, type de signal). Myri-10G est disponible depuis la fin 2006.
Le nanoréseau est un système permettant de connecter jusqu'à 31 micro-ordinateurs Thomson (TO7, TO7/70, MO5, etc.), appelés « nanomachines », avec une machine plus puissante appelée « tête de réseau » ; les nanomachines bénéficient ainsi des capacités supérieures de la tête de réseau (notamment l'accès à des disquettes et à une imprimante). En France, à la suite du plan « Informatique pour tous » (plan IPT), le nanoréseau a été mis en place en 1985 dans les lycées, les collèges et dans une école sur cinq.
Le NET ID est, dans une adresse IP (IPv4), le ou les octets situés en début d'adresse qui identifient le réseau, par opposition à l'HOST ID qui identifie la machine dans ce réseau.
Dans un réseau d'ordinateurs, en particulier pour IRC (Internet Relay Chat), le terme anglais netsplit désigne une déconnexion d'un nœud du réseau auquel il était connecté au préalable.
Le network allocation vector (NAV) est un mécanisme d'écoute virtuelle de support utilisé par les protocoles de réseaux sans fils tels que IEEE 802.11 et IEEE 802.16 (WiMax). L'écoute virtuelle du support est une abstraction logique limitant la nécessité d'écoute d'un récepteur et permettant par la même occasion d'économiser de l'énergie. Les entêtes de trames de la couche MAC[incompréhensible] contiennent un champ durée spécifiant le temps requis de transmission pour celle-ci, durant lequel le moyen sera occupé. Les stations écoutant le canal lisent cette durée et détermine une NAV en conséquence. Cette NAV est une durée pendant laquelle une station ne doit pas accéder au medium de communication. Le NAV peut être vu comme un compteur comptant à rebours. Le compteur arrivé à zéro indique que le canal est inactif et lorsqu'il est différent de zéro, qu'il est actif.
Le network centric warfare est un concept popularisé sous le nom de guerre en réseau et qui est apparu à la fin du XXe siècle dans les doctrines militaires. Terme d'origine américaine, il décrit une manière de conduire des opérations militaires en exploitant les capacités des systèmes d'informations et des réseaux disponibles à cette époque. L'évolution principale par rapport aux doctrines antérieures concerne le partage de l'information. Il s'agit de la capacité de relier entre elles les différentes armées (terre, marine, air) ainsi que les armées de pays alliés, de récupérer des informations grâce à des drones, des satellites, de les diffuser en temps réel aux unités afin de frapper plus vite et plus précisément. Le concept network centric s'est élargi depuis plusieurs années au domaine civil aux États-Unis, et appuie aujourd'hui l'influence américaine dans les grandes entreprises et organisations internationales.
La New Network Architecture désigne l'un des premiers ensemble de produits pour le transport des données entre ordinateurs à avoir été conçus à des fins de téléinformatique, pour le compte de la Compagnie internationale pour l'informatique dans les années 1970.
North American Network Operators' Group (NANOG) est un forum d'opérateurs de réseaux nord-américains dont le but est la diffusion d'information techniques et la coordination entre opérateurs. NANOG se matérialise par des rencontres et dispose d'une liste de diffusion influente dans le monde des gestionnaires de réseau, à l'instar du RIPE en Europe et du FRnOG en France.
La NumALAT ou Numérisation de l'Aviation Légère de l'Armée de Terre (en France) est l'intégration des unités de l'ALAT dans le cadre plus global de la numérisation de l'espace de bataille (NEB), déclinaison du concept de Network Centric Warfare. Outre les aspects techniques, il s'agit de proposer aux équipages une image situationnelle[Quoi ?] commune de l'espace de bataille en implémentant des Systèmes d'Information Spatio-Temporels (SIST) dans les ordinateurs de bord des hélicoptères et aéronefs utilisés par l'ALAT.
La Numérisation de l'Espace de Bataille (NEB) permet, à l'armée de terre française, d'obtenir, de diffuser et de traiter, en temps réel, les informations nécessaires aux différents échelons de commandement sur le champ de bataille. Il s'agit d'une déclinaison du concept de Network Centric Warfare. C'est un système d'aide à la prise de décision destiné à renseigner les chefs (de tous les niveaux) sur la situation actuelle et à venir.
OpenFlow est un protocole réseau standard qui permet de réaliser une architecture Software-defined networking (SDN). Il est publié par l'Open Networking Foundation (en) (ONF). Ce protocole est constitué d'instructions qui permet de programmer le plan de contrôle d’un équipement réseau. Une instruction est définie par un pattern (IP source ou destination, adresse MAC, port, etc.) et une action correspondante (transmettre sur un port, rejeter le paquet, etc.).
OpenID Connect (OIDC) est une simple couche d'identification basée sur OAuth 2.0, un dispositif d'autorisation. Ce standard est géré par la fondation OpenID.
OpenWrt est une distribution GNU/Linux minimaliste pour matériel embarqué (routeurs, tablettes, téléphones…). Historiquement développée pour remplacer le firmware des routeurs basés sur des systèmes sur une puce Broadcom (par exemple les routeurs WLAN d'Asus, Belkin, Dell, Linksys, US Robotics, Viewsonic), OpenWrt fournit une interface Web avec le firmware Whiterussian (webif) et Kamikaze via les projets X-Wrt (en) et LuCI.
Dans les télécommunications, l'orienté connexion décrit un moyen de transmettre des données dans lequel : les appareils à chaque extrémité, utilisent un protocole préliminaire pour établir une connexion de bout en bout avant que les données soient envoyées ; les données sont envoyées le long d'un même chemin pendant la communication. Le service offert par un protocole orienté connexion est souvent, mais pas toujours, un service réseau « fiable », qui fournit la garantie que les données arriveront dans le bon ordre. Concernant les communications en mode circuit, le réseau de téléphone public, RNIS, SDH/SONET et DWDM sont des exemples de communications orientées connexion. Les communications en mode paquet peuvent aussi être orientées connexion, et sont appelées des communications en mode circuit virtuel. Un protocole de commutation de paquets orienté connexion n'a pas à associer d'informations de routage (les adresses complètes de source et de destination), mais seulement le numéro de canal/flux de données, souvent appelé l'identifiant de circuit virtuel (Virtual Circuit Identifier - VCI). L'information de routage peut être fournie aux nœuds réseau pendant la phase d'établissement de la connexion, où le VCI est alors défini dans les tables de routage de chaque nœud. L'alternative à la transmission orienté connexion est la communication sans connexion en mode paquet, aussi connu sous le nom de communication datagramme, dans laquelle les données sont envoyées d'un point à un autre sans arrangement à l'avance, et aucune garantie n'est fournie. Dans la commutation de datagrammes, chaque paquet de données doit contenir des informations complètes d'adresse, du fait que les paquets sont routés individuellement. Les paquets peuvent être délivrés par différents chemins et sans aucune garantie, suivant une politique « au mieux » (best-effort). Les protocoles sans connexion sont généralement décrits comme étant sans état (stateless) car les points de terminaison n'ont pas de façon, définies par protocole, de se souvenir où ils en sont dans leur « conversation » par échange de messages. Grâce à leur capacité à suivre une conversation, les protocoles orientés connexion sont parfois qualifiés de « protocoles à état » (stateful).
Dans le contexte d'un réseau informatique, le paquet est l'entité de transmission de la couche réseau (couche 3 du modèle OSI).
Le partage généralisé du processeur ou PGP (en anglais : Generalized processor sharing, abrégé GPS) est un algorithme d'ordonnancement idéal pour les ordonnanceurs de paquets. Il est en rapport avec le principe de file d'attente équitable, qui consiste à grouper les paquets dans des classes, et à partager les capacités du serveur entre elles. Le PGP distribue cette capacité en fonction de certains coefficients de pondération fixés. En ordonnancement, le partage généralisé du processeur est « un algorithme  d'ordonnancement idéalisé qui permet d'obtenir une équité parfaite. Tous les ordonnanceurs, en pratique, approximent un PGP et l'utilise comme référence pour mesurer l'équité. » Le partage généralisé du processeur suppose que le trafic est fluide (que la taille des paquets est infinitésimale) et donc peut être arbitrairement divisé. Il existe plusieurs techniques qui permettent de quasiment atteindre les performances du PGP, comme la théorie des files d'attente pondérées équitables (weighted fair queueing, WFQ), également connu comme le partage généralisé paquet par paquet du processeur (packet-by-packet generalized processor sharing, PGPS).
En informatique, une passerelle (en anglais, gateway) est le nom générique d'un dispositif permettant de relier deux réseaux informatiques de types différents, par exemple un réseau local et le réseau Internet. Ainsi, un répéteur est une passerelle de niveau 1, un pont une passerelle de niveau 2 et un relais, souvent appelé routeur, une passerelle de niveau 3. Cependant, le terme passerelle désigne plus couramment le modem-routeur ou box qui permet de relier un réseau local au réseau Internet. Une passerelle effectue donc le routage des paquets mais peut également effectuer des traitements plus évolués sur ceux-ci. Le plus souvent, elle sert également de pare-feu, de proxy, effectue de la qualité de service, etc. Par exemple, dans le routage (IGP et BGP par exemple), le terme de passerelle par défaut (default gateway) est utilisé alors qu'il s'agit de routage au niveau IP ; il est donc utile de préciser dans ce cas que l'on parle de passerelle applicative pour éviter toute ambiguïté.
Une passerelle multimédia est un appareil électronique permettant, au moyen d'une connexion à un réseau informatique par câble (Ethernet) ou sans-fil (Wi-Fi), d'accéder à des fichiers multimédias (audio, photos et vidéo) stockés dans un ordinateur ou un serveur informatique, pour les lire sur des équipements plus adaptés à ce type de divertissements : une chaîne hi-fi, une télévision, voire un home cinema.
Le Piazza telematica (en français : Place télématique) est un terme employé pour la première fois durant la coupe du monde de football Italia 1990 afin de décrire les centraux informatiques établis pour les journalistes dans chacune des 12 villes italiennes accueillant les tournois de qualification. Quelques années plus tard, en novembre 1993, trois ingénieurs, trois architectes, un journaliste et un étudiant universitaire ont fondé Piazze telematiche (Places télématiques), une association à but non lucratif favorisant la planification et la réalisation d'un réseau de Piazze Telematiche, un pour chacune des 8 100 communes italiennes. En 1997 l'Union européenne, à travers sa division XVI pour la cohésion sociale et le développement soutenable, a approuvé le placement d'un Projet Pilote Urbain (PPU) de Piazza Telematica à Naples dans le quartier de Scampia. L’ouverture au grand public eu lieu en décembre 2004. Actuellement Piazza Telematica est employé : pour identifier de nouveaux endroits en ville équipés d’outils de NTIC, comme c’est le cas pour la Piazza Telematica de Scampia ou pour la Piazza telematica de l'université de Rome III. pour évoquer sur Internet un espace virtuel ouvert à tout citoyen pour un échange d’idées, de propositions et plus généralement de la participation à la vie sociale, culturelle et économique de la communauté, comme pour le cas Piazza Telematica de la municipalité Provaglio d'Iseo.
Dans le domaine du routage le plan de commutation parfois appelé plan de données (Forwarding Plane ou Data Plane en anglais) définit la partie de l'architecture d'un routeur qui décide ce qu'il faut faire avec les paquets arrivant sur une interface d'entrée. Le plus souvent, il se réfère à un tableau dans lequel le routeur recherche l'Adresse de destination contenue dans le paquet entrant et récupère les informations nécessaires pour déterminer le chemin, depuis l'élément récepteur, à travers la structure de commutation interne du routeur, jusqu'à l'interface(s) de sortie appropriée(s).
Dans le domaine du routage, le plan de contrôle (control plane an anglais) est la partie de l'architecture du routeur qui est concerné par l'élaboration de la topologie réseau ou les informations d'une table de routage (éventuellement augmentée) qui définit ce qu'il faut faire avec les paquets entrants. Les fonctions du plan de contrôle, comme la participation à des protocoles de routage, tourne dans l'architecture plan de contrôle. Dans la plupart des cas, la table de routage contient une liste d'adresses de destination et l'interface(s) de sortie(s) associée(s). La logique du plan de contrôle peut également définir que certains paquets doivent être rejetés, ou au contraire doivent être traités de manière préférentielle afin d'obtenir un haut niveau de qualité de service. Ceci est permis grâce à des mécanismes de différenciation de services. En fonction de la mise en œuvre spécifique du routeur, il peut y avoir une base d'informations de transmission distincte qui est peuplée (c'est-à-dire chargée) par le plan de contrôle, mais utilisée par le plan de commutation pour rechercher les paquets, à très grande vitesse, et de décider comment les gérer.
Un Point d'accès réseau ou PA (ou AP pour Access Point en anglais), dans le domaine des réseaux de télécommunications, est un endroit équipé d'une structure ad-hoc permettant de se connecter matériellement à un réseau filaire ou radio, en suivant une procédure logicielle à l'aide d'un terminal.
Dans la suite des protocoles Internet et correspondant à la couche de transport du modèle OSI, la notion de port logiciel permet, sur un ordinateur donné, de distinguer différents interlocuteurs. Ces interlocuteurs sont des programmes informatiques qui, selon les cas, écoutent ou émettent des informations sur ces ports. Un port est distingué par son numéro. Le terme port est aussi parfois utilisé pour désigner les interfaces de connexion, un concept sensiblement différent.
Project 25 (P25) ou APCO-25 désigne un ensemble de normes de télécommunications pour les radio numériques en usage parmi les personnels de sécurité en Amérique du Nord, que ce soit des policiers ou des pompiers. Utilisé aux niveaux fédéral, provincial (au Canada), de l'État (aux États-Unis) et municipal, il permet aux personnels de communiquer avec tout professionnel dans le cas d'urgence ou d'une demande d'aide. Il ressemble beaucoup au système européen TETRA.
Un protocole orienté bit (ou BOP, pour bit-oriented protocol), aussi appelé protocole orienté binaire, est un protocole de communication qui transmet les flux de données comme un ensemble de bits sans sémantique (il n'a pas de notion de caractère, de signification).
Un protocole réseau orienté connexion est un protocole qui livre un flux de données dans le même ordre que celui dans lequel il a été envoyé, après avoir d'abord établi une session de communication. Cela peut être une connexion de type commutation de circuits, ou de type circuit virtuel dans le cas d'un réseau de commutation de paquets. Dans ce dernier cas, il identifie les flux de trafic par un identifiant de connexion plutôt que par l'utilisation explicite des adresses source et destination. Typiquement, cet identifiant de connexion est un nombre entier (10 bits pour Frame relay, 24 bits pour ATM par exemple). Cela permet aux commutateurs réseau d'être beaucoup plus rapide (puisque les tables de routage ne sont que de simples tables de correspondance, et il est donc facile de les implémenter au niveau du matériel). L'impact est si grand, en fait, que des protocoles à l'origine sans connexion, comme le trafic IP, sont étiquetés par des entêtes orientées connexion (comme avec MPLS ou le champ Flow ID existant en IPv6). Un exemple de protocole orienté connexion au niveau de la couche transport est le protocole TCP. Les protocoles orientés connexion ne sont pas nécessairement des protocoles fiables. ATM et Frame Relay, par exemple, sont deux exemples de protocoles orientés connexion non fiable. Il y a aussi des protocoles fiables sans connexion, comme AX.25 quand il passe les données dans des trames I. Mais cette combinaison est rare, et le fiable/sans connexion est inhabituel dans les réseaux commerciaux et académiques. Les protocoles orientés connexion supportent beaucoup plus efficacement le trafic temps réel que les protocoles sans connexion, c'est pourquoi ATM est remplacé par des protocoles Ethernet avec connexion (TCP, SCTP) pour porter les flux de trafic isochrone temps-réel, en particulier dans des réseaux fortement agrégés comme les dorsales (backbones), où la devise "la bande passante est peu chère" n'arrive pas encore à tenir ses promesse[réf. souhaitée]. Des protocoles orientés connexion ont été conçus ou modifiés pour s'adapter à la fois aux données orientés connexion et sans connexion.
Un protocole propriétaire est un protocole de communication dont les spécifications ne sont pas publiques, à la manière d'un logiciel propriétaire. Les protocoles de communication des réseaux MSN et Skype sont des exemples de protocoles propriétaires. La conséquence directe est de fausser la libre concurrence entre créateurs de logiciels utilisant ces protocoles. En effet, celui qui a créé le protocole devient la référence, et peut le modifier à sa guise. Les autres créateurs de logiciels sont donc toujours en retard sur la gestion des nouveautés du protocole, car ils doivent les deviner (souvent à partir de techniques de rétro-ingénierie). L'utilisation d'un protocole propriétaire va à l'encontre de la notion d'interopérabilité.   Portail des télécommunications
La qualité de service (QDS) ou quality of service (QoS) est la capacité à véhiculer dans de bonnes conditions un type de trafic donné, en termes de disponibilité, débit, délais de transmission, gigue, taux de perte de paquets… La qualité de service est un concept de gestion qui a pour but d’optimiser les ressources d'un réseau (en management du système d'information) ou d'un processus (en logistique) et de garantir de bonnes performances aux applications critiques pour l'organisation. La qualité de service permet d’offrir aux utilisateurs des débits et des temps de réponse différenciés par applications (ou activités) suivant les protocoles mis en œuvre au niveau de la structure. Elle permet ainsi aux fournisseurs de services (départements réseaux des entreprises, opérateurs…) de s’engager formellement auprès de leurs clients sur les caractéristiques de transport des données applicatives sur leurs infrastructures IP.
Queries per second (anglais : « requêtes par seconde »), abrégé en QPS, est un terme informatique correspondant au nombre de requêtes qu'un serveur informatique accepte de traiter par seconde.  Portail de l’informatique
Réseaux Associés pour la Recherche Européenne (ou RARE) était une organisation de recherche fondée en 1986 par plusieurs gestionnaires de réseaux européens afin de promouvoir les standards réseau ouverts (et plus spécifiquement les protocoles OSI). En 1995, RARE a été fusionné avec EARN pour former TERENA.
Le rate limiting est utilisé pour contrôler le débit du trafic envoyé ou reçu sur une interface réseau. Le trafic inférieur ou égal à la limite spécifiée est envoyé, tandis que le trafic excédant cette limite est rejeté ou retardé. Un appareil qui remplit cette fonction est un limiteur de débit. Le rate limiting est effectué par traffic policing (rejet des paquets en excès), queuing (retard des paquets en transit) ou par contrôle de congestion (manipulation des mécanismes de contrôle de congestion du protocole utilisé). Policing et queuing peuvent être appliqués à tous les réseaux informatiques. Le contrôle de congestion peut seulement être appliqué aux protocoles réseaux utilisant des mécanismes de contrôle de congestion, tels que TCP.  Portail de l’informatique
Le relayage de trames (ou FR, pour l'anglais Frame Relay) est un protocole à commutation de paquets situé au niveau de la couche de liaison (niveau 2) du modèle OSI, utilisé pour les échanges intersites (WAN). Il a été inventé par Eric Scace, ingénieur chez Sprint International[réf. nécessaire]. Sur le plan technique, il peut être vu : comme un successeur de X.25 : il a en effet remplacé ce protocole pour le raccordement des sites des entreprises aux infrastructures des opérateurs qui offrent des services RPV. comme une étape vers l'ATM : il a souvent été présenté ainsi par les opérateurs très « UIT », c'est-à-dire les opérateurs ayant « voulu » X.25 et l'ATM, comme France Télécom par exemple. Le Frame Relay est en effet issu d'une volonté américaine, de l'ANSI en particulier, X.25 n'ayant jamais été très populaire aux États-Unis comme faisant partie du RNIS (ISDN) : c'est ainsi que l'UIT l'a considéré et a défini des normes qui n'ont jamais été implémentées
REMIP est le Réseau régional de la recherche en Midi-Pyrénées. C'est un réseau métropolitain de télécommunications reliant les Établissements d’Enseignement Supérieur et de Recherche de l’agglomération toulousaine. L’architecture construite est constituée d’une boucle reliant les sites de concentration et d’étoiles de connexions point à point reliant les sites terminaux aux sites de concentration.  Portail des télécommunications
Le réseau Abilene est la dorsale Internet des États-Unis mise en place par la communauté de l’Internet2. Il offre un haut débit à plus de 220 institutions membres qui sont principalement des universités, mais aussi des entreprises et institutions associées. Il s'étend à travers tout le territoire américain, on le retrouve aussi bien à Washington qu'à Porto Rico.
Le Réseau d'informations scientifiques du Québec (RISQ) est un organisme sans but lucratif québécois qui offre des services dans le domaine des technologies et de la télécommunication. Il dessert l'ensemble des universités et des cégeps québécois ainsi que de nombreux instituts gouvernementaux et de recherche scientifique, tel l'observatoire du Mont-Mégantic. Il dessert environ 750 000 utilisateurs.  Créé en 1988 par les dirigeants universitaires de la province, il vise à fournir à ses membres un accès Internet à large bande. Il gère aujourd'hui un réseau de 6 000 km de fibre optique.
Un réseau d'objets est une vision futuriste de l'Internet qui, dans le futur, reliera non seulement des personnes et des sites Web, mais aussi une multitude d'objets qui seront munis de circuits électroniques leur permettant de communiquer entre eux et avec des personnes à travers Internet. En plus des ordinateurs, de nombreux objets sont déjà branchés à Internet : des téléphones, des tablettes électroniques, des consoles de jeux, des téléviseurs, etc. L'avènement des radioétiquettes (RFID) accélère ce phénomène.
Un réseau de diffusion de contenu (RDC) ou en anglais content delivery network (CDN), est constitué d’ordinateurs reliés en réseau à travers Internet et qui coopèrent afin de mettre à disposition du contenu ou des données à des utilisateurs. Ce réseau est constitué : de serveurs d'origine, d'où les contenus sont « injectés » dans le RDC pour y être répliqués ; de serveurs périphériques, typiquement déployés à plusieurs endroits géographiquement distincts, où les contenus des serveurs d'origine sont répliqués ; d'un mécanisme de routage permettant à une requête utilisateur sur un contenu d'être servie par le serveur le « plus proche », dans le but d’optimiser le mécanisme de transmission / livraison. Les serveurs (ou nœuds) sont généralement connectés à Internet à travers différentes dorsales Internet. L’optimisation peut se traduire par la réduction des coûts de bande passante, l’amélioration de l’expérience utilisateur, voire les deux. Le nombre de nœuds et de serveurs qui constituent un RDC varie selon les choix d’architecture, certains pouvant atteindre plusieurs milliers de nœuds et des dizaines de milliers de serveurs.
Un réseau étendu, souvent désigné par son acronyme anglais WAN (Wide Area Network), est un réseau informatique ou un réseau de télécommunications couvrant une grande zone géographique, typiquement à l'échelle d'un pays, d'un continent, ou de la planète entière. Le plus grand WAN est le réseau Internet.
Un réseau ethernet métropolitain, également appelé réseau ethernet urbain ou Metro Ethernet (terminologie anglo-saxonne très répandue), est un réseau de télécommunications à haut débit basé sur le standard Ethernet qui couvre une zone géographique étendue. Alors qu'à l'origine le protocole Ethernet était utilisé principalement dans les réseaux locaux privés des clients (entreprises et particuliers), depuis 2003 environ, cette technologie tend à se développer également au sein des réseaux plus larges tels que ceux des fournisseurs de services de télécommunications, notamment dans les segments d'accès et métropolitain. Elle permet aux opérateurs de raccorder à haut débit leurs clients et de leur fournir différents services: services aux entreprises (interconnexion de succursales, accès distants à un site central, mise en réseau de stockage...), accès haut débit pour le grand public (incluant internet, voix, IPTV...), réseau de collecte des opérateurs mobiles etc.
Un réseau local, souvent désigné par l'acronyme anglais LAN de Local Area Network, est un réseau informatique tel que les terminaux qui y participent (ordinateurs, etc.) s'envoient des trames au niveau de la couche de liaison sans utiliser d’accès à internet. On définit aussi le LAN par le domaine de diffusion, c'est-à-dire l'ensemble des stations qui reçoivent une même trame de diffusion (en anglais broadcast frame). Au niveau de l'adressage IP, un réseau local correspond généralement à un sous-réseau IP (même préfixe d'adresse IP). On interconnecte les réseaux locaux au moyen de routeurs. Une autre approche consiste à définir le réseau local par sa taille physique. C'est généralement un réseau à une échelle géographique relativement restreinte, par exemple une salle informatique, une habitation particulière, un bâtiment ou un site d'entreprise. Dans le cas d'un réseau d'entreprise, on utilise aussi le terme RLE pour réseau local d'entreprise.
Un réseau métropolitain (en anglais Metropolitan Area Network, MAN) désigne un réseau composé d'ordinateurs habituellement utilisé dans les campus ou dans les villes. Le réseau utilise généralement des fibres optiques.
Un Réseau national de la recherche et de l'enseignement (parfois abrégé en NREN pour National Research and Education Network) est une organisation chargée de fournir et de gérer une infrastructure réseau pour les centres de recherche, les écoles et les universités. Elle est en général nationale et financée publiquement pour l'essentiel. Ces réseaux voient le jour à la fin des années 1980 et au début des années 1990 et se développent avec l'avènement d'Internet. En Europe, on peut citer: France : RENATER, Belgique : Belnet, Pays-Bas : SURFnet, Suisse : SWITCH, Allemagne : DFN, Grande-Bretagne : JANET, Italie : GARR, Grèce : GRNET, Suède, Islande, Danemark, Norvège : NORDUnet, Espagne : RedIRIS, Luxembourg : RESTENA, Irlande : HEAnet République Tchèque : CESNET Croatie : CARnet Portugal : FCCN Hongrie : NIIF/HUNGARNET Pologne : PIONIER etc. Les NREN européens se fédèrent autour de DANTE afin de se connecter entre eux et de négocier plus efficacement avec la Commission européenne, les opérateurs de télécommunications et les grands réseaux nord-américains.  Portail de l’informatique  Portail d’Internet  Portail des télécommunications
Le Réseau national de télécommunications pour la technologie, l'enseignement et la recherche (RENATER) est le réseau informatique français reliant les différentes universités et les différents centres de recherche entre eux en France métropolitaine et dans les départements d'outre-mer. Il a été créé en 1993.
Un réseau personnel (ou Personal Area Network, PAN) désigne un type de réseau informatique restreint en termes d'équipements, généralement mis en œuvre dans un espace d'une dizaine de mètres. D'autres appellations pour ce type de réseau sont : réseau domestique ou réseau individuel. L'idée est d'envoyer des informations entre des périphériques proches. Au lieu d'envoyer via un LAN ou un WLAN nécessitant une infrastructure, une nouvelle classification, PAN est créée. PAN est généralement utilisé pour créer un réseau d'appareils de personnel, comme un ordinateur portable, smartphone, tablette et des dispositifs portables tels que smartwatch ou lunettes de réalité virtuelle. Les PAN sont destinés à couvrir de petites zones telles que les maisons privées ou les espaces de travail individuels. PAN peut être connecté par fil ou sans fil. La technologie de réseau filaire peut être USB ou firewire. La technologie de réseau sans fil peut être IrDA, HomeRF, Bluetooth, UWB, ZigBee.
Néologisme technologique du monde de l'informatique en réseau d'objets, le terme réseau pervasif est une traduction littérale de pervasive network, c’est-à-dire un réseau pénétrant ou infiltrant, (notion de perméabilité). "Pervasive computing" ou "ubiquitous computing" (aussi "ubicomputing"), ont été d'abord traduit par "informatique diffuse", "informatique omniprésente" ou "informatique ubiquitaire". Cette traduction aura évolué vers celle plus générique d'Intelligence Ambiante. En fait, ce concept exprime la tendance vers la connexion en réseau, la miniaturisation des dispositifs électroniques et leur intégration dans n'importe quel objet du quotidien, favorisant ainsi l'accès aux informations dont on a besoin partout et à tout moment. L'informatique omniprésente fait donc référence à l'utilisation de plus en plus répandue de minuscules systèmes numériques communiquant spontanément les uns avec les autres et qui, grâce à leurs dimensions très réduites, seront intégrés dans les objets de la vie quotidienne, jusqu'à devenir presque invisibles pour les utilisateurs.
Le RIPE (Réseaux IP Européens) est un forum ouvert à toutes les parties ayant un intérêt dans le développement de l'Internet en Europe. L'objectif de la communauté RIPE est d'assurer la coordination technique et administrative nécessaire au développement d'Internet. Le RIPE n'est pas un organisme de normalisation comme l'IETF et ne gère pas des noms de domaines. Ses équivalent sont le NANOG en Amérique du Nord et le FRnOG en France. RIPE n'est pas une entité légale et n'a pas de structure associative formelle. Toute personne manifestant un intérêt peut joindre les listes de discussions, les groupes de travail et participer aux réunions du RIPE. Le RIPE a un président qui veille à l'organisation des meetings et aux liaisons avec l'extérieur. Rob Blokzijl était le porte-parole initial du RIPE et en a assuré la présidence jusqu'en mai 2014. Hans Petter Holen lui succède,. Le RIPE NCC est une organisation distincte du RIPE. Le RIPE NCC fournit le support administratif à la communauté RIPE, comme la partie logistique des réunions du RIPE et l'administration des listes de discussions du groupe de travail du RIPE. Le RIPE NCC a été fondé en 1992 par la communauté RIPE.
ROCAD est le Réseau Optique du Campus de la Doua (campus scientifique de Villeurbanne, au nord de Lyon). C'est le réseau métropolitain ou MAN (Metropolitan Area Network) qui interconnecte à très haut débit les écoles et universités du campus. Il est relié à Renater par le biais de LyRES (réseau métropolitain Lyon Recherche et Enseignement Supérieur).  Portail des télécommunications  Portail de la métropole de Lyon
Le routage est le mécanisme par lequel des chemins sont sélectionnés dans un réseau pour acheminer les données d'un expéditeur jusqu'à un ou plusieurs destinataires. Le routage est une tâche exécutée dans de nombreux réseaux, tels que le réseau téléphonique, les réseaux de données électroniques comme Internet, et les réseaux de transports. Sa performance est importante dans les réseaux décentralisés, c'est-à-dire où l'information n'est pas distribuée par une seule source, mais échangée entre des agents indépendants.
Une salle de(s) marchés est une salle où sont rassemblés les opérateurs de marché intervenant sur les marchés financiers.  L'usage désigne souvent la salle des marchés par le terme de front-office. Les pays anglo-saxons et d'autres pays utilisent le terme de trading-room, voire de dealing-room ou de trading-floor. Le terme de floor (parquet) est inspiré de celui d'une bourse cotant à la criée. Avec l'effacement progressif de la cotation à la criée au profit des bourses électroniques, la salle des marchés devient le seul lieu de vie emblématique du marché financier. C'est aussi le plus souvent là que sont mises en œuvre les technologies de l'information les plus récentes avant d'être disséminées dans les autres secteurs de l'établissement financier.
Le SciNet consortium est un consortium canadien, détenu par l'Université de Toronto et des hôpitaux ontariens affiliés, qui exploite un superordinateur. Le consortium a reçu des fonds du gouvernement du Canada, du gouvernement de l'Ontario, de l'Université de Toronto et des hôpitaux affiliés.
Sega Channel était un réseau informatique de jeu en ligne développé par Sega pour la console de jeu Mega Drive, servant de Content delivery network. À partir de juin 1993, des tests marketing sont effectués, Sega Channel est fournie par Time Warner Cable et TCI à travers des services de distribution de télévision par câble par le biais d'un câble coaxial. C'était un service de pay to play à travers lequel les clients pouvaient accéder au mode en ligne des jeux Mega Drive compatibles, essayer des démos de jeux et obtenir des cheat codes
Le Sega Meganet était un réseau informatique mis en place au Japon pour les personnes utilisant la Mega Drive. Les joueurs branchaient le Megamodem (un modem avec une vitesse entre 1600 et 2400 bit/s) au port DE-9 à l'arrière de la Mega Drive et pouvaient ainsi jouer en ligne avec d'autres utilisateurs.   Portail des télécommunications  Portail du jeu vidéo
Semi-Automatic Ground Environment (SAGE) est le nom d'un système d'arme américain ultra-secret opérationnel de 1952 à 1984.  Conçu autour d'un réseau de macroordinateurs (mainframe computer), de nombreux sites de stations radar dont ceux de la ligne Pinetree puis la ligne DEW, et de la liste des capacités offensives disponibles (missiles, chasse, etc.), il permettait de produire en temps réel une image unifiée de l'espace aérien à l'échelle du territoire national et de pouvoir apporter une réponse tactique immédiate en cas de danger. SAGE est le cœur du dispositif du Commandement de la défense aérospatiale de l'Amérique du Nord et du Air Defense Command face à une hypothétique attaque aérienne . Son caractère secret, ses énormes ordinateurs et ses écrans gigantesques ont fondé une partie du folklore de la guerre froide, ainsi qu'on peut le voir dans des films comme Docteur Folamour et Le cerveau d'acier. Le système SAGE fonctionnait grâce au plus gros ordinateur jamais construit alors, le AN/FSQ-7 d'IBM. Celui-ci équipait les Centres de direction ("DC", au nombre de 23), et il était capable de recueillir automatiquement les données radar brutes, de calculer les trajectoires des objets détectés, et de proposer les réponses appropriées en fonction de la distance et de la disponibilité. Les cartes étaient ensuite affichées sur des écrans, où des officiers saisissaient à l'aide de pointeurs lumineux le statut des objets détectés (F pour Friend, T pour Target), les options tactiques, ainsi que les ordres de commandement éventuels. Les commandes étaient enfin envoyées par téléscripteur, sans intervention humaine, aux unités armées concernées (plus tard, les missiles CIM-10 Bomarc et les chasseurs de l'United States Air Force verront leur plan de vol automatiquement actualisé, y compris en vol). Chaque Centre de direction rapportait à un Centre de combat ("CC", au nombre de 4) pour la "supervision des différents secteurs à l'intérieur d'une division", donnant à « chaque Centre de combat [...] la capacité de coordonner la défense de la nation tout entière »  Le système SAGE est reconnu pour être l'un des tout premiers réseaux numériques de l'histoire de l'informatique (mêlant capacités de calcul, interfaces de saisie, interfaces de lecture, téléphonie, modems, téléscripteurs, télégraphie). Devenu peu efficace pour détecter les ICBM déployés par les Soviétiques dans les années 1960, le système SAGE restait cependant vital pour surveiller l'espace aérien et intercepter d'éventuelles intrusions par avions. Il a donc été maintenu sous sa forme initiale jusqu'en 1984, puis a été remplacé par des systèmes modernisés.
En informatique, la sérialisation (de l'anglais américain serialization) est le codage d'une information sous la forme d'une suite d'informations plus petites (dites atomiques, voir l'étymologie de atome) pour, par exemple, sa sauvegarde (persistance) ou son transport sur le réseau (proxy, RPC…). L'activité réciproque, visant à décoder cette suite pour créer une copie conforme de l'information d'origine, s'appelle la désérialisation (ou unmarshalling). Le terme marshalling (mobilisation, canalisation, organisation) est souvent employé de façon synonyme, de même que le terme linéarisation. Les termes marshalling et unmarshalling s'emploient le plus souvent dans le contexte d'échanges entre programmes informatiques, alors que les termes sérialisation et désérialisation sont plus généraux. D'apparence simple, ces opérations posent en réalité un certain nombre de problèmes, comme la gestion des références entre objets ou la portabilité des encodages. Par ailleurs, les choix entre les diverses techniques de sérialisation ont une influence sur les critères de performances comme la taille des suites d'octets sérialisées ou la vitesse de leur traitement.
La simulation des réseaux est une technique par laquelle un logiciel (simulateur) modélise le comportement d'un réseau, soit par le calcul de l'interaction entre les entités du réseau en utilisant des formules mathématiques, ou en capturant et reproduisant des observations à partir d'un réseau réel.  Portail de l’informatique
Le Sneakernet (ou ‘'Tennis-Net'’, ‘'Armpit-Net’', ‘'Floppy-Net'’ ou ‘'ShoeNet'’) est une méthode de transfert de fichier sans réseau informatique, qui fonctionne par exemple par l'intermédiaire de clés USB ou de disques durs externes. Dans certains cas, ce type de réseau peut être plus rapide qu'internet : ainsi pour transférer un fichier de 3 Go avec une connexion ADSL 512K, en vitesse montante réellement très lente (12 ko/s), il est parfois plus rapide d'envoyer une clé USB de 4 Go contenant le fichier à son correspondant. Historiquement, on utilisait souvent des outils logiciels de découpage de fichier pour pouvoir transférer par disquettes. Ce procédé est également protégé de toute surveillance car chaque périphérique de stockage ne peut être suivi. De multiples notions reposent sur ce principe ou l'utilisent de manière indirecte : Dead Drop, Copie privée...   Portail des télécommunications
Le software-defined networking (SDN) est un modèle d’architecture réseau qui permet aux administrateurs de réseaux de gérer les services de réseaux par Abstraction de fonctionnalités.
La segmentation réseau est une technique ayant pour objectif de diviser un réseau informatique en plusieurs sous-réseaux. La segmentation est principalement utilisée afin d'augmenter les performances globales du réseau et améliorer sa sécurité.
Un sous-réseau est une subdivision logique d'un réseau de taille plus importante. Le masque de sous-réseau permet de distinguer la partie de l'adresse utilisée pour le routage et celle utilisable pour numéroter des interfaces. Un sous-réseau correspond typiquement à un réseau local sous-jacent. Historiquement, on appelle également sous-réseau chacun des réseaux connectés à Internet. La subdivision d'un réseau en sous-réseaux permet de limiter la propagation des broadcast, ceux-ci restant limités au réseau local et leur gestion étant coûteuse en bande passante et en ressource au niveau des commutateurs réseau. Les routeurs sont utilisés pour la communication entre les machines appartenant à des sous-réseaux différents.
StanNet est un réseau métropolitain de télécommunication à haut-débit, mis en service en 1998, avec le soutien du Conseil régional de Lorraine, dans l'agglomération de Nancy.
Strava est un site internet et une application mobile utilisée pour enregistrer des activités sportives via GPS. Le siège social se situe à San Francisco en Californie. Le cyclisme et la course à pied concentrent la majorité des activités enregistrées sur le site. En suédois, Strava signifie « s'efforcer à faire quelque chose sans relâche ».
Un sur-réseau (ou supernet) est une normalisation (RFC 1519) ainsi qu'une technique de CIDR qui permet de définir un préfixe réseau englobant plusieurs sous-réseaux. La technique du sur-réseau consiste à agréger plusieurs réseaux IP en un seul. Cette technique est utilisée dans les tables de routage. Elle permet de réduire le nombre de lignes dans la table tout en conservant la totalité des destinations. Cette technique n'est pas sans risque car elle peut autoriser des destinations qui ne sont pas prévues initialement.
La synchronisation de fichiers (ou de répertoires) est le processus permettant de faire correspondre les contenus de deux (ou plus) emplacements de stockage. Lorsqu'un utilisateur ajoute, modifie, ou supprime un fichier à l'endroit A, le processus de synchronisation entre A et B ajoutera, modifiera, ou supprimera le même fichier à l'endroit B afin que les contenus des deux répertoires restent bien des copies conformes
Le synchronisme désigne le caractère de ce qui se passe en même temps, à la même vitesse. L'adjectif synchrone définit deux processus qui se déroulent de manière synchronisée. Il s'oppose à asynchrone. Il est particulièrement important dans le domaine technique, celui de l'électrotechnique et l'électronique. En particulier, on parle en électrotechnique de machines synchrones pour des machines tournantes dont la fréquence de rotation est égale à celle du réseau électrique. Il s'utilise aussi dans le domaine de l'audiovisuel pour la connectique. En informatique, les systèmes sont fréquemment fondés sur ce principe, qui permet des transferts d'information fiables, que ce soit au niveau des réseaux ou au niveau du processeur. Du point de vue du programmeur, une méthode est synchrone si elle attend une réponse avant de retourner la sienne. En histoire, le synchronisme retrace la simultanéité des événements dans un cadre déterminé (pays, continent, monde) sans que ces évènements soient obligatoirement dépendants directement les uns des autres. Dans le domaine du doublage, le synchronisme consiste à caler sur les mouvements des lèvres des acteurs originaux le texte doublé.
Le système d’information du marché intérieur est un réseau informatique qui relie les organismes publics de l’Espace Économique Européen. Il a été conçu par la Commission européenne en étroite coopération avec les États membres de l’Union européenne dans le but d’accélérer la coopération administrative transfrontalière. IMI permet aux autorités publiques aux niveaux national, régional et local d’identifier leurs homologues dans d’autres pays et d’échanger des informations avec eux. Des questions et réponses pré-traduites ainsi qu’un outil de traduction automatique leur permettent d’utiliser leur propre langue pour communiquer.
Une table de routage est une structure de données utilisée par un routeur ou un ordinateur en réseau et qui associe des préfixes à des moyens d'acheminer les datagrammes vers leur destination.
Le taux d'erreur ou B.E.R., abréviation de l'expression anglaise Bit Error Rate, désigne une valeur, relative au taux d'erreur, mesurée à la réception d'une transmission numérique, relative au niveau d'atténuation et/ou de perturbation d'un signal transmis. Ce phénomène survient également lors de l'échantillonnage (numérisation), lors de la lecture et de la sauvegarde des données (CD-R, DVD-R, disque dur, RAM...). Ce taux détermine le nombre d'erreurs apparues entre la modulation et juste après la démodulation du signal. Ce taux d’erreur ne tient généralement pas compte du codage des données sauf dans le cas d'une norme de transmission combinant modulation et correction d'erreur (exemple : QPSK). La cause des perturbations, donc de l'augmentation du taux d'erreur, peut être multiple : équipement ou réseau défectueux, pointage incorrect d'une antenne, interférences, longueur des câbles, etc. Le taux d'erreur (BER) s'exprime en puissance négative. Par exemple,                                    10                        −             3                                     {\displaystyle 10^{-3}}    signifie que l'on a en moyenne une erreur binaire pour mille bits transmis.  Portail de l’informatique  Portail des télécommunications
En informatique, le téléchargement est l'opération de transmission d'informations (logiciels, données, images, sons, vidéos) d'un ordinateur à un autre via un canal de transmission, en général Internet ou un intranet. En télécommunications lors d'échanges entre serveur et client, on différencie le téléchargement du téléversement : le premier désigne un transfert du serveur vers le client tandis que le second est du client vers le serveur. On distingue différentes formes de téléchargements : le téléchargement direct qui aboutit au stockage d'un fichier ; la lecture en continu, (anglais : streaming) (dans ce cas, le stockage du fichier est provisoire et n'apparait pas sur le disque dur du destinataire) ; le pair-à-pair, qui est un échange de données entre ordinateurs qui ont un double rôle de clients et de serveurs ; la navigation sur le Web qui stocke temporairement ou non les fichiers distants (pages web et contenu multimédia) pour les afficher. Le téléchargement peut être payant et soumis à des réglementations diverses. Il permet un échange de données licites ou illicites, en fonction du contenu téléchargé, et des pays d'origine et de destination de l'information ; les usages illicites font partie de ce que l'on appelle cybercriminalité. L'utilisation d'un fichier téléchargé (notamment une œuvre pouvant subir une atteinte au droit d’auteur) peut être contrôlée par une gestion numérique des droits (DRM).
La télématique est un terme qui recouvre les applications associant les télécommunications et l'informatique, apparu en France à l'occasion de la filière technologique qui allait donner vie au Minitel.
Télétel est le nom du réseau informatique français utilisé par le Minitel. Il est basé sur Transpac, le réseau de commutation de paquets X.25 de France Télécom. L'accès au réseau Télétel se fait sans abonnement supplémentaire pour les clients de France Télécom. La connexion est facturée à la durée. Les coûts de connexion apparaissaient sur la facture de la ligne téléphonique. En pratique, l'utilisateur passe par son commutateur téléphonique de rattachement en composant un numéro de téléphone particulier (36 13, 36 14, 36 15, etc.). Dès que le numéro de téléphone particulier est détecté, le commutateur téléphonique est mis en relation à un commutateur spécial E10.5/PAVI. Ainsi, le terminal Minitel est donc mis en relation avec un PAV (Point d'accès Vidéotex). Les E10.5/PAVI sont répartis sur tout le territoire national. Dans la terminologie X.25, ce sont des PAD (Packet assembler/disassembler). Après composition d'un code de service, le réseau Transpac met l'utilisateur en relation avec le serveur souhaité. Le commutateur E10.5/PAVI appartient à la grande famille 1000-E10 des commutateurs téléphoniques fabriqués par Alcatel, mais il a été adapté à la norme X.25. En France, le dernier commutateur E10.5/PAVI a été démonté après l'arrêt du Minitel, en 2012.  Portail des télécommunications  Portail de l’informatique
Terrestrial Trunked Radio (ou TETRA) est un système de radio numérique mobile professionnel bi-directionnel (comme des talkie-walkies évolués), spécialement conçu pour des services officiels tels que services de secours, forces de polices, ambulances et pompiers, services de transport public et pour l'armée. L’European Telecommunications Standards Institute a élaboré cette norme pour harmoniser les moyens de télécommunications en Europe, conformément aux normes ETSI EN 300 392-1 et EN 300 392-2. La norme a permis d’homogénéiser les différentes spécifications associées à la ressource radio dans les bandes de spectre radio définies. Un réseau de type TETRA offre un canal radio partagé ouvert en permanence, et réservé à un groupe d'utilisateurs. Chaque utilisateur peut pousser sur un bouton de son appareil pour parler, et tous les autres utilisateurs entendront sa voix. Ceci permet d'établir une communication immédiate entre un utilisateur sur le terrain et un dispatcher, ou un groupe d'utilisateurs. Les appels de groupe sont essentiels lorsque les instructions d'un officier ou d'un dispatcher doivent être entendues immédiatement par un groupe d'utilisateurs.
Le Time to Live (« temps de vie » ou « durée de vie »), abrégé TTL, indique le temps pendant lequel une information doit être conservée, ou le temps pendant lequel une information doit être gardée en cache.
Claude Touzet est un spécialiste français de l'apprentissage tant automatique que biologique. Il est né à Neuilly-sur-Seine le 16 novembre 1963. Ses principaux domaines de recherche ont d'abord concerné l'apprentissage automatique (réseaux de neurones artificiels, robotique autonome, coopération entre robots) avant de se tourner vers l'apprentissage biologique (programmes d'entraînement de la mémoire et de l'attention pour les seniors et personnes souffrants de diverses neuro-pathologies). Sa Théorie neuronale de la Cognition (TnC, 2010) est une proposition d'organisation neuronale visant à expliquer la réalisation des différentes fonctions cognitives avec le seul usage de hiérarchies de cartes corticales.
Le Traffic policing, ou limitation du flux, consiste à vérifier que les flux réseau se conforment à un accord de service et à prendre les mesures pour faire respecter un tel contrat. Les sources de données qui sont informées de l'existence d'un tel accord peuvent appliquer le traffic shaping (régulation de flux) pour faire en sorte que ce qu'elles envoient reste dans les limites de ce contrat. Les paquets échangés dépassant ce qui est prévu dans l'accord peuvent être immédiatement jetés, marqués comme étant en excès, ou laissés inchangés par le processus de limitation du flux. Le sort réservé aux données excédentaires est fonction de choix administratifs et des caractéristiques des données échangées.
Le Traffic shaping ou régulation de flux est le contrôle du volume des échanges sur un réseau informatique dans le but d’optimiser ou de garantir les performances, une latence plus basse ou d’augmenter la bande passante utilisable en retardant les paquets qui correspondent à certains critères. Plus particulièrement, le trafic shaping désigne toute action sur un groupe de paquets (un flux réseau) qui impose un délai supplémentaire à ces paquets pour qu’ils se conforment à une contrainte prédéterminée (contractuelle ou liée à un certain type de trafic). Le traffic shaping offre un moyen de contrôler le volume des échanges en train d’être envoyés à un réseau pendant une période donnée (bandwith throttling), le débit maximum auquel le trafic peut être envoyé (rate limiting), ou encore des critères plus complexes tels que l'algorithme GCRA (generic cell rate algorithm) dans le cadre de ATM. Ce contrôle peut être utilisé de différentes manières et pour diverses raisons ; cependant, le trafic shaping se fait toujours en retardant certains paquets (aucune perte). Le trafic shaping est couramment appliqué aux réseaux d’extrémités pour contrôler le trafic entrant sur le réseau, mais peut également être utilisé à la source du trafic (par exemple, sur un ordinateur ou une carte réseau) ou par un élément du réseau (routeur ou commutateur). Le Traffic policing est une pratique distincte, mais liée.
Dans les réseaux informatiques, une trame est le PDU de la couche 2 (Liaison de données) dans le modèle OSI. Une trame est délimitée par une série de bits particulière appelée drapeau, fanion. Une trame est composée d'un header, des informations que l'on veut transmettre, et d'un postambule (trailer). Un paquet (dans le cas d'IP par exemple) ne peut transiter directement sur un réseau : il est encapsulé à l'intérieur d'une trame qui elle-même finit en un enchaînement de bits qui circule sur le support physique. Il existe trois versions différentes dont une qui a été abandonnée : Ethernet Type I (créée par Xerox) abandonnée à l'heure actuelle ; Ethernet Type II (propriétaire Intel, Digital, Xerox) ; IEEE 802.3.
Transiris est le premier logiciel de routage pour le transport des données entre ordinateurs à avoir été installé au sein même du système d'exploitation dans une logique de réseau et de partage des données appelée téléinformatique. Fonctionnalité importante de l'Iris 80 de la Compagnie internationale pour l'informatique , il a ensuite donné naissance à Datanet.
La transmission de données désigne le transport, quel que soit le type d'information, d'un endroit à un autre, par un moyen physique.
Dans un réseau à commutation de paquets, la transmission en mode non-connecté ou transmission en mode sans-connexion est une transmission de données dans laquelle chaque paquet est préfixé par un entête contenant une adresse de destination, suffisante pour permettre la livraison autonome du paquet, sans recours à d'autres instructions. Un paquet transmis en mode non-connecté est fréquemment appelé un datagramme. Dans une communication orientée connexion, les stations qui sont prêtes à échanger des données doivent d'abord se déclarer comme voulant effectivement le faire. Ceci est appelé l'« établissement d'une connexion ». Une connexion est parfois définie par une relation logique entre les parties échangeant des données. Un avantage du mode non-connecté par rapport au mode connecté est qu'il permet les opérations de multicast et de broadcast, qui peuvent économiser encore plus de données quand la même donnée doit être transmise à plusieurs destinataires. Par contre, une connexion est toujours de type unicast (point à point). Malheureusement, dans une transmission en mode non connecté d'un paquet, le fournisseur du service de transmission ne peut garantir qu'il n'y aura pas de perte, d'insertion d'erreurs, de mauvaise livraison, de duplication, ou de dé-séquencement de la livraison des paquets. Malgré tout, ces risques peuvent être réduits en fournissant un service de transmission fiable à une couche de protocole de plus haut niveau dans le modèle de référence OSI Un autre inconvénient du mode non-connecté est qu'il n'est pas possible d'avoir des optimisations quand il y a un envoi de plusieurs trames entre deux parties. En établissant une connexion au début d'un tel échange de données, les composants (routeurs, ponts réseau) le long du chemin réseau, seraient en mesure de précalculer (et donc de mettre en cache) les informations de routage, évitant par là, son recalcul pour chaque paquet. Les composants réseau pourraient aussi réserver de la capacité pour le transfert des trames ultérieures, par exemple, lors du téléchargement d'une vidéo. La distinction entre une transmission en mode non-connecté et connecté peut prendre place au niveau des différentes couches du modèle de référence OSI: Au niveau de la couche de transport : TCP est un protocole de transport orienté connexion. UDP est en mode non-connecté. Au niveau de la couche réseau : IPv4 est un protocole réseau en mode non-connecté, IPv6 avec le champ FlowID peut fonctionner en mode connecté. Au niveau de la couche de liaison de données : le protocole IEEE 802.2 définissant la sous-couche de Contrôle de la liaison logique (Logical Link Control - LLC) de la couche de liaison de données, peut fournir à la fois des services en mode connecté et non-connecté. En fait, certains protocoles, comme le « Path Control » de SNA à ses débuts, nécessitait une couche de liaison de données orientée connexion. Mais avec l'apparition de IBM Advanced Peer-to-Peer Networking (en), SNA a pu fonctionner avec une couche de liaison de données en mode non-connecté.
Transpac était une filiale de l'opérateur de télécommunications France Télécom, créée le 14 décembre 1978 et spécialisée dans la fourniture d'accès réseaux pour les entreprises (lignes spécialisées, Relais de trames, ATM). Elle détenait le monopole commercial pour la fourniture d'accès X.25 de 1979 jusqu'à 2006. Avec l'arrivée d'Internet, son activité a évolué à la fin des années 1990 vers la fourniture de réseaux IP (réseaux VPN, téléphonie sur IP (ToIP)), mais également vers les offres d'hébergement et de fourniture de services. Cette filiale a fusionné avec France Télécom le 1er janvier 2006. Après le 1er juin 2006, les offres commerciales de Transpac ont été exploitées par Orange Business Services.
Un tunnel, dans le contexte de réseaux informatiques, est une encapsulation de données d'un protocole réseau dans un autre, situé dans la même couche du modèle en couches, ou dans une couche de niveau supérieur. Par exemple, pour faire passer le protocole IPv6 dans l'Internet actuel (qui est presque entièrement en IPv4) on va créer un tunnel entre deux machines IPv4 ; ce tunnel, pour le protocole IPv6, semblera un simple lien point-à-point (un logiciel comme traceroute ne verra donc pas le tunnel). En sécurité, on crée souvent des tunnels chiffrés, par exemple comme le fait SSH. Les données peuvent alors y circuler sans craindre d'être écoutées. Les tunnels peuvent être utilisés pour créer des réseaux privés virtuels (VPN). Le tunnel HTTP est le cas particulier qui consiste à faire passer une connexion interdite par un pare-feu (SSH par exemple) dans un protocole HTTP presque toujours autorisé. C'est particulièrement utile quand le pare-feu ne bloque pas simplement les ports associés (22 pour SSH par exemple), mais analyse aussi les protocoles utilisés.
Le terme unicast définit une connexion réseau point à point, c'est-à-dire d'un hôte vers un (seul) autre hôte. On entend par unicast le fait de communiquer entre deux ordinateurs identifiés chacun par une adresse réseau unique. Les paquets de données sont acheminés sur le réseau suivant l'adresse du destinataire « encapsulée » dans la trame transmise. Normalement, seul le destinataire intercepte et décode le paquet qui lui est adressé. Dans le protocole IP, les adresses doivent être uniques dans la mesure où les paquets sont acheminés au niveau du LAN (Local Area Network) ou du WAN (Wide Area Network). Exemple d'une adresse IP locale (privée) pouvant servir à une communication unicast : 192.168.1.32 Exemple d'une adresse IP Internet (publique) pouvant servir à une communication unicast : 66.102.11.99 Pour une diffusion multipoints, on utilisera une diffusion broadcast ou multicast.
La valence est le nombre d'états possibles d'un signal transmis. En pratique, la valence est souvent choisie de sorte que ce soit une puissance de 2 (2, 4, 8, 16, etc). Pour une valence de                                    2                        n                                     {\displaystyle 2^{n}}   , n est le nombre de bits nécessaire pour écrire ces états en binaire. Par exemple, pour une valence de 4, il y a 4 états possibles codés sur 2 bits (                        2         =         l         o         g         2         (         4         )                 {\displaystyle 2=log2(4)}   ), ce qui donne en binaire : 00, 01, 10 et 11. Lorsqu'un signal a une valence                                    2                        n                                     {\displaystyle 2^{n}}   , une rapidité de modulation d'un baud équivaut à un débit binaire de n bit/s. D = log2(V) * R (D débit en bit/s, V valence en symbole/baud, R rapidité de modulation en baud)  Portail de l’informatique  Portail des télécommunications
Dans l'industrie des télécommunications et de l'informatique, on désigne par "Wide Area File Services (WAFS)" une technologie, utilisée principalement dans les entreprises ayant des succursales distribuées géographiquement, qui permet d'optimiser les accès distants à un serveur de fichiers central au travers d'un réseau étendu (WAN).
Le réseau métropolitain sans fil (WMAN pour Wireless Metropolitan Area Network), aussi connu sous le nom de Boucle Locale Radio (BLR), est une technologie informatique pour le réseau. Les WMAN sont basés sur la norme IEEE 802.16, et offrent un débit utile de 1 à 10 Mbit/s pour une portée de 4 à 10 kilomètres. Ce type de connexion est donc principalement destiné aux opérateurs de télécommunication.
Le réseau personnel sans fil (appelé également réseau individuel sans fil ou réseau domestique sans fil et noté WPAN pour Wireless Personal Area Network) concerne les réseaux sans fil d'une faible portée : de l'ordre de quelques dizaines mètres. Ce type de réseau sert généralement à relier des périphériques (imprimante, téléphone portable, appareils domestiques, ...) ou un assistant personnel (PDA) à un ordinateur sans liaison filaire ou bien à permettre la liaison sans fil entre deux machines très peu distantes. Il existe plusieurs technologies utilisées pour les WPAN.
Z-Wave est un protocole radio conçu pour la domotique (éclairage, chauffage…).
1024 est le bulletin de la Société informatique de France (SIF).
ABC Informatique est une encyclopédie hebdomadaire diffusée entre 1984 et 1985 par les Éditions Atlas dont la thématique est l'informatique. La société a créé une filiale nommée Altisoft.
ACM Computing Surveys est une revue scientifique basée sur l'évaluation par les pairs publiée par l'Association for Computing Machinery. La revue a été fondée en 1969. Elle publie des articles sur l'informatique et les ordinateurs. Selon le Journal Citation Reports, ACM Computing Surveys a le facteur d'impact le plus élevé des revues informatiques. Dans un classement de revues informatiques de 2008, ACM Computing Surveys a reçu la plus haute note : A*.
Acta Informatica est une revue scientifique évaluée par des pairs qui publie des articles de recherche originale en informatique La revue est surtout connue pour ses publications en informatique théorique. L'un des deux articles de 1988 couronnés du prix Gödel en 1995 est paru dans ce journal.
Algorithmica est une revue mensuelle d'informatique théorique, publiée par Springer. Elle aborde à la fois les aspects théoriques de l'algorithmique et des aspects plus expérimentaux. En 2013, l'éditeur-en-chef est Ming-Yang Kao. ISSN : 0178-4617
Combinatorics, Probability and Computing est une revue scientifique revue par les pairs en mathématiques, publiée par Cambridge University Press. Son rédacteur en chef est Béla Bollobás (DPMMS (en) et Université de Memphis). Le journal couvre les champs de la combinatoire, la théorie des probabilités et l'informatique théorique. Actuellement, il publie six numéros par an. Comme avec d'autres revues du même éditeur, il suit une politique hybride green/gold libre accès, dans laquelle les auteurs peuvent placer des copies de leurs articles en dépôt institutionnel après six mois de période d'embargo, ou payer des frais pour un accès libre afin de rende leurs articles libres lisibles sur le site du journal. Le journal a été créé par Bollobás en 1992. Le lauréat de la médaille Fields Timothy Gowers l'a qualifié de "favori personnel" parmi les revues de combinatoire et écrit qu'il "maintient un niveau élevé".
Communications of the ACM (CACM) est la principale revue mensuelle de l'Association for Computing Machinery (ACM). Créé en 1957, CACM est envoyé à tous les membres de l'ACM, environ 80 000 actuellement. Les articles sont écrits pour des lecteurs ayant des connaissances dans tous les domaines de l'informatique et des systèmes d'information. L'accent est mis sur les applications concrètes des découvertes sur les technologies de l'information et sur les problèmes de gestion des systèmes d'information. L'ACM publie aussi de nombreuses autres revues plus théoriques.  CACM est à la frontière d'un magazine de vulgarisation scientifique, d'un magazine professionnel, et d'une revue scientifique. Le contenu est ainsi soumis au processus d'évaluation par les pairs comme dans une revue scientifique (et est pris en compte dans de nombreux comités d'évaluations des résultats de recherche universitaire), mais ressemble plus a des synthèses de travaux déjà publiés. Les contenus publiés doivent être accessibles à une large audience. CACM est catégorisé comme un magazine par son éditeur .
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) est un journal scientifique spécialisé en reconnaissance des formes et vision par ordinateur. TPAMI est un journal mensuel de l'IEEE Computer Society, qui aborde l'ensemble des thématiques liées à la vision par ordinateur et à la compréhension et l'interprétation automatique des images. Il aborde également tous les thèmes de la reconnaissance de formes, ainsi que certains sujets en intelligence artificielle. Des thèmes d'intérêt particuliers sont par exemple la reconnaissance de l'écriture manuscrite, la détection d'objets, la recherche d'images par le contenu, l'imagerie médicale, ou la reconnaissance de visage.
Information and Computation est une revue scientifique informatique mensuelle publiée par Elsevier (anciennement Academic Press). La revue est fondée en 1957 sous son ancien titre Information and Control. Le rédacteur en chef en est Albert R. Meyer (Laboratoire d'informatique, MIT). La revue publie 12 numéros par an. En 2012, le facteur d'impact de la revue est de 0,699 et de 0,890 sur 5 ans.
International Journal of Computer Vision (IJCV) est un journal scientifique spécialisé dans le domaine de la vision par ordinateur. Le journal est l'un des meilleurs du domaine, avec un très fort facteur d'impact (3,508 en 2009), se classant parmi les premiers journaux en informatique et intelligence artificielle.
Journal of the ACM (Journal de l'ACM) est la revue scientifique majeure de l'Association for Computing Machinery (ACM). C'est une revue à comité de lecture qui couvre l'informatique en général, et plus particulièrement les aspects théoriques. Son directeur de publication courant est Éva Tardos, qui succède à Victor Vianu (en). Elle a été créée en 1954. Elle est estimée dans la communauté scientifique, comme peut l'attester la citation Lowry, Romans et Curtis 2004, « computer scientists universally hold the Journal of the ACM (JACM) in high esteem. » (« les informaticiens tiennent universellement Journal of the ACM en haute estime. »)
Journal of Artificial Intelligence Research est une revue scientifique à accès libre et comité de lecture qui couvre la recherche dans tous les domaines de l'intelligence artificielle. Elle a été établie en 1993 comme l'une des premières revues scientifiques distribuées en ligne. Les publications sur papier sont imprimées par AAAI Press. La rédactrice en chef est Francesca Rossi (Université de Padoue).
Le Journal of Cryptology, en abrégé JoC est un journal scientifique qui parle de cryptologie. Le journal est publié trimestriellement par l’International Association for Cryptologic Research. Son éditeur en chef est Kenneth Paterson.
Journal of Functional Programming (JFP) est une revue scientifique spécialisée dans le domaine de la programmation fonctionnelle.
Le Journal of Machine Learning Research (généralement abrégé JMLR) est une revue scientifique axée sur l'apprentissage automatique, un sous-domaine de l'intelligence artificielle. Il a été fondé en 2000. La revue a été fondée comme une alternative libre accès à la revue Machine Learning. En 2001, quarante éditeurs de Machine Learning ont démissionné afin de soutenir JMLR, disant qu'à l'ère de l'Internet, il était préjudiciable pour les chercheurs de continuer à publier leurs papiers dans des journaux coûteux avec des archives d'accès payant. Lors de leur démission, ils ont écrit qu'ils appuyaient le modèle de JMLR, dans lequel les auteurs conservaient le droit d'auteur sur leurs publications et les archives étaient disponibles gratuitement sur Internet. Les éditions imprimées de JMLR ont été publiées par MIT Press jusqu'en 2004, et par Microtome Publishing par la suite. Depuis l'été 2007, JMLR publie également Machine Learning Open Source Software.
Le Journal of Symbolic Computation est une revue scientifique à comité de lecture consacrée à la recherche en calcul formel et dans les domaines apparentés.
Machine Learning est une revue scientifique révisée par les pairs traitant d'apprentissage automatique, publiée depuis 1986. En 2001, quarante éditeurs de Machine Learning ont démissionné afin de soutenir Journal of Machine Learning Research, disant qu'à l'ère de l'Internet, il était préjudiciable pour les chercheurs de continuer à publier leurs papiers dans des journaux coûteux avec des archives d'accès payant. Lors de leur démission, ils ont écrit qu'ils appuyaient le modèle du Journal of Machine Learning Research, dans lequel les auteurs conservaient le droit d'auteur sur leurs publications et les archives étaient disponibles gratuitement sur Internet.
Management Information Systems Quarterly (abrégé en MIS Quarterly ou MISQ) est une revue scientifique qui publie des articles de recherche originaux évalués par des pairs dans le domaine du management des systèmes d'information.
Molecular Informatics (abrégé en Mol. Inf.) est une revue scientifique à comité de lecture. Ce mensuel publie des articles de recherches originales sur tous les aspects de la bio-informatique et de la chémoinformatique. D'après le Journal Citation Reports, le facteur d'impact de ce journal était de 1,647 en 2014. Actuellement, les directeurs de publication sont Knut Baumann (Université technique Carolo-Wilhelmina de Brunswick), Gerhard Ecker (Université de Vienne), Jordi Mestres (Institut Municipal d'Investigació Mèdica de Barcelone) et Gisbert Schneider (École polytechnique fédérale de Zurich).
PeerJ est un journal scientifique de biologie et de médecine, couvrant de larges domaines — mega journal (en) — libre d'accès et revu par les pairs. C'est le nom de l'entreprise faîtière qui édite également, depuis février 2015, le PeerJ Computer Science.
Les Proceedings of the IEEE est un périodique scientifique à comité de lecture publié par l’Institute of Electrical and Electronics Engineers (IEEE) qui se concentre sur le génie électrique et l'informatique. Selon le Journal Citation Reports, les Proceedings of the IEEE ont l'un des plus grands facteurs d'impact en 2010 dans le domaine du génie électrique.
Protecta est un magazine mensuel italien technique-scientifique, économique et environnemental fondé en 1987. Publié par le groupe éditorial Ecoedizioni Internazionali, le magazine est imprimé en roto offset. C’est aujourd’hui une filiale rédactionnellement indépendante du groupe Ecoedizioni Internazionali. En 2008, l’édition italienne tire une moyenne de 240 000 exemplaires, tandis que le tirage de Protecta et de ses éditions imprimées s’élève à 150 000. Depuis 2009, il existe aussi une édition électronique.
La revue SIAM Journal on Computing (abrégée en SICOMP) est une revue scientifique centrée sur les aspects mathématiques et formelles de l’informatique. Elle est publiée par la Society for Industrial and Applied Mathematics (SIAM). C'est une revue bimestrielle, paraissant depuis 1972.
Theoretical Computer Science (abrégé en TCS) est une revue scientifique publiée par Elsevier Science, dont la parution a commencé en 1975 et qui couvre l'ensemble de l'informatique théorique. Maurice Nivat en le fondateur et l'éditeur en chef pendant environ vingt-cinq ans, et Grzegorz Rozenberg est l'éditeur fondateur de la section C : Theory of Natural Computing.
Le Cercle (titre original : The Circle) est un roman dystopique de science-fiction de Dave Eggers, paru en octobre 2013. Il évoque les activités de Mae Holland, embauchée par une puissante compagnie agissant sur internet — fusion de Google, Facebook et Apple — et qui découvre peu à peu les buts secrets et terrifiants de ses dirigeants. L'ouvrage a fait l'objet d'une adaptation au cinéma en 2017, The Circle, réalisée par James Ponsoldt, avec Emma Watson dans le rôle de Mae Holland.
Binaire est un site d'information français spécialisé dans l’informatique (la science et la technique). Si le blog est indépendant, il a des liens forts avec la Société informatique de France. Il collabore régulièrement avec le site Interstices, le magazine The Conversation (en), et le journal scientifique Le Bulletin 1024. Selon le blog lui-même : « Le but de ce blog est de parler d’informatique, de communiquer sur ce qu’est vraiment l’informatique en tant que science et technique, ses définitions, ses progrès, ses dangers, ses questionnements, ses succès et impacts, ses enjeux, ses métiers, son enseignement… »
BrowserChoice.eu, aussi appelé ballot screen ou écran de choix du navigateur, est un site Web de Microsoft permettant de choisir son navigateur Web, lancé en mars 2010 et fermé le 17 décembre 2014. Il est le résultat d'un procès intenté par l'Union européenne à Microsoft pour abus de position dominante.
CNET est un site Web d'information traitant de micro-informatique, d'Internet et de nouvelles technologies. Fondé en 1994 par Halsey Minor et Shelby Bonnie, propriété de CNET Networks dont il constituait le site phare, il appartient aujourd'hui à CBS Interactive, filiale de CBS Corporation.
CodinGame est un site dédié à la programmation informatique ludique, proposant d’un côté des casse-têtes de difficulté croissante à résoudre dans l’un des vingt-cinq langages de programmation disponibles, et de l’autre des jeux d’intelligence artificielle multijoueurs, ou des défis de résolution de problèmes en temps limité, ou de code golf.
Computer Channel était une chaîne de télévision par satellite ainsi qu'un portail internet dédié à l'informatique et aux télécommunications, disparu en 2002. Fondée en 1991, elle a été détenue notamment par France Télécom et Wanadoo,,.
Developpez.com est une communauté francophone dédiée au développement informatique. Developpez.com est à la fois un hébergeur gratuit et un club d'entraide. Le site propose aux lecteurs des ressources et services accessibles gratuitement et sans inscription préalable : blogs, forum, réseau social, plus de 14 000 cours et tutoriels, formations en ligne, plus de 100 FAQ , des codes sources et un magazine en ligne. Le site a été lancé en 1999. Il est lu par plus de deux millions de lecteurs dans le monde. L'équipe de rédaction de Developpez.com, qui va des responsables de rubriques aux rédacteurs et modérateurs, est bénévole et comprend plus de 1 700 contributeurs. Les forums sont divisés en plus de 2 000 sous-forums qui abordent de très nombreuses technologies et langages de programmation.
Gitorious était une forge logicielle libre et un service communautaire basé sur git. D'abord racheté en août 2013 par Powow AS (une société norvégienne et polonaise à la fois), il a ensuite été racheté par Gitlab le 3 mars 2015, qui continue à proposer le code source de la forge sous licence libre sur son propre site, mais a fermé le site de Gitorious.
interstices est une revue de culture scientifique en ligne, créée par des chercheurs de l'INRIA pour rendre accessible à un large public la recherche en informatique,. Ce site « a pour but de faire connaître la variété des recherches dans le domaine des sciences et technologies de l'information et de la communication ». Lancé en 2004 à l'initiative de l'INRIA, Institut national de recherche en informatique et en automatique, il se développe en partenariat avec le CNRS, les universités et l'association des sciences et technologies de l'information. Le magazine Sciences et Avenir d'octobre 2010 le classe parmi les cent meilleurs sites scientifiques.
Love2recycle.fr est une marque et un site Web français créés par le groupe Anovo en 2009. L’objectif de Love2recycle.fr est de racheter et recycler les téléphones mobiles, les tablettes les ordinateurs portables. Le site propose aussi depuis 2014 une boutique de smartphones et tablettes reconditionnés.
Mac4ever est un site Web communautaire francophone traitant notamment de l'actualité du monde Mac et Apple.
MacPlus, fondé en septembre 1997, par Romain Bisseret et Olivier Royer, est le plus ancien des sites web d'information sur Apple encore en activité. C'est une association loi de 1901 qui regroupe trois sites web : MacPlus.net (actualité Mac, iPod, iPhone et iPad), Mac2Sell.net (cote de l'occasion des produits Apple) et FunTouch.net (actualité des jeux sur iOS). En janvier 2016, la rédaction ferme le site « parce que l’envie n’y est tout simplement plus »,.
Mashable est un site d'actualité et un blog d'information fondé par Pete Cashmore. Le site web se concentre principalement sur les actualités fournies par les médias sociaux, mais traite aussi des nouveautés et du développement de la téléphonie mobile, du divertissement, des vidéos en ligne, du monde des affaires, de la programmation web ainsi que des technologies en général. Mashable fut fondé en juillet 2005 par Pete Cashmore, depuis sa résidence à Aberdeen (Écosse).
Le Monde informatique est un magazine hebdomadaire français d'informatique professionnel créé en 1980 par le groupe IDG (basé à Boston), sans aucun lien avec le journal Le Monde. 44 numéros sont publiés par an. Il correspond en France à d'autres titres édités par IDG ailleurs dans le monde comme ComputerWorld, ComputerWoche… Son dessinateur caricaturiste était François Cointe. Les autres blogs des journalistes de l'hebdomadaire sont accessibles depuis la page d'accueil du site. L'hebdomadaire a publié son dernier numéro en octobre 2007, décision prise peu avant le rachat d'actifs du groupe IDG par le groupe AdThink Media. L'édition en ligne du même nom continue elle à exister.
Numerama (anciennement Ratiatum) est un site web d'actualité sur l'informatique et le numérique. Le site vise à examiner ses enjeux et son environnement économique, législatif, social ainsi que la technologie et la culture web. Il a été créé par Guillaume Champeau en avril 2002 et est édité par la société PressTIC. En avril 2015, cette dernière est rachetée par le groupe Humanoid.
OMG! Ubuntu! est un blog anglophone à propos d'Ubuntu destiné au grand public. Il fournit des informations sur la distribution Linux Ubuntu et certains logiciels libres.
OpenClassrooms est une école en ligne. Chaque visiteur peut à la fois être un lecteur ou un rédacteur. Les cours peuvent être réalisés aussi bien par des membres, par l'équipe du site, ou éventuellement par des professeurs d'universités ou de grandes écoles partenaires. Initialement orientée autour de la programmation informatique, la plate-forme couvre depuis 2013 des thématiques plus larges tels que le marketing, l'entrepreneuriat et les sciences. Créé en 1999 sous le nom de Site du Zéro, il se forme essentiellement sur la base de contributions de bénévoles proposant des tutoriels vulgarisés avec un ton léger portant sur des sujets informatiques divers. À la suite du succès et de la fin des études des gérants, l'entreprise Simple IT, renommée ensuite OpenClassrooms, est fondée dans le but de pérenniser le site. Celle-ci base son business model sur la délivrance de certifications payantes et propose un abonnement pour être suivi par un mentor,. En 2015, le site déclare compter 1 million de comptes depuis sa création, ainsi qu'un trafic de 2,5 millions de visiteurs uniques par mois. La même année, l'abonnement Premium est proposé gratuitement à tous les demandeurs d'emplois français, en partenariat avec Pôle emploi et le gouvernement.
SecurityFocus est un site web anglophone de référence, consacré à la sécurité informatique. Il héberge un système de suivi de bugs et des vulnérabilités trouvés, plusieurs listes de diffusion sur la plupart des technologies liées à la sécurité, et publie des offres d'emploi. SecurityFocus a été acheté par la société Symantec en 2002, éditrice de Norton Antivirus.
SELFHTML est une documentation gratuite et complète sur le langage HTML et le développement de sites Web en général. Cette documentation se présente sous la forme de fichiers HTML statiques. SELFHTML est un projet en allemand conduit au départ par Stefan Münz. Il a été traduit en français par Serge François. La version 8.0 est la dernière version traduite avant la mort de l'auteur ; elle ne peut plus être consultée en ligne mais peut être téléchargée avec un complément mis en ligne en 2008.
Stack Overflow est un site web proposant des questions et réponses sur un large choix de thèmes concernant la programmation informatique. Il fait partie du réseau de sites Stack Exchange. Stack Overflow a été créé par Jeff Atwood et Joël Spolsky en 2008 comme une alternative aux autres sites Q&A, comme Experts-Exchange. Chaque membre peut voter pour les questions et réponses postées, faisant gagner des points, appelés réputation, à leurs auteurs. Il est également possible de voter contre (vote down) pour pénaliser l'auteur de la réponse et indiquer aux futurs lecteurs que cette réponse peut être incorrecte. Le but de ces votes est de mettre en avant les réponses de qualité, tout en récompensant leurs auteurs, leur donnant accès à des privilèges quand certains seuils de réputation sont atteints (par exemple: pouvoir voter, voir moins de publicité, pouvoir fermer les questions)
Tom’s Hardware est un site web traitant du matériel informatique. Le site propose de suivre l’actualité au travers de news quotidiennes, de dossiers et tests de produits. Le site est géré par la société Bestofmedia. Selon Nielsen/NetRatings, le site affichait 1,5 million de visiteurs uniques et 20 millions de pages vues en février 2008 en France. Quant à la communauté du site, elle a récemment dépassé le million.
The Verge est un site américain lancé le 21 octobre 2011 qui traite de l'actualité technologique, de l'information et des médias. Il est édité par Vox Media à Manhattan, et fait concurrence à des sites comme Engadget, Gizmodo, Mashable, CNET, Wired, SlashGear et TechCrunch. Il publie des dépêches, des traitements de fonds, des tests de matériel, et s'appuie beaucoup sur la vidéo à travers des podcasts télévisés. Son financement est assuré par la publicité et le sponsoring. L'éditeur en chef de The Verge est Nilay Patel, remplaçant Joshua Topolsky depuis juillet 2014. Son responsable des contenus est Marty Moe. Le site web est basé sur Chorus, la plateforme de publication développée par Vox Media qui anime également Polygon.com et bientôt les blogs partenaires du groupe. Le logo du site est l’œuvre de l'entreprise de design Area 17, et se base sur un triangle de Penrose.
W3Schools est un site Web populaire destiné à l'apprentissage en ligne des technologies Web. Son contenu inclut des didacticiels et des références relatives à HTML, CSS, JavaScript, JSON, PHP, AngularJS, SQL, Bootstrap, Node.js, jQuery, XQuery, AJAX et XML. Il reçoit plus de 10 millions de visiteurs uniques chaque mois. Créé en 1998, son nom est dérivé du World Wide Web, mais n'est pas affiliés au W3C (World Wide Web Consortium). Il est géré par Refsnes Data, basé en Norvège. W3Schools présente des milliers d'exemples de code. À l'aide d'un éditeur en ligne, les lecteurs peuvent modifier des exemples et exécuter le code dans un bac à sable.
YouOS est un environnement de bureau Web expérimental, créé par WebShaka, qui reproduit l'environnement de bureau d'un système d'exploitation moderne sur une page web, utilisant JavaScript afin de communiquer avec le serveur. Cela permet aux utilisateurs de sauvegarder un environnement où ils pourront retourner plus tard, et pour plusieurs utilisateurs de collaborer en utilisant un seul environnement. Le logiciel est actuellement en stage de développement alpha, son développement ayant commencé à la fin du mois de décembre 2005. Un API - accompagné de quelque 293 applications - est fourni pour YouOS. Sans aucune explication, le site YouOS a été fermé le 30 juillet 2008 par ses développeurs.
Zataz est un site web français d'information traitant principalement de la délinquance informatique.
ZDNet est un réseau de sites web spécialisés dans les nouvelles de technologie. Détenu par CBS Interactive (filiale de CBS Corporation), il a été fondé le 1er avril 1991 par Ziff Davis. En 2011, le réseau diffuse des nouvelles dans plusieurs langues, dont le français, l'anglais, l'allemand et le japonais.
La sûreté de fonctionnement des systèmes informatiques est le domaine de la sûreté de fonctionnement qui traite de l'aspect « produit » de la qualité des systèmes informatiques, c'est-à-dire la partie « qualité des processus d'ingénierie des systèmes ».
La double authentification (Two-factor authentication en anglais, 2FA) ou vérification en deux étapes est une méthode par laquelle un utilisateur peut accéder à une ressource informatique (un ordinateur, un téléphone intelligent ou encore un site web) après avoir présenté deux preuves d'identité distinctes à un mécanisme d'authentification,,,. Un exemple de ce processus est l'accès à un compte bancaire grâce à un guichet automatique bancaire : seule la combinaison de la carte bancaire (que l'usager détient) et du NIP (que l'usager connaît) permet de consulter le solde du compte et de retirer de l'argent, par exemple. La multiple authentification, plus communément appelée authentification à facteurs multiples ou l'authentification multi-facteurs (Multi-factor authentication en anglais, MFA) exige, quant à elle, plusieurs preuves d'identité.
Jean-Claude Laprie, né le 22 décembre 1944 à Paris et mort le 17 octobre 2010 à Toulouse, est un chercheur en informatique. Il a contribué à l'essor théorique de la sûreté de fonctionnement des systèmes informatiques.
Uptime Institute est un consortium d'entreprises créé en 1993 dont l'objectif est de maximiser l'efficacité des centres de traitement de données. Uptime Institute est connu en particulier pour avoir défini la notion de « Tier » pour les datacenters, largement adopté dans le monde,. Uptime Institute a été racheté par The 451 Group (un des concurrents du Gartner) en 2009. La classification comporte les niveaux Tier I, Tier II, Tier III et Tier IV. Un datacenter doit être certifié par l’Uptime Institute pour revendiquer un niveau de Tier. La classification d’un site est fixée par son sous-système de plus bas niveau. Les notions intermédiaires de type Tier III+ n’existe pas bien qu'elles soient parfois mentionnées pour désigner une conformité incomplète au niveau supérieur. En 2015, Uptime Institute est le seul organisme à délivrer des certifications de datacenter. Les autres organismes proposant des classifications de datacenter (BICSI 002, TIA 942, Syska Hennessy) ne proposent pas de certification.
La cartographie d'information est une discipline qui trouve ses fondamentaux dans une pluralité de connaissances à la croisée de la gestion des connaissances, de la sémiologie graphique et des sciences cognitives. Elle fait appel à une grande variété de concepts issus de domaines et sous domaines parfois très spécialisés. Ce sont des outils privilégiés d'exploration et d'analyse de la complexité et des espaces de représentation (dimensions des données, réduction, projection/spatialisation),. Les cartes d'informations diffèrent des cartes géographiques, des cartes ou schémas heuristiques, des cartes éditoriales ou infographies, des cartes d'investigation et s'apparentent aux graphes, aux cartes sémantiques et dynamiques.
Un système à criticité mixte est un système informatique temps réel, dont la particularité réside dans le fait qu’il se compose de fonctionnalités critiques et non-critiques. Ce type de système est typiquement soumis à de multiples procédures de certifications, qui ont pour objectif de valider la sûreté du système, et la sécurité de ses utilisateurs. On retrouve typiquement ces systèmes informatiques déployés sur des plates-formes embarquées, par exemple dans des dispositifs aériens et automobiles.
La gestion de logs (LM) comprend une approche de la gestion de grands volumes des messages de log  générés par l'ordinateur (aussi connu comme journaux d'évènements, journalisation, etc.). La gestion des logs concerne en général: La collecte des logs L'agrégation centralisée des logs Le stockage à long terme et la durée de rétention des logs La rotation des fichiers de logs L'analyse des logs (en temps réel et en vrac après une période de stockage) Les rapports et l'étude des logs.
En informatique, I/O bound est l'état d'un ordinateur lorsque le temps nécessaire pour exécuter un programme ou une partie d'un programme est déterminé principalement par la vitesse des opérations d'entrées-sorties. Typiquement, lorsqu'un ordinateur est en état I/O bound, le processeur est inoccupé parce qu'il attend la complétion d'opérations d'entrées-sorties sur un disque dur, un réseau ou un autre périphérique. Le terme français pour I/O bound est tributaire des entrées-sorties, mais ce terme est peu utilisé et les informaticiens utilisent presque toujours le terme I/O bound.
Wedus.org (WEb EDUcation System) est un projet développé par l'association EFREI Aides Humanitaires - EAH. Le projet poursuit plusieurs buts: Créer une communauté d'écoles, d'élèves et de professeurs Apporter aux écoles des pays en voie de développement une solution leur permettant de récupérer de vastes contenus éducatifs à moindre coût Apporter aux élèves et aux professeurs des moyens d'expression (forums, blogs, etc.) Initier les élèves des pays en voie de développement aux Nouvelles Technologies de l'Information et de la Communication (NTIC) Wedus.org est divisé en 2 sous-projets: le projet Wedus, qui correspond au développement du logiciel en lui-même le projet Wedux, qui consiste en la création de distributions Linux basées sur Ubuntu permettant d'installer simplement et automatiquement Wedus
stat est le nom d'un appel système et d'une commande Unix faisant partie du paquetage Coreutils (ou GNU Core Utilities). Cet outil permet d'obtenir des informations sur des fichiers ou répertoires. Par défaut, cette commande affiche 3 dates : La date à laquelle le contenu du fichier a été lu La date à laquelle le contenu du fichier a été modifié La date à laquelle les permissions sur ce fichier ont été modifiés
Syslog-ng est une implémentation du protocole syslog pour les architectures de type UNIX.
Un système cyber-physique (sigle : CPS) est un système où des éléments informatiques collaborent pour le contrôle et la commande d'entités physiques.
L'ARM Cortex-R est un microprocesseur temps réel, conçu par ARM, prévu pour être implémenté dans des SoC d'architecture ARM. Il utilise un jeu d'instruction nommé ARMv7-R, version temps réel du ARMv7, qui existe aussi dans une version dédiée aux processeurs d'application ARM Cortex-A (ARMv7-A) et aux processeurs servant également de microcontrôleurs, ARM Cortex-M (ARMv7-M). Il est actuellement décliné sous 4 formes en ARMv7 : ARM Cortex-R4 ARM Cortex-R5 ARM Cortex-R7 ARM Cortex-R8 Et une en ARMv8 : ARM Cortex-R52 Parmi les fabricants de semi-conducteurs utilisant cette technologie, on peut citer : Broadcom, Fujitsu, Infineon, LCI, Renesas, Texas Instruments, Toshiba.
La terminologie informatique est l'ensemble des termes et des sigles utilisés dans le domaine de l'informatique. La terminologie informatique regroupe en particulier des termes relatifs à des notions, des techniques, des normes, des produits – logiciels ou matériels, ainsi que des applications pratiques et des métiers de l'informatique. Depuis ses débuts en 1950, le domaine de l'informatique a donné lieu à de nombreux néologismes, souvent empruntés à l'anglais. Ces emprunts sont d'usage courant dans le jargon des professionnels et le grand public. En France et au Québec, les gouvernements ont mis en place des organismes publics pour formuler des recommandations sur la terminologie et proposer leurs propres néologismes.
En informatique et en télécommunications, l'acquittement d'une donnée ou d'une information consiste à informer son émetteur de sa bonne réception. On utilise souvent le terme Ack pour un acquittement, ce terme correspond à l'équivalent anglais du terme : acknowledgement. Il est possible d'émettre un « acquittement négatif » (Nack) pour indiquer une mauvaise réception, ou aucune réception au bout d'un certain temps. L'acquittement est notamment au cœur du protocole Go-Back-N ARQ utilisé par plusieurs protocoles de communication.
Application portfolio management (APM) ou gestion du portefeuille applicatif GPA est une méthode apparue dans les années 1990 dans les moyennes et grandes entreprises qui cherchaient à mettre en œuvre une stratégie informatique. L'objectif est de gérer le capital, les actifs de matière grise constitués par plusieurs vagues d'informatisation d'entreprise en appliquant les recettes de retour sur investissement, de coût de maintenance, de développement ou de rachat propres à la gestion financière.
Une appliquette (applet en anglais) est un logiciel ou un mini-logiciel qui s'exécute dans la fenêtre d'une autre application, en général un navigateur web.
En informatique, le terme assembly désigne : en anglais, un langage assembleur (voir aussi Assembly language (en) sur wikipedia.eng) un anglicisme informatique, désignant un ensemble logiciel cohérent constitué en une unité de déploiement indivisible à partir de plusieurs 'briques' logicielles (dll, ressources, méta données, dépendances...), particulièrement employé dans le monde Microsoft, mais également en Java (fichiers jar)
L’asynchronisme désigne le caractère de ce qui ne se passe pas à la même vitesse, que ce soit dans le temps ou dans la vitesse proprement dite, par opposition à un phénomène synchrone.
L’attente active, en génie logiciel, ou polling (parfois appelée aussi « scrutation ») est une technique de programmation que les processus utilisent lorsqu'ils vérifient de façon répétée si une condition est vraie, comme l'attente d'une entrée (clavier ou autre) ou encore la libération d'un verrou. Cette technique peut également être utilisée pour mettre en attente un programme pour une durée déterminée. Cela était nécessaire sur d'anciens systèmes d'exploitation dont le matériel sous-jacent ne proposait pas de méthode spécifique pour suspendre l'exécution du flot d'instruction pendant une période déterminée. Sur les plateformes modernes proposant des horloges et plusieurs vitesses de processeur, cette forme de suspension du programme est souvent imprécise et signe de programmation naïve. L'attente active peut, par contre, être une stratégie valide dans certaines circonstances, le plus souvent dans l'implémentation des spinlocks au sein de systèmes d'exploitation conçus pour fonctionner sur des systèmes à processeurs multiples. À part ce genre de cas, les attentes actives devraient être évitées, puisque le CPU pourrait être réattribué à une autre tâche.
En informatique, un back-end (parfois aussi appelé un arrière-plan) est un terme désignant un étage de sortie d'un logiciel devant produire un résultat. On l'oppose au front-end (aussi appelé un frontal) qui lui est la partie visible de l'iceberg.
En électronique, la bande passante d'un système est l'intervalle de fréquences dans lequel l'affaiblissement du signal est inférieur à une valeur spécifiée (CEI). C'est une façon sommaire de caractériser la fonction de transfert d'un système, pour indiquer la gamme de fréquences qu'un système peut raisonnablement traiter (Dic. Phys.). La bande passante doit être distinguée de la largeur de bande d'une définition plus générale et qui concerne aussi bien les systèmes que les signaux (CEI). Par analogie, dans le domaine des réseaux informatiques, spécialement les accès à internet à haut débit, on utilise le terme bande passante pour désigner le débit binaire maximal d'une voie de transmission. La ligne de transmission jusqu'à l'usager est le composant déterminant. La notion de bande passante s'applique mal aux lignes de transmission, qui atténuent progressivement les fréquences et connaissent des phénomènes de déphasage et de réflexion d'ondes, et dont le débit numérique dépend en outre du niveau de bruit de fond.
En informatique, une base de données distribuée (en anglais : distributed database) est une base de données dont la gestion est traitée par un réseau d'ordinateurs interconnectés qui stockent des données de manière distribuée. Ce stockage peut être soit partitionné entre différents nœuds du réseau, soit répliqué entièrement sur chacun d'eux, ou soit organisé de façon hybride.
Benevolent Dictator for Life (BDFL) (littéralement « Dictateur bienveillant à vie ») est le surnom donné à une personne respectée de la communauté de développement open source qui définit des orientations générales d'un projet donné. Ce nom est un jeu de mots entre dictateur bienveillant et président à vie. Le BDFL est la personne qui détient effectivement des pouvoirs similaires à ceux d'un dictateur sur ce projet. Cependant, les autres développeurs et utilisateurs lui font confiance pour qu'il n'abuse pas de ses pouvoirs. Le terme est utilisé pour plaisanter, car les « sujets » du chef du projet contribuent volontairement, et le produit final pourra être utilisé par tout le monde. Dans ce contexte, un dictateur n'a du pouvoir que sur le processus, et ce uniquement tant que la confiance règne. Une variante est Self-Appointed Benevolent Dictator for Life (SABDFL), littéralement « bienveillant dictateur à vie autoproclamé ». C'est le pseudonyme du milliardaire sud-africain Mark Shuttleworth, reflétant ainsi l'influence qu'il exerce sur la communauté Ubuntu. Par extension, cette dénomination a également été donnée à certains fondateurs de tiers-lieux, lieux où la culture libre et l'absence de hiérarchie est mise en avant. Antoine Burret , auteur de "Tiers-lieux et plus si affinités"  insiste sur la différence avec un "chef", ou supérieur : "Il y a une différence importante, c’est qu’il n’y a pas de lien hiérarchique. Celui qui ne veut pas suivre peut partir. J’insiste sur un point : il peut partir et reprendre le projet pour aller dans un autre sens que celui voulu par le premier promoteur. Le pouvoir n’appartient au dictateur bienveillant que dans la mesure où la communauté lui donne sa confiance. Chacun est libre."
En informatique, brick est un terme anglais (« brique ») utilisé pour désigner une mort logicielle ou matérielle d'un produit, généralement difficilement voire impossible à réparer. Cela peut se produire notamment à cause d'un virus (comme pour la DS le cheval de Troie DS.Brick.A/B), ou d'un plantage au cours d'une mise à jour importante, par exemple d'un logiciel embarqué (firmware). Utilisé en particulier dans le domaine des consoles de jeux vidéo, le terme possède à la base une connotation humoristique, car un appareil brické est à peu près aussi utile qu'une brique.
En informatique, un bug (mot anglais bug) ou bogue (au Nouveau-Brunswick, au Québec et en France) est un défaut de conception d'un programme informatique à l'origine d'un dysfonctionnement. La gravité du dysfonctionnement peut aller de bénigne (défauts d’affichage mineurs) à majeure (crash système pouvant entraîner de graves accidents : par exemple, l'explosion du Vol 501 d'Ariane 5). Un bug peut résider dans un logiciel applicatif, dans les logiciels tiers utilisés par ce logiciel applicatif, voire dans le firmware d'un composant matériel comme ce fut le cas du bug de la division du Pentium. Un patch (terme francisé en « retouche » ou « correctif ») est un morceau de logiciel destiné à corriger un ou plusieurs bugs.
En programmation informatique, un bug logiciel inhabituel est un bug considéré comme très difficile à détecter et à réparer. Il en existe plusieurs types souvent désignés d'après le nom de scientifiques qui ont découvert des principes non intuitifs. Selon la nature, la probabilité d'apparition ou la gravité, on trouve ainsi : Schrödinbug ; Heisenbug ; Bohr bug ; Mandelbug.
Le Centre facilitant la recherche et l’innovation dans les organisations (CEFRIO), (précédemment Centre francophone d'information des organisations) à l’aide des technologies de l'information et de la communication (TIC), regroupe plus de 150 membres universitaires, industriels et gouvernementaux ainsi que 74 chercheurs associés et invités. Ensemble, ils œuvrent à la transformation des pratiques par le numérique de la société québécoise grâce à l'usage des technologies afin d’améliorer l'innovation sociale et organisationnelle.
En informatique, une chaîne de compilation (en anglais : « toolchain ») désigne l'ensemble des paquets utilisés dans le processus de compilation d'un programme, pour un processeur donné. Le compilateur n'est qu'un élément de cette chaîne, laquelle varie selon l'architecture matérielle cible.
Dans un réseau informatique, un client est le logiciel qui envoie des demandes à un serveur. Il peut s'agir d'un logiciel manipulé par une personne, ou d'un bot. Est appelé client aussi bien l'ordinateur depuis lequel les demandes sont envoyées que le logiciel qui contient les instructions relatives à la formulation des demandes et la personne qui opère les demandes. L'ordinateur client est généralement un ordinateur personnel ordinaire, équipés de logiciels relatifs aux différents types de demandes qui vont être envoyées, comme un navigateur web, un logiciel client pour le World wide web.
Le terme codebase, ou code base est utilisé en développement de logiciel pour désigner l'ensemble du code source utilisé pour construire un logiciel ou un composant. En général, le codebase n'inclut que des fichiers sources écrits par des humains et non, par exemple, des fichiers sources générés par d'autres outils ou par des programmes. Le codebase d'un projet est habituellement versionné grâce à un système de gestion de versions. Il se trouve alors conservé au milieu d'une quantité importante de code source (public ou privé). Ces systèmes sont souvent utilisés dans des projets faisant intervenir plusieurs développeurs afin de gérer les versions successives du code source. Subversion est un système de gestion de versions populaire pour gérer ce workflow et est largement utilisé dans les projets open source. Dire que plusieurs codebases sont distincts signifie que leurs implémentations sont indépendantes (elles ne partagent pas de code source) et que historiquement ces implémentations ne dérivent pas d'un codebase commun. Dans le cas de standards, ceci prouve une interopérabilité puisque deux programmes indépendants peuvent implémenter un même standard.[pas clair]
En informatique, un commettant (de l'anglais « principal ») désigne une entité pouvant être authentifiée par un système informatique ou en réseau.
La Commission spécialisée de terminologie et de néologie de l'informatique et des composants électroniques (CSTIC) est, en France, une Commission spécialisée de terminologie et de néologie qui intervient dans le domaine de la terminologie informatique. Elle fait partie du dispositif gouvernemental coordonné par la Commission générale de terminologie et de néologie, mis en place par le décret du 3 juillet 1996 relatif à l'enrichissement de la langue française. Elle est rattachée au Ministère de l'Économie, de l'Industrie et de l'Emploi. Elle a été créée en 1997.
La configuration d'un logiciel, d'un matériel, ou d'un réseau informatique est un ensemble de caractéristiques techniques qui ne dépendent pas du constructeur mais découlent des choix de l'acheteur et de l'utilisateur. Des caractéristiques qui sont donc susceptibles de différer largement même pour des objets de construction identique. La configuration matérielle découle des choix relatifs au matériel : type de matériel acheté, budget alloué… La configuration, c'est aussi l'activité qui consiste à modifier des paramètres de configuration. C'est une activité typique de l'administration système.
Le démarrage d'un ordinateur (boot en anglais) est la procédure de démarrage d’un ordinateur et comporte notamment le chargement du programme initial (l’amorçage ou boostrap en anglais). On distingue : le « démarrage à froid » (cold boot), obtenu en allumant la machine, ou en l’éteignant puis en la rallumant ; du « démarrage à chaud » (warm boot), ou « re-boot », obtenu en rechargeant le programme initial ; il ne s’agit pas d’un redémarrage au sens strict (coupure puis remise de l’alimentation électrique) ; l’option est présente au niveau du système d’exploitation.
Les données biomédicales ouvertes révèlent que la médecine et l’open data (« données ouvertes ») sont deux domaines qui entretiennent des relations nouvelles, complexes, en pleine évolution et parfois apparemment contradictoires. En effet, introduire l'open data dans le monde médical, vise à répondre à deux enjeux parfois a priori opposés : permettre une large ouverture des données de santé ; le faire en respectant l'éthique biomédicale et le droit de la santé, le secret de fabrication, qui sont très contraignants notamment en termes de protection de la confidentialité de certaines données, en particulier les données personnelles, surtout quand elle concerne la santé (Les données de santé sont dans ce cadre les « données susceptibles de révéler la maladie d’une personne »,) et du secret médical, ce qui suppose un système fiable d’anonymisation des données, assez sophistiqué pour éviter les doubles comptages et d’autres biais lors de l’utilisation de ces données. Un projet de règlement européen retient une définition plus large des données de santé : « toute information relative à la santé physique ou mentale d'une personne, ou à la prestation de services de santé à cette personne ». Avec l’émergence du smartphone, des tablettes (utilisés par plus de 3 milliards de personnes) et d’autres objets connectés, ainsi que d’applications médicales (ex. : surveillance du poids, de l’activité physique et sportive, de la qualité du sommeil, etc.) et d’une informatique de plus en plus ubiquitaire (17 millions de bracelets et de montres intelligentes auraient été vendus en 2014), des enjeux nouveaux apparaissent, notamment concernant la confidentialité, le stockage et la protection des données médicales personnelles au sein du et « big data ». Se pose aussi la question de leur utilisation par des tiers ou par le secteur privé (Apple, dont l'iOS 8 peut pour la première fois gérer des données personnelles de santé) a ainsi interdit la commercialisation des données de santé HealthKit).
Les données de santé sont toutes les données médicales et/ou relatives aux déterminants généraux de santé, et à la santé d'une personne, d'un groupe de personnes (couple, famille, quartier, ville, région, ethnie, pays, etc.) ou de populations (santé publique, santé au travail, santé reproductive). Cette formulation est généralement utilisée pour la santé humaine plutôt que vétérinaire. Ces données sont utilisées pour le suivi et l'évaluation des systèmes et politiques de santé, pour établir des budgets prévisionnels, faire de la prospective en santé et croiser ces indicateurs avec d'autres (environnement, alimentation, précarité, éducation, usages de psychotropes, alcoolisme, tabagisme, etc.). En France, un « projet de loi de santé » (porté par Marisol Touraine, présenté le 15 octobre 2014 en Conseil des ministres et qui sera débattu au Parlement en 2015) propose la création d'un « Système national des données de santé » (SNDS) qui devrait unifier les bases de données existantes, et serait géré par la Caisse nationale de l'assurance maladie des travailleurs salariés (CNAMTS) qui devra mettre à disposition du public (en Open Data) les données suivantes (anonymisées, avec une ouverture graduelle des données, en fonction du risque de réidentification des patients) : Données issues des systèmes d'information hospitaliers; Données du système d’information de l'assurance maladie ; Données sur les causes de décès ; Certaines données de remboursement transmises par les organismes d’assurance maladie complémentaire.
Le downgrade est un terme informatique signifiant le retour à une version antérieure de système ou de logiciel. Ce terme anglophone est le contraire de upgrade, qui signifie mise à niveau dans cette même langue.  Portail de l’informatique
Dans le contexte des bases de données, la durabilité est la propriété qui garantit qu'une transaction informatique qui a été confirmée survit de façon permanente, quels que soient les problèmes rencontrés par la base de données ou le système informatique où cette transaction a été traitée. Par exemple, dans un système de réservation de sièges d'avion, la durabilité assure qu'une réservation confirmée restera enregistrée quels que soient les problèmes rencontrés par l'ordinateur qui gère le système de réservation (panne d'électricité, écrasement de la tête sur le disque dur, etc.). La durabilité est l'une des quatre propriétés ACID qui garantissent qu'une transaction informatique est exécutée de façon fiable. Plusieurs système de gestion de base de données implémentent la durabilité en écrivant les transactions sur un journal des transactions qui peut être utilisé pour recréer la base de données dans l'état où elle était immédiatement avant une panne. Une transaction est confirmée seulement après son enregistrement dans le journal des transactions. Dans le cas de transactions distribuées, tous les serveurs impliqués doivent se coordonner pour émettre une confirmation uniquement lorsque la transaction est enregistrée de façon permanente sur tous les serveurs. Cela est habituellement fait au moyen d'un two-phase commit protocol (en).
L'écriture bufferisée en informatique est une technique qui consiste à stocker des données temporairement en mémoire vive (ou mémoire tampon), avant de les écrire par blocs sur un support (disque dur, base de données, CD-ROM, ...). L'objectif de l'écriture bufferisée est de profiter de la vitesse très supérieure d'accès à la mémoire vive pour minimiser ceux à des supports usuellement beaucoup plus lents. Ceci présente deux avantages : N'utilisant pas le support d'écriture plus lent, le logiciel n'a pas à attendre que l'écriture ait été effectuée. En conséquence, celui-ci est plus rapide. Des supports ne permettant qu'un accès à un moment donné (comme un disque dur ou un graveur de CDROM) sont sollicités moins souvent. Ceci permet de traiter d'un coup toutes les écritures d'un logiciel donné, et de réduire les temps d'accès dans le cas de plusieurs logiciels devant y accéder.
En général, un système (ordinateur, réseau) et par extension son utilisation ou ce qu'il contient, est dit en ligne s'il est connecté à un autre réseau ou système (au moyen d'une « ligne » de communication, bien que ce ne soit pas toujours le moyen ; c'est comme « être au téléphone »). Diverses significations plus spécifiques existent : En langage courant, le réseau est en général Internet si bien que « en ligne » décrit un service, une information accessible par Internet ou l'état d'un ordinateur et le fait que son utilisateur y soit connecté, etc. ; L'utilisation de services internet (tels que la lecture et rédaction de courriels) est dite « en ligne » si elle nécessite une connexion (cas de la messagerie web) ou par opposition « hors ligne » si, entre d'éventuels brefs échanges de données en ligne (un courriel par exemple), un programme (client de messagerie) est capable de réaliser ce service de manière locale et autonome sans être constamment connecté à un serveur externe.
End-of-file (littéralement « fin de fichier » en anglais), couramment abrégé en EOF, est une condition dans un système d'exploitation informatique indiquant qu'aucune donnée supplémentaire ne peut être lue depuis une source de données. La source de données est d'habitude appelée un fichier, ou un flux. Dans le langage de programmation C, ou plus exactement, dans sa bibliothèque standard, les fonctions d'accès fichier et autres flux d'entrées-sorties peuvent retourner une valeur égale à la valeur symbolique EOF afin d'indiquer que la condition end-of-file est survenue. La valeur réelle de EOF est un nombre négatif, dépendante du système d'exploitation, communément -1. La macro EOF prend sa valeur grâce au préprocesseur, avant de compiler le code source. Sous tout environnement UNIX, une indication d'end-of-file peut être envoyée depuis un shell (console) en pressant les touches CTRL+D. Sous MS-DOS et Windows, elle correspond aux touches CTRL+Z. Dans certains cas, lorsqu'on a affaire à des fichiers texte ou à une lecture depuis un périphérique caractère (un périphérique orienté flux comme une carte vidéo par exemple), le shell MS-DOS de Microsoft (COMMAND.COM) ou des programmes utilitaires du système d'exploitation ajouteront, historiquement, un caractère control-Z en code ASCII à la fin du fichier disque (bien que les appels d'écriture fichier du kernel basique MSDOS.SYS n'en ajoutent jamais). Cela a été fait afin de conserver la compatibilité ascendante avec certaines particularités du CP/M, puisque le système de fichiers du CP/M n'enregistrait la longueur des fichiers qu'en termes de « combien d'enregistrements de 128 octets étaient alloués ». Le système de fichiers MS-DOS, quant à lui, a toujours enregistré la taille exacte, en octets, des fichiers, et ce depuis sa toute première version. Les caractères de contrôle ASCII sont non-imprimables dans des flux de caractères. Ils sont normalement représentés de manière plus lisible.
Les règles d'usage de l'information visent à lutter contre les abus liés aux usages des systèmes d'information, afin d'allier compétitivité, productivité, et conformité juridique de l'entreprise. À cet effet, le Cercle d'éthique des affaires et le Club informatique des grandes entreprises françaises (Cigref) ont publié une déontologie des usages des systèmes d'information. Les usages de l'information peuvent également être soumis à des règles de terminologie, qui peuvent être sensibles dans le cas de la terminologie informatique. En France, ces règles ont pris une forme juridique avec le décret du 3 juillet 1996 relatif à l'enrichissement de la langue française, qui impose l'usage de termes en français dans les administrations françaises, sous certaines conditions (termes proposés par la Commission générale de terminologie et de néologie, publiés au Journal officiel de la République française, et disponibles sur le site web FranceTerme).
En informatique, un environnement désigne, pour une application, l'ensemble des matériels et des logiciels système, dont le système d'exploitation, sur lesquels sont exécutés les programmes de l'application. On précise souvent le type d'environnement dont il s'agit. Ce sont généralement les suivants, par ordre d'apparition dans le cycle de vie : L'environnement de développement, sur lequel sont développés les programmes de l'application, L'environnement de qualification, sur lequel sont testés les programmes de l'application, L'environnement de formation, sur lequel les utilisateurs sont formés à l'application, L'environnement de production, sur lequel sont exécutés les programmes opérationnellement. Un environnement est le plus souvent implanté sur un seul serveur, mais il peut y avoir des exceptions (cas de l'informatique distribuée). Il peut y avoir plusieurs environnements hébergés sur un même serveur (l'environnement de qualification et l'environnement de formation par exemple). L'environnement de production nécessite tout particulièrement une surveillance continue, car tout dysfonctionnement peut interrompre l'utilisation opérationnelle de l'application, ce qui peut avoir des conséquences graves. Les incidents des traitements par lots peuvent bloquer l'utilisation de l'application par des populations entières d'utilisateurs.
Un environnement d'exécution ou runtime est un logiciel responsable de l'exécution des programmes informatiques écrits dans un langage de programmation donné. Un runtime offre des services d'exécution de programmes tels que les entrées-sorties, l'arrêt des processus, l'utilisation des services du système d'exploitation, le traitement des erreurs de calcul, la génération d'événements, l'utilisation de services offerts dans un autre langage de programmation, le débogage, le profilage et le ramasse-miette. Contrairement à un logiciel de développement permettant de programmer et développer son application, un runtime ne permet que l'exécution d'un programme. Un runtime peut être vu comme une machine virtuelle : de la même manière qu'un code natif est exécuté par le processeur, un code objet est exécuté par le runtime. Le runtime sert alors à exécuter du code objet en mettant le code natif ad hoc à disposition du processeur pour exécution. La définition est informelle, par exemple le livre IWarp anatomy définit un runtime comme suit : « nous définissons le runtime comme étant le composant logiciel responsable de l'exécution des programmes sur les systèmes iWarp, il facilite l'exécution des programmes en cachant les détails bassement matériels de la machine. ».
Un environnement pervasif (du latin "pervadere" pour "s'insinuer, se propager, s'étendre, envahir,...", également mentionné sous le vocable environnement ubiquitaire) correspond à un fonctionnement global de la communication où une informatique diffuse permet à des objets communicants de se reconnaitre entre eux et de se localiser automatiquement.
Une erreur de page (anglais : page fault) est dans le fonctionnement d'un ordinateur la découverte d'une différence entre le plan d'adressage et la mémoire vive. Cette interruption ou exception est émise par le matériel (l'unité de gestion mémoire du processeur) en direction du logiciel (le système d'exploitation). Cette erreur est fréquente et tout à fait bénigne dans les systèmes d'exploitation modernes car ceux-ci utilisent une grande quantité de mémoire virtuelle sans en avertir les applications.
En informatique, une erreur interne désigne une erreur qui est due à un défaut dans la conception d'un programme ou dans sa configuration, ou encore à des facteurs externes que le programme n'est pas capable de gérer (manque de mémoire, matériel défaillant…). Elle est en général la manifestation d'un bug. Ce type d'erreur s'oppose aux erreurs dues à une mauvaise manipulation de l'utilisateur final et ne peut en général être résolu que par les développeurs du programme ou les administrateurs responsables de sa configuration.
En informatique, le terme désigne deux choses assez différentes : Dans le contexte des processus, un état est l'ensemble formé par les données entrées, les données temporaires, les données calculées et le pointeur d'instruction (et généralement l'état des registres) par un processus ; Similairement, dans le contexte d'un protocole de communication structuré en requêtes vers un serveur suivi de réponses, le protocole peut nécessiter que le serveur conserve un état dépendant de requêtes passées (contraire de serveur stateless). Exemple : dans une session FTP un état est conservé entre deux requêtes (le serveur doit se souvenir si l'utilisateur s'est identifié, quel sont le dossier courant et les paramètres de transfert). Une requête HTTP peut nécessiter ou non un état, mais si l'architecture du serveur est ReST il n'aura pas besoin d'état, toute information nécessaire étant fournie dans l'URL de la requête HTTP. En gestion des données, un état, ou état d'impression, est une mise en forme (disposition, calculs) d'informations extraites d'une base de données, en vue de leur affichage ou de leur impression. La constitution d'états se fait sans qu'il y ait de modification des données utilisées, leur objet est de constituer diverses présentations synthétiques de ces données dans l'objectif de comprendre ce qu'elles recouvrent : statistiques, évolutions dans le temps, cumuls et sous-totaux, etc. Ce terme d'état est progressivement remplacé par l'anglicisme de 'reporting' en informatique décisionnelle.  Portail de l’informatique
Everyware (informatique omniprésente) est un mot-valise (formé à partir de everywhere et (hard|soft)ware) et un néologisme de l'auteur américain Adam Greenfield pour englober les termes d'informatique ubiquitaire, d'informatique pervasive, d'informatique ambiante et de médias tangibles.
Le facteur d'authentification est un facteur physique, cognitif ou biologique produisant une empreinte qu'un utilisateur peut prendre pour être authentifié par un système informatique. L'empreinte doit être personnelle à l'utilisateur et doit pouvoir être numérisée.
Le facteur de mobilité est à la fois une norme et un chiffre-clé de l’industrie informatique permettant d’évaluer les paramètres de performance mobile des ordinateurs portables. Trois indicateurs comptent particulièrement pour les utilisateurs professionnels et les hommes d’affaires en déplacement : l’autonomie, le poids et la robustesse. Le facteur de mobilité simple met en relation l’autonomie d'un ordinateur portable et son poids en kilogrammes (accumulateurs compris). On obtient le facteur de mobilité élargi en multipliant le quotient (autonomie/poids en kg) par la résistance de l’ordinateur portable à la pression (kgf). Ces indicateurs permettent d’effectuer une comparaison transparente entre les différents modèles d’une catégorie d’ordinateurs portables. Le facteur de mobilité fait passer au second plan les valeurs élevées des différents paramètres.  Portail de l’informatique
Un fichier informatique est au sens commun, une collection, un ensemble de données numériques réunies sous un même nom, enregistrées sur un support de stockage permanent, appelé mémoire de masse, tel qu'un disque dur, un CD-ROM, une mémoire flash ou une bande magnétique, et manipulées comme une unité,. Techniquement un fichier est une information numérique constituée d'une séquence d'octets, c'est-à-dire d'une séquence de nombres, permettant des usages divers. En vue de faciliter leur organisation, les fichiers sont disposés dans des systèmes de fichiers qui permettent de placer les fichiers dans des emplacements appelés répertoires ou dossiers eux-mêmes organisés selon le même principe de manière à former une hiérarchie arborescente. Un fichier comporte un nom de fichier qui sert à désigner le contenu et y accéder. Ce nom comporte souvent — notamment dans l'environnement windows — un suffixe - l´extension, qui renseigne sur la nature des informations contenues dans le fichier et donc des logiciels utilisables pour le manipuler. Chaque fichier comporte un certain nombre de métadonnées — informations concernant les informations— telles que suivant le système de fichier, la longueur du fichier, son auteur, les personnes autorisées à le manipuler, ou la date de la dernière modification. L'essence du fichier sont les informations qu'il contient. Le format de fichier est la convention selon laquelle les informations ainsi que les métadonnées sont numérisées et séquencées dans le fichier. Le format du fichier est propriétaire lorsque la convention n'est connue que de son auteur et n'a jamais été publiée. Le format du fichier est ouvert lorsque la convention est rendue publique en vue de permettre l'interopérabilité des logiciels le manipulant. Selon la nature et le format du contenu, les fichiers peuvent être qualifiés d'exécutables, de compressés, de textes, de documents, d'images, d'audio ou de vidéos. Avant l'arrivée des ordinateurs, les fichiers étaient des piles de fiches réunies dans des classeurs. L'utilisation des ordinateurs et du stockage magnétique a facilité et accéléré la manipulation de grands fichiers tels que ceux utilisés dans le commerce et l'administration publique. La manipulation des fichiers est un des services classiques offerts par les systèmes d'exploitation.
Dans un système informatique, un firmware (ou micrologiciel, microcode, logiciel interne, logiciel embarqué ou encore microprogramme) est un programme intégré dans un matériel informatique (ordinateur, photocopieur, automate (API, APS), disque dur, routeur, appareil photo numérique, etc.) pour qu'il puisse fonctionner.
Sur un réseau de commutation de paquets, le flux de paquets, le flux du trafic ou le flux du réseau désigne une succession de paquets d'un ordinateur source vers une destination, qui peut être une autre machine, un groupe multicast, ou un réseau broadcast. La RFC 2722 définit le flux de paquets comme une « logique artificielle équivalente à un appel ou une connexion ». La RFC 3697 définit le flux du trafic comme « succession de paquets envoyés d'une source donnée vers une destination particulière de type unicast, anycast ou multicast que la source souhaite labelliser comme étant un flux. Le flux peut être considéré comme la somme de tous les paquets [en mode connecté sur la couche transport] ou [un flux audio ou vidéo].  ». Le flux est également défini dans la RFC 3917 comme « un ensemble de paquets IP [passé sur un point donné du] réseau durant un certain intervalle de temps.  ».
Le formatage est l'action de formater, « prédisposer à reconnaître certains signaux et non d’autres, et les interpréter de façon prédéfinie ». C'est l’action de préparer un support de données informatique (disquettes, disques durs, clés USB, etc.) en y inscrivant un système de fichiers, de façon à ce qu'il soit reconnu par le système d'exploitation de l'ordinateur. Il existe de nombreux systèmes de fichiers différents : FAT, FAT32, NTFS, HFS, ext2, ext3, ext4, UFS, entre autres. Les disques de grande capacité peuvent recevoir plusieurs systèmes de fichiers, divisés en partitions logiques ; on parle alors de partitionnement. En pratique, on partitionne surtout des disques durs. Les autres périphériques de stockage peuvent l'être également, mais cette opération est souvent moins intéressante, et peut poser des problèmes de compatibilité. Le formatage peut rendre impossible l'accès aux données précédemment présentes sur le disque. En effet, si le formatage est un formatage à zéro, chaque bit de donnée est remplacé par un zéro, et les données sont perdues. Par contre, si le formatage est une simple réécriture de l'index, alors il est possible (quoique difficile et souvent aléatoire) de retrouver tout ou une partie des données.
En programmation informatique, un framework (appelé aussi cadre applicatif, cadre d'applications, cadriciel, socle d'applications ou encore infrastructure de développement) désigne un ensemble cohérent de composants logiciels structurels, qui sert à créer les fondations ainsi que les grandes lignes de tout ou d’une partie d'un logiciel (architecture). Un framework se distingue d'une simple bibliothèque logicielle principalement par : son caractère générique, faiblement spécialisé, contrairement à certaines bibliothèques ; un framework peut à ce titre être constitué de plusieurs bibliothèques chacune spécialisée dans un domaine. Un framework peut néanmoins être spécialisé, sur un langage particulier, une plateforme spécifique, un domaine particulier : communication de données, data mapping, etc. ; le cadre de travail (traduction littérale de l'anglais : framework) qu'il impose de par sa construction même, guidant l'architecture logicielle voire conduisant le développeur à respecter certains patrons de conception ; les bibliothèques le constituant sont alors organisées selon le même paradigme. Les frameworks sont donc conçus et utilisés pour modeler l'architecture des logiciels applicatifs, des applications web, des middlewares et des composants logiciels. Les frameworks sont acquis par les informaticiens, puis incorporés dans des logiciels applicatifs mis sur le marché, ils sont par conséquent rarement achetés et installés séparément par un utilisateur final.
Un logiciel gratuit, gratuiciel, ou freeware est un logiciel propriétaire distribué gratuitement sans toutefois conférer à l'utilisateur certaines libertés d'usage,, associées au logiciel libre. Les termes « gratuiciel » ou « logiciel gratuit », dont l'usage est préconisé par la commission générale de terminologie et de néologie en France, sont des traductions du mot anglais freeware, qui est une contraction de free (gratuit) et software (logiciel) — contraction qui prête à confusion en anglais avec free software qui désigne en anglais un logiciel libre. Un logiciel freeware peut fonctionner gratuitement pour une durée de temps illimité. L'auteur d'un logiciel freeware pourrait limiter les droits de copie et/ou de distribution de son logiciel. En tant que logiciel propriétaire le principe de freeware peut être une stratégie de marketing fondée sur des revenus indirects (support, produits liés, ...) plutôt que sur la vente du logiciel.
Un frontal (aussi appelé frontend ou front-end en anglais) est un équipement informatique. On l'oppose généralement au backend. En informatique, un frontal peut désigner une interface de communication entre plusieurs applications hétérogènes ou un point d'entrée uniformisé pour des services différents. Par exemple, dans les architectures web, on peut utiliser un serveur frontal HTTP pour traiter les requêtes générales et renvoyer certaines demandes de service vers un conteneur d'application (comme Tomcat) ou un serveur d'applications (comme JBoss, GlassFish, TomEE (en), Resin (en), ...). Plus généralement, il s'agit de la mise en place d'un serveur permettant la dissimulation d'un autre. Dans ce cas, le serveur frontal intercepte les requêtes utilisateur et les ré-envoie vers le serveur backend. Le serveur frontal agit donc comme un proxy. La mise en place d'un tel système crée un temps de latence relatif à la distance entre les deux serveurs.
Le fait de générer une entité dans le jargon informatique signifie produire cette entité et la matérialiser. Cette entité peut être de diverses natures : texte, son, image, données binaires, vidéo (en 2D ou 3D), etc. La matérialisation peut être volatile, c'est-à-dire non destinée à être stockée (affichage à l'écran, production d'un son dans des haut-parleurs, etc.) ou au contraire stockée c'est-à-dire enregistrée sur un support à long terme (disque dur, impression sur papier, CD-ROM, etc.).  Portail de l’informatique
En Informatique des algorithmes existent qui permettent de calculer une entrée valide pour produire une sortie prédéfinie. En anglais, ces algorithmes sont appelés goal seeking (recherche de but), what-if analysis ou bien back solving. Pour les algorithmes utilisés deux options existent : Les méthodes d'approximation d'un résultat, ou celui-ci est amélioré à fur et à mesure, Les méthodes qui utilisent la logique mathématique. La plupart des tableurs modernes possèdent cette fonctionnalité. Selon O'Brien et Marakas des analyses d'optimisation peuvent être faites pour obtenir une amélioration. Au lieu d'essayer de trouver une valeur spécifique, on essaye de trouver la valeur optimale pour une ou plusieurs variables, en tenant compte d'une ou plusieurs contraintes, comme on le fait dans l'optimisation linéaire.
Un groupware, collecticiel, ou encore logiciel de groupe, est un type de logiciel qui permet à un groupe de personnes de partager des documents à distance pour favoriser le travail collaboratif.
Le mot hackathon désigne un événement où un groupe de développeurs volontaires se réunissent pour faire de la programmation informatique collaborative, sur plusieurs jours. C'est un processus créatif fréquemment utilisé dans le domaine de l'innovation numérique.
Un hackerspace est un tiers-lieu où des gens avec un intérêt commun (souvent autour de l'informatique, de la technologie, des sciences, des arts...) peuvent se rencontrer et collaborer. Les Hackerspaces peuvent être vus comme des laboratoires communautaires ouverts où des gens (les hackers) peuvent partager ressources et savoir. Une traduction française est Laboratoire ouvert ou Transformatoire. Beaucoup de hackerspaces utilisent et participent à des projets autour du logiciels libres, du matériel libre, des ressources documentaires sous licence libre ou des médias alternatifs alimentant ainsi un patrimoine informationnel commun. Ils sont souvent physiquement installés dans des maisons des associations ou des universités, mais dès que le nombre d'adhérents et l'éventail des activités augmentent, ils déménagent généralement dans des espaces industriels ou d'anciens entrepôts.
En informatique, le concept d'historique des événements ou de journalisation désigne l'enregistrement séquentiel dans un fichier ou une base de données de tous les événements affectant un processus particulier (application, activité d'un réseau informatique…). Le journal (en anglais log file ou plus simplement log), désigne alors le fichier contenant ces enregistrements. Généralement datés et classés par ordre chronologique, ces derniers permettent d'analyser pas à pas l'activité interne du processus et ses interactions avec son environnement. Un fichier texte peut être la structure sous-jacente d'un fichier journal.  
Un ordinateur hôte est un terme général pour décrire tout ordinateur relié à un réseau informatique, qu'il fournisse des services à d'autres systèmes ou utilisateurs (serveur informatique, ou « système hôte ») ou soit un simple client. On utilise aussi le terme de système hôte pour désigner le système qui héberge un système virtuel par opposition à celui-ci. Le système virtuel utilise les ressources du système hôte. Cela s'appelle la virtualisation. Un périphérique est l'hôte d'une machine serveur quand il y est raccordé en tant que client.  
IMACD est un acronyme anglais dans le domaine de l'Infogérance: il signifie install (installation de nouveaux équipements), Move (déplacement de matériel informatique), Add (ajout de matériel ou de logiciel articles onsite ou offsite), Change (remplacement des pièces ou des swaps de modules), Disposal (recyclage et traitement des déchets) qui correspond à cinq étapes importantes dans le cycle de vie des processus d'installation d'équipements informatiques. Les services IMACD opèrent principalement dans le secteur B2B, les grandes entreprises, certaines PME, des institutions publiques et départements gouvernementaux. Malgré le fait que les services IMACD sont proposés par des professionnels de l'installation et de la gestion de parcs informatiques à travers le monde, et qu'ils représentent une part non négligeable du marché de l'informatique, leurs processus sont mal compris par de nombreux spécialistes informatiques qui à l'ère du PC (Personal Computer) oublient que l'installation informatique dans des entreprises est un cycle complexe fait de nombreux processus. Un autre problème, et celui-ci est inquiétant, concerne le recyclage (ou non) des déchets engendrés par ce secteur.
En informatique, une image système (en anglais, system image) est une copie de l'état d'un ordinateur sauvegardée sur un support non volatil comme un fichier. On dit qu'un ordinateur peut utiliser une image système s'il peut être arrêté puis restauré dans le même état au moyen de l'image système. Dans un tel cas, l'image système sert de sauvegarde.  Portail de l’informatique
Une interface est la couche limite par laquelle ont lieu les échanges et les interactions entre deux éléments. En informatique et en électronique, une interface est un dispositif qui permet des échanges et interactions entre différents acteurs : Une interface humain-machine permet des échanges entre un humain et une machine ; Une interface de programmation permet des échanges entre plusieurs logiciels ; Une interface, dans certains langages objet (Java, C++…), est la déclaration de signature(s) d'une fonction que toutes les classes héritantes devront dûment implémenter ; Il y a de nombreuses interfaces électroniques entre les différents dispositifs électroniques d'un appareil informatique.  En chimie et en physique, une interface est une surface entre deux produits où ont lieu des phénomènes tels que la catalyse (voir science des surfaces). En géographie une interface est une zone limitrophe entre deux espaces (région, ville, pays..) qui sert à des échanges commerciaux et culturels. Un hub est un point central d'échange d'un réseau de transport. Le terme d'interface est aussi utilisé pour désigner une personne qui sert d'intermédiaire pour des échanges entre des sourds-muets et des personnes entendantes.
Une interface définit la frontière de communication entre deux entités, comme des éléments de logiciel, des composants de matériel informatique, ou un utilisateur. Elle se réfère généralement à une image abstraite qu'une entité fournit d'elle-même à l'extérieur. Cela permet de distinguer les méthodes de communication avec l'extérieur et les opérations internes, et autorise à modifier les opérations internes sans affecter la façon dont les entités externes interagissent avec elle, en même temps qu'elle en fournit des abstractions multiples. On appelle aussi interfaces des dispositifs fournissant un moyen de traduction entre des entités qui n'utilisent pas le même langage, comme entre un être humain et un ordinateur. Étant donné que ces interfaces réalisent des traductions et des adaptations, elles entraînent des coûts de développement supplémentaires par rapport à des communications directes. Variétés : L'interface entre un être humain et un ordinateur est appelée interface utilisateur ; Les interfaces entre des composants de matériel informatique sont des interfaces physiques. Cet article traite des interfaces logicielles qui existent entre différents composants logiciels et fournissent un mode de programmation par lequel ces composants peuvent communiquer.
En informatique, iowait est une métrique logicielle appliquée aux systèmes d’exploitation. Elle correspond au temps d’attente du système pour l’écriture ou la lecture de données. Un « iowait » de 10 % signifie que 10 % de l’activité du processeur n’est pas destinée à faire des calculs mais à attendre l’exécution d’une opération de lecture/écriture. Dans les environnements UNIX, cette mesure peut être trouvée grâce à des commandes comme top ou sar (en)… Elle est toutefois dénuée de sens en environnement multi-cœur.
En informatique, le journal des transactions (en anglais, transaction log, transaction journal, database log, binary log ou audit trail) est la liste des transactions informatiques exécutées sur une base de données. Cette liste de transactions est utilisée pour rétablir l'intégrité de la base de données dans les cas de problèmes logiciels ou matériels du système qui gère la base de données. Physiquement, le journal des transactions est un fichier contenant une copie des modifications apportées à la base de données. Ce fichier est conservé sur un support non volatil pour être accessible même dans un cas de mauvais fonctionnement de l'ordinateur qui le gère. Lors du démarrage d'une base de données, le système de gestion de la base de données s'assure que la base de données a été fermée correctement et que la base de données est dans un état cohérent. Si ce n'est pas le cas, le système de gestion de la banque de données utilise le journal des transactions pour faire un retour en arrière (rollback) sur les transactions qui ne sont pas validées (committed). De plus, toujours en utilisant le journal des transactions, il réapplique les transactions validées dont les changements n'apparaissent pas dans la base de données. Ces corrections sont faites pour assurer l'atomicité et la durabilité des transactions.
En gestion de configuration, un Label est une étiquette qui est apposée sur un ensemble d'éléments de configuration. Cette marque permet généralement de marquer une configuration dans un certain état tel que la version d'un produit livré la dernière fusion de branches  Portail de l’informatique
Le débat « Linux ou GNU/Linux » est une controverse divisant les partisants du logiciel libre sur le nom à donner aux systèmes d'exploitation fondés sur le système GNU et un noyau Linux. Les plus nombreux (avec le grand public) l'appellent simplement « Linux », les autres (peut-être plus proches du projet GNU) l'appellent « GNU/Linux ».
Un livrable est un produit destiné à la livraison. En gestion de projet informatique, un livrable désigne tout composant matérialisant le résultat d'une prestation de réalisation (production ou TMA) à la Direction des Systèmes d'Information (DSI), c’est-à-dire toute production émise par le titulaire au cours du projet : document, courrier, module de code logiciel, dossiers de tests, application intégrée… Un livrable peut faire l'objet d'un bordereau de livraison, qui liste le contenu, la date, le fournisseur, le client, l'approbation par le client, et autres métadonnées.
En informatique, un logiciel spécifique est un logiciel développé sur commande à l'attention d'un client donné, par opposition à un logiciel standard, qui est un développé sur initiative d'un éditeur, et vendu à de nombreux clients. Le terme anglais correspondant à logiciel spécifique est "custom software", ou "bespoke software". Les Britanniques parlent de bespoke development pour désigner le développement spécifique (développement d'un logiciel spécifique). La construction d'un logiciel spécifique est une prestation de service, qui consiste à fournir l'expertise technique et la main d'œuvre nécessaire. Les fonctionnalités, le planning de livraison, et les conditions de paiement font l'objet d'un contrat entre le prestataire et le client. Le consommateur est fortement impliqué dans le processus de construction et signe la réussite du travail. La quasi totalité des logiciels spécifiques sont des logiciels applicatifs. Les acheteurs de logiciels spécifiques sont des moyennes et grandes entreprises. La construction de logiciels spécifiques est pratiquée depuis les années 1960, et elle était initialement la seule manière d'obtenir des logiciels applicatifs. En 1998 dans l'Union européenne, 45 % de la production de logiciels concerne des logiciels spécifiques.
Le concept de machine to machine (terme issu de l'anglais), parfois abrégé par le signe M2M, utilise les télécommunications et l'informatique pour permettre des communications entre machines, et ceci sans intervention humaine. En français, le M2M se traduit par « la communication de machine à machine », « la communication entre machines » ou encore « la communication intermachines ». Une définition plus générale de « la communication machine à machine » est l'association des technologies de l'information et de la communication (abréviation TIC), avec des objets dits intelligents et communicants et cela dans le but de fournir à ces derniers les moyens d'interagir sans intervention humaine avec le système d'information. Ce dernier peut appartenir indifféremment à une organisation ou à une entreprise. Comme toutes les technologies qui émergent, sa définition continue d'évoluer mais elle se réfère généralement à la télémétrie ou à la télématique. Cette technologie fonctionne en utilisant des réseaux et plus particulièrement les réseaux mobiles publics (comme le GPRS ou l'UMTS), ou des liaisons sans fil à courte distance, comme le Wi-Fi, le bluetooth ou le RFID. Apparaît alors la notion d'appareils connectés sur une boucle locale, cette dernière étant connectée à un concentrateur de données qui fait office de passerelle vers les bases de données des gros serveurs centraux ou vers le cloud.
La notion de médias, images et technologies de l'information et de la communication, ou MITIC, créée en Suisse à Genève par le service chargé du domaine concerné au Département de l'instruction publique, tend à élargir le concept de TIC (Technologies de l'information et de la communication) pour, d’une part, prendre en compte les convergences que le numérique a amené entre le domaine de l’informatique et celui des médias (presse, radio, télévision, édition multimédia, web), aussi bien que la banalisation d’internet comme environnement de travail quotidien et, d’autre part, dans le champ scolaire, rapprocher les domaines de l’informatique et de la critique de l’information (éducation aux médias) dont le développement s’est fait pendant longtemps de manière indépendante. Le domaine des MITIC est l’une des cinq thématiques transversales de la formation générale intégrée dans le Plan d’étude romand (PER) de la scolarité obligatoire qui a été introduit à partir de 2011 dans les cantons de Suisse romande. Les trois principaux objectifs d’apprentissage sont les suivants: Exercer un regard sélectif et critique (cycle 1), décoder la mise en scène de divers types de messages (cycle 2) et exercer des lectures multiples dans la consommation et la production de médias et d’informations (cycle 3).
Un métalangage est un formalisme conçu pour décrire rigoureusement un langage.
La méthode du canard en plastique, appelée également méthode du canard en caoutchouc, est une pratique de revue du code source utilisée en génie logiciel dans les phases de débogage.
La micro-informatique désigne ce qui a rapport avec les micro-ordinateurs. Ce dernier terme fut introduit par les développeurs du Micral (REE), pour désigner dans leurs brevets le premier ordinateur ayant un microprocesseur comme unité centrale, possibilité ouverte par les progrès en matière de circuits intégrés. Le préfixe micro- tient au fait qu'à cette époque, il existait déjà depuis les années 1940 des ordinateurs centraux qui tenaient dans de vastes salles machines et à partir de 1965 des mini-ordinateurs. Ce préfixe évoque le microprocesseur au cœur de la création et de la commercialisation de micro-ordinateurs. Dans le contexte de l'époque, la désignation du micro-ordinateur sous l'appellation « ordinateur personnel » est à ce titre très précise. En effet, l'ensemble du micro-ordinateur, c'est-à-dire l'unité processeur, la console clavier-écran et ses unités périphériques (disques, imprimante), tenait sur une table et n'était à la disposition que d'un utilisateur à la fois. En France, au début des années 1980, le terme « ordinateur individuel » était aussi employé ; c'est d'ailleurs resté comme titre d'un magazine informatique. Depuis la fin des années 1970 jusqu'au début du XXIe siècle, la micro-informatique a eu pour effet d'introduire l'informatique dans les petites et moyennes entreprises comme chez les particuliers. Cette démocratisation aurait été considérablement ralentie si IBM avait breveté les composants de son Personal Computer. En effet, l'apparition de machines compatibles facilement extensibles et le marché de masse ont permis d'avoir des composants génériques toujours plus puissants et bon marché. Cela a précipité la fin des ordinateurs centraux et des mini-ordinateurs qui sont remplacés par des fermes de serveurs voire par des grilles de calcul massivement parallèles ; et enfin cela a souvent permis l'éclosion d'une industrie locale du logiciel dont la société Microsoft est le plus bel exemple.
En architecture informatique, un middleware (anglicisme) ou intergiciel est un logiciel tiers qui crée un réseau d'échange d'informations entre différentes applications informatiques. Le réseau est mis en œuvre par l'utilisation d'une même technique d'échange d'informations dans toutes les applications impliquées à l'aide de composants logiciels. Les composants logiciels du middleware assurent la communication entre les applications quels que soient les ordinateurs impliqués et quelles que soient les caractéristiques matérielles et logicielles des réseaux informatiques, des protocoles réseau, des systèmes d'exploitation impliqués. Les techniques les plus courantes d'échange d'informations sont l'échange de messages, l'appel de procédures à distance et la manipulation d'objets à distance. Les middlewares sont typiquement utilisés comme ciment pour relier des applications informatiques disparates des systèmes d'information des entreprises et des institutions.
Une migration est, en informatique, le passage d'un état existant d'un système d'information ou d'une application vers une cible définie dans un projet ou un programme. La migration de données est généralement réalisée par programmation pour parvenir à un traitement automatisé, en libérant des ressources humaines de tâches embarrassantes. La migration est nécessaire lorsque des organisations ou des individus procèdent au changement des systèmes informatiques ou à leur mise à niveau.
Une mise à niveau désigne dans le vocabulaire informatique l'actualisation d'un logiciel ou d'un matériel. Le terme correspondant en anglais est upgrade. On parle parfois de « montée de version ». L'expression mise à niveau est peu utilisée car fréquemment remplacée par l'expression mise à jour par cause de méconnaissance des différences (subtiles) entre elles.
En informatique, le terme mock-up (qui vient du même mot anglais qui signifie une maquette à l'échelle 1:1) désigne un prototype d'interface utilisateur. Un mock-up a ainsi pour rôle de présenter les idées sur l'utilisation d'un logiciel.   Portail de l’informatique
Un mode de compatibilité est un mécanisme logiciel qui émule un processeur, système d'exploitation, et/ou une plateforme matérielle afin de permettre à des logiciels obsolètes de rester compatible avec le nouveau matériel ou logiciel. Cela diffère d'un émulateur. Un émulateur crée typiquement une architecture virtuelle sur le système hôte. Un mode de compatibilité translate les anciens appels de fonctions en appels que le système hôte peut comprendre. Le Mode Classic de Mac OS X et le mode de compatibilité de Windows XP, permettent tous deux à des applications conçues pour d'anciennes versions du système d'exploitation de tourner.  Portail de la programmation informatique
Le mode paysage est une expression du monde de l'informatique pour indiquer que la surface d'affichage est dans le sens de la largeur. En effet, la plupart des surfaces d'affichage (écran, feuille de papier...) sont rectangulaires. On pourrait donc les utiliser dans le sens de la hauteur ou dans le sens de la largeur. À l'opposé, l'utilisation de la surface dans le sens de la hauteur est appelé mode portrait. Les Xerox Alto utilisaient des écrans en mode portrait. En impression, les modes portrait et paysage sont nommés format à la française et format à l'italienne (voir l'article Reliure). Les écrans sont souvent utilisés dans le sens de la largeur. Les tablet PC ou les téléphones basculent en général leur mode d'affichage, soit sur demande de l'utilisateur, soit automatiquement grâce à un détecteur de gravité. Cette fonction permet d'optimiser l'utilisation de la surface d'affichage. Si ce que l'on souhaite afficher est plus large que haut, on utilisera le mode paysage, et vice-versa. De nombreux écrans et logiciels permettent l'usage du mode pivot.   Portail de l’informatique
On nomme mode pivot la possibilité de basculer un écran du mode paysage en mode portrait comme on le fait avec un téléphone ou une tablette. La taille et le poids des écrans demande que leur pied soit réglable en hauteur et muni d'un système de compensation de poids. Quelques écrans sont munis d'un capteur d'orientation que leur pilote sait gérer. L'écran se réorganise alors, au moins sous Windows. Dans le cas contraire, si l'écran est tactile, il est facile de commander sa réorientation avant ou après rotation. S'il ne l'est pas, on indique en général qu'on va procéder à celle-ci au préalable, car le maniement d'un pointeur de souris est plus délicat lorsque ses mouvements en horizontal et vertical ne correspondent plus à ceux de la souris. Le mode pivot a existé sur quelques dispositifs de traitement de texte dès les années 1980, mais n'est devenu vraiment populaire qu'avec la généralisation des écrans plats. Le mode portrait est particulièrement utile pour éviter le scrolling continuel dans les opérations suivantes : développement logiciel ; lecture de documents PDF ou d'aide en ligne ; consultation de sites Internet.  Portail de l’informatique
Le mode portrait est une expression du monde de l'informatique pour indiquer que la surface d'affichage est dans le sens de la hauteur. En effet, la plupart des surfaces d'affichage (écran, feuille de papier...) sont rectangulaires. On pourrait donc les utiliser dans le sens de la hauteur ou dans le sens de la largeur. À l'opposé, l'utilisation de la surface dans le sens de la largeur est appelé mode paysage. Les Xerox Alto utilisaient des écrans en mode portrait. En impression, les modes portrait et paysage sont nommés format à la française et format à l'italienne (voir l'article Reliure). Les écrans sont souvent utilisés dans le sens de la largeur. Les tablettes tactiles ou les téléphones basculent en général leur mode d'affichage, soit sur demande de l'utilisateur, soit automatiquement grâce à un détecteur de gravité. Cette fonction permet d'optimiser l'utilisation de la surface d'affichage. Si ce que l'on souhaite afficher est plus large que haut, on utilisera le mode paysage, et vice-versa. De nombreux écrans et logiciels permettent l'usage du mode pivot.
En informatique, un moniteur transactionnel (ou « TP monitor » pour « Transaction Processing Monitor ») est un système transactionnel de partage des ressources machine (mémoire, base de données, etc.). Les éléments gérés par le moniteur transactionnel sont des transactions dont le champ d'application varie suivant les produits. Le moniteur transactionnel gère les ressources et s'appuie sur des gestionnaires de transactions externes (comme un moteur de base de données, par exemple, ou un système de queues transactionnelles). Il ne gère pas directement les transactions elles-mêmes mais les coordonne.
En informatique, le multitâche préemptif désigne la capacité d'un système d'exploitation à exécuter ou arrêter une tâche planifiée en cours. Un ordonnanceur préemptif présente l'avantage d'une meilleure réactivité du système et de son évolution, mais l'inconvénient vient des situations de compétition (lorsque le processus d'exécution accède à la même ressource avant qu'un autre processus (préempté) ait terminé son utilisation).
La multitextualité est un concept émergent (2005). Elle peut être : présentielle par l’existence conjointe, sur une feuille de papier ou sur un écran, de deux ou plusieurs documents de nature hétérogène ; langagière s'il s'agit du même texte en deux langues différentes. Ou encore d’un document dynamique et du code qui l’a généré — PHP, HTML, etc. ; hypertextuelle par la coprésence, via des hyperliens, de deux ou plusieurs documents de nature hétérogène. Par exemple, un article d’encyclopédie en ligne et d’autres documents de types différents — document extensionnel, publication en ligne, etc. La multitextualité hypertextuelle — au contraire du texte livresque séquentiel — permet de présenter des concepts dans un « volume ». Dans chaque texte, à chaque endroit opportun, un lien peut donner accès à : un concept de niveau plus élevé — une classe par exemple ; un concept de niveau inférieur — sous-classe ; une instanciation ou un exemple (texte ou image) ; une définition ; un commentaire ; etc. La relation du lecteur au texte et au sens s’en trouve profondément modifiée. Si, comme le suggèrent les travaux de la médiologie, la pensée humaine évolue par le moyen du texte et du sens, de nouveaux modes de pensée devraient émerger au fil de l’hypermodernité.  Portail de l’informatique
Le terme neek provient d'une contraction les termes nerd et geek, soit en anglais "a cross between a nerd and a geek.". Les termes nerd et geek sont proches : Nerd appuie l'idée de connaissances certaines en un domaine défini, ici l'informatique, Geek souligne l'aspect passionné de la personne, le plus souvent aussi dans le domaine de l'informatique. Or, nerd a une connotation plus péjorative car le terme nerd désigne une personne introvertie et solitaire.
L'expression not invented here (littéralement « pas inventé ici » ou de manière développée « ce truc n'a pas été développé par nous ») désigne de manière péjorative l'attitude de membres d'une organisation qui rejettent les innovations survenues hors de son contrôle. Selon ses détracteurs, cette attitude peut amener à privilégier des solutions moins efficaces ou à dupliquer des recherches déjà effectuées ailleurs. Les raisons (bonnes ou mauvaises) sont souvent : les standards de qualité de l'entité extérieure ne sont pas les mêmes l'adaptation courent au besoin flexibilité aux besoins futurs indépendance vis-à-vis des aléas de vie d'une entité extérieure favoritisme purement relationnel Les effets (négatifs ou positifs) sont : la duplication de la recherche produit une vérification de validité et une évolution des produits, au contraire de l'adoption d'une norme diminuer l'interopérabilité des outils ainsi construits des outils moins diffusés et utilisés sont moins testés et ont moins de développeurs des outils moins diffusés et utilisés ont moins de chance de correspondre à d'autres besoins généraux et sont donc de moins en moins utilisés (cercle vicieux) En français, l'expression réinventer la roue ou réinventer la roue carrée désignent encore plus péjorativement des attitudes voisines.  Portail de la production industrielle  Portail de l’informatique
La désignation open source, ou « code source ouvert », s'applique aux logiciels (et s'étend maintenant aux œuvres de l'esprit) dont la licence respecte des critères précisément établis par l'Open Source Initiative, c'est-à-dire les possibilités de libre redistribution, d'accès au code source et de création de travaux dérivés. Mis à la disposition du grand public, ce code source est généralement le résultat d'une collaboration entre programmeurs. Le mouvement open source s'est constitué en 1998 sous l'impulsion d'Eric Raymond et s'est développé en opposition au mouvement du logiciel libre qui prône des valeurs philosophiques et politiques de justice, tandis que l'open source se focalise sur des considérations techniques de développement logiciel et ne voit pas de problème à l'utilisation de systèmes intégrés combinant logiciels propriétaires et logiciels open source. Dans la pratique toutefois, la très grande majorité des logiciels open source sont également libres, l'exception la plus notable étant les logiciels pratiquant la tivoisation. L’open source a déjà investi tous les grands domaines du système d’information des administrations françaises : environnements serveurs, domaines applicatifs, outils d’ingénierie, solutions de réseaux et sécurité. Les solutions open source sont désormais au même rang que les solutions propriétaires dans le paysage des logiciels du secteur public. Les décideurs effectuent d’ailleurs de plus en plus leur choix à partir d’un jugement éclairé, en comparant systématiquement solutions propriétaires et solutions libres.
En informatique, une page-écran désigne ce qu'un écran peut afficher à un moment donné. Ce terme date de l'époque où les interfaces utilisateur étaient en mode caractère.  Portail de l’informatique
Dans le monde informatique, la paramétrabilité couvre tout l'aspect des possibilités de configuration d'une application.
En informatique, le partitionnement d'un support de stockage (disque dur, SSD, carte-mémoire...) est l'opération qui consiste à le diviser en régions — ou partitions dans lesquelles les systèmes d'exploitation présents sur la machine peuvent gérer leurs informations de manière séparée et privée. Chaque système d'exploitation est libre de diviser les partitions qu'il utilise de la manière qui lui convient; il le fait généralement en y créant un système de fichiers qui n'est autre qu'une manière de subdiviser l'espace disponible en y plaçant des repères “logiques” (par opposition aux subdivisions matérielles qui n'ont pas d'incidence pour l'utilisateur) et en maintenant à jour des tables d'allocations pour déterminer à quoi sont utilisés les différents sous-espaces. Les différentes partitions apparaissent aux systèmes d'exploitation qui les repèrent, chacun à sa manière, en leur donnant en général le nom de "disque" (désormais impropre puisque pouvant désigner des unités de stockage basés sur la mémoire flash comme les SSD) ou "volume". Les systèmes Windows les présentent généralement à l'utilisateur en les repérant par des lettres (C:, D:, etc.), alors que les systèmes Mac OS les présentent sous forme d'icônes sur le bureau. Dans les systèmes UNIX ou Linux, elles apparaissent à l'utilisateur de façons différentes selon l'outil utilisé et peuvent être parfois masquées car noyées dans l'arborescence unique des dossiers (typiquement, la désignation est de la forme /dev/sda pour un support matériel, et si deux partitions sont créées elles seront désignées par /dev/sda1, /dev/sda2). On nomme « partition d'amorçage » (parfois par abus de langage « partition primaire ») celle dans laquelle le micro-code, après avoir accompli l'initialisation du matériel, va chercher les premières instructions à exécuter pour continuer le processus de démarrage. En général, ce micro-code y trouve un chargeur d'amorçage qui lui permet, soit de démarrer l'unique système d'exploitation présent sur l'ordinateur, soit de présenter à l'utilisateur un choix entre différents systèmes chargeables. Un support de stockage peut être partitionné pour différentes architectures. On trouve ainsi le partitionnement de type MBR (partitionnement Intel) longtemps employé sur la majorité des ordinateurs personnels de type PC pour les supports d'une capacité inférieure ou égale à 2 To (le partitionnement MBR étant limité par l'adressage en 32 bits), ou GPT pour des architectures plus récentes (Macintosh, Linux, et PC depuis les années 2010) conçues pour exploiter pleinement les supports de capacité supérieure à 2 To.
Le passage à l'échelle, anglicisme pour le redimensionnement et la mise à l'échelle, est la faculté qu’a un système à pouvoir changer de taille ou de volume selon les besoins des utilisateurs. La disponibilité et la latence (temps de réponse) font partie des critères de qualité d'un site web. Le nombre d’utilisateurs d’un site variant de jour en jour, il s’avère difficile et coûteux de prévoir les ressources nécessaires au bon fonctionnement du site. Ainsi, posséder la faculté d'être capable de passer facilement l’application à l’échelle s'avère donc un atout majeur pour un site. Le cloud computing répond à ce besoin. Il permet d’ajuster les ressources matérielles de manière dynamique et transparente en fonction de la charge utilisateurs pour assurer une disponibilité continue du site et un délai de réponse correct.
Un patch ou correctif, est une section de code que l'on ajoute à un logiciel, pour y apporter des modifications : correction d'un bug, traduction, crack.
La physicalisation, qui s'oppose dans ce sens à la virtualisation, est un terme définissant le fait d'assembler un grand nombre de serveurs physiques dédiés équipés de processeurs très basse consommation (ultra-low power) dans un même châssis. Ce système s'intègre dans une « informatique éco-responsable » (éco-TIC). Son concept peut se rapprocher de celui du serveur lame.
Un pilote informatique, souvent abrégé en pilote, est un programme informatique destiné à permettre à un autre programme (souvent un système d'exploitation) d'interagir avec un périphérique. En général, chaque périphérique a son propre pilote. De manière simpliste, un pilote d'imprimante est un logiciel qui explique à l'ordinateur comment utiliser l'imprimante. Sans pilote, l'imprimante ou la carte graphique par exemple ne pourraient pas être utilisées. Certains systèmes d'exploitation comme Windows proposent leurs propres pilotes génériques censés fonctionner de manière satisfaisante avec la plupart des périphériques pour une utilisation courante. Si ces pilotes gèrent les grandes fonctions communes à tous les matériels, ils n'ont pas toujours toutes les capacités des pilotes de constructeurs, qui seuls connaissent parfaitement et en détail les spécifications du matériel piloté.
Dans le jargon informatique, un plantage (parfois appelé, par anglicisme, « crash ») est une interruption anormale, souvent inattendue, d’un logiciel (lequel peut aussi bien être une application qu’un système d'exploitation) due à une panne, un incident ou un bug. Un plantage peut avoir plusieurs conséquences : le programme s’interrompt abruptement (parfois avec un message d’erreur produit par le programme lui-même ou par le système d’exploitation), le programme ne répond plus (fenêtre aréactive), l'image à l'écran se fige (la machine ne réagit plus aux sollicitations des périphériques d'entrée, et aucune évolution de son état n’est apparente), ou l'ordinateur peut redémarrer plus ou moins subitement (après en avoir averti l’utilisateur ou non). Un plantage a souvent pour origine soit une division par 0, soit une instruction non attendue (ou utilisant une valeur non attendue). Le plantage peut se produire à 3 niveaux différents : Logiciels : la conséquence est une aréaction. De nos jours, la plupart des systèmes d'exploitation permettent de reprendre la main (automatiquement ou manuellement) et d'arrêter le fonctionnement du logiciel, en perdant les données non sauvegardées dans le logiciel en question, mais sans avoir besoin de redémarrer l'ordinateur. Système d'exploitation : Techniquement, c'est un logiciel, sauf qu'en cas de plantage, l'utilisateur n'a en général plus de moyen d'action sur son ordinateur (redémarrage forcé avec perte de toutes les données non sauvegardées). Matériel : plantage d'un processeur (surchauffe, rayon cosmique, fréquence trop haute, erreur de conception, etc.) Le terme peut être utilisé comme verbe : « le système s'est planté ». Il est recommandé par l’office québécois de la langue française et usuellement utilisé dans d'autres pays francophones comme la France. On utilise parfois les termes plus généraux d’« incident » et de « panne ».
Le concept de plasticité désigne, en informatique et particulièrement dans le domaine des interfaces homme-machines, « la capacité d'une interface à s'adapter aux contraintes matérielles et environnementales dans le respect de son utilisabilité ».
En informatique, une plate-forme est un environnement permettant la gestion et/ou l'utilisation de services applicatifs. La terminologie peut notamment désigner : un système d'exploitation ou un noyau ; un environnement d'exécution comme une machine virtuelle ; un environnement de développement, à condition qu'il intègre son environnement d'exécution ; un serveur web ou d'applications, notamment une plate-forme de téléchargement  ; une application web ou logicielle, par exemple : un ENT, un SGBD, une bibliothèque multimédia, une plate-forme de jeux vidéo, un centre de maintenance pour un appareil ou un réseau informatique.   Portail de l’informatique
Un point de contrôle ou un point de contrôle d'application (en anglais, application checkpoint) est un point d'un programme informatique où s'effectue, lors de l'exécution du programme, un arrêt au cours duquel sont copiées, sur un support externe (par exemple, un disque dur), les informations nécessaires afin de permettre, en cas d'arrêt ultérieur, la reprise du traitement à partir du dernier point de contrôle.
Le power-on self-test (POST, l’auto-test au démarrage) désigne la première étape du processus plus général appelé amorçage. Lors de cette étape, le programme intégré à la carte-mère (le BIOS pour Basic Input/Output System) teste la présence des divers périphériques et tente de leur attribuer les ressources nécessaires à un fonctionnement sans conflit. Le POST terminé, le contrôle sera cédé au système d’exploitation, qui éventuellement corrigera ou affinera les réglages du BIOS. Le POST existe aussi pour d’autres architectures telles que certains routeurs, certaines imprimantes, mais c’est grâce aux ordinateurs personnels qu’il s’est démocratisé.
Un program temporary fix, abrégé PTF est un sigle popularisé par IBM pour les envois réguliers (tous les quatre à six mois) de correctifs à ses clients. Il définit un ensemble de patches et de corrections (correctifs ou évolutifs) à appliquer à un logiciel (ou groupe de logiciel : en l'occurrence au moins le système d'exploitation). PTF et APAR sont souvent utilisés comme synonymes (surtout dans le monde IBM mainframe), mais intrinsèquement ce n'est pas pareil.
Dans les réseaux informatiques et les télécommunications, un protocole de communication est une spécification de plusieurs règles pour un type de communication particulier. Initialement, on nommait protocole ce qui est utilisé pour communiquer sur une même couche d'abstraction entre deux machines différentes. Par extension de langage, on utilise parfois ce mot aussi aujourd'hui pour désigner les règles de communication entre deux couches sur une même machine.
Queries per second (anglais : « requêtes par seconde »), abrégé en QPS, est un terme informatique correspondant au nombre de requêtes qu'un serveur informatique accepte de traiter par seconde.  Portail de l’informatique
Une solution quick-and-dirty (anglicisme signifiant littéralement « rapide et sale » qu’on peut traduire par « vite fait mal fait ») est, en informatique et en biologie, une stratégie ou un protocole élaboré avec peu de soin, mais présentant l’avantage d’être une façon rapide de résoudre un problème particulier.
La remise à zéro est un terme utilisé en informatique pour désigner le fait d'affecter à une ou plusieurs variables une valeur prédéfinie, souvent zéro. Il est parfois synonyme de réinitialisation, mais on peut faire une RAZ sur une seule variable, ou un seul champ dans un formulaire. C'est la version française de l'anglicisme reset. Il s'agit d'un acronyme surtout employé en informatique et dans les terminaux de réception, TNS Télévision Numérique Satellite, TNT (adaptateur) et console client ADSL..  Portail de l’informatique
Les « réseaux intelligents » sont des réseaux matériels de distributions de fluides (électricité, eau, gaz, pétrole...), et/ou d'information (télécommunications) qui ont été « augmentés » (rendus intelligents) par des systèmes informatiques, capteurs, interfaces informatiques et électromécaniques leur donnant des capacités d'échange bidirectionnel et parfois une certaine capacité d'autonomie en matières de calcul et gestion de flux et traitement d'information.
Le rétroportage, ou backporting en anglais, est l'action consistant à aller piocher une modification développée pour une nouvelle version d'un logiciel afin d'en faire bénéficier une version plus ancienne de ce logiciel (au moyen d'un logiciel patch). Cela permet de maintenir sélectivement à jour une version d'un logiciel sans avoir à la remplacer entièrement par une version plus récente qui nécessiterait de nouveaux tests de qualité.
Le rPerf est une unité de mesure de puissance de serveur informatique propre à IBM, destinée à estimer les performances d'un système POWER AIX d'un point de vue commercial, et à comparer les puissances relatives de ces systèmes dans un environnement transactionnel. Contrairement au TpmC, il n'existe pas de publication officielle soumise à une organisation indépendante, ni de publication pour des modèles d'autres constructeurs. L'unité dérive d'un modèle analytique, à la fois du TPC et du SPEC basé sur un modèle bien défini : le pSeries 640 monoprocesseur équipé d'un CPU Power3-II à 375 MHz, possédant un cache primaire de 32/64 Ko et un cache secondaire de 4 Mo. Par définition le rperf de ce système a été déclaré égal à 1 rPerf. Le rPerf est utilisé pour passer outre certaines contraintes de publication officielle des TPC-C. Il n'y a pas de correspondance officielle, car les tests ne sont pas totalement identiques. La mesure de rPerf a remplacé le ROLTP (Relative On Line Transaction Performance). Une mesure rPerf ne tient pas compte des entrées/sorties disques ni du réseau. Toutefois, le constat est que ces étalonnages représentent la puissance d'un système utilisé en transactionnel, et l'usage est de considérer qu'un rPerf vaut environ 8 900 tpmC.
En informatique, la sauvegarde (backup en anglais) est l'opération qui consiste à dupliquer et à mettre en sécurité les données contenues dans un système informatique. Certains utilisateurs ont pour objectif final de sauvegarder leurs fichiers dès le moment de leur enregistrement comme celui qui vient de saisir un texte de loi dans un traitement de texte. Ce terme est à distinguer de deux notions proches : l'enregistrement des données, qui consiste à écrire des données sur un périphérique, tel qu'un disque dur, une clé USB, des bandes magnétiques, où les informations demeureront même après l'extinction de la machine, contrairement à la mémoire vive. l'archivage, qui consiste à enregistrer des données de manière à garantir sur le long terme leur conformité à un état donné, en général leur état au moment où elles ont été validées par leurs auteurs. La sauvegarde passe forcément par un enregistrement des données, mais pas nécessairement dans un but d'archivage.
En informatique matérielle et logicielle et en télécommunications, la scalability ou scalabilité (calque de traduction) désigne la capacité d'un produit à s'adapter à un changement d'ordre de grandeur de la demande (montée en charge), en particulier sa capacité à maintenir ses fonctionnalités et ses performances en cas de forte demande. Selon René J. Chevance, le mot anglais scalability, formé sur l'adjectif scalable dérivé du verbe to scale (« changer d'échelle »), « n'a pas d'équivalent communément admis en français ». Les traductions utilisées sont extension graduelle, évolutivité, facteur d'échelle, extensibilité, passage à l'échelle, ou capacité à monter en charge. En radio-télévision, on parle d’échelonnabilité . Le calque français scalabilité est également utilisé ou alors le mot anglais scalability est conservé tel quel. La scalabilité peut faire référence à la capacité d’un système à accroître sa capacité de calcul sous une charge accrue quand des ressources (généralement du matériel) sont ajoutées. L’expression porte un sens similaire quand elle est utilisée dans un contexte commercial, quand l’extensibilité d’une entreprise implique que le modèle économique offre le potentiel d’une croissance économique pour l’entreprise. Cet aspect doit être étudié de près dans tout service réseau, sujet en particulier à des afflux très brusques de trafic (ex: publicité télévisée, réaction à une actualité sensible, début d'un évènement sportif, effet Slashdot, etc.), lesquels peuvent provoquer une augmentation importante du temps de réponse entraînant le découragement des usagers, une baisse de rentabilité, et parfois jusqu'à l'impossibilité technique de servir tous les clients.
Dans le domaine de l'informatique, le terme schéma est relatif à l'organisation de données, en particulier dans les annuaires et les bases de données en général.
Un schéma directeur informatique est une étape majeure pour la définition, la formalisation, la mise en place ou l'actualisation d'un système d’information. Ce document de synthèse est établi par la direction informatique et validé par la direction générale de l'organisation. Pour un horizon déterminé, le document final décrit de manière concrète comment le système d'information et l'informatique vont être déployés pour répondre aux objectifs fixés et fournir les services attendus. L'élaboration d'un tel document résulte généralement d'une démarche projet censée offrir une vue globale de l’état actuel du système, une spécification des besoins et la définition des orientations à prendre. C'est dire qu'il résulte d'une démarche anticipatrice et normative. Un schéma directeur peut donner lieu à suggestion de plusieurs scénarios dont le sort est tranché par la direction générale. Au bout du compte toutes les décisions arrêtées sont clairement explicitées et font l'objet d'un échelonnement et d'une programmation dans le temps : finalités visées, procédures à réaliser, sélection des moyens et ressources nécessaires et suffisantes, séquences et étapes à respecter. Outre la planification des actions, le schéma directeur doit déboucher sur une évaluation d'un budget associé pour ce faire. Par son ampleur, le schéma directeur constitue l'étape fondatrice du cycle de vie d’un système informatique : il fait la lumière sur les options possibles, les choix opérés et la programmation échelonnée dans le temps des actions retenues pour accompagner l'organisation dans son développement. Le lancement de l’étape schéma directeur doit nécessairement intervenir dès lors que l'informatique représente un facteur clé de succès dans une organisation ( du fait des enjeux en cause ou de son importance). Cette étape est également indispensable lorsque le système en place, fiable à une époque précédente, n’est plus fiable aujourd’hui (on dit de ce système qu’il a épuisé son cycle de vie).
SD-WAN est l'acronyme de Software-Defined Wide Area Network, soit réseau étendu à définition logicielle, et est présenté comme la nouvelle évolution majeure des télécommunications. Un SD-WAN facilite la gestion en séparant la partie contrôle de la partie réseau.
Dans du texte, la casse (minuscule ou majuscule) des lettres peut parfois changer la signification. Les mots en lettres capitales (haut de casse) n'ont pas toujours le même sens s'ils sont écrits en minuscules (bas-de-casse). Par exemple, Rennes est une ville mais rennes désigne des caribous. Dans le Système international d'unités, M est le symbole du préfixe méga et m celui de milli. L'unité newton doit son nom à Newton. Et en astronomie, l'étoile HD 189733 B n'est pas la planète HD 189733 b. La sensibilité à la casse (traduction de l'anglais case sensitivity) est une notion informatique signifiant que dans un contexte donné, le sens d'une chaîne de caractères (par exemple un mot, un nom de variable, un nom de fichier, un code alphanumérique etc) dépend de la casse (capitale ou bas-de-casse) des lettres qu'elle contient. Ce terme tire son origine de la casse en typographie. La sensibilité à la casse peut intervenir dans le tri par ordre alphabétique comme dans les comparaisons. La sensibilité à la casse s'applique typiquement : Aux systèmes de gestion des fichiers (nommage des fichiers), surtout dans les systèmes de la famille Unix ; Aux mots clés et aux identifiants de nombreux langages informatiques (C, C++, C#, Java, Python, PHP, XML et de nombreux autres). Cependant, d'autres langages sont insensibles à la casse, comme Fortran, BASIC, Pascal, Ada, Common Lisp, HTML, ou encore SQL ; Aux comptes d'utilisateur ; Aux mots de passe.  Lorsqu'un ordinateur compare plusieurs textes pour évaluer leur égalité, il peut le faire en ou non les considérer égaux seulement en fonction de leur casse. La comparaison de deux chaînes de caractères alphanumériques est généralement plus complexe en mode insensible à la casse et dépend du codage des chaînes de caractères données ; plus il est simple, plus simple est le calcul. Ainsi, une comparaison sur une chaîne codée en ASCII et ne contenant donc pas de caractères accentués sera sans doute sensiblement plus rapide qu'en UTF-8 (unicode/ ISO-10646). Les fonctions de hachage en MD2, en MD5 ou en SHA256 sont également sensible à la casse.
Un serveur bureautique est un équipement informatique utilisé pour centraliser les fonctions d’impression, de partage de fichiers ou de connexion Internet, par exemple. Ce terme regroupe principalement les serveurs suivants : Serveur d'impression ; Serveur de fichiers.  Portail de l’informatique
En informatique, un serveur central ou un serveur multiservice est un serveur qui centralise plusieurs services. Les serveurs centraux sont utilisés généralement dans une architecture centralisée a contrario d'une architecture décentralisée (dite distribuée). On trouve généralement ce type de serveur au sein de petite structure (PME, PMI, TPE et TPI) qui possède un budget limité pour organiser leur système d'information, mais qui privilégie une architecture centralisée de type client-serveur plus facilement évolutive (en ajoutant un serveur supplémentaire par exemple) que l'historique architecture terminal-serveur (en) commandé par un unique ordinateur central. On parle de serveur central dans un réseau d'utilisateurs ou de machines. Il existe différents types de centralisation : la centralisation de données - qui stocke l'ensemble des données sur un serveur concernant l'ensemble des éléments du réseau informatique ; la centralisation de direction - où un seul serveur décide de quel machine ou quel utilisateur fait quoi et quand ; la centralisation de communication - toutes les communications passent par le serveur central. La centralisation de données, de direction et de communication était à l'origine la principale organisation des réseaux informatiques.
Un serveur d'impression est un serveur qui permet de partager une ou plusieurs imprimantes entre plusieurs utilisateurs (ou ordinateurs) situés sur un même réseau informatique. Le serveur dispose donc : d'une connexion réseau (par exemple, un port RJ45 pour un réseau ethernet) gérant les protocoles réseaux (par exemple, TCP/IP, NetBEUI, AppleTalk); d'une ou plusieurs connexions à des imprimantes. La plupart des serveurs d'impression disposent de connexions USB ; certains disposent également de ports parallèles. Certains serveurs d'impressions ne sont pas connectés directement par leur câble d'interface aux imprimantes. Ces dernières sont connectées via le réseau, en effet, les imprimantes professionnelles sont généralement connectées directement sur le réseau pour permettre une répartition au sein des locaux de l'entreprise. Le serveur d'impression peut être constitué d'un ordinateur qui partage une imprimante qui lui est directement connectée (ou à travers le réseau), ce peut également être un petit appareil spécialisé dédié. L'avantage de cette dernière solution est son faible prix. Un serveur d'impression doit toujours rester sous tension et il est préférable qu'il ait une adresse IP fixe. Il peut être situé sur un poste client : à partir du moment où l'imprimante est connectée sur un ordinateur et que celle-ci est partagée, ce poste devient ce que l'on nomme un serveur d'impression. Les documents à imprimer sont placés sur des files d'attente (spool) puis envoyés petit à petit à l'imprimante. Le système d'impression qui est le plus utilisé aujourd'hui sous Linux et Unix est CUPS (Common Unix Printing System). Pour communiquer avec les imprimantes et les clients, les serveurs d'impressions utilisent une grande variété de protocoles tels LPD/LPR, IPP utilisé par CUPS, NetBIOS, AppSocket utilisé par les serveurs d'impression JetDirect ou encore IPX/SPX.
Un serveur de démarrage (boots servers en anglais) a comme tâche principale de partager avec les terminaux un système de fichiers qu'ils utilisent entre autres pour effectuer leur démarrage.
Un serveur dédié infogéré est un serveur dédié qui est géré par un hébergeur ou une société spécialisée. Souvent il s'agit d'une option payante. Peu d'hébergeurs français offrent l'infogérance d'un serveur dédié. L'infogérance d'un serveur dédié est un service qui comprend le matériel, le logiciel et le bon fonctionnement d'un serveur dédié. En cas de dysfonctionnement, l'hébergeur en est immédiatement informé et se charge de relancer les services corrompus (par exemple, en cas de problème de base de données, surcharge du serveur, de processus bloqués, etc.) Suivant les contrats, une équipe est disponible 24/24 pour surveiller le serveur et intervenir si nécessaire. Bénéficier d'un service d'infogérance est avant tout un souci en moins car une équipe se charge des mises à jour de sécurité et du bon fonctionnement du serveur.  Portail de l’informatique
Un serveur informatique est un dispositif informatique matériel ou logiciel qui offre des services, à un ou plusieurs clients (parfois des milliers). Les services les plus courants sont : l'accès aux informations du World Wide Web ; le courrier électronique ; le partage d'imprimantes ; le commerce électronique ; le stockage en base de données ; la gestion de l'authentification et du contrôle d'accès ; le jeu et la mise à disposition de logiciels applicatifs (optique Logiciel en tant que service). Un serveur fonctionne en permanence, répondant automatiquement à des requêtes provenant d'autres dispositifs informatiques (les clients), selon le principe dit client-serveur. Le format des requêtes et des résultats est normalisé, se conforme à des protocoles réseaux et chaque service peut être exploité par tout client qui met en œuvre le protocole propre à ce service. Les serveurs sont utilisés par les entreprises, les institutions et les opérateurs de télécommunication. Ils sont courants dans les centres de traitement de données et le réseau Internet. D'après le cabinet Netcraft, il y a en mars 2014 plus de 412 millions de serveurs web dans le monde, et leur nombre est en augmentation constante depuis l'invention du World Wide Web en 1991.
Un serveur sans état est un serveur qui traite chaque requête de façon indépendante, sans relation avec les requêtes précédentes. Il s'appuie sur un protocole sans état qui contient au minimum le format de la requête en entrée et le format de la réponse en sortie. Les protocoles HTTP et IP sont des exemples de protocoles sans état.  Portail de l’informatique
En informatique et en télécommunication, une session est une période délimitée pendant laquelle un appareil informatique est en communication et réalise des opérations au service d'un client - un usager, un logiciel ou un autre appareil.
Un shareware, partagiciel ou contribuciel, est un logiciel qui peut être utilisé gratuitement généralement durant une certaine période ou avec des fonctionnalités limitées. Après cette période d'essai, l'utilisateur doit rétribuer l'auteur s'il veut continuer à utiliser le logiciel ou avoir accès à la version complète.  
En informatique, le spooling est une technique qui consiste à mettre des informations dans une file d'attente (spool) avant de les envoyer à un périphérique. Les informations sont mises en attente par un premier processus en même temps qu'un deuxième processus (le spooler) les lit et les envoie au périphérique. Cette technique est utilisée depuis 1960 pour exploiter des périphériques lents tels que les imprimantes, lecteurs de carte perforée, modems, etc.. Le mot spool est à l'origine l'acronyme de simultaneous processing operations on line. Il a été introduit par IBM en 1960, puis est devenu un mot du vocabulaire informatique. Par extension, le spool d'impression désigne le fichier d'édition envoyé au périphérique d'impression. Il regroupe l'ensemble des ordres que l'imprimante doit exécuter pour mener à bien l'impression d'un ou plusieurs documents. La technique du spooling a été mise en œuvre la première fois dans l'ordinateur IBM 7070. En 2011 de nombreux systèmes d'exploitation comportent un spooler pour les imprimantes. Certaines imprimantes (Hewlett-Packard, Apple) ont un spooler incorporé,.
Stepping, ou step, est un terme informatique utilisé pour indiquer une « révision » d'un microprocesseur, ce qui n'implique pas forcément de modifications importantes dans son architecture. Il s'agit d'un concept utilisé principalement par les grands producteurs mondiaux de microprocesseurs, Intel et AMD, pour indiquer que la conception d'un processeur spécifique a évolué par rapport à son design original. Ce terme est généralement accompagné d'une combinaison de lettres ou de chiffres.  Portail de l’informatique  Portail de l’électricité et de l’électronique
Les stéréotypes sont un des trois mécanismes d'extensibilité en Unified Modeling Language. Ils permettent aux concepteurs d'étendre le vocabulaire de l'UML, afin de créer des éléments de ce nouveau modèle, dérivés de ceux qui existent déjà, mais qui ont des propriétés spécifiques qui sont adaptées à un domaine particulier ou autre usage spécialisé. Par exemple, lors de la modélisation d'un réseau, il peut être nécessaire d'avoir des symboles pour représenter les routeurs et concentrateurs. Graphiquement, un stéréotype est rendu comme un nom encadré par des guillemets et placé au-dessus du nom d'un autre élément. Par exemple, dans un diagramme de classe les stéréotypes peuvent être utilisés pour classer les méthodes par comportement telles que « constructor » et « getter ». Malgré son apparence, l'« interface » n'est pas un stéréotype, mais un classificateur UML.  Portail de l’informatique
Sucre syntaxique et sel syntaxique sont des expressions utilisées en informatique à propos des langages de programmation.
Le synchronisme désigne le caractère de ce qui se passe en même temps, à la même vitesse. L'adjectif synchrone définit deux processus qui se déroulent de manière synchronisée. Il s'oppose à asynchrone. Il est particulièrement important dans le domaine technique, celui de l'électrotechnique et l'électronique. En particulier, on parle en électrotechnique de machines synchrones pour des machines tournantes dont la fréquence de rotation est égale à celle du réseau électrique. Il s'utilise aussi dans le domaine de l'audiovisuel pour la connectique. En informatique, les systèmes sont fréquemment fondés sur ce principe, qui permet des transferts d'information fiables, que ce soit au niveau des réseaux ou au niveau du processeur. Du point de vue du programmeur, une méthode est synchrone si elle attend une réponse avant de retourner la sienne. En histoire, le synchronisme retrace la simultanéité des événements dans un cadre déterminé (pays, continent, monde) sans que ces évènements soient obligatoirement dépendants directement les uns des autres. Dans le domaine du doublage, le synchronisme consiste à caler sur les mouvements des lèvres des acteurs originaux le texte doublé.
En informatique, le syndrome du plat de spaghettis est une dégradation qui touche les systèmes informatiques trop fortement couplés. Le système devient coûteux à maintenir et sujet aux pannes.
Big Blue = IBM Bill = Billiou = Bill Gates Box = boîtier en anglais, comprendre la boîte métallique qui contient les éléments de l'ordinateur browser = navigateur Web butiner = consulter le World Wide Web Cupertino = Apple (Apple est basé à Cupertino, en Californie) explorateur web = navigateur Web (« navigateur » vient de Netscape Navigator, « explorateur » vient de Internet Explorer) fureteur = navigateur Web (surtout utilisé au Canada) mail = email = courrier électronique = courriel (officiel depuis 2003 en France) Le Net = Internet La Toile = le Web = World Wide Web Redmond = Microsoft (Microsoft est basé à Redmond, près de Seattle) surfer = consulter le World Wide Web librairie = bibliothèque (certains informaticiens considèrent qu'il s'agit d'un abus de langage résultant d'une mauvaise traduction du faux ami en anglais library mais le mot librairie était déjà synonyme de bibliothèque en France au Moyen Âge avant d'arriver dans la langue anglaise, voir référence 1 dans l'article bibliothèque   Portail de l’informatique
Un système hérité, système patrimonial ou legacy system en anglais est un matériel et/ou logiciel continuant d'être utilisé dans une organisation (entreprise ou administration), alors qu'il est supplanté par des systèmes plus modernes. L'obsolescence de ces systèmes et leur criticité les rendent difficilement remplaçables sans engendrer des projets coûteux et risqués. Par exemple, les banques et assurances qui ont informatisé leur traitement des informations dans les années 70 ont des applications qui tournent avec du code hérité souvent en COBOL ou en Fortran. Les risques pris pour réécrire l'application dans un autre langage et les coûts inhérents au changement dissuadent souvent la modernisation du système voire son remplacement.
Le terme systèmes ouverts peut désigner à la fois des systèmes informatiques : - qui fournissent un ensemble d'avantages en interopérabilité, portabilité, et standards ouvert de logiciel; - ou configurés pour permettre des accès non restreints par des personnes et/ou des ordinateurs ; Cet article ne traite que du premier sens. Le terme trouve son origine à la fin des années 1970 et au début des années 1980, principalement pour décrire les systèmes basés sur le système d'exploitation Unix, spécialement par opposition aux systèmes dits propriétaires, ordinateurs centraux et micro-ordinateurs en usage à cette époque. Au contraire de ces anciens systèmes propriétaires, la nouvelle génération de systèmes Unix s'est caractérisée sur des interfaces de programmation standardisées et sur des interconnexions de périphériques ; le développement du matériel et du logiciel par des tiers fut encouragé, en partant de la norme de cette époque, qui voyait des compagnies comme Amdahl et Hitachi se tourner vers la justice pour obtenir le droit de vendre des systèmes et des périphériques qui étaient compatibles avec les ordinateurs centraux d'IBM. On peut dire que la définition de "système ouvert" s'est formalisée dans les années 1990, avec l'émergence de standards de logiciels administrés de façon indépendante comme Single UNIX Specification de l'Open Group. Bien que les utilisateurs d'ordinateurs aujourd'hui soient habitués à un haut niveau d'interopérabilité entre le matériel informatique et le logiciel, avant l'an 2000, les systèmes ouverts purent faire l'objet d'une promotion de la part de fournisseurs Unix, car ils avaient un avantage compétitif significatif. IBM et d'autres compagnies résistèrent à cette tendance pendant des décennies, dont un exemple significatif est donné par la déclaration d'un commercial d'IBM en 1991, selon lequel on devrait être « prudent de ne pas rester enfermé dans les systèmes ouverts ». Cependant, dans la première partie du XXIe siècle, beaucoup de ces fournisseurs de systèmes propriétaires, particulièrement IBM et Hewlett-Packard, commencèrent à adopter Linux comme une partie de leur stratégie commerciale d'ensemble, avec open source markété comme un atout de « système ouvert ». En conséquence, un ordinateur central IBM avec open source Linux on zSeries est markété comme étant plutôt un système ouvert que des serveurs utilisant Microsoft Windows — ou même ceux utilisant Unix, en dépit de leur héritage de systèmes ouverts. En réaction, de plus en plus de compagnies ouvrent le code source de leurs produits, avec l'exemple notable de Sun Microsystems et la création de Openoffice.org et des projets OpenSolaris, basés sur leurs produits logiciels dont le code source était préalablement fermé, StarOffice et Solaris.
En informatique, une tâche est une unité d’exécution dite aussi unité de travail. Il s'agit d'un terme global qui peut être précisé par une dénomination plus spécifique tel que le processus, le processus léger, le fil d'exécution et le mécanise de requête-réponse (en). Dans le diagramme adjacent, on trouve un exemple de tâche ou une file d'attente de travaux à effectuer (task queue) est rangée en entrée d'un groupement de fils d’exécution (thread pool) afin d'être traités puis stockés en sortie dans une file de travaux terminés (completed tasks). L'ensemble du travail ou les tâches qui effectuent ce travail peuvent tous être appelés « tâches ».
Technologies de l'information et de la communication (TIC : transcription de l'anglais information and communication technologies, ICT) est une expression, principalement utilisée dans le monde universitaire, pour désigner le domaine de la télématique, c'est-à-dire les techniques de l'informatique, de l'audiovisuel, des multimédias, d'Internet et des télécommunications qui permettent aux utilisateurs de communiquer, d'accéder aux sources d'information, de stocker, de manipuler, de produire et de transmettre l'information sous toutes les formes : texte, musique, son, image, vidéo et interface graphique interactive (IHM). Les textes juridiques et réglementaires utilisent la locution communications électroniques. Les nouvelles technologies de l'information et de la communication (NTIC) ouvrent des problématiques résultant de l'intégration de ces techniques au sein des systèmes institutionnels, recouvrant notamment les produits, les pratiques et les procédés potentiellement générés par cette intégration.
La télémétrie est une technologie qui permet la mesure à distance et la journalisation d'informations d'intérêt vers le concepteur du système ou un opérateur. Le mot est dérivé des racines grecques tele (à distance) et metron (mesure). Ces systèmes requièrent des instructions et des données à envoyer dans le but de réaliser l'exploitation requise. La contrepartie de la télémétrie est la télécommande.[pas clair] La télémétrie est utilisée en particulier dans les dernières versions de Microsoft Windows, Windows 7, Windows 8 et Windows 10.
Dans le domaine de l'informatique, le temps de réponse est une mesure de la performance d'une application interactive. Il peut être défini comme le temps qui s'écoule entre la fin d'une demande adressée à un ordinateur et le début de la réponse. C'est une notion qui comporte des aspects techniques (serveurs, réseau informatique,...) mais aussi des aspects liés à l'ergonomie de l'interface entre l'utilisateur et le système.
Cet article traite de la terminologie en langue française de la distribution informatique.
La théorie du matelas d'eau (en anglais, Waterbed theory) est l'observation, décrite par Larry Wall que quelques systèmes ont une complexité minimale, et que tenter de réduire cette complexité à un endroit ne fera que l'augmenter à un autre endroit. Ce comportement est assimilé à celui d'un matelas d'eau sur lequel lorsque l'on fait pression à un endroit, l'eau se répartit sur le reste du matelas, parce que l'eau est incompressible. La théorie du matelas d'eau a été citée comme partie significative de la philosophie de design du Perl 6.
En informatique, et particulièrement dans les bases de données, une transaction telle qu'une réservation, un achat ou un paiement est mise en œuvre via une suite d'opérations qui font passer la base de données d'un état A — antérieur à la transaction — à un état B postérieur et des mécanismes permettent d'obtenir que cette suite soit à la fois atomique, cohérente, isolée et durable (ACID). atomique : la suite d'opérations est indivisible, en cas d'échec en cours d'une des opérations, la suite d'opérations doit être complètement annulée (rollback) quel que soit le nombre d'opérations déjà réussies. cohérente : le contenu de la base de données à la fin de la transaction doit être cohérent sans pour autant que chaque opération durant la transaction donne un contenu cohérent. Un contenu final incohérent doit entraîner l'échec et l'annulation de toutes opérations de la transaction. isolée : lorsque deux transactions A et B sont exécutées en même temps, les modifications effectuées par A ne sont ni visibles par B, ni modifiables par B tant que la transaction A n'est pas terminée et validée (commit). durable : Une fois validé, l'état de la base de données doit être permanent, et aucun incident technique (exemple: crash) ne doit pouvoir engendrer une annulation des opérations effectuées durant la transaction. La majorité des systèmes de gestion de base de données hiérarchiques comme relationnels du marché permettent de réaliser des transactions atomiques, cohérentes, isolées et durables. Les systèmes NoSQL n'y prétendent pas .
Le terme ubimedia a été inventé par Adam Greenfield (en). Il caractérise l’informatique omniprésente. Ce terme décrit une informatique qui envahit notre quotidien justifiée par une simplification des tâches effectuées par les individus. Cette informatique est censée nous faire gagner du temps, soulager notre cerveau en externalisant la mémorisation de certaines informations logistiques (numéros de téléphone, itinéraires, dates de rendez-vous, etc.), autrement dit, nous simplifier la vie.
L'ubiquité ou l'omniprésence est la capacité d'être présent en tout lieu ou en plusieurs lieux simultanément. Le terme est dérivé du latin « ubique » qui signifie « partout ». La notion se rencontre aussi bien dans les domaines de l'informatique, de la biologie, de l'écologie, de la géologie, de la sociologie, et des mathématiques.
Undecimber ou Undecember est un terme anglais désignant un treizième mois d'un calendrier qui en comporte normalement douze. Duodecember représente de la même façon un quatorzième mois.
En informatique, un utilisateur intensif (power user en anglais) est un utilisateur d'un ordinateur personnel qui a la capacité d'utiliser des fonctions avancées des programmes qui sont généralement hors de portée des utilisateurs « normaux ». Il n'est pas nécessairement capable de programmer ou d'administrer des systèmes. Dans le domaine des logiciels d'entreprise (en), l'utilisateur intensif peut désigner un spécialiste d'un processus particulier.
En technique ou informatique, la versatilité exprime l'affinité d'un appareil terminal, mobile ou fixe, vis-à-vis des différents réseaux auxquels on peut le connecter : filaires (LAN avec support Ethernet), cellulaires (GSM, GPRS, UMTS, i-mode) et sans-fil (WPAN puce BlueTooth, WLAN avec carte ou puce WI-FI ou HomeRF). Plus un terminal possède de possibilités de passage d'un réseau à l'autre, plus il sera versatile. Employé dans ce sens, il s'agit d'un anglicisme. L'adjectif français correct est polyvalent. On parlera donc de terminal polyvalent pour désigner un terminal multi-standard ou multi-mode. Le terme de versatilité renvoie à l'inconstance, la fugacité et l'inconsistance d'un sujet.  Portail de l’informatique
La virtualisation consiste à exécuter sur une machine hôte dans un environnement isolé des systèmes d'exploitation — on parle alors de virtualisation système — ou des applications — on parle alors de virtualisation applicative. Ces ordinateurs virtuels sont appelés serveur privé virtuel (Virtual Private Server ou VPS) ou encore environnement virtuel (Virtual Environment ou VE).
Virtuel est un adjectif (qui peut être substantivé : le virtuel) utilisé pour désigner ce qui est seulement en puissance, sans effet actuel. Il s'emploie souvent pour signifier l'absence d'existence. À partir des années 1980, cet adjectif est aussi utilisé pour désigner ce qui se passe dans un ordinateur ou sur Internet, c'est-à-dire dans un « monde numérique » par opposition au « monde physique ».
Web-brigades (en russe : Веб-бригады) est un mot-valise créé par la journaliste russe Anna Polianskaïa, calquée sur les Red brigades (Brigades rouges, en russe : Красные бригады) pour dénommer des groupes d'utilisateurs du réseau informatique international, contributeurs ou hackers, qui interviennent sur Internet pour faire valoir un point de vue, effacer des données déplaisantes, défendre une cause commerciale, religieuse, idéologique, politique ou autre, et détruire les sites ou les contributions de leurs contradicteurs. Ces groupes peuvent soit se former spontanément en raison de l'identité, des convictions ou d'intérêts de leurs membres, soit être constitués et coordonnés par les instances communicantes d'une entreprise, confession, boîte à idées, parti politique ou tout autre groupe de pression.
En informatique, un wordmark (en français, marque de mot) est un bit utilisé sur certains ordinateurs ayant des mots de longueur variable (par exemple, les ordinateurs de la série IBM 1400 ou l'ordinateur IBM 1620) pour indiquer la fin d'un mot. Parfois, un autre terme que wordmark est utilisé (par exemple, flag) lorsque le bit servant au marquage de la fin d'un mot peut aussi servir à d'autres usages.  Portail de l’informatique
.BAT est l'extension d'un fichier de commandes MS-DOS. Réaliser un tel fichier permet de concevoir des scripts qui seront interprétés par le "shell" ou interpréteur de commandes (command.com ou cmd.exe) pour notamment exécuter des fichiers .EXE ou .COM. Cette extension est principalement utilisée sur les systèmes d'exploitation de Microsoft (DOS et Windows). Elle peut être assimilée (dans une certaine mesure) à l'extension .sh des scripts shell Unix (ceux du Bourne shell plus exactement). .BAT tire son nom de l'anglais batch, qui signifie lot. .BAT sert à créer des programmes facilement à l'aide du bloc note. Si le fichier est inconnu, il est préférable de ne pas le lancer (à l'aide d'un double clic), mais au contraire il faut l'éditer (à l'aide d'un clic droit puis éditer) pour voir ce qu'il contient. En effet, il peut s'agir d'un script malveillant tel qu'un cheval de Troie. Dans l'interpréteur COMMAND.COM (et CMD.EXE), il existe quelques commandes spécifiques aux fichiers de traitements par lots. Voici la liste : CHOICE (externe) ECHO (interne) FOR (interne) GOTO (interne) IF (interne) PAUSE (interne) REM (interne) SHIFT (interne) TYPE (interne) CALL (interne)
.cat est un domaine de premier niveau commandité créé pour la langue et la culture catalane. Il a été approuvé le 16 septembre 2005 par l'ICANN. Il est géré par la Fundació puntCat. Le domaine .cat est le premier domaine de premier niveau réservé à une communauté linguistique. Il sert de domaine aux pages web en langue catalane ou ayant trait à la culture catalane.
Un fichier cue sheet est un fichier texte ayant l’extension .cue et contenant les métadonnées d’une image CD ou DVD. Il accompagne souvent un fichier .bin, .flac ou .ape car ces formats ne permettent pas un découpage en pistes. Il accompagne souvent aussi un fichier audio correspondant à un enregistrement non-stop tel qu'un mix ou enregistrement en concert, permettant ainsi d'accéder aux différentes parties de l'enregistrement.
Le .exe est une extension de nom de fichier qui désigne un fichier exécutable. Cette extension identifie le fichier principal de tous les programmes exécutables fonctionnant sous les différentes versions de ces systèmes d'exploitation[Lesquels ?]. Un double clic permettra d'exécuter les tâches prévues pour ces programmes. Elle est utilisée pour nommer les fichiers exécutables des systèmes d'exploitation Microsoft Windows, DOS ou encore OpenVMS, Symbian OS ou OS/2. Ces fichiers sont (généralement) distribués aux utilisateurs parmi les applications au travers des stores d'applications, des programmes d'installation de logiciel, ou préinstallés par le constructeur de l'ordinateur. Sous cette extension se cachent différents formats de fichiers, définis avec l'évolution de DOS et Windows et les autres OS, dont l'utilisateur n'a généralement pas à se soucier. La convention, sous Windows, pour lancer une application, pour l'utilisateur lambda est d'accéder à celle-ci au travers du "menu démarrer" ou au travers de fichiers de raccourcis sur le bureau pour les versions anciennes de Windows, ou encore au travers de l'interface Modern UI pour les versions récentes. Les techniciens peuvent aussi utiliser des interfaces textuelles comme command.com et powershell au travers de la console windows en tapant le nom d'une commande, sans son extension. À chaque application correspond un ou plusieurs fichiers à l'extension .exe qui permettent d'exécuter le programme afin de l'utiliser. On peut trouver des fichiers .exe en lançant l'explorateur de fichier de Windows explorer, en prenant soin de le configurer afin d'afficher les extensions des noms de fichiers et en ouvrant le dossier d'installation des applications sous Windows, généralement un dossier nommé "Program Files", ou en ouvrant le dossier d'installation de windows, généralement C:\Windows dans lequel on peut voir par exemple l'éditeur de texte notepad.exe, livré avec le système d'exploitation, ou des utilitaires d'administration comme l'éditeur de base de registre.
.info est un domaine de premier niveau générique non-restreint d'Internet. Ce TLD est géré par les services de la société Afilias et a été activé dans les root server le 27 juillet 2001. Ce domaine est ouvert à tous sans restriction mais est destiné aux sites Web qui veulent informer leurs utilisateurs plutôt que de vendre quelque chose ou être simplement une vitrine. Le domaine .info, avec en janvier 2015 environ 5.5 millions de domaines, fut dès le départ de loin le domaine le plus populaire des 7 nouveaux domaines créés en 2001-2002 (.aero, .biz, .coop, .info, .museum, .name et .pro).
Le format .ipa est le format de fichiers utilisé par les applications du système d'exploitation iOS.
.Mac (prononcer Dot Mac) est un service payant d'Apple principalement destiné aux ordinateurs Macintosh, les autres plateformes n'ont alors pas accès à tous les services de .Mac. Durant l'été 2008, Apple a lancé MobileMe, un service multi-plateforme qui remplace .Mac au même tarif. Puis, le 12 octobre 2011, c'est iCloud qui prend le relais. Service gratuit, il permet de faire les mêmes choses seuls sont absents le service iDisk, la publication web via l'application iWeb et une galerie privée pour mettre des photos.
Les fichiers .MCO (Messenger Content Objets) sont des winks (ou des clins d’œil en français), qui sont envoyés sous forme d'animations Flash à un contact sur Windows Live Messenger.  Portail de l’informatique
.NET Micro Framework est une version de .NET adaptée au monde de l'embarqué et plus spécifiquement aux appareils ayant les ressources les plus restreintes. Les sources du .NET Micro Framework sont disponibles sur CodePlex.
.nfo, une extension de nom de fichier accompagnant généralement des fichiers ou dossiers pirates, est une « marque de fabrique » (souvent avec des logos en ASCII art) des différentes équipes travaillant sur des projets de crackage et de piratage de logiciels, tel que des jeux, des programmes multimédia, ou des logiciels utilitaires. Il est écrit en texte brut. Pour voir l'art ASCII fait en nfo, il faut utiliser le logiciel notepad avec une petite taille de police, ou un lecteur spécialisé comme DAMN NFO Viewer. Il est aussi possible de visualiser un fichier .nfo avec le wordpad de windows en choisissant l'option (ouvrir avec…) et de sélectionner Wordpad. Microsoft Windows utilise aussi les fichiers d'extension nfo par le programme System Information, mais la fréquence de ces fichiers est beaucoup plus rare.
.pro est un domaine de premier niveau générique restreint d'Internet qui est actif depuis juin 2004. Ce domaine est destiné aux personnes ayant un statut professionnel reconnu. Il s'agit notamment de comptables, de médecins, d'avocats, de notaires, d'ingénieurs, d'électriciens, d'architectes ou d'autres professionnels pouvant démontrer leurs qualifications. Le bureau d'enregistrement exige que le demandeur fournisse un numéro de compagnie valide ou une preuve qu'il a une certification valide et vérifiable émise par un ordre professionnel reconnu dans son pays (ou l'équivalent). À noter qu'il ne faut pas prendre cela comme une garantie que le registraire a correctement vérifié cette information, ou que le propriétaire du domaine n'a pas été suspendu depuis par son ordre professionnel.
.quebec est un domaine de premier niveau parrainé créé pour les sites en rapport avec la province du Québec.
L’extension de fichier *.sys est utilisée pour un codage exécutable (non sous forme de texte), souvent pour Mac OS ou Windows. Sur Windows c'est l'extension des pilotes de périphériques. Il était utilisé par exemple pour les fichiers io.sys (driver du DOS), msdos.sys (noyau du DOS) et config.sys (configuration du DOS). Les fichiers .sys sont stockés habituellement dans le dossier C:\Windows\system32\drivers sur les systèmes Windows.
˽ (02FD en Unicode) est un caractère typographique utilisé en informatique pour signaler explicitement la présence d'une espace, concurremment au tiret bas.  Portail de l’écriture  Portail de l’informatique
Le $, ou symbole dollar, est principalement utilisé en référence à plusieurs unités monétaires que sont le dollar et le peso.
Le ฿, ou symbole baht, est utilisé comme symbole de quelques unités monétaires dont le baht, le balboa ou il s'agit aussi d'un des symboles non officiels utilisé pour le Bitcoin.
01net magazine, anciennement L'Ordinateur individuel (OI), est une revue d'informatique mensuelle grand public, créée en octobre 1978 par le groupe Tests (aujourd'hui groupe 01), alors connu pour ses revues destinées aux professionnels de l'informatique : 01 Informatique, Informatique et Gestion (en liaison avec l'association AFCET) et Bureau Gestion.
La 1T-SRAM (1-Transistor Static Random Access Memory) est un type de mémoire vive développée par MoSys. Elle a un temps de latence de 1 ns, et elle est utilisée dans la GameCube de Nintendo, ainsi que dans son successeur, la Wii. Le '1T' fait référence à 'un transistor' alors que le bit d'une SRAM est généralement composé de 6 transistors.   Portail de l’électricité et de l’électronique  Portail de l’informatique
La 3D est interactive lorsqu'il est possible d'agir dessus en temps réel. Si le contenu de l’affichage est calculé en temps réel, cela signifie que l'utilisateur peut modifier a tout moment l'un de ses paramètres. Le contenu de la scène va donc évoluer en fonction des actions de l'utilisateur. Le jeu vidéo en est l'application la plus populaire. En effet, un joueur peut dialoguer avec des personnages, influer sur la trajectoire d'une voiture, modifier l'apparence de son avatar, ou tout simplement naviguer dans un menu. Pourtant le terme 3D Interactive est surtout utilisé dans les secteurs de l'industrie et de la communication pour la différencier de la fonction ludique du jeu vidéo. En effet, dans ces secteurs la 3D est utilisée pour de la formation, de la présentation de maquette, de la communication produit. Les applications les plus courantes sont : Visite Virtuelle Maquette 3D Réalité virtuelle Réalité augmentée Lorsque l'application utilise le ludique, on parle de : Advergame s'il s’agit d'un jeu visant à promouvoir un produit. Serious Game lorsque le jeu est un moyen ludique pour enseigner des sujets sérieux.
Le 3D XML est un format de données 3D créé par Dassault Systèmes, éditeur de logiciels CAO (CATIA, SolidWorks). La spécification a été rendue publique le 15 juin 2005. La société, dont tous les produits de la gamme supportent le format, propose également des lecteurs gratuits: 3D XML Player est disponible sur plateforme Windows pour Microsoft Office, Internet Explorer et Firefox. 3DVIA Player, édité par sa filiale Virtools, est disponible pour la plupart des navigateurs sous Windows et Mac OS X. IBM permet également d'intégrer des objets 3D XML dans les documents Lotus Notes. Le 3D XML est un container zip pouvant contenir des données en XML et des données propriétaires. Il ne faut pas le confondre avec le format ouvert X3D malgré la proximité des noms. Dassault Systèmes a d'ailleurs quitté le consortium Web3D afin de développer ce format. Dassault Systemes of America, filiale américaine de Dassault Systemes, accorde une licence annuelle gratuite, tacitement reconductible ne couvrant que les usages internes.
Le format 3DMF est un ancien format utilisé notamment sous Mac OS 9 dans le fameux Album. Ce format était utilisé pour représenter des images en 3 dimensions. Ces images pouvait être grossies, déplacées, tournées. Ce format est encore utilisé aujourd'hui. Il est possible de réaliser ses propres images 3DMF sur Mac ou Windows.  Portail de l’informatique
Le 3DNow! est un jeu d'instructions multimédia développé par AMD. Il est introduit en 1997 avec le K6-2. Cette technologie basée sur les opérations SIMD (Single Instruction Multiple Data), a donc pour vocation d’accélérer le traitement du processeur. Elle regroupe 21 instructions et huit registres 64 bits FP / MMX. Chaque instruction 3DNow! permet de traiter jusqu’à deux opérations entières ou flottantes par cycle d’horloge, ce qui en théorie double les performances. La mise en œuvre du 3DNow! dans les processeurs AMD permettait d’exécuter jusqu’à deux instructions 3DNow! par cycle d’horloge lorsqu’il s’agissait par exemple de deux additions ou de deux multiplications. On arrivait donc, en théorie, à des performances multipliées par quatre. Le 3DNow! a été créé par AMD pour pallier la faiblesse de ses K6 dans les calculs en virgule flottante. Les performances dans ce domaine commençaient en effet à devenir de plus en plus cruciales pour les jeux et les applications multimédias et Intel avait pris une grande avance en la matière avec son Pentium II. Le 3DNow! a donc servi de palliatif à AMD en attendant l’Athlon. Le 3Dnow! permettait également de prendre une longueur d'avance en accélérant les calculs vectoriels pour la première fois sur une technologie CISC, ce genre de technologie existant sur les processeur RISC, tel que, par exemple, le PowerPC, dès le début des années 1990. Devant l'efficacité de cette méthode, en 1999, Intel répond au 3Dnow! en ajoutant à ses processeurs x86 le SSE, qui est similaire mais incompatible. Le 20 août 2010, AMD annonce l'abandon du jeu d'instructions 3DNow! pour ses futurs processeurs à l'exception de deux instructions (PREFETCH et PREFETCHW), au profit du jeu d'instructions SSE et de ses successeurs. Ces instructions sont aussi disponibles sur certains processeurs Intel.
3ivx est un système de compression vidéo MPEG-4 propriétaire et commercial qui permet de faire de la vidéo de qualité supérieure aux formats MPEG-4 usuels à débit similaire[réf. nécessaire]. Il est ainsi possible de stocker plus de deux heures de vidéo de qualité légèrement inférieure à celle d'un DVD sur un seul cédérom ou d'effectuer du flux proche de la qualité DVD avec une connexion câble ou ADSL. Le décodeur 3ivx permet de lire les variantes MPEG-4 (y compris DivX 3, 4 et 5, Apple MPEG-4 et Xvid). Le décodeur 3ivx produit une vidéo de bonne qualité grâce à la qualité de ses filtres de traitement et à sa conception de base ainsi qu'à son optimisation entretenant une vitesse de décodage élevée. Cela permet d'encoder de la vidéo MPEG-4 en fichiers .avi, .mov ou .mp4 (avec AAC) avec l'application d'encodage de votre choix. Contrairement à XviD qui est totalement libre et gratuit ou à DivX dont seul l'encodeur est payant, 3ivx est payant à tous les stades d'utilisation (licence à 6,95 USD pour le seul décodeur ou à 19,95 USD pour l'ensemble codeur et décodeur en version personnelle et à 29,95 USD et 99,95 USD pour la version professionnelle).
3PAR Inc. est un constructeur américain de matériel informatique spécialisé dans le stockage. Il produit notamment une gamme de baies de stockage disque pour les entreprises. 3PAR a été fondé en 1999, et a son siège à Fremont (Californie).En 2008, 3PAR pénètre le marché français par un contrat de revente exclusive avec une entreprise française (AntemetA). En avril 2010, il a été reconnu comme étant la quatrième société de technologie en termes de croissance par le magazine Forbes. 3PAR a été racheté par Hewlett-Packard le 2 septembre 2010, après une série de surenchères avec Dell, pour un montant de 2,4 milliards de dollars ce qui représente plus de 10 fois son chiffre d'affaires. Parmi les innovations de 3PAR figurent l'introduction de la notion de thin provisioning, qui simplifie l'allocation d'espace disque et introduit la possibilité de surréservation, ainsi que la virtualisation du stockage.
6rd (IPv6 Rapid Deployment) est un système permettant de faire transiter des paquets IPv6 sur un réseau IPv4. Il repose sur les mécanismes mis en place pour le 6to4 classique mais diffère notamment par l'utilisation d'un préfixe IPv6 spécifique au fournisseur d'accès (à la place du préfixe global 6to4 2002::/16). Ce protocole a été mis au point en 2007 par Rémi Després, un des concepteurs de Transpac. Une description ainsi qu'un exemple de déploiement du protocole au sein du FAI Free ont été publiés en janvier 2010 par l'IETF dans la RFC 5569. En se basant sur cette publication, le groupe de travail softwires de l'IETF a entrepris l'élaboration des spécifications du protocole en vue de son intégration dans la procédure de normalisation. Ces spécifications ont été publiées le 10 août 2010 dans la RFC 5969.
10PASS-TS est une spécification de la couche physique (PHY) de l'IEEE 802.3-2008. Elle définit les communications Ethernet full-duplex sur des faibles distances en utilisant des câbles à paires torsadées de catégorie 1. 10PASS-TS permet de délivrer un minimum de 10 Mbit/s à des distances allant jusqu'à 750 mètres, en faisant appel à la technologie VDSL sur une paire de cuivre. Elle permet aussi le support optionnel d'une agrégation de paires multiples. Contrairement à 10BASE-T, 100BASE-T et 1000BASE-T qui fournissent des taux respectifs de 10, 100 et 1000 Mbit/s, le taux des liens 10PASS-TSpeut varier en fonction des caractéristiques de la paire à savoir principalement: la longueur du câble. la section du câble. le nombre de paires si le lien est agrégé.
Le 68EC000 est un microprocesseur 16 bits cadencé à 11,3 MHz produit par Motorola. Il s'agit d'une version à bas coût du 68000. Ce CPU a notamment servi comme contrôleur son sur la console de salon Sega Saturn et avait déjà également servi de processeur central de la Mega Drive.  Portail de l’informatique  Portail de l’électricité et de l’électronique
80 PLUS est une initiative lancée en 2004 par Ecos Consulting pour promouvoir le rendement électrique des blocs d'alimentation des ordinateurs. Elle certifie qu'au moins 80 % de l'énergie reçue en entrée est effectivement transmise à la machine (c'est-à-dire moins de 20 % de pertes par effet Joule). De nombreuses entreprises ont adhéré à ce programme, comme Dell, HP et Lenovo. Depuis 2004, plusieurs évolutions du label ont vu le jour, augmentant à chaque fois le rendement exigé. La liste des certifications « 80 Plus » est disponible ci-dessous.
100BASE-FX est un lien en fibre optique multimode à gradient d’indice.  Le 100BASE-FX est défini pour fonctionner avec deux brins de fibre optique multimode (MMF) par lien, un pour la transmission de données, l’autre pour la réception de données. La fibre utilisée est du type (62.5/125) et ce sur une longueur d’onde de 1350 nanomètre. Il peut également fonctionner sur deux brins de fibre optique monomode (SMF) en duplex, fibre de type 9/125. En multimode, la longueur des brins entre deux équipements est au maximum de 412 mètres en mode « half-duplex » et deux kilomètres en « full-duplex », cette limitation étant imposée par le temps que le signal passe dans les équipements du réseau. En monomode, la longueur maximale varie de 10 à 80km suivant le type de fibre et d'optique. Le standard Fast Ethernet indique un débit de 100Mbit/s.
Le 100BASE-T4 est une norme de câblage réseau mise au point pour l'élaboration du Fast Ethernet 100BASE-T en améliorant la norme 10BASE-T afin d’augmenter la vitesse de transmission des réseaux informatiques et Ethernet.
386BSD est un système d'exploitation libre de type BSD, dérivé de 4.3BSD NET/2 sur les architectures i386. 386BSD a été principalement écrit par Lynne et William Jolitz. La première version fonctionnelle date de 1992, et le développement s'est arrêté en 1997. Aujourd'hui encore le développement de 386BSD continue sur des systèmes dérivés (tels que Darwin d'Apple et OpenBSD).
480i est une nomenclature utilisée en vidéo numérique pour désigner une image vidéo de définition standard composée de 480 lignes entrelacées à une fréquence de rafraîchissement d'environ 60 Hz (et donc à une cadence d'environ 30 images par seconde) ; l'image peut avoir un rapport d'image de 4/3 ou de 16/9.
La norme 1000BASE-T, aussi appelée Gigabit Ethernet, est une évolution de l’Ethernet classique. Celle-ci autorise des débits de 1 000 Mbit/s sur 4 paires de fils de cuivre de catégorie 5 ou supérieure (utilisation de connecteurs RJ45), sur une longueur maximale de 100 m. 1000BASE-T utilise chacune des 4 paires torsadées en mode full duplex, chaque paire transmettant 2 bits par baud, à l’aide d’un code à 5 moments. Soit un total de 1 octet par top d’horloge sur l’ensemble des 4 paires, dans chaque sens. Ce standard est compatible avec 100BASE-TX et 10BASE-T, il assure la détection automatique des taux d’envoi et de réception assurée. Celui-ci permet un fonctionnement sans switch, en mode « point à point ».
